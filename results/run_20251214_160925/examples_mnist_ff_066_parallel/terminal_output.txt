================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_066_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_066_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -b 124
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3832875
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -b 124
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 124
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:53:57...

training layer:  0
Loss:  1.1267627477645874
Loss:  0.7443103194236755
Loss:  0.6934111714363098
Loss:  0.6897458434104919
Loss:  0.674945592880249
Loss:  0.6683623194694519
Loss:  0.6625562310218811
Loss:  0.656387984752655
Loss:  0.6494399905204773
Loss:  0.6411351561546326
Loss:  0.6308230757713318
Loss:  0.6180617809295654
Loss:  0.6027079224586487
Loss:  0.5851505398750305
Loss:  0.566244900226593
Loss:  0.546939492225647
Loss:  0.5278598666191101
Loss:  0.5092669725418091
Loss:  0.4913437068462372
Loss:  0.47412562370300293
Loss:  0.4575965404510498
Loss:  0.44178682565689087
Loss:  0.42673003673553467
Loss:  0.4124317765235901
Loss:  0.39890480041503906
Loss:  0.3861309587955475
Loss:  0.374062180519104
Loss:  0.3626343011856079
Loss:  0.3517872095108032
Loss:  0.3414681553840637
Loss:  0.33163049817085266
Loss:  0.3222402334213257
Loss:  0.31327423453330994
Loss:  0.3047029972076416
Loss:  0.29650744795799255
Loss:  0.2886680066585541
Loss:  0.2811598479747772
Loss:  0.27396753430366516
Loss:  0.2670741379261017
Loss:  0.26046302914619446
Loss:  0.25411859154701233
Loss:  0.24802300333976746
Loss:  0.2421664446592331
Loss:  0.23654739558696747
Loss:  0.2311563789844513
Loss:  0.2259838581085205
Loss:  0.22102217376232147
Loss:  0.21626007556915283
Loss:  0.2116858810186386
Loss:  0.20728735625743866
Loss:  0.20305195450782776
Loss:  0.19897936284542084
Loss:  0.19507010281085968
Loss:  0.1913163810968399
Loss:  0.18770888447761536
Loss:  0.18423934280872345
Loss:  0.18090085685253143
Loss:  0.1776868999004364
Loss:  0.17459240555763245
Loss:  0.17161217331886292
Loss:  0.1687411665916443
Loss:  0.16597385704517365
Loss:  0.1633051186800003
Loss:  0.16073103249073029
Loss:  0.15824778378009796
Loss:  0.15585090219974518
Loss:  0.15353608131408691
Loss:  0.1512986272573471
Loss:  0.14913304150104523
Loss:  0.14703728258609772
Loss:  0.14501214027404785
Loss:  0.14305511116981506
Loss:  0.14116217195987701
Loss:  0.13932840526103973
Loss:  0.13754573464393616
Loss:  0.13580936193466187
Loss:  0.13413262367248535
Loss:  0.1325126439332962
Loss:  0.13094596564769745
Loss:  0.12942904233932495
Loss:  0.1279590129852295
Loss:  0.12653367221355438
Loss:  0.12515099346637726
Loss:  0.12380921095609665
Loss:  0.12250664830207825
Loss:  0.12124159187078476
Loss:  0.12001272290945053
Loss:  0.11881913989782333
Loss:  0.11765924841165543
Loss:  0.11653165519237518
Loss:  0.11543513089418411
Loss:  0.11436860263347626
Loss:  0.11333093047142029
Loss:  0.11232109367847443
Loss:  0.11133840680122375
Loss:  0.11038195341825485
Loss:  0.10945093631744385
Loss:  0.10854462534189224
Loss:  0.10766211897134781
Loss:  0.10680260509252548
training layer:  1
Loss:  1.1266961097717285
Loss:  0.7240374088287354
Loss:  0.7079824805259705
Loss:  0.6880238056182861
Loss:  0.6710212230682373
Loss:  0.6571623086929321
Loss:  0.644867479801178
Loss:  0.6340330243110657
Loss:  0.625156819820404
Loss:  0.6171202659606934
Loss:  0.6093111038208008
Loss:  0.6015726923942566
Loss:  0.5938785672187805
Loss:  0.586235761642456
Loss:  0.578637421131134
Loss:  0.5710929036140442
Loss:  0.5635671019554138
Loss:  0.5560544729232788
Loss:  0.5485564470291138
Loss:  0.541067361831665
Loss:  0.5336076021194458
Loss:  0.5262211561203003
Loss:  0.5189643502235413
Loss:  0.5118694305419922
Loss:  0.5049553513526917
Loss:  0.49824801087379456
Loss:  0.4917605221271515
Loss:  0.4854702949523926
Loss:  0.47933921217918396
Loss:  0.47336387634277344
Loss:  0.4675639867782593
Loss:  0.4619179666042328
Loss:  0.4564191401004791
Loss:  0.45105305314064026
Loss:  0.4458167552947998
Loss:  0.44070494174957275
Loss:  0.4357176125049591
Loss:  0.4308529496192932
Loss:  0.42610734701156616
Loss:  0.4214724004268646
Loss:  0.4169423282146454
Loss:  0.41251200437545776
Loss:  0.40817221999168396
Loss:  0.40392056107521057
Loss:  0.3997560441493988
Loss:  0.3956791162490845
Loss:  0.3916877508163452
Loss:  0.3877790868282318
Loss:  0.38394948840141296
Loss:  0.38019856810569763
Loss:  0.3765256404876709
Loss:  0.37292730808258057
Loss:  0.3694019913673401
Loss:  0.3659493327140808
Loss:  0.36256635189056396
Loss:  0.35924822092056274
Loss:  0.35599252581596375
Loss:  0.3527967035770416
Loss:  0.3496575951576233
Loss:  0.34657394886016846
Loss:  0.34354570508003235
Loss:  0.3405728042125702
Loss:  0.33765220642089844
Loss:  0.33478084206581116
Loss:  0.33195507526397705
Loss:  0.3291732668876648
Loss:  0.32643672823905945
Loss:  0.3237437307834625
Loss:  0.3210914433002472
Loss:  0.31847822666168213
Loss:  0.3159019947052002
Loss:  0.31335964798927307
Loss:  0.310850590467453
Loss:  0.3083764612674713
Loss:  0.3059391677379608
Loss:  0.30353569984436035
Loss:  0.3011630177497864
Loss:  0.29881829023361206
Loss:  0.29650014638900757
Loss:  0.2942103147506714
Loss:  0.2919464409351349
Loss:  0.28970691561698914
Loss:  0.28748762607574463
Loss:  0.2852921783924103
Loss:  0.28312984108924866
Loss:  0.2809963822364807
Loss:  0.27888956665992737
Loss:  0.2768067419528961
Loss:  0.27474796772003174
Loss:  0.27271440625190735
Loss:  0.2707076072692871
Loss:  0.2687241733074188
Loss:  0.2667646110057831
Loss:  0.2648286819458008
Loss:  0.26291462779045105
Loss:  0.26102203130722046
Loss:  0.2591538727283478
Loss:  0.25731006264686584
Loss:  0.25548991560935974
Loss:  0.2536941170692444
train error: 0.8709677457809448
test error: 0.8932000026106834

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:53:57
[0;34m[INFO][0m Training End: 2025-12-15 16:54:05
[0;34m[INFO][0m Total Duration: 0h 0m 8s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 10.6799997389316600%
[0;34m[INFO][0m Test Error: 0.8932000026106834
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.8709677457809448
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_066_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_066_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
