================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_062_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_062_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -l 0.019657
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3831095
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -l 0.019657
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.019657
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:46:56...

training layer:  0
Loss:  1.1267799139022827
Loss:  0.7505919933319092
Loss:  0.7027677297592163
Loss:  0.707294225692749
Loss:  0.6977653503417969
Loss:  0.6944159269332886
Loss:  0.6930745840072632
Loss:  0.6917925477027893
Loss:  0.6904959082603455
Loss:  0.6891300082206726
Loss:  0.687339723110199
Loss:  0.6849851608276367
Loss:  0.6819214820861816
Loss:  0.6780439615249634
Loss:  0.6733083128929138
Loss:  0.6677103638648987
Loss:  0.6612901091575623
Loss:  0.6541500687599182
Loss:  0.6464625597000122
Loss:  0.6384485363960266
Loss:  0.6303170919418335
Loss:  0.6222136616706848
Loss:  0.6142139434814453
Loss:  0.6063458919525146
Loss:  0.5986146330833435
Loss:  0.5910207629203796
Loss:  0.583561360836029
Loss:  0.5762370824813843
Loss:  0.5690502524375916
Loss:  0.5620041489601135
Loss:  0.5551030039787292
Loss:  0.5483492016792297
Loss:  0.5417432188987732
Loss:  0.5352863073348999
Loss:  0.5289778709411621
Loss:  0.52281653881073
Loss:  0.5168011784553528
Loss:  0.5109312534332275
Loss:  0.5052050948143005
Loss:  0.4996206760406494
Loss:  0.4941772222518921
Loss:  0.4888741970062256
Loss:  0.4837096631526947
Loss:  0.4786810278892517
Loss:  0.47378525137901306
Loss:  0.46901825070381165
Loss:  0.4643769860267639
Loss:  0.4598599374294281
Loss:  0.45546361804008484
Loss:  0.45118358731269836
Loss:  0.4470149874687195
Loss:  0.4429537057876587
Loss:  0.4389965236186981
Loss:  0.4351412355899811
Loss:  0.4313846528530121
Loss:  0.42772313952445984
Loss:  0.4241529107093811
Loss:  0.4206700026988983
Loss:  0.4172708988189697
Loss:  0.4139517545700073
Loss:  0.4107091724872589
Loss:  0.4075402319431305
Loss:  0.4044421315193176
Loss:  0.401412695646286
Loss:  0.3984501361846924
Loss:  0.39555227756500244
Loss:  0.39271682500839233
Loss:  0.38994133472442627
Loss:  0.38722342252731323
Loss:  0.38456079363822937
Loss:  0.3819512724876404
Loss:  0.37939247488975525
Loss:  0.3768821656703949
Loss:  0.3744180500507355
Loss:  0.37199804186820984
Loss:  0.36962053179740906
Loss:  0.36728397011756897
Loss:  0.3649870753288269
Loss:  0.36272844672203064
Loss:  0.3605065941810608
Loss:  0.3583199679851532
Loss:  0.3561673164367676
Loss:  0.354047954082489
Loss:  0.3519611656665802
Loss:  0.34990614652633667
Loss:  0.3478822112083435
Loss:  0.3458887040615082
Loss:  0.34392476081848145
Loss:  0.3419894874095917
Loss:  0.34008198976516724
Loss:  0.3382015526294708
Loss:  0.33634716272354126
Loss:  0.33451804518699646
Loss:  0.3327135145664215
Loss:  0.3309328854084015
Loss:  0.3291754424571991
Loss:  0.3274405598640442
Loss:  0.3257276713848114
Loss:  0.3240363299846649
Loss:  0.3223658502101898
training layer:  1
Loss:  1.1266595125198364
Loss:  0.7262384295463562
Loss:  0.6442103981971741
Loss:  0.6120543479919434
Loss:  0.5975024104118347
Loss:  0.5911136865615845
Loss:  0.5865911245346069
Loss:  0.5798782110214233
Loss:  0.5711120963096619
Loss:  0.5604243278503418
Loss:  0.5479170680046082
Loss:  0.5341506004333496
Loss:  0.5196146368980408
Loss:  0.504922091960907
Loss:  0.49061083793640137
Loss:  0.4771318733692169
Loss:  0.46461132168769836
Loss:  0.4531159996986389
Loss:  0.4427422285079956
Loss:  0.43347206711769104
Loss:  0.4252028167247772
Loss:  0.4178231358528137
Loss:  0.4111989140510559
Loss:  0.4052058756351471
Loss:  0.3997393548488617
Loss:  0.39469921588897705
Loss:  0.3900105357170105
Loss:  0.3856196105480194
Loss:  0.38149896264076233
Loss:  0.3776186406612396
Loss:  0.3739479184150696
Loss:  0.37045979499816895
Loss:  0.3671307861804962
Loss:  0.3639434576034546
Loss:  0.36088812351226807
Loss:  0.35795068740844727
Loss:  0.35512182116508484
Loss:  0.3523940443992615
Loss:  0.349759578704834
Loss:  0.34721308946609497
Loss:  0.34475061297416687
Loss:  0.3423673212528229
Loss:  0.3400578796863556
Loss:  0.33781737089157104
Loss:  0.3356406092643738
Loss:  0.3335234820842743
Loss:  0.3314628303050995
Loss:  0.3294563293457031
Loss:  0.3275015950202942
Loss:  0.3255961239337921
Loss:  0.32373711466789246
Loss:  0.3219218850135803
Loss:  0.320147842168808
Loss:  0.318412721157074
Loss:  0.3167145252227783
Loss:  0.3150516152381897
Loss:  0.3134225606918335
Loss:  0.31182602047920227
Loss:  0.3102606236934662
Loss:  0.3087250292301178
Loss:  0.3072179853916168
Loss:  0.3057384788990021
Loss:  0.30428555607795715
Loss:  0.3028583526611328
Loss:  0.30145618319511414
Loss:  0.3000783622264862
Loss:  0.2987240254878998
Loss:  0.2973926365375519
Loss:  0.29608359932899475
Loss:  0.2947964370250702
Loss:  0.29353052377700806
Loss:  0.292285293340683
Loss:  0.29106009006500244
Loss:  0.28985437750816345
Loss:  0.2886674404144287
Loss:  0.28749871253967285
Loss:  0.28634753823280334
Loss:  0.28521353006362915
Loss:  0.2840960919857025
Loss:  0.28299465775489807
Loss:  0.28190872073173523
Loss:  0.28083768486976624
Loss:  0.2797811031341553
Loss:  0.2787383794784546
Loss:  0.27770912647247314
Loss:  0.27669304609298706
Loss:  0.27568984031677246
Loss:  0.27469921112060547
Loss:  0.2737208604812622
Loss:  0.272754430770874
Loss:  0.2717995345592499
Loss:  0.27085575461387634
Loss:  0.2699226438999176
Loss:  0.2689998745918274
Loss:  0.2680869698524475
Loss:  0.267183393239975
Loss:  0.2662885785102844
Loss:  0.26540204882621765
Loss:  0.26452314853668213
Loss:  0.26365113258361816
train error: 0.09140002727508545
test error: 0.09229999780654907

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:46:56
[0;34m[INFO][0m Training End: 2025-12-15 16:47:14
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 90.77000021934509300%
[0;34m[INFO][0m Test Error: 0.09229999780654907
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.09140002727508545
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_062_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_062_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
