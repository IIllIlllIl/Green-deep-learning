================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_072_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_072_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff --seed 2980
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3836088
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff --seed 2980
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 2980
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 17:04:50...

training layer:  0
Loss:  1.1267637014389038
Loss:  0.7565072178840637
Loss:  0.701678454875946
Loss:  0.7049230337142944
Loss:  0.6948540210723877
Loss:  0.691719114780426
Loss:  0.6898685693740845
Loss:  0.6874222755432129
Loss:  0.6840393543243408
Loss:  0.6793684363365173
Loss:  0.6731508374214172
Loss:  0.6652756929397583
Loss:  0.6558040380477905
Loss:  0.6449941992759705
Loss:  0.6333308219909668
Loss:  0.6213923692703247
Loss:  0.6095949411392212
Loss:  0.5981099605560303
Loss:  0.5869610905647278
Loss:  0.5761388540267944
Loss:  0.5656366348266602
Loss:  0.5554561614990234
Loss:  0.5456023812294006
Loss:  0.5360762476921082
Loss:  0.5268791317939758
Loss:  0.5180153250694275
Loss:  0.5094801783561707
Loss:  0.5012673735618591
Loss:  0.4933667480945587
Loss:  0.48576727509498596
Loss:  0.47845765948295593
Loss:  0.4714265465736389
Loss:  0.4646633267402649
Loss:  0.45815539360046387
Loss:  0.45189139246940613
Loss:  0.4458600580692291
Loss:  0.4400496482849121
Loss:  0.43444979190826416
Loss:  0.4290502965450287
Loss:  0.4238396883010864
Loss:  0.418807715177536
Loss:  0.4139437973499298
Loss:  0.40923842787742615
Loss:  0.40468308329582214
Loss:  0.4002693295478821
Loss:  0.39598923921585083
Loss:  0.3918359875679016
Loss:  0.3878030478954315
Loss:  0.3838827908039093
Loss:  0.3800686001777649
Loss:  0.37635520100593567
Loss:  0.3727380633354187
Loss:  0.36921262741088867
Loss:  0.3657745122909546
Loss:  0.3624187409877777
Loss:  0.3591403365135193
Loss:  0.35593435168266296
Loss:  0.35279685258865356
Loss:  0.3497249484062195
Loss:  0.3467167019844055
Loss:  0.34377068281173706
Loss:  0.3408854901790619
Loss:  0.3380599021911621
Loss:  0.3352928161621094
Loss:  0.3325830101966858
Loss:  0.3299292325973511
Loss:  0.3273296654224396
Loss:  0.3247823119163513
Loss:  0.32228508591651917
Loss:  0.31983625888824463
Loss:  0.31743404269218445
Loss:  0.3150769770145416
Loss:  0.3127634823322296
Loss:  0.31049206852912903
Loss:  0.30826130509376526
Loss:  0.30606961250305176
Loss:  0.30391576886177063
Loss:  0.3017984926700592
Loss:  0.29971644282341003
Loss:  0.2976682484149933
Loss:  0.2956525683403015
Loss:  0.29366832971572876
Loss:  0.29171445965766907
Loss:  0.2897898852825165
Loss:  0.28789350390434265
Loss:  0.2860241234302521
Loss:  0.28418081998825073
Loss:  0.2823624312877655
Loss:  0.2805679142475128
Loss:  0.2787964642047882
Loss:  0.27704745531082153
Loss:  0.2753203809261322
Loss:  0.27361464500427246
Loss:  0.2719297707080841
Loss:  0.2702651917934418
Loss:  0.26862046122550964
Loss:  0.26699480414390564
Loss:  0.2653878927230835
Loss:  0.2637990117073059
Loss:  0.26222774386405945
training layer:  1
Loss:  1.126665472984314
Loss:  0.6240895986557007
Loss:  0.5442208647727966
Loss:  0.5349575877189636
Loss:  0.5285581946372986
Loss:  0.5166710019111633
Loss:  0.508786141872406
Loss:  0.4988187253475189
Loss:  0.4876975417137146
Loss:  0.474860817193985
Loss:  0.46099188923835754
Loss:  0.4465658962726593
Loss:  0.43221020698547363
Loss:  0.4184187352657318
Loss:  0.4055539071559906
Loss:  0.39376237988471985
Loss:  0.3830207288265228
Loss:  0.37322378158569336
Loss:  0.3642476797103882
Loss:  0.35601603984832764
Loss:  0.34846633672714233
Loss:  0.34150683879852295
Loss:  0.3350461423397064
Loss:  0.3290325999259949
Loss:  0.3234074115753174
Loss:  0.3181297779083252
Loss:  0.3131745755672455
Loss:  0.3084801435470581
Loss:  0.30402693152427673
Loss:  0.2998420298099518
Loss:  0.29588812589645386
Loss:  0.2921339273452759
Loss:  0.2885763943195343
Loss:  0.2852006256580353
Loss:  0.2819841802120209
Loss:  0.2789095640182495
Loss:  0.2759585976600647
Loss:  0.27311229705810547
Loss:  0.2703742980957031
Loss:  0.2677505910396576
Loss:  0.26523256301879883
Loss:  0.2628103792667389
Loss:  0.2604755163192749
Loss:  0.25822117924690247
Loss:  0.2560436427593231
Loss:  0.2539392411708832
Loss:  0.2519031763076782
Loss:  0.2499307543039322
Loss:  0.24801823496818542
Loss:  0.24616236984729767
Loss:  0.24435999989509583
Loss:  0.24260760843753815
Loss:  0.24090144038200378
Loss:  0.23923736810684204
Loss:  0.2376120537519455
Loss:  0.23602421581745148
Loss:  0.2344723641872406
Loss:  0.2329552173614502
Loss:  0.23147305846214294
Loss:  0.23002591729164124
Loss:  0.22861303389072418
Loss:  0.22723595798015594
Loss:  0.22589445114135742
Loss:  0.22458671033382416
Loss:  0.22331082820892334
Loss:  0.22206494212150574
Loss:  0.2208474725484848
Loss:  0.21965722739696503
Loss:  0.21849291026592255
Loss:  0.2173534631729126
Loss:  0.2162378430366516
Loss:  0.2151450663805008
Loss:  0.21407413482666016
Loss:  0.21302421391010284
Loss:  0.21199437975883484
Loss:  0.2109839767217636
Loss:  0.20999227464199066
Loss:  0.2090187966823578
Loss:  0.20806291699409485
Loss:  0.20712406933307648
Loss:  0.20620165765285492
Loss:  0.20529505610466003
Loss:  0.20440369844436646
Loss:  0.20352700352668762
Loss:  0.20266439020633698
Loss:  0.20181535184383392
Loss:  0.2009795904159546
Loss:  0.20015670359134674
Loss:  0.19934625923633575
Loss:  0.19854770600795746
Loss:  0.1977604627609253
Loss:  0.19698402285575867
Loss:  0.19621796905994415
Loss:  0.19546198844909668
Loss:  0.19471599161624908
Loss:  0.1939801126718521
Loss:  0.1932542622089386
Loss:  0.19253817200660706
Loss:  0.1918315589427948
Loss:  0.19113409519195557
train error: 0.0667000412940979
test error: 0.08380001783370972

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 17:04:50
[0;34m[INFO][0m Training End: 2025-12-15 17:05:07
[0;34m[INFO][0m Total Duration: 0h 0m 17s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 91.61999821662902800%
[0;34m[INFO][0m Test Error: 0.08380001783370972
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.0667000412940979
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_072_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_072_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
