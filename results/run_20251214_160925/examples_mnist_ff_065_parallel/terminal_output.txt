================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_065_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_065_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -b 56
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3832489
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -b 56
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 56
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:52:14...

training layer:  0
Loss:  1.1267669200897217
Loss:  0.7441275715827942
Loss:  0.6891710162162781
Loss:  0.6857953667640686
Loss:  0.6707214117050171
Loss:  0.6622804999351501
Loss:  0.6549695134162903
Loss:  0.6471932530403137
Loss:  0.6382855176925659
Loss:  0.6274903416633606
Loss:  0.613956868648529
Loss:  0.5971381068229675
Loss:  0.5770478248596191
Loss:  0.5546267628669739
Loss:  0.5311194658279419
Loss:  0.5074912905693054
Loss:  0.48443281650543213
Loss:  0.46257632970809937
Loss:  0.4421716630458832
Loss:  0.423221617937088
Loss:  0.4056166112422943
Loss:  0.38920801877975464
Loss:  0.37388700246810913
Loss:  0.35955384373664856
Loss:  0.34612157940864563
Loss:  0.33351537585258484
Loss:  0.32167279720306396
Loss:  0.31054574251174927
Loss:  0.3000882863998413
Loss:  0.2902570962905884
Loss:  0.2810095548629761
Loss:  0.2723042964935303
Loss:  0.26410171389579773
Loss:  0.2563741207122803
Loss:  0.24909169971942902
Loss:  0.24222218990325928
Loss:  0.23573490977287292
Loss:  0.22960177063941956
Loss:  0.2237972766160965
Loss:  0.2182980179786682
Loss:  0.213082417845726
Loss:  0.20813104510307312
Loss:  0.20342586934566498
Loss:  0.1989501565694809
Loss:  0.19468854367733002
Loss:  0.1906270682811737
Loss:  0.18675284087657928
Loss:  0.18305395543575287
Loss:  0.1795194298028946
Loss:  0.17613960802555084
Loss:  0.17290543019771576
Loss:  0.16980835795402527
Loss:  0.16684027016162872
Loss:  0.1639939248561859
Loss:  0.16126257181167603
Loss:  0.15863974392414093
Loss:  0.15611930191516876
Loss:  0.15369614958763123
Loss:  0.1513652801513672
Loss:  0.14912211894989014
Loss:  0.14696215093135834
Loss:  0.14488112926483154
Loss:  0.14287501573562622
Loss:  0.14094004034996033
Loss:  0.13907280564308167
Loss:  0.13727007806301117
Loss:  0.13552887737751007
Loss:  0.13384610414505005
Loss:  0.1322188824415207
Loss:  0.13064470887184143
Loss:  0.12912094593048096
Loss:  0.12764616310596466
Loss:  0.12621846795082092
Loss:  0.1248352974653244
Loss:  0.12349489331245422
Loss:  0.12219570577144623
Loss:  0.12093586474657059
Loss:  0.11971348524093628
Loss:  0.11852668225765228
Loss:  0.11737342178821564
Loss:  0.11625327169895172
Loss:  0.11516569554805756
Loss:  0.11410918086767197
Loss:  0.11308236420154572
Loss:  0.11208400130271912
Loss:  0.11111295968294144
Loss:  0.11016819626092911
Loss:  0.10924872756004333
Loss:  0.1083536222577095
Loss:  0.10748196393251419
Loss:  0.10663285106420517
Loss:  0.10580553859472275
Loss:  0.10499922931194305
Loss:  0.1042131707072258
Loss:  0.10344666987657547
Loss:  0.10269901156425476
Loss:  0.10196954756975174
Loss:  0.10125763714313507
Loss:  0.1005626991391182
Loss:  0.0998842641711235
training layer:  1
Loss:  1.126662254333496
Loss:  0.7676493525505066
Loss:  0.7137770056724548
Loss:  0.7033481001853943
Loss:  0.6872473359107971
Loss:  0.6725962162017822
Loss:  0.6626943349838257
Loss:  0.6542013883590698
Loss:  0.646320104598999
Loss:  0.6386889219284058
Loss:  0.6312190890312195
Loss:  0.6238204836845398
Loss:  0.6166272163391113
Loss:  0.609682559967041
Loss:  0.603015124797821
Loss:  0.5966241359710693
Loss:  0.5904549360275269
Loss:  0.584411084651947
Loss:  0.578403115272522
Loss:  0.5723661184310913
Loss:  0.5662856101989746
Loss:  0.5601054430007935
Loss:  0.5538021922111511
Loss:  0.5473988652229309
Loss:  0.5409330129623413
Loss:  0.5344421863555908
Loss:  0.5279511213302612
Loss:  0.5214702486991882
Loss:  0.5150612592697144
Loss:  0.5087396502494812
Loss:  0.5025084614753723
Loss:  0.49636930227279663
Loss:  0.49032220244407654
Loss:  0.484363317489624
Loss:  0.47848939895629883
Loss:  0.4727103114128113
Loss:  0.4670464098453522
Loss:  0.4614931046962738
Loss:  0.4560326933860779
Loss:  0.4506601393222809
Loss:  0.44537681341171265
Loss:  0.4401859641075134
Loss:  0.4350864887237549
Loss:  0.4300641119480133
Loss:  0.42512229084968567
Loss:  0.42026451230049133
Loss:  0.41549399495124817
Loss:  0.41080671548843384
Loss:  0.40620890259742737
Loss:  0.4017105996608734
Loss:  0.397302508354187
Loss:  0.3929850161075592
Loss:  0.38876232504844666
Loss:  0.38462671637535095
Loss:  0.3805757462978363
Loss:  0.3766101896762848
Loss:  0.3727326989173889
Loss:  0.3689418137073517
Loss:  0.3652391731739044
Loss:  0.3616253435611725
Loss:  0.35809534788131714
Loss:  0.35464897751808167
Loss:  0.351284384727478
Loss:  0.3479980528354645
Loss:  0.34478822350502014
Loss:  0.3416520655155182
Loss:  0.3385886549949646
Loss:  0.3355964720249176
Loss:  0.3326724171638489
Loss:  0.32981473207473755
Loss:  0.3270203471183777
Loss:  0.3242873251438141
Loss:  0.32161441445350647
Loss:  0.31899896264076233
Loss:  0.3164384961128235
Loss:  0.313930869102478
Loss:  0.31147313117980957
Loss:  0.30906400084495544
Loss:  0.30670166015625
Loss:  0.3043850362300873
Loss:  0.30211353302001953
Loss:  0.2998860478401184
Loss:  0.29770058393478394
Loss:  0.2955552339553833
Loss:  0.29344794154167175
Loss:  0.2913779616355896
Loss:  0.2893427312374115
Loss:  0.28734061121940613
Loss:  0.28537148237228394
Loss:  0.2834359407424927
Loss:  0.2815326750278473
Loss:  0.27966082096099854
Loss:  0.27781930565834045
Loss:  0.27600765228271484
Loss:  0.27422550320625305
Loss:  0.27247151732444763
Loss:  0.27074477076530457
Loss:  0.2690443992614746
Loss:  0.2673701047897339
Loss:  0.2657214403152466
train error: 0.8392857015132904
test error: 0.9307999983429909

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:52:14
[0;34m[INFO][0m Training End: 2025-12-15 16:52:22
[0;34m[INFO][0m Total Duration: 0h 0m 8s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 6.9200001657009100%
[0;34m[INFO][0m Test Error: 0.9307999983429909
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.8392857015132904
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_065_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_065_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
