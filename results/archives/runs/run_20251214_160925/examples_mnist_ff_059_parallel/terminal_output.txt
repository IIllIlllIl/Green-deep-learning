================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_059_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_059_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -l 0.019757
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3828982
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -l 0.019757
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.019757
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:41:20...

training layer:  0
Loss:  1.1267870664596558
Loss:  0.7484891414642334
Loss:  0.7027416825294495
Loss:  0.7076483964920044
Loss:  0.697626531124115
Loss:  0.6946186423301697
Loss:  0.693138062953949
Loss:  0.6918899416923523
Loss:  0.6905409693717957
Loss:  0.6891117691993713
Loss:  0.6872400045394897
Loss:  0.6847882866859436
Loss:  0.6816120147705078
Loss:  0.6775945425033569
Loss:  0.6726800799369812
Loss:  0.6668688058853149
Loss:  0.6602211594581604
Loss:  0.6528581380844116
Loss:  0.6449697017669678
Loss:  0.6367761492729187
Loss:  0.6284739971160889
Loss:  0.6202082633972168
Loss:  0.6120644211769104
Loss:  0.6040741801261902
Loss:  0.59624844789505
Loss:  0.5885891318321228
Loss:  0.5810918807983398
Loss:  0.573752224445343
Loss:  0.5665696859359741
Loss:  0.5595461130142212
Loss:  0.5526833534240723
Loss:  0.545983612537384
Loss:  0.5394490361213684
Loss:  0.5330802798271179
Loss:  0.5268770456314087
Loss:  0.5208380818367004
Loss:  0.5149617195129395
Loss:  0.5092441439628601
Loss:  0.5036818981170654
Loss:  0.49827101826667786
Loss:  0.49300727248191833
Loss:  0.4878857433795929
Loss:  0.4829005300998688
Loss:  0.4780462682247162
Loss:  0.473318874835968
Loss:  0.4687151610851288
Loss:  0.4642322063446045
Loss:  0.4598662555217743
Loss:  0.45561346411705017
Loss:  0.4514695107936859
Loss:  0.4474303722381592
Loss:  0.4434918761253357
Loss:  0.4396499991416931
Loss:  0.43590062856674194
Loss:  0.4322400689125061
Loss:  0.428664892911911
Loss:  0.42517176270484924
Loss:  0.42175766825675964
Loss:  0.4184195101261139
Loss:  0.4151547849178314
Loss:  0.4119609296321869
Loss:  0.4088355302810669
Loss:  0.40577635169029236
Loss:  0.4027819335460663
Loss:  0.3998505175113678
Loss:  0.3969804644584656
Loss:  0.39416995644569397
Loss:  0.39141708612442017
Loss:  0.38871946930885315
Loss:  0.3860749304294586
Loss:  0.3834814429283142
Loss:  0.3809368908405304
Loss:  0.37843942642211914
Loss:  0.37598738074302673
Loss:  0.3735794425010681
Loss:  0.37121421098709106
Loss:  0.3688904345035553
Loss:  0.36660677194595337
Loss:  0.36436179280281067
Loss:  0.3621542453765869
Loss:  0.3599829375743866
Loss:  0.3578472435474396
Loss:  0.35574638843536377
Loss:  0.35367944836616516
Loss:  0.35164520144462585
Loss:  0.3496425747871399
Loss:  0.3476703464984894
Loss:  0.3457275927066803
Loss:  0.3438132107257843
Loss:  0.3419262766838074
Loss:  0.34006601572036743
Loss:  0.33823153376579285
Loss:  0.3364221453666687
Loss:  0.3346370458602905
Loss:  0.3328755795955658
Loss:  0.33113694190979004
Loss:  0.3294205367565155
Loss:  0.3277255892753601
Loss:  0.32605141401290894
Loss:  0.3243972957134247
training layer:  1
Loss:  1.1266571283340454
Loss:  0.725993812084198
Loss:  0.6406427621841431
Loss:  0.6071343421936035
Loss:  0.5935431718826294
Loss:  0.588103711605072
Loss:  0.5844240784645081
Loss:  0.5790050029754639
Loss:  0.5728805065155029
Loss:  0.565852701663971
Loss:  0.5577115416526794
Loss:  0.5487568974494934
Loss:  0.539125382900238
Loss:  0.5290451049804688
Loss:  0.5187676548957825
Loss:  0.5085473656654358
Loss:  0.4985347390174866
Loss:  0.48893284797668457
Loss:  0.47988754510879517
Loss:  0.47143176198005676
Loss:  0.4635261595249176
Loss:  0.4561886489391327
Loss:  0.4493805170059204
Loss:  0.4430249035358429
Loss:  0.437042236328125
Loss:  0.43144071102142334
Loss:  0.4261806905269623
Loss:  0.42118483781814575
Loss:  0.4164033532142639
Loss:  0.41180381178855896
Loss:  0.4073953330516815
Loss:  0.4031832814216614
Loss:  0.3991699516773224
Loss:  0.3953273296356201
Loss:  0.3916395902633667
Loss:  0.38810205459594727
Loss:  0.3847026824951172
Loss:  0.3814300298690796
Loss:  0.3782760202884674
Loss:  0.3752283751964569
Loss:  0.3722669780254364
Loss:  0.3693634569644928
Loss:  0.36652296781539917
Loss:  0.3637695610523224
Loss:  0.36109232902526855
Loss:  0.35847947001457214
Loss:  0.3559393882751465
Loss:  0.3534742593765259
Loss:  0.35107576847076416
Loss:  0.3487491011619568
Loss:  0.34649360179901123
Loss:  0.3443019986152649
Loss:  0.3421671986579895
Loss:  0.34008124470710754
Loss:  0.33803337812423706
Loss:  0.3360113799571991
Loss:  0.3340231478214264
Loss:  0.33208468556404114
Loss:  0.33019769191741943
Loss:  0.3283594250679016
Loss:  0.32656705379486084
Loss:  0.32481804490089417
Loss:  0.3231099545955658
Loss:  0.32144033908843994
Loss:  0.3198068141937256
Loss:  0.31820717453956604
Loss:  0.31663960218429565
Loss:  0.31510213017463684
Loss:  0.3135928511619568
Loss:  0.3121103048324585
Loss:  0.31065329909324646
Loss:  0.30922091007232666
Loss:  0.30781200528144836
Loss:  0.3064250946044922
Loss:  0.30505847930908203
Loss:  0.30371028184890747
Loss:  0.30237844586372375
Loss:  0.3010609447956085
Loss:  0.29975569248199463
Loss:  0.2984607219696045
Loss:  0.29717573523521423
Loss:  0.29590320587158203
Loss:  0.2946404218673706
Loss:  0.29337942600250244
Loss:  0.2921079397201538
Loss:  0.2908139228820801
Loss:  0.28949663043022156
Loss:  0.28816938400268555
Loss:  0.2868425250053406
Loss:  0.28551748394966125
Loss:  0.284197598695755
Loss:  0.2828918397426605
Loss:  0.2816106379032135
Loss:  0.2803586721420288
Loss:  0.27913662791252136
Loss:  0.2779431939125061
Loss:  0.27677592635154724
Loss:  0.27563291788101196
Loss:  0.2745121121406555
Loss:  0.2734113335609436
train error: 0.0854000449180603
test error: 0.08980000019073486

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:41:20
[0;34m[INFO][0m Training End: 2025-12-15 16:41:38
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 91.01999998092651400%
[0;34m[INFO][0m Test Error: 0.08980000019073486
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.0854000449180603
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_059_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_059_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
