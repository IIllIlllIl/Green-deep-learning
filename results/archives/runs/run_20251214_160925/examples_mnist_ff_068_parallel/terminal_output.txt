================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_068_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_068_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff --seed 44
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3833661
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff --seed 44
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 44
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:57:23...

training layer:  0
Loss:  1.1267863512039185
Loss:  0.7550684809684753
Loss:  0.7026407718658447
Loss:  0.7050521969795227
Loss:  0.6946481466293335
Loss:  0.6918755769729614
Loss:  0.6901928782463074
Loss:  0.6879434585571289
Loss:  0.6848505735397339
Loss:  0.6805548667907715
Loss:  0.6747773885726929
Loss:  0.667404294013977
Loss:  0.6584743857383728
Loss:  0.6481940150260925
Loss:  0.63697749376297
Loss:  0.6253219842910767
Loss:  0.6136459112167358
Loss:  0.602175772190094
Loss:  0.5909991264343262
Loss:  0.5801406502723694
Loss:  0.5696036219596863
Loss:  0.5593835115432739
Loss:  0.5494709610939026
Loss:  0.5398606061935425
Loss:  0.5305555462837219
Loss:  0.5215590596199036
Loss:  0.5128694772720337
Loss:  0.5044838190078735
Loss:  0.4963971674442291
Loss:  0.48860427737236023
Loss:  0.48109787702560425
Loss:  0.47386863827705383
Loss:  0.46690693497657776
Loss:  0.46020418405532837
Loss:  0.4537525177001953
Loss:  0.4475421905517578
Loss:  0.4415610134601593
Loss:  0.43579548597335815
Loss:  0.43023309111595154
Loss:  0.42486289143562317
Loss:  0.41967537999153137
Loss:  0.41466224193573
Loss:  0.4098162055015564
Loss:  0.40513020753860474
Loss:  0.4005967080593109
Loss:  0.39620810747146606
Loss:  0.3919563293457031
Loss:  0.387833833694458
Loss:  0.3838336169719696
Loss:  0.3799511790275574
Loss:  0.37618231773376465
Loss:  0.3725208044052124
Loss:  0.36896029114723206
Loss:  0.3654947280883789
Loss:  0.362118124961853
Loss:  0.35882508754730225
Loss:  0.3556109368801117
Loss:  0.3524714410305023
Loss:  0.3494025766849518
Loss:  0.3464004099369049
Loss:  0.34346210956573486
Loss:  0.34058627486228943
Loss:  0.33777087926864624
Loss:  0.33501389622688293
Loss:  0.3323133587837219
Loss:  0.3296670913696289
Loss:  0.3270729184150696
Loss:  0.32452866435050964
Loss:  0.3220321834087372
Loss:  0.31958135962486267
Loss:  0.31717410683631897
Loss:  0.31480881571769714
Loss:  0.31248408555984497
Loss:  0.3101986348628998
Loss:  0.3079511523246765
Loss:  0.3057400584220886
Loss:  0.30356380343437195
Loss:  0.30142128467559814
Loss:  0.29931142926216125
Loss:  0.29723334312438965
Loss:  0.29518601298332214
Loss:  0.29316845536231995
Loss:  0.29117944836616516
Loss:  0.289218008518219
Loss:  0.28728318214416504
Loss:  0.2853742241859436
Loss:  0.28349047899246216
Loss:  0.2816314995288849
Loss:  0.2797965705394745
Loss:  0.2779850959777832
Loss:  0.27619633078575134
Loss:  0.2744296193122864
Loss:  0.2726842164993286
Loss:  0.27095943689346313
Loss:  0.26925453543663025
Loss:  0.2675687372684479
Loss:  0.2659013569355011
Loss:  0.2642517685890198
Loss:  0.2626194655895233
Loss:  0.26100388169288635
training layer:  1
Loss:  1.1266908645629883
Loss:  0.630732536315918
Loss:  0.5516596436500549
Loss:  0.5395717620849609
Loss:  0.5323512554168701
Loss:  0.5189807415008545
Loss:  0.5078533291816711
Loss:  0.4943161904811859
Loss:  0.4793350398540497
Loss:  0.4637921154499054
Loss:  0.4486325979232788
Loss:  0.4344542920589447
Loss:  0.4215713143348694
Loss:  0.4100455939769745
Loss:  0.39970463514328003
Loss:  0.39023086428642273
Loss:  0.38136786222457886
Loss:  0.37314915657043457
Loss:  0.3655749559402466
Loss:  0.3585818409919739
Loss:  0.352079302072525
Loss:  0.3460105359554291
Loss:  0.34033337235450745
Loss:  0.33497822284698486
Loss:  0.3299025297164917
Loss:  0.32512450218200684
Loss:  0.3206118941307068
Loss:  0.3163301944732666
Loss:  0.3122505843639374
Loss:  0.30835428833961487
Loss:  0.30462217330932617
Loss:  0.3010370433330536
Loss:  0.29758888483047485
Loss:  0.29426833987236023
Loss:  0.2910654544830322
Loss:  0.28797483444213867
Loss:  0.2849947512149811
Loss:  0.2821197211742401
Loss:  0.2793414890766144
Loss:  0.2766539454460144
Loss:  0.2740517854690552
Loss:  0.2715311348438263
Loss:  0.26908889412879944
Loss:  0.2667195200920105
Loss:  0.26441776752471924
Loss:  0.26218080520629883
Loss:  0.26000967621803284
Loss:  0.2579056918621063
Loss:  0.25586646795272827
Loss:  0.25388768315315247
Loss:  0.25196489691734314
Loss:  0.2500934600830078
Loss:  0.24826888740062714
Loss:  0.2464880347251892
Loss:  0.24474968016147614
Loss:  0.24305327236652374
Loss:  0.24139630794525146
Loss:  0.2397753894329071
Loss:  0.2381865233182907
Loss:  0.23662616312503815
Loss:  0.23509269952774048
Loss:  0.23358939588069916
Loss:  0.23211872577667236
Loss:  0.23067982494831085
Loss:  0.2292715460062027
Loss:  0.2278929203748703
Loss:  0.22654300928115845
Loss:  0.2252209633588791
Loss:  0.22392594814300537
Loss:  0.2226572483778
Loss:  0.2214140146970749
Loss:  0.22019557654857635
Loss:  0.2190011590719223
Loss:  0.21783016622066498
Loss:  0.2166818529367447
Loss:  0.2155556082725525
Loss:  0.21445073187351227
Loss:  0.21336659789085388
Loss:  0.21230243146419525
Loss:  0.21125748753547668
Loss:  0.21023093163967133
Loss:  0.20922201871871948
Loss:  0.20823001861572266
Loss:  0.20725443959236145
Loss:  0.20629535615444183
Loss:  0.20535224676132202
Loss:  0.20442421734333038
Loss:  0.20351092517375946
Loss:  0.20261231064796448
Loss:  0.20172818005084991
Loss:  0.200858473777771
Loss:  0.2000030279159546
Loss:  0.19916146993637085
Loss:  0.19833341240882874
Loss:  0.19751840829849243
Loss:  0.19671601057052612
Loss:  0.19592587649822235
Loss:  0.1951475739479065
Loss:  0.1943807601928711
Loss:  0.1936250627040863
train error: 0.06920003890991211
test error: 0.08329999446868896

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:57:23
[0;34m[INFO][0m Training End: 2025-12-15 16:57:40
[0;34m[INFO][0m Total Duration: 0h 0m 17s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 91.67000055313110400%
[0;34m[INFO][0m Test Error: 0.08329999446868896
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.06920003890991211
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_068_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_068_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
