================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_063_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_063_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -b 73
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3831704
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -b 73
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 73
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:48:48...

training layer:  0
Loss:  1.1267634630203247
Loss:  0.7387564778327942
Loss:  0.6841729283332825
Loss:  0.6765178442001343
Loss:  0.6571374535560608
Loss:  0.645540714263916
Loss:  0.6350591778755188
Loss:  0.6246916055679321
Loss:  0.6142182946205139
Loss:  0.6032804846763611
Loss:  0.5915142893791199
Loss:  0.5785526633262634
Loss:  0.5642015337944031
Loss:  0.5485200881958008
Loss:  0.5318242907524109
Loss:  0.5147706866264343
Loss:  0.49791938066482544
Loss:  0.48163285851478577
Loss:  0.465972900390625
Loss:  0.4509255886077881
Loss:  0.4365305006504059
Loss:  0.4228438138961792
Loss:  0.4098232686519623
Loss:  0.39747968316078186
Loss:  0.3857908248901367
Loss:  0.3746981620788574
Loss:  0.3641614615917206
Loss:  0.3541456460952759
Loss:  0.34462499618530273
Loss:  0.33556076884269714
Loss:  0.32691413164138794
Loss:  0.31865376234054565
Loss:  0.31075212359428406
Loss:  0.30318954586982727
Loss:  0.29594847559928894
Loss:  0.2890099883079529
Loss:  0.2823503911495209
Loss:  0.27594858407974243
Loss:  0.26979997754096985
Loss:  0.26389068365097046
Loss:  0.25820794701576233
Loss:  0.25273963809013367
Loss:  0.24747435748577118
Loss:  0.24240370094776154
Loss:  0.23751908540725708
Loss:  0.2328108698129654
Loss:  0.22827017307281494
Loss:  0.22388896346092224
Loss:  0.21966058015823364
Loss:  0.2155781090259552
Loss:  0.21163475513458252
Loss:  0.2078242152929306
Loss:  0.2041398137807846
Loss:  0.20057536661624908
Loss:  0.1971251219511032
Loss:  0.19378630816936493
Loss:  0.1905548870563507
Loss:  0.18742656707763672
Loss:  0.18439699709415436
Loss:  0.18146184086799622
Loss:  0.17861641943454742
Loss:  0.17585523426532745
Loss:  0.17317014932632446
Loss:  0.1705530434846878
Loss:  0.1680164635181427
Loss:  0.1655614823102951
Loss:  0.16318291425704956
Loss:  0.16087602078914642
Loss:  0.15863826870918274
Loss:  0.1564672738313675
Loss:  0.1543603092432022
Loss:  0.15231476724147797
Loss:  0.1503278762102127
Loss:  0.14839689433574677
Loss:  0.14652013778686523
Loss:  0.14469745755195618
Loss:  0.14292730391025543
Loss:  0.14120744168758392
Loss:  0.13953594863414764
Loss:  0.1379111260175705
Loss:  0.13633118569850922
Loss:  0.13479457795619965
Loss:  0.1333000659942627
Loss:  0.1318461298942566
Loss:  0.13043113052845
Loss:  0.12905336916446686
Loss:  0.12771061062812805
Loss:  0.1263999193906784
Loss:  0.12511691451072693
Loss:  0.12385942041873932
Loss:  0.12263445556163788
Loss:  0.12144837528467178
Loss:  0.12029766291379929
Loss:  0.11917880922555923
Loss:  0.1180896982550621
Loss:  0.11702900379896164
Loss:  0.11599553376436234
Loss:  0.11498834192752838
Loss:  0.11400651186704636
Loss:  0.11304916441440582
training layer:  1
Loss:  1.1266911029815674
Loss:  0.7803011536598206
Loss:  0.7146876454353333
Loss:  0.6964976191520691
Loss:  0.6827535033226013
Loss:  0.6687440276145935
Loss:  0.6557021141052246
Loss:  0.6446018815040588
Loss:  0.6338875889778137
Loss:  0.6235297918319702
Loss:  0.6134824156761169
Loss:  0.6036049723625183
Loss:  0.5939035415649414
Loss:  0.5844879150390625
Loss:  0.5753450989723206
Loss:  0.566470205783844
Loss:  0.5577913522720337
Loss:  0.5492991209030151
Loss:  0.5410007834434509
Loss:  0.5328266620635986
Loss:  0.5247812867164612
Loss:  0.51690673828125
Loss:  0.5092219710350037
Loss:  0.5017275810241699
Loss:  0.49444350600242615
Loss:  0.48739033937454224
Loss:  0.4805530905723572
Loss:  0.47391247749328613
Loss:  0.4674665331840515
Loss:  0.4612089693546295
Loss:  0.4551398754119873
Loss:  0.44924643635749817
Loss:  0.44351252913475037
Loss:  0.4379221796989441
Loss:  0.43245843052864075
Loss:  0.4271344244480133
Loss:  0.42193883657455444
Loss:  0.4168650209903717
Loss:  0.41191744804382324
Loss:  0.4070911109447479
Loss:  0.4023841619491577
Loss:  0.39779385924339294
Loss:  0.3933122158050537
Loss:  0.38893550634384155
Loss:  0.3846573531627655
Loss:  0.38047146797180176
Loss:  0.3763716518878937
Loss:  0.3723542094230652
Loss:  0.36841949820518494
Loss:  0.3645668029785156
Loss:  0.3608067035675049
Loss:  0.3571481704711914
Loss:  0.35358795523643494
Loss:  0.3501187562942505
Loss:  0.346737265586853
Loss:  0.3434371054172516
Loss:  0.340213418006897
Loss:  0.33705848455429077
Loss:  0.33397141098976135
Loss:  0.3309556841850281
Loss:  0.3280107378959656
Loss:  0.3251352608203888
Loss:  0.3223245441913605
Loss:  0.31957176327705383
Loss:  0.3168741464614868
Loss:  0.31423327326774597
Loss:  0.31164541840553284
Loss:  0.3091103136539459
Loss:  0.3066273629665375
Loss:  0.3041929006576538
Loss:  0.3018026351928711
Loss:  0.29945656657218933
Loss:  0.2971540689468384
Loss:  0.29489296674728394
Loss:  0.2926729917526245
Loss:  0.29049280285835266
Loss:  0.2883501946926117
Loss:  0.28624385595321655
Loss:  0.28417378664016724
Loss:  0.28213974833488464
Loss:  0.2801406979560852
Loss:  0.2781759798526764
Loss:  0.27624520659446716
Loss:  0.2743474245071411
Loss:  0.2724815309047699
Loss:  0.27064692974090576
Loss:  0.2688426077365875
Loss:  0.2670677900314331
Loss:  0.26532161235809326
Loss:  0.26360318064689636
Loss:  0.2619112730026245
Loss:  0.2602444589138031
Loss:  0.2586042284965515
Loss:  0.2569902539253235
Loss:  0.2554015517234802
Loss:  0.2538374364376068
Loss:  0.25229722261428833
Loss:  0.2507803440093994
Loss:  0.24928627908229828
Loss:  0.24781428277492523
train error: 0.9452054798603058
test error: 0.9255000054836273

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:48:48
[0;34m[INFO][0m Training End: 2025-12-15 16:48:56
[0;34m[INFO][0m Total Duration: 0h 0m 8s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 7.4499994516372700%
[0;34m[INFO][0m Test Error: 0.9255000054836273
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.9452054798603058
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_063_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_063_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
