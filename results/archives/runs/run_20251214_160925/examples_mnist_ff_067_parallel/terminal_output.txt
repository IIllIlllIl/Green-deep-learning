================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_067_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_067_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -b 33
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3833268
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -b 33
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 33
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:55:40...

training layer:  0
Loss:  1.1267598867416382
Loss:  0.7353624105453491
Loss:  0.6932201385498047
Loss:  0.6849789023399353
Loss:  0.6689220070838928
Loss:  0.6605098843574524
Loss:  0.6515912413597107
Loss:  0.6414347887039185
Loss:  0.629422664642334
Loss:  0.6147885322570801
Loss:  0.5972034931182861
Loss:  0.5769619941711426
Loss:  0.5551038980484009
Loss:  0.5328524112701416
Loss:  0.5108692646026611
Loss:  0.4894750416278839
Loss:  0.46893101930618286
Loss:  0.44941622018814087
Loss:  0.430951863527298
Loss:  0.41353392601013184
Loss:  0.3971548080444336
Loss:  0.3817783296108246
Loss:  0.36735060811042786
Loss:  0.35380515456199646
Loss:  0.3410828113555908
Loss:  0.3291207551956177
Loss:  0.3178626596927643
Loss:  0.30726030468940735
Loss:  0.297267347574234
Loss:  0.2878393828868866
Loss:  0.2789367437362671
Loss:  0.2705235481262207
Loss:  0.2625664174556732
Loss:  0.2550335228443146
Loss:  0.24789544939994812
Loss:  0.24112564325332642
Loss:  0.2347000390291214
Loss:  0.22859600186347961
Loss:  0.22279268503189087
Loss:  0.21727082133293152
Loss:  0.21201258897781372
Loss:  0.2070007026195526
Loss:  0.20221760869026184
Loss:  0.19764597713947296
Loss:  0.19328127801418304
Loss:  0.18911293148994446
Loss:  0.18512383103370667
Loss:  0.181309312582016
Loss:  0.17766013741493225
Loss:  0.1741652637720108
Loss:  0.1708155870437622
Loss:  0.16760283708572388
Loss:  0.16451963782310486
Loss:  0.16155946254730225
Loss:  0.15871500968933105
Loss:  0.15597765147686005
Loss:  0.15333685278892517
Loss:  0.15079516172409058
Loss:  0.1483524888753891
Loss:  0.1459985226392746
Loss:  0.1437365561723709
Loss:  0.1415611058473587
Loss:  0.13946418464183807
Loss:  0.1374402940273285
Loss:  0.13548368215560913
Loss:  0.13359589874744415
Loss:  0.13177290558815002
Loss:  0.13001090288162231
Loss:  0.12831075489521027
Loss:  0.1266687661409378
Loss:  0.12508104741573334
Loss:  0.12354470789432526
Loss:  0.1220569908618927
Loss:  0.12061524391174316
Loss:  0.11921633034944534
Loss:  0.11785949766635895
Loss:  0.11654714494943619
Loss:  0.11527775228023529
Loss:  0.11404546350240707
Loss:  0.11284854263067245
Loss:  0.11169186979532242
Loss:  0.11057206988334656
Loss:  0.10948564857244492
Loss:  0.10843099653720856
Loss:  0.10740683227777481
Loss:  0.10641176998615265
Loss:  0.10544437915086746
Loss:  0.10450338572263718
Loss:  0.10358721762895584
Loss:  0.10269278287887573
Loss:  0.10182472318410873
Loss:  0.10098391771316528
Loss:  0.10016743838787079
Loss:  0.09937354177236557
Loss:  0.0986010953783989
Loss:  0.09784934669733047
Loss:  0.097117580473423
Loss:  0.09640513360500336
Loss:  0.0957113727927208
Loss:  0.09503567218780518
training layer:  1
Loss:  1.1266789436340332
Loss:  0.777034342288971
Loss:  0.7358631491661072
Loss:  0.7208421230316162
Loss:  0.7046664953231812
Loss:  0.6953709721565247
Loss:  0.6905604004859924
Loss:  0.6855953335762024
Loss:  0.6807973384857178
Loss:  0.6756211519241333
Loss:  0.6699063181877136
Loss:  0.6634359955787659
Loss:  0.656227171421051
Loss:  0.6484774947166443
Loss:  0.6404848098754883
Loss:  0.6326186060905457
Loss:  0.6251826286315918
Loss:  0.6182980537414551
Loss:  0.6119413375854492
Loss:  0.6060630679130554
Loss:  0.6006125211715698
Loss:  0.5955398678779602
Loss:  0.5908019542694092
Loss:  0.5863574743270874
Loss:  0.5821722745895386
Loss:  0.5782067179679871
Loss:  0.5744098424911499
Loss:  0.5707564949989319
Loss:  0.5672251582145691
Loss:  0.5637988448143005
Loss:  0.5604739785194397
Loss:  0.5572484731674194
Loss:  0.5541168451309204
Loss:  0.551088273525238
Loss:  0.548171877861023
Loss:  0.5453612208366394
Loss:  0.5426382422447205
Loss:  0.5399893522262573
Loss:  0.5374062061309814
Loss:  0.5348838567733765
Loss:  0.5324167013168335
Loss:  0.5300008654594421
Loss:  0.5276313424110413
Loss:  0.5253005623817444
Loss:  0.5230115652084351
Loss:  0.5207656025886536
Loss:  0.518561065196991
Loss:  0.5163946747779846
Loss:  0.5142671465873718
Loss:  0.5121767520904541
Loss:  0.5101237297058105
Loss:  0.508108913898468
Loss:  0.5061312913894653
Loss:  0.5041892528533936
Loss:  0.5022794008255005
Loss:  0.5003976821899414
Loss:  0.498542457818985
Loss:  0.496712327003479
Loss:  0.4949053227901459
Loss:  0.49312061071395874
Loss:  0.4913579821586609
Loss:  0.4896170496940613
Loss:  0.487897127866745
Loss:  0.4861964285373688
Loss:  0.4845133125782013
Loss:  0.4828470051288605
Loss:  0.4811958372592926
Loss:  0.47955867648124695
Loss:  0.47793543338775635
Loss:  0.4763253629207611
Loss:  0.4747278094291687
Loss:  0.47314193844795227
Loss:  0.47156694531440735
Loss:  0.4700027108192444
Loss:  0.4684486985206604
Loss:  0.46690377593040466
Loss:  0.4653683304786682
Loss:  0.4638422429561615
Loss:  0.4623250365257263
Loss:  0.46081653237342834
Loss:  0.4593169391155243
Loss:  0.4578261971473694
Loss:  0.4563436806201935
Loss:  0.45486980676651
Loss:  0.4534045457839966
Loss:  0.45194798707962036
Loss:  0.45049986243247986
Loss:  0.4490602910518646
Loss:  0.44762900471687317
Loss:  0.4462059736251831
Loss:  0.4447910487651825
Loss:  0.44338417053222656
Loss:  0.4419853687286377
Loss:  0.44059449434280396
Loss:  0.4392114281654358
Loss:  0.4378361403942108
Loss:  0.43646860122680664
Loss:  0.4351087510585785
Loss:  0.4337564706802368
Loss:  0.4324118196964264
train error: 0.8787878751754761
test error: 0.9394000023603439

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:55:40
[0;34m[INFO][0m Training End: 2025-12-15 16:55:48
[0;34m[INFO][0m Total Duration: 0h 0m 8s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 6.0599997639656100%
[0;34m[INFO][0m Test Error: 0.9394000023603439
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.8787878751754761
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_067_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_067_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
