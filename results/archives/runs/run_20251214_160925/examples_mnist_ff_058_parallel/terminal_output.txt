================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_058_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_058_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -l 0.017243
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3828372
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -l 0.017243
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.017243
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:39:29...

training layer:  0
Loss:  1.126779317855835
Loss:  0.7984247803688049
Loss:  0.7177110314369202
Loss:  0.6996755599975586
Loss:  0.6995001435279846
Loss:  0.6953982710838318
Loss:  0.6932715773582458
Loss:  0.6922304630279541
Loss:  0.6910732388496399
Loss:  0.6899589896202087
Loss:  0.6886667013168335
Loss:  0.6870525479316711
Loss:  0.6850309371948242
Loss:  0.6825064420700073
Loss:  0.6794081926345825
Loss:  0.675686240196228
Loss:  0.6713143587112427
Loss:  0.6662964820861816
Loss:  0.6606810092926025
Loss:  0.6545567512512207
Loss:  0.648034930229187
Loss:  0.6412440538406372
Loss:  0.6343070268630981
Loss:  0.6273279786109924
Loss:  0.6203727126121521
Loss:  0.6134730577468872
Loss:  0.6066468954086304
Loss:  0.599907636642456
Loss:  0.5932656526565552
Loss:  0.5867288708686829
Loss:  0.5803008079528809
Loss:  0.5739811658859253
Loss:  0.5677695870399475
Loss:  0.5616675615310669
Loss:  0.555679976940155
Loss:  0.549808919429779
Loss:  0.5440555810928345
Loss:  0.538421094417572
Loss:  0.5329076051712036
Loss:  0.527515709400177
Loss:  0.5222455859184265
Loss:  0.5170983076095581
Loss:  0.5120733976364136
Loss:  0.5071685910224915
Loss:  0.5023805499076843
Loss:  0.4977056384086609
Loss:  0.4931409955024719
Loss:  0.4886848032474518
Loss:  0.484334260225296
Loss:  0.48008549213409424
Loss:  0.4759347140789032
Loss:  0.47188007831573486
Loss:  0.4679202139377594
Loss:  0.46405354142189026
Loss:  0.4602782130241394
Loss:  0.45659139752388
Loss:  0.45299050211906433
Loss:  0.4494730234146118
Loss:  0.4460359215736389
Loss:  0.44267624616622925
Loss:  0.43939071893692017
Loss:  0.4361763000488281
Loss:  0.43303021788597107
Loss:  0.42994987964630127
Loss:  0.42693251371383667
Loss:  0.42397528886795044
Loss:  0.42107558250427246
Loss:  0.4182310402393341
Loss:  0.4154401123523712
Loss:  0.4127015471458435
Loss:  0.41001376509666443
Loss:  0.4073747992515564
Loss:  0.40478262305259705
Loss:  0.4022350609302521
Loss:  0.39973026514053345
Loss:  0.3972669541835785
Loss:  0.39484408497810364
Loss:  0.39246058464050293
Loss:  0.39011523127555847
Loss:  0.38780680298805237
Loss:  0.3855341076850891
Loss:  0.3832961618900299
Loss:  0.38109198212623596
Loss:  0.3789204955101013
Loss:  0.37678077816963196
Loss:  0.37467190623283386
Loss:  0.37259310483932495
Loss:  0.37054362893104553
Loss:  0.36852285265922546
Loss:  0.36653026938438416
Loss:  0.3645654022693634
Loss:  0.36262771487236023
Loss:  0.36071687936782837
Loss:  0.3588325083255768
Loss:  0.35697436332702637
Loss:  0.3551417887210846
Loss:  0.3533344566822052
Loss:  0.3515518605709076
Loss:  0.34979331493377686
Loss:  0.3480583131313324
training layer:  1
Loss:  1.126666784286499
Loss:  0.7113537192344666
Loss:  0.6606789827346802
Loss:  0.6345909237861633
Loss:  0.6222231984138489
Loss:  0.6157326698303223
Loss:  0.6123685836791992
Loss:  0.6099032759666443
Loss:  0.6066741943359375
Loss:  0.60252445936203
Loss:  0.5973405241966248
Loss:  0.590906023979187
Loss:  0.5833659768104553
Loss:  0.5748269557952881
Loss:  0.5654613375663757
Loss:  0.5555540919303894
Loss:  0.5453861355781555
Loss:  0.5350686311721802
Loss:  0.5245959162712097
Loss:  0.514289140701294
Loss:  0.5044233202934265
Loss:  0.49520838260650635
Loss:  0.48671865463256836
Loss:  0.4789024293422699
Loss:  0.4716760516166687
Loss:  0.46494099497795105
Loss:  0.45871591567993164
Loss:  0.4529871940612793
Loss:  0.4476737082004547
Loss:  0.44271111488342285
Loss:  0.43801072239875793
Loss:  0.4334850013256073
Loss:  0.42911556363105774
Loss:  0.42491960525512695
Loss:  0.42087793350219727
Loss:  0.41695281863212585
Loss:  0.4131052494049072
Loss:  0.40934836864471436
Loss:  0.4056745171546936
Loss:  0.40210869908332825
Loss:  0.3986324071884155
Loss:  0.3952663838863373
Loss:  0.39202752709388733
Loss:  0.3889094889163971
Loss:  0.3859011232852936
Loss:  0.3829908072948456
Loss:  0.38016679883003235
Loss:  0.3774201571941376
Loss:  0.37474584579467773
Loss:  0.372130423784256
Loss:  0.36957332491874695
Loss:  0.36710646748542786
Loss:  0.36472517251968384
Loss:  0.3624069392681122
Loss:  0.3601434528827667
Loss:  0.3579429090023041
Loss:  0.3558085560798645
Loss:  0.35373541712760925
Loss:  0.35171905159950256
Loss:  0.3497548997402191
Loss:  0.3478391170501709
Loss:  0.34596899151802063
Loss:  0.3441433608531952
Loss:  0.34236013889312744
Loss:  0.34061673283576965
Loss:  0.33891117572784424
Loss:  0.33724164962768555
Loss:  0.33560630679130554
Loss:  0.3340033292770386
Loss:  0.33243075013160706
Loss:  0.3308867812156677
Loss:  0.3293694853782654
Loss:  0.3278771936893463
Loss:  0.3264082074165344
Loss:  0.3249603807926178
Loss:  0.32353103160858154
Loss:  0.32211747765541077
Loss:  0.32071852684020996
Loss:  0.31933555006980896
Loss:  0.31797167658805847
Loss:  0.31662896275520325
Loss:  0.31530821323394775
Loss:  0.31400930881500244
Loss:  0.3127318024635315
Loss:  0.31147491931915283
Loss:  0.3102380633354187
Loss:  0.30902034044265747
Loss:  0.3078208565711975
Loss:  0.3066383898258209
Loss:  0.3054714798927307
Loss:  0.30431875586509705
Loss:  0.303180992603302
Loss:  0.30205848813056946
Loss:  0.30095112323760986
Loss:  0.29985886812210083
Loss:  0.29878151416778564
Loss:  0.2977188229560852
Loss:  0.29667040705680847
Loss:  0.29563596844673157
Loss:  0.294615238904953
train error: 0.08880001306533813
test error: 0.08890002965927124

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:39:29
[0;34m[INFO][0m Training End: 2025-12-15 16:39:46
[0;34m[INFO][0m Total Duration: 0h 0m 17s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 91.10999703407287600%
[0;34m[INFO][0m Test Error: 0.08890002965927124
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.08880001306533813
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_058_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_058_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
