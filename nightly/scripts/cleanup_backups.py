#!/usr/bin/env python3
"""
å¤‡ä»½æ–‡ä»¶æ¸…ç†è„šæœ¬

ç›®æ ‡:
1. è¯†åˆ«æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
2. è¯„ä¼°ä¿ç•™å¿…è¦æ€§
3. å½’æ¡£æˆ–åˆ é™¤ä¸å¿…è¦çš„å¤‡ä»½

ç‰ˆæœ¬: v1.0
æ—¥æœŸ: 2025-12-19
"""

import os
import shutil
from datetime import datetime
from pathlib import Path

def analyze_backups():
    """åˆ†ææ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""

    print("=" * 100)
    print("å¤‡ä»½æ–‡ä»¶åˆ†æ")
    print("=" * 100)
    print()

    # æŸ¥æ‰¾æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
    results_dir = Path('results')
    backup_files = []

    for pattern in ['*.backup*', '*.bak*']:
        backup_files.extend(results_dir.rglob(pattern))

    # æŒ‰ä½ç½®åˆ†ç±»
    categories = {
        'resultsæ ¹ç›®å½•': [],
        'archivedç›®å½•': [],
        'backup_archiveç›®å½•': [],
        'sessionç›®å½•': []
    }

    for file in backup_files:
        rel_path = file.relative_to(results_dir)
        parts = rel_path.parts

        if len(parts) == 1:
            categories['resultsæ ¹ç›®å½•'].append(file)
        elif 'archived' in parts[0] or 'archive' in parts[0]:
            if 'backup_archive' in str(file):
                categories['backup_archiveç›®å½•'].append(file)
            else:
                categories['archivedç›®å½•'].append(file)
        elif parts[0].startswith('run_'):
            categories['sessionç›®å½•'].append(file)
        else:
            categories['resultsæ ¹ç›®å½•'].append(file)

    # è¾“å‡ºåˆ†æç»“æœ
    total_size = 0
    total_files = 0

    for category, files in categories.items():
        if files:
            print(f"\nã€{category}ã€‘: {len(files)}ä¸ªæ–‡ä»¶")
            print("-" * 100)

            category_size = 0
            for file in sorted(files):
                size = file.stat().st_size
                size_mb = size / 1024 / 1024
                mtime = datetime.fromtimestamp(file.stat().st_mtime)

                category_size += size
                total_size += size
                total_files += 1

                rel_path = file.relative_to(results_dir)
                print(f"  {str(rel_path):<70} {size_mb:>6.2f}MB  {mtime.strftime('%Y-%m-%d %H:%M')}")

            print(f"  å°è®¡: {len(files)}ä¸ªæ–‡ä»¶, {category_size/1024/1024:.2f}MB")

    print()
    print("=" * 100)
    print(f"æ€»è®¡: {total_files}ä¸ªå¤‡ä»½æ–‡ä»¶, {total_size/1024/1024:.2f}MB")
    print("=" * 100)
    print()

    return categories, total_size

def recommend_cleanup(categories):
    """æ¨èæ¸…ç†æ–¹æ¡ˆ"""

    print("\n" + "=" * 100)
    print("æ¸…ç†å»ºè®®")
    print("=" * 100)
    print()

    # 1. resultsæ ¹ç›®å½•çš„å¤‡ä»½
    root_backups = categories['resultsæ ¹ç›®å½•']
    if root_backups:
        print("ã€1. resultsæ ¹ç›®å½•å¤‡ä»½ã€‘:")
        print()

        # æŒ‰æ–‡ä»¶ååˆ†ç»„
        data_csv_backups = [f for f in root_backups if 'data.csv' in f.name]
        raw_data_backups = [f for f in root_backups if 'raw_data.csv' in f.name]

        if data_csv_backups:
            print("  data.csvå¤‡ä»½:")
            latest = max(data_csv_backups, key=lambda f: f.stat().st_mtime)
            for f in sorted(data_csv_backups, key=lambda x: x.stat().st_mtime, reverse=True):
                age_days = (datetime.now().timestamp() - f.stat().st_mtime) / 86400
                if f == latest:
                    print(f"    âœ… ä¿ç•™: {f.name} (æœ€æ–°, {age_days:.1f}å¤©å‰)")
                else:
                    print(f"    ğŸ—‘ï¸ å¯åˆ é™¤: {f.name} ({age_days:.1f}å¤©å‰)")

        if raw_data_backups:
            print(f"\n  raw_data.csvå¤‡ä»½:")
            latest = max(raw_data_backups, key=lambda f: f.stat().st_mtime)
            for f in sorted(raw_data_backups, key=lambda x: x.stat().st_mtime, reverse=True):
                age_days = (datetime.now().timestamp() - f.stat().st_mtime) / 86400
                if f == latest:
                    print(f"    âœ… ä¿ç•™: {f.name} (æœ€æ–°, {age_days:.1f}å¤©å‰)")
                else:
                    print(f"    ğŸ—‘ï¸ å¯åˆ é™¤: {f.name} ({age_days:.1f}å¤©å‰)")

    # 2. backup_archiveç›®å½•
    archive_backups = categories['backup_archiveç›®å½•']
    if archive_backups:
        print(f"\nã€2. backup_archiveç›®å½•ã€‘:")
        print(f"  ğŸ“¦ å·²å½’æ¡£: {len(archive_backups)}ä¸ªæ–‡ä»¶")
        print(f"  ğŸ’¡ å»ºè®®: ä¿ç•™å½’æ¡£ï¼ˆå·²æ•´ç†ï¼‰")

    # 3. archivedç›®å½•çš„å¤‡ä»½
    archived_backups = categories['archivedç›®å½•']
    if archived_backups:
        print(f"\nã€3. archived/summary_archiveç›®å½•ã€‘:")
        print(f"  ğŸ“¦ å·²å½’æ¡£: {len(archived_backups)}ä¸ªæ–‡ä»¶")
        print(f"  ğŸ’¡ å»ºè®®: ä¿ç•™å½’æ¡£ï¼ˆå†å²å‚è€ƒï¼‰")

    # 4. sessionç›®å½•çš„å¤‡ä»½
    session_backups = categories['sessionç›®å½•']
    if session_backups:
        print(f"\nã€4. sessionç›®å½•ã€‘:")
        print(f"  ğŸ“‚ Sessionå¤‡ä»½: {len(session_backups)}ä¸ªæ–‡ä»¶")
        print(f"  ğŸ’¡ å»ºè®®: è¯„ä¼°sessionä»·å€¼åå†³å®š")

    print()
    print("=" * 100)
    print("æ¸…ç†ç­–ç•¥:")
    print("=" * 100)
    print("  âœ… ä¿ç•™: æ¯ä¸ªæ–‡ä»¶çš„æœ€æ–°å¤‡ä»½ï¼ˆ1ä¸ªï¼‰")
    print("  ğŸ—‘ï¸ åˆ é™¤: æ—§çš„é‡å¤å¤‡ä»½ï¼ˆå¦‚data.csv.backup_before_refixï¼‰")
    print("  ğŸ“¦ ä¿ç•™: æ‰€æœ‰å½’æ¡£ç›®å½•çš„å¤‡ä»½ï¼ˆå·²æ•´ç†ï¼‰")
    print("  ğŸ’¾ ä¿ç•™: backup_archive_20251219ç›®å½•ï¼ˆæ‰¹é‡å½’æ¡£ï¼‰")
    print()

def execute_cleanup(dry_run=True):
    """æ‰§è¡Œæ¸…ç†æ“ä½œ"""

    print("\n" + "=" * 100)
    if dry_run:
        print("æ¸…ç†é¢„è§ˆï¼ˆdry-runæ¨¡å¼ï¼‰")
    else:
        print("æ‰§è¡Œæ¸…ç†")
    print("=" * 100)
    print()

    results_dir = Path('results')

    # å®šä¹‰åˆ é™¤è§„åˆ™
    to_delete = []
    to_keep = []

    # 1. data.csvå¤‡ä»½ - åªä¿ç•™æœ€æ–°çš„
    data_csv_backups = list(results_dir.glob('data.csv.backup*'))
    if data_csv_backups:
        latest = max(data_csv_backups, key=lambda f: f.stat().st_mtime)
        for f in data_csv_backups:
            if f == latest:
                to_keep.append(('data.csvæœ€æ–°å¤‡ä»½', f))
            else:
                to_delete.append(('data.csvæ—§å¤‡ä»½', f))

    # 2. raw_data.csvå¤‡ä»½ - åªä¿ç•™æœ€æ–°çš„
    raw_data_backups = list(results_dir.glob('raw_data.csv.backup*'))
    if raw_data_backups:
        latest = max(raw_data_backups, key=lambda f: f.stat().st_mtime)
        for f in raw_data_backups:
            if f == latest:
                to_keep.append(('raw_data.csvæœ€æ–°å¤‡ä»½', f))
            else:
                to_delete.append(('raw_data.csvæ—§å¤‡ä»½', f))

    # è¾“å‡ºæ¸…ç†è®¡åˆ’
    if to_delete:
        print("ğŸ—‘ï¸ è®¡åˆ’åˆ é™¤:")
        total_delete_size = 0
        for reason, file in to_delete:
            size = file.stat().st_size / 1024 / 1024
            total_delete_size += size
            print(f"  - {file.name:<60} {size:>6.2f}MB  ({reason})")
        print(f"  å°è®¡: {len(to_delete)}ä¸ªæ–‡ä»¶, {total_delete_size:.2f}MB")

    if to_keep:
        print(f"\nâœ… ä¿ç•™:")
        total_keep_size = 0
        for reason, file in to_keep:
            size = file.stat().st_size / 1024 / 1024
            total_keep_size += size
            print(f"  - {file.name:<60} {size:>6.2f}MB  ({reason})")
        print(f"  å°è®¡: {len(to_keep)}ä¸ªæ–‡ä»¶, {total_keep_size:.2f}MB")

    print()
    print("ğŸ“¦ å½’æ¡£ç›®å½•ï¼ˆä¸æ¸…ç†ï¼‰:")
    print("  - archived/summary_archive/ - 10ä¸ªå¤‡ä»½æ–‡ä»¶")
    print("  - backup_archive_20251219/ - 26ä¸ªå¤‡ä»½æ–‡ä»¶")

    # æ‰§è¡Œåˆ é™¤
    if not dry_run and to_delete:
        print()
        confirm = input("ç¡®è®¤åˆ é™¤ä»¥ä¸Šæ–‡ä»¶? (yes/no): ")
        if confirm.lower() == 'yes':
            for reason, file in to_delete:
                file.unlink()
                print(f"  âœ“ å·²åˆ é™¤: {file.name}")
            print(f"\nâœ“ æ¸…ç†å®Œæˆ: åˆ é™¤{len(to_delete)}ä¸ªæ–‡ä»¶")
        else:
            print("\nå–æ¶ˆæ¸…ç†")

    return to_delete, to_keep

if __name__ == '__main__':
    # åˆ†æå¤‡ä»½æ–‡ä»¶
    categories, total_size = analyze_backups()

    # æ¨èæ¸…ç†æ–¹æ¡ˆ
    recommend_cleanup(categories)

    # é¢„è§ˆæ¸…ç†ï¼ˆdry-runï¼‰
    to_delete, to_keep = execute_cleanup(dry_run=True)

    print()
    print("ä¸‹ä¸€æ­¥:")
    print("  python3 scripts/cleanup_backups.py  # é¢„è§ˆæ¸…ç†")
    print("  # æ£€æŸ¥æ¸…ç†è®¡åˆ’åï¼Œä¿®æ”¹è„šæœ¬è®¾ç½®dry_run=Falseæ‰§è¡Œå®é™…æ¸…ç†")
