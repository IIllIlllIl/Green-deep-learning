# 11ä¸ªæ¨¡å‹çš„æ·±åº¦å­¦ä¹ æ¶æ„è¯¦è§£

**ç‰ˆæœ¬**: v4.3.0
**æœ€åæ›´æ–°**: 2025-11-18

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°å®éªŒä¸­ä½¿ç”¨çš„11ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç½‘ç»œæ¶æ„ã€‚

---

## ğŸ“Š æ¶æ„æ€»è§ˆè¡¨

| # | æ¨¡å‹ | æ¶æ„ç±»å‹ | ä¸»è¦ç»„ä»¶ | å±‚æ•° | å‚æ•°é‡ | ç‰¹ç‚¹ |
|---|------|---------|---------|------|--------|------|
| 1 | MRT-OAST | Transformer | Multi-head Attention + MLP + ResNet | ~12å±‚ | ~10M | å¤šè¡¨ç¤ºèåˆ |
| 2 | bug-localization | DNN + IR | DNN (2éšè—å±‚) | 3å±‚ | ~1K | æ··åˆæ–¹æ³• |
| 3 | resnet20 | ResNet | Residual Blocks | 20å±‚ | 0.27M | æ®‹å·®è¿æ¥ |
| 4 | VulBERTa_mlp | Transformer + MLP | BERT + MLP | 12+2å±‚ | 125M | é¢„è®­ç»ƒ+å¾®è°ƒ |
| 5 | densenet121 | DenseNet | Dense Blocks | 121å±‚ | 7.98M | å¯†é›†è¿æ¥ |
| 6 | hrnet18 | HRNet | Parallel Multi-scale | 18å±‚ | 21.3M | é«˜åˆ†è¾¨ç‡ä¿æŒ |
| 7 | pcb | CNN + Partition | ResNet-50 + 6éƒ¨åˆ† | 50+6å±‚ | 25.6M | éƒ¨åˆ†ç‰¹å¾æå– |
| 8 | mnist | CNN | Conv + Pooling | 3å±‚ | ~44K | ç»å…¸CNN |
| 9 | mnist_rnn | RNN | LSTM | 2å±‚ | ~100K | åºåˆ—å¤„ç† |
| 10 | mnist_ff | Forward-Forward | FF Layers | 4å±‚ | ~50K | æ–°å‹å­¦ä¹ ç®—æ³• |
| 11 | siamese | Siamese CNN | å­ªç”Ÿç½‘ç»œ | 3Ã—2å±‚ | ~88K | å¯¹æ¯”å­¦ä¹  |

---

## ğŸ” è¯¦ç»†æ¶æ„åˆ†æ

### 1. MRT-OAST (Multi-Representation Transformer)

**ä»»åŠ¡**: ä»£ç å…‹éš†æ£€æµ‹

**æ¶æ„**: Transformer-based Multi-Representation Learning

```
è¾“å…¥å±‚:
â”œâ”€ Code Tokens (ä»£ç è¯å…ƒ)
â”œâ”€ AST (æŠ½è±¡è¯­æ³•æ ‘)
â””â”€ OAST (ä¼˜åŒ–çš„æŠ½è±¡è¯­æ³•æ ‘)
         â†“
    Embedding Layer (è¯åµŒå…¥å±‚)
    - Dimension: 512
         â†“
    Transformer Encoder
    â”œâ”€ Multi-head Self-Attention (8 heads)
    â”œâ”€ Feed-Forward Network
    â”œâ”€ Layer Normalization
    â””â”€ Residual Connection
    Ã— 12 layers
         â†“
    Representation Fusion (è¡¨ç¤ºèåˆ)
    - Token representation
    - AST representation
    - OAST representation
         â†“
    Similarity Network (ç›¸ä¼¼åº¦ç½‘ç»œ)
    - MLP: 512 â†’ 256 â†’ 128 â†’ 1
         â†“
    Output: Similarity Score (0-1)
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ å¤šè¡¨ç¤ºå­¦ä¹ ï¼ˆToken + AST + OASTï¼‰
- ğŸ”¥ Transformeræ¶æ„ï¼ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰
- ğŸ”¥ ç«¯åˆ°ç«¯è®­ç»ƒ

**å‚æ•°é‡**: ~10M

---

### 2. bug-localization (DNN + rVSM)

**ä»»åŠ¡**: è½¯ä»¶ç¼ºé™·å®šä½

**æ¶æ„**: ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº (MLP)

```
è¾“å…¥å±‚ (5ä¸ªç‰¹å¾):
â”œâ”€ rVSM similarity (æ–‡æœ¬ç›¸ä¼¼åº¦)
â”œâ”€ Collaborative filter (ååŒè¿‡æ»¤)
â”œâ”€ Classname similarity (ç±»åç›¸ä¼¼åº¦)
â”œâ”€ Bug recency (bugæ–°è¿‘åº¦)
â””â”€ Bug frequency (bugé¢‘ç‡)
         â†“
    Hidden Layer 1
    - Neurons: 10
    - Activation: ReLU
    - Dropout: 0.2
         â†“
    Hidden Layer 2
    - Neurons: 10
    - Activation: ReLU
    - Dropout: 0.2
         â†“
    Output Layer
    - Neurons: 1
    - Activation: Sigmoid
         â†“
    Output: Relevance Score (0-1)
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ ç®€å•ä½†æœ‰æ•ˆçš„MLP
- ğŸ”¥ ç»“åˆä¼ ç»ŸIRç‰¹å¾ï¼ˆrVSMï¼‰å’ŒDNN
- ğŸ”¥ 10æŠ˜äº¤å‰éªŒè¯

**å‚æ•°é‡**: ~1,000 (éå¸¸å°)

**ç½‘ç»œç»“æ„**:
```
Input(5) â†’ Dense(10, relu) â†’ Dense(10, relu) â†’ Dense(1, sigmoid)
```

---

### 3. resnet20 (ResNet-20 for CIFAR-10)

**ä»»åŠ¡**: å›¾åƒåˆ†ç±»ï¼ˆCIFAR-10ï¼‰

**æ¶æ„**: æ®‹å·®ç½‘ç»œ (Residual Network)

```
Input: 32Ã—32Ã—3 (CIFAR-10å›¾åƒ)
         â†“
    Conv1: 3Ã—3, 16 filters
         â†“
    Residual Block Stack 1 (16 filters)
    â”œâ”€ Residual Block Ã— 3
    â”‚  â”œâ”€ Conv: 3Ã—3, 16
    â”‚  â”œâ”€ BatchNorm + ReLU
    â”‚  â”œâ”€ Conv: 3Ã—3, 16
    â”‚  â”œâ”€ BatchNorm
    â”‚  â””â”€ Shortcut (identity)
         â†“
    Residual Block Stack 2 (32 filters, stride=2)
    â”œâ”€ Residual Block Ã— 3
         â†“
    Residual Block Stack 3 (64 filters, stride=2)
    â”œâ”€ Residual Block Ã— 3
         â†“
    Global Average Pooling
         â†“
    Fully Connected: 10 classes
         â†“
    Softmax
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ æ®‹å·®è¿æ¥ï¼ˆSkip Connectionï¼‰è§£å†³æ¢¯åº¦æ¶ˆå¤±
- ğŸ”¥ BatchNormåŠ é€Ÿè®­ç»ƒ
- ğŸ”¥ ä¸¥æ ¼æŒ‰ç…§åŸè®ºæ–‡å®ç°

**å‚æ•°é‡**: 0.27M

**Residual Blockç»“æ„**:
```
x â†’ Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ (+) â†’ ReLU
â†“_________________________________â†‘
        (shortcut/identity)
```

---

### 4. VulBERTa_mlp (ä»£ç æ¼æ´æ£€æµ‹)

**ä»»åŠ¡**: æºä»£ç æ¼æ´æ£€æµ‹

**æ¶æ„**: BERT + MLP Classifier

```
Input: C/C++ Source Code
         â†“
    Custom Tokenizer (ä»£ç ä¸“ç”¨)
    - Vocabulary: 50,000
         â†“
    RoBERTa Base Encoder
    â”œâ”€ Embedding Layer: 768
    â”œâ”€ Transformer Encoder Ã— 12
    â”‚  â”œâ”€ Multi-head Attention (12 heads)
    â”‚  â”œâ”€ Feed-Forward (3072)
    â”‚  â””â”€ Layer Norm + Residual
         â†“
    [CLS] Token Representation (768-dim)
         â†“
    MLP Classifier
    â”œâ”€ Dense: 768 â†’ 256
    â”œâ”€ ReLU + Dropout(0.1)
    â”œâ”€ Dense: 256 â†’ 128
    â”œâ”€ ReLU + Dropout(0.1)
    â””â”€ Dense: 128 â†’ 2
         â†“
    Softmax
         â†“
    Output: [Vulnerable, Non-Vulnerable]
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ åŸºäºRoBERTaé¢„è®­ç»ƒæ¨¡å‹
- ğŸ”¥ è‡ªå®šä¹‰ä»£ç tokenizer
- ğŸ”¥ åœ¨Devignæ•°æ®é›†ä¸Šfine-tune

**å‚æ•°é‡**: ~125M (BERT: 123M + MLP: 2M)

**é¢„è®­ç»ƒ**: DrapGHæ•°æ®é›†ï¼ˆå¼€æºC/C++ä»£ç ï¼‰

---

### 5. densenet121 (DenseNet-121)

**ä»»åŠ¡**: è¡Œäººé‡è¯†åˆ«

**æ¶æ„**: å¯†é›†è¿æ¥ç½‘ç»œ (Densely Connected Network)

```
Input: 256Ã—128Ã—3 (è¡Œäººå›¾åƒ)
         â†“
    Conv1: 7Ã—7, 64, stride=2
    MaxPool: 3Ã—3, stride=2
         â†“
    Dense Block 1 (6 layers)
    â”œâ”€ [BN-ReLU-Conv(1Ã—1)-BN-ReLU-Conv(3Ã—3)] Ã— 6
    â”œâ”€ Growth rate: 32
    â””â”€ Dense connections (æ¯å±‚è¿æ¥å‰é¢æ‰€æœ‰å±‚)
         â†“
    Transition Layer 1
    â”œâ”€ BN-Conv(1Ã—1)-AvgPool(2Ã—2)
    â””â”€ Compression: 0.5
         â†“
    Dense Block 2 (12 layers)
         â†“
    Transition Layer 2
         â†“
    Dense Block 3 (24 layers)
         â†“
    Transition Layer 3
         â†“
    Dense Block 4 (16 layers)
         â†“
    Global Average Pooling
         â†“
    Fully Connected: 751 IDs (Market-1501)
         â†“
    Output: Person ID
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ Dense Connectionï¼ˆæ¯å±‚ä¸å‰é¢æ‰€æœ‰å±‚è¿æ¥ï¼‰
- ğŸ”¥ ç‰¹å¾é‡ç”¨ï¼Œå‚æ•°æ•ˆç‡é«˜
- ğŸ”¥ ç¼“è§£æ¢¯åº¦æ¶ˆå¤±

**å‚æ•°é‡**: 7.98M

**Dense Blockæ ¸å¿ƒ**:
```
xâ‚€ â†’ Hâ‚ â†’ xâ‚ â”€â”
xâ‚€ â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â†’ Concat â†’ Hâ‚‚ â†’ xâ‚‚ â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Concat â†’ Hâ‚ƒ â†’ xâ‚ƒ
```

---

### 6. hrnet18 (High-Resolution Net)

**ä»»åŠ¡**: è¡Œäººé‡è¯†åˆ«

**æ¶æ„**: é«˜åˆ†è¾¨ç‡ç½‘ç»œ (å¹¶è¡Œå¤šå°ºåº¦)

```
Input: 256Ã—128Ã—3
         â†“
    Stem: 2Ã— Conv(3Ã—3, 64)
         â†“
    Stage 1: Single Resolution (1/4)
    â””â”€ Bottleneck Ã— 4
         â†“
    Stage 2: Parallel Branches (1/4 + 1/8)
    â”œâ”€ High-Res Branch (1/4)
    â””â”€ Low-Res Branch (1/8)
    â””â”€ Multi-scale Fusion
         â†“
    Stage 3: Parallel Branches (1/4 + 1/8 + 1/16)
    â”œâ”€ High-Res Branch (1/4)
    â”œâ”€ Med-Res Branch (1/8)
    â””â”€ Low-Res Branch (1/16)
    â””â”€ Multi-scale Fusion
         â†“
    Stage 4: Parallel Branches (1/4 + 1/8 + 1/16 + 1/32)
    â””â”€ Multi-scale Fusion
         â†“
    Global Average Pooling
         â†“
    FC: 751 classes
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ å§‹ç»ˆä¿æŒé«˜åˆ†è¾¨ç‡è¡¨ç¤º
- ğŸ”¥ å¹¶è¡Œå¤šå°ºåº¦åˆ†æ”¯
- ğŸ”¥ è·¨åˆ†æ”¯ä¿¡æ¯èåˆ

**å‚æ•°é‡**: 21.3M

**Multi-scale Fusion**:
```
High-Res â”€â”€â”¬â”€â†’ High-Res Output
           â”‚
Med-Res â”€â”€â”€â”¼â”€â†’ Med-Res Output
           â”‚
Low-Res â”€â”€â”€â”´â”€â†’ Low-Res Output
(é€šè¿‡ä¸Šé‡‡æ ·/ä¸‹é‡‡æ ·äº’ç›¸äº¤æ¢ä¿¡æ¯)
```

---

### 7. pcb (Part-based Convolutional Baseline)

**ä»»åŠ¡**: è¡Œäººé‡è¯†åˆ«

**æ¶æ„**: ResNet-50 + éƒ¨åˆ†åˆ†å‰²

```
Input: 384Ã—128Ã—3
         â†“
    ResNet-50 Backbone (å»æ‰æœ€åçš„stride)
    â”œâ”€ Conv1: 7Ã—7, 64
    â”œâ”€ MaxPool: 3Ã—3
    â”œâ”€ Layer1 (Bottleneck Ã— 3)
    â”œâ”€ Layer2 (Bottleneck Ã— 4)
    â”œâ”€ Layer3 (Bottleneck Ã— 6)
    â””â”€ Layer4 (Bottleneck Ã— 3)
         â†“
    Feature Map: 24Ã—8Ã—2048
         â†“
    Uniform Partition (å‡åŒ€åˆ†å‰²)
    â”œâ”€ Part 1: [0:4, :, :]   (å¤´éƒ¨)
    â”œâ”€ Part 2: [4:8, :, :]   (ä¸Šèº¯å¹²)
    â”œâ”€ Part 3: [8:12, :, :]  (è…°éƒ¨)
    â”œâ”€ Part 4: [12:16, :, :] (å¤§è…¿)
    â”œâ”€ Part 5: [16:20, :, :] (å°è…¿)
    â””â”€ Part 6: [20:24, :, :] (è„šéƒ¨)
         â†“
    Part-level Feature Extraction (æ¯éƒ¨åˆ†ç‹¬ç«‹)
    â”œâ”€ Global Average Pooling (æ¯ä¸ªpart)
    â”œâ”€ FC: 2048 â†’ 256 (æ¯ä¸ªpart)
    â””â”€ L2 Normalization
         â†“
    Concatenation: 256Ã—6 = 1536-dim
         â†“
    Classification: 751 classes
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ éƒ¨åˆ†åˆ†å‰²ï¼ˆ6ä¸ªæ°´å¹³æ¡å¸¦ï¼‰
- ğŸ”¥ ç»†ç²’åº¦ç‰¹å¾æå–
- ğŸ”¥ å¯¹é®æŒ¡é²æ£’

**å‚æ•°é‡**: 25.6M (ResNet-50: 23.5M + 6Ã—FC: 2.1M)

---

### 8. mnist (ç»å…¸CNN)

**ä»»åŠ¡**: æ‰‹å†™æ•°å­—è¯†åˆ«

**æ¶æ„**: ç®€å•3å±‚CNN

```
Input: 28Ã—28Ã—1 (ç°åº¦å›¾åƒ)
         â†“
    Conv1: 3Ã—3, 32 filters
    ReLU
    MaxPool: 2Ã—2
         â†“
    Conv2: 3Ã—3, 64 filters
    ReLU
    MaxPool: 2Ã—2
         â†“
    Flatten: 7Ã—7Ã—64 = 3136
         â†“
    FC1: 3136 â†’ 128
    ReLU
    Dropout: 0.5
         â†“
    FC2: 128 â†’ 10
         â†“
    Softmax
         â†“
    Output: 10 classes (0-9)
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ ç»å…¸CNNç»“æ„
- ğŸ”¥ ç®€å•ä½†æœ‰æ•ˆ
- ğŸ”¥ æ•™å­¦ç¤ºä¾‹

**å‚æ•°é‡**: ~44K

---

### 9. mnist_rnn (LSTM for MNIST)

**ä»»åŠ¡**: æ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆåºåˆ—æ–¹å¼ï¼‰

**æ¶æ„**: LSTMå¾ªç¯ç¥ç»ç½‘ç»œ

```
Input: 28Ã—28 â†’ é‡å¡‘ä¸ºåºåˆ— 28 steps Ã— 28 features
         â†“
    LSTM Layer 1
    - Hidden units: 128
    - Return sequences: True
    - Dropout: 0.2
         â†“
    LSTM Layer 2
    - Hidden units: 128
    - Return sequences: False
    - Dropout: 0.2
         â†“
    Last Hidden State: 128-dim
         â†“
    Fully Connected: 128 â†’ 10
         â†“
    Softmax
         â†“
    Output: 10 classes
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ å°†å›¾åƒè§†ä¸ºåºåˆ—ï¼ˆ28è¡Œï¼Œæ¯è¡Œ28åƒç´ ï¼‰
- ğŸ”¥ LSTMå¤„ç†æ—¶åºä¾èµ–
- ğŸ”¥ æ¼”ç¤ºRNNåœ¨CVä¸­çš„åº”ç”¨

**å‚æ•°é‡**: ~100K

**LSTM Cellç»“æ„**:
```
Input Gate:  i_t = Ïƒ(W_iÂ·[h_{t-1}, x_t] + b_i)
Forget Gate: f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t] + b_f)
Cell State:  C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ tanh(W_cÂ·[h_{t-1}, x_t] + b_c)
Output Gate: o_t = Ïƒ(W_oÂ·[h_{t-1}, x_t] + b_o)
Hidden:      h_t = o_t âŠ™ tanh(C_t)
```

---

### 10. mnist_ff (Forward-Forward Network)

**ä»»åŠ¡**: æ‰‹å†™æ•°å­—è¯†åˆ«

**æ¶æ„**: Forward-Forwardç®—æ³•ï¼ˆHinton 2022ï¼‰

```
Input: 28Ã—28Ã—1 â†’ Flatten: 784
         â†“
    FF Layer 1
    - Input: 784
    - Output: 500
    - Positive pass: Real data
    - Negative pass: Negative data
    - Local loss: Goodness function
         â†“
    FF Layer 2
    - Input: 500
    - Output: 500
    - Local loss: Goodness function
         â†“
    FF Layer 3
    - Input: 500
    - Output: 500
    - Local loss: Goodness function
         â†“
    FF Layer 4
    - Input: 500
    - Output: 10
    - Local loss: Goodness function
         â†“
    Output: 10 classes
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ æ— åå‘ä¼ æ’­ï¼ˆä¸éœ€è¦BPï¼‰
- ğŸ”¥ å±€éƒ¨å­¦ä¹ è§„åˆ™
- ğŸ”¥ æ¯å±‚ç‹¬ç«‹è®­ç»ƒ

**å‚æ•°é‡**: ~50K

**Goodness Function (æ ¸å¿ƒ)**:
```python
# Positive data: æœ€å¤§åŒ– goodness
goodness_pos = Î£(activationÂ²)

# Negative data: æœ€å°åŒ– goodness
goodness_neg = Î£(activationÂ²)

# Local loss
loss = -log(Ïƒ(goodness_pos - threshold))
       -log(Ïƒ(threshold - goodness_neg))
```

**è®­ç»ƒæ–¹å¼**:
- âœ… æ¯å±‚å±€éƒ¨è®­ç»ƒï¼ˆä¸éœ€è¦å…¨å±€æ¢¯åº¦ï¼‰
- âœ… Positive samples: çœŸå®æ•°æ®
- âœ… Negative samples: æ ‡ç­¾é”™è¯¯çš„æ•°æ®

---

### 11. siamese (å­ªç”Ÿç½‘ç»œ)

**ä»»åŠ¡**: ç›¸ä¼¼åº¦å­¦ä¹ 

**æ¶æ„**: Siamese CNN

```
Image 1: 28Ã—28Ã—1 â”€â”€â”
                    â”œâ”€â†’ Shared CNN â”€â”€â”
Image 2: 28Ã—28Ã—1 â”€â”€â”˜                 â”‚
                                     â”‚
Shared CNN:                          â”‚
â”œâ”€ Conv1: 3Ã—3, 64                   â”‚
â”œâ”€ ReLU + MaxPool                   â”‚
â”œâ”€ Conv2: 3Ã—3, 128                  â”‚
â”œâ”€ ReLU + MaxPool                   â”‚
â”œâ”€ Flatten                          â”‚
â””â”€ FC: 128                          â”‚
                                     â†“
                          [Feature 1, Feature 2]
                                     â†“
                          Distance Calculation
                          (Euclidean / Cosine)
                                     â†“
                          Contrastive Loss
                          - Similar pairs: minimize distance
                          - Dissimilar pairs: maximize distance
                                     â†“
                          Output: Similarity Score
```

**å…³é”®ç‰¹ç‚¹**:
- ğŸ”¥ æƒé‡å…±äº«ï¼ˆä¸¤ä¸ªåˆ†æ”¯ä½¿ç”¨ç›¸åŒçš„CNNï¼‰
- ğŸ”¥ å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
- ğŸ”¥ å­¦ä¹ è·ç¦»åº¦é‡

**å‚æ•°é‡**: ~88K (å•ä¸ªåˆ†æ”¯44K Ã— æƒé‡å…±äº«)

**Contrastive Loss**:
```python
loss = (1 - Y) * 0.5 * DÂ² + Y * 0.5 * max(margin - D, 0)Â²

å…¶ä¸­:
- Y: æ ‡ç­¾ (0=ç›¸ä¼¼, 1=ä¸ç›¸ä¼¼)
- D: æ¬§æ°è·ç¦»
- margin: é—´éš”ï¼ˆé€šå¸¸ä¸º1.0ï¼‰
```

---

## ğŸ“Š æ¶æ„åˆ†ç±»ç»Ÿè®¡

### æŒ‰æ¶æ„ç±»å‹åˆ†ç±»

| æ¶æ„ç±»å‹ | æ¨¡å‹æ•°é‡ | æ¨¡å‹åˆ—è¡¨ |
|---------|---------|---------|
| **CNN** | 5 | resnet20, densenet121, hrnet18, pcb, mnist |
| **RNN/LSTM** | 1 | mnist_rnn |
| **Transformer** | 2 | MRT-OAST, VulBERTa_mlp |
| **MLP** | 1 | bug-localization |
| **Siamese** | 1 | siamese |
| **Forward-Forward** | 1 | mnist_ff |

### æŒ‰å‚æ•°è§„æ¨¡åˆ†ç±»

| è§„æ¨¡ | èŒƒå›´ | æ¨¡å‹æ•°é‡ | æ¨¡å‹åˆ—è¡¨ |
|------|------|---------|---------|
| **æå°** | < 100K | 4 | bug-localization, mnist, mnist_rnn, mnist_ff, siamese |
| **å°** | 100K-1M | 1 | resnet20 |
| **ä¸­** | 1M-10M | 2 | densenet121, MRT-OAST |
| **å¤§** | 10M-30M | 2 | hrnet18, pcb |
| **è¶…å¤§** | > 100M | 1 | VulBERTa_mlp |

### æŒ‰åˆ›æ–°æ€§åˆ†ç±»

| ç±»åˆ« | æ¨¡å‹ | åˆ›æ–°ç‚¹ |
|------|------|--------|
| **ç»å…¸æ¶æ„** | mnist, mnist_rnn | æ•™å­¦ç¤ºä¾‹ |
| **æ®‹å·®å­¦ä¹ ** | resnet20 | Skip Connection |
| **å¯†é›†è¿æ¥** | densenet121 | Dense Connection |
| **å¤šå°ºåº¦** | hrnet18 | Parallel Multi-scale |
| **éƒ¨åˆ†åˆ†å‰²** | pcb | Part-based Features |
| **é¢„è®­ç»ƒ** | VulBERTa_mlp | BERT for Code |
| **å¤šè¡¨ç¤º** | MRT-OAST | Token + AST + OAST |
| **å¯¹æ¯”å­¦ä¹ ** | siamese | Contrastive Learning |
| **æ–°å‹å­¦ä¹ ** | mnist_ff | No Backpropagation |

---

## ğŸ¯ æ¶æ„é€‰æ‹©å»ºè®®

### å›¾åƒåˆ†ç±»ä»»åŠ¡

- **å°æ•°æ®é›†**: resnet20 (è½»é‡çº§)
- **ä¸­ç­‰æ•°æ®é›†**: densenet121 (ç‰¹å¾é‡ç”¨)
- **éœ€è¦å¤šå°ºåº¦**: hrnet18 (ä¿æŒé«˜åˆ†è¾¨ç‡)

### è¡Œäººé‡è¯†åˆ«ä»»åŠ¡

- **æ ‡å‡†æ–¹æ³•**: densenet121
- **ç»†ç²’åº¦**: pcb (éƒ¨åˆ†ç‰¹å¾)
- **é«˜ç²¾åº¦**: hrnet18 (å¤šå°ºåº¦)

### ä»£ç åˆ†æä»»åŠ¡

- **å…‹éš†æ£€æµ‹**: MRT-OAST (å¤šè¡¨ç¤º)
- **æ¼æ´æ£€æµ‹**: VulBERTa (é¢„è®­ç»ƒ)
- **ç¼ºé™·å®šä½**: bug-localization (è½»é‡çº§)

### æ•™å­¦ç¤ºä¾‹

- **CNNå…¥é—¨**: mnist
- **RNNå…¥é—¨**: mnist_rnn
- **åº¦é‡å­¦ä¹ **: siamese
- **æ–°ç®—æ³•**: mnist_ff

---

## ğŸ“š å‚è€ƒèµ„æ–™

### åŸå§‹è®ºæ–‡

1. **ResNet**: He et al., "Deep Residual Learning for Image Recognition", CVPR 2016
2. **DenseNet**: Huang et al., "Densely Connected Convolutional Networks", CVPR 2017
3. **HRNet**: Wang et al., "Deep High-Resolution Representation Learning", CVPR 2019
4. **PCB**: Sun et al., "Beyond Part Models: Person Retrieval with Refined Part Pooling", ECCV 2018
5. **VulBERTa**: Hanif & Maffeis, "VulBERTa: Simplified Source Code Pre-Training", IJCNN 2022
6. **Forward-Forward**: Hinton, "The Forward-Forward Algorithm", 2022
7. **Siamese**: Bromley et al., "Signature Verification using a Siamese Time Delay Neural Network", 1993

### å®ç°å‚è€ƒ

- PyTorchå®˜æ–¹: https://pytorch.org/docs/
- timmåº“: https://github.com/rwightman/pytorch-image-models
- HuggingFace Transformers: https://huggingface.co/docs/transformers

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-18
**ä½œè€…**: Green
