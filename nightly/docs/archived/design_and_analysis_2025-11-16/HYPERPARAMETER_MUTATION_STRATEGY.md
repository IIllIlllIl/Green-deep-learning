# è¶…å‚æ•°å˜å¼‚èŒƒå›´å’Œæ–¹å¼è®¾è®¡

**æ—¥æœŸ**: 2025-11-09
**ä½œè€…**: Green
**é¡¹ç›®**: Mutation-Based Training Energy Profiler

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä¸ºæ¯ä¸ªè¶…å‚æ•°æä¾›ç§‘å­¦åˆç†çš„å˜å¼‚èŒƒå›´å’Œå˜å¼‚æ–¹å¼å»ºè®®ï¼Œç¡®ä¿å˜å¼‚åçš„æ¨¡å‹æ—¢èƒ½ä¿æŒæ€§èƒ½åˆç†æ€§ï¼Œåˆèƒ½æœ‰æ•ˆæ¢ç´¢èƒ½è€—-æ€§èƒ½æƒè¡¡ç©ºé—´ã€‚

**è®¾è®¡åŸåˆ™**:
1. âœ… **æ€§èƒ½ä¸‹ç•Œ**: é¿å…æ€§èƒ½è¿‡å·®å¯¼è‡´èƒ½è€—ç ”ç©¶æ— æ„ä¹‰
2. âœ… **æ¢ç´¢å……åˆ†**: èŒƒå›´è¶³å¤Ÿå¤§ä»¥è§‚å¯Ÿèƒ½è€—å˜åŒ–è¶‹åŠ¿
3. âœ… **å®ç”¨æ€§**: åŸºäºæœºå™¨å­¦ä¹ é¢†åŸŸæœ€ä½³å®è·µ
4. âœ… **å¯å¤ç°æ€§**: æ˜ç¡®çš„å˜å¼‚åˆ†å¸ƒå’Œé‡‡æ ·ç­–ç•¥

---

## ğŸ¯ é€šç”¨è¶…å‚æ•°å˜å¼‚è®¾è®¡

### 1. Epochsï¼ˆè®­ç»ƒè½®æ•°ï¼‰

#### æ¨èå˜å¼‚èŒƒå›´

**è¡¨è¾¾å¼**: `[default * 0.5, default * 2.0]`

**å˜å¼‚æ–¹å¼**: **å¯¹æ•°å‡åŒ€åˆ†å¸ƒ** (Log-Uniform Distribution)

**å…·ä½“å®ç°**:
```python
import numpy as np

def mutate_epochs(default, min_epochs=5):
    """
    å¯¹æ•°å‡åŒ€åˆ†å¸ƒé‡‡æ ·epochs

    Args:
        default: é»˜è®¤epochså€¼
        min_epochs: æœ€å°epochsçº¦æŸï¼ˆé¿å…è¿‡å°ï¼‰

    Returns:
        å˜å¼‚åçš„epochså€¼
    """
    lower = max(default * 0.5, min_epochs)
    upper = default * 2.0

    # å¯¹æ•°å‡åŒ€åˆ†å¸ƒé‡‡æ ·
    log_lower = np.log(lower)
    log_upper = np.log(upper)
    log_value = np.random.uniform(log_lower, log_upper)

    return int(np.exp(log_value))
```

**åˆ†æ¡£å»ºè®®**ï¼ˆç¦»æ•£å˜å¼‚ï¼‰:
```python
# å¯¹äºéœ€è¦ç¦»æ•£é‡‡æ ·çš„åœºæ™¯
def mutate_epochs_discrete(default):
    """
    ç¦»æ•£åˆ†æ¡£é‡‡æ ·

    è¿”å›: default * {0.5, 0.75, 1.0, 1.5, 2.0} ä¹‹ä¸€
    """
    factors = [0.5, 0.75, 1.0, 1.5, 2.0]
    factor = np.random.choice(factors)
    return int(default * factor)
```

#### å„æ¨¡å‹å…·ä½“å»ºè®®

| æ¨¡å‹ç»„ | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | å˜å¼‚æ–¹å¼ | è¯´æ˜ |
|--------|--------|---------|---------|------|
| **MRT-OAST** | 10 | [5, 20] | ç¦»æ•£: {5, 8, 10, 15, 20} | å°æ•°æ®é›†ï¼Œè¿‡å¤šepochæ˜“è¿‡æ‹Ÿåˆ |
| **pytorch_resnet** | 200 | [100, 300] | å¯¹æ•°: [100, 400] | å¤§æ•°æ®é›†(CIFAR-10)ï¼Œéœ€å……åˆ†è®­ç»ƒ |
| **VulBERTa** | 10 | [5, 20] | ç¦»æ•£: {5, 8, 10, 15, 20} | é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼Œå°‘epochè¶³å¤Ÿ |
| **Person_reID** | 60 | [30, 120] | å¯¹æ•°: [30, 120] | æ£€ç´¢ä»»åŠ¡ï¼Œéœ€è¾ƒå¤šepoch |
| **examples** | 10 | [5, 20] | ç¦»æ•£: {5, 8, 10, 15, 20} | MNISTç®€å•ä»»åŠ¡ |

#### åŸå› å’Œå‚è€ƒ

**ç†è®ºä¾æ®**:
1. **èƒ½è€—å½±å“**: epochsä¸èƒ½è€—æˆçº¿æ€§å…³ç³»ï¼Œæ˜¯æœ€ç›´æ¥çš„èƒ½è€—æ§åˆ¶å› ç´ 
2. **æ€§èƒ½æƒè¡¡**: è¿‡å°‘epochå¯¼è‡´æ¬ æ‹Ÿåˆï¼Œè¿‡å¤šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œèƒ½è€—æµªè´¹
3. **0.5-2.0å€èŒƒå›´**:
   - ä¸‹ç•Œ0.5å€ï¼šä¿è¯åŸºæœ¬æ”¶æ•›ï¼ˆå‚è€ƒï¼šDeep Learning, Goodfellow et al., 2016ï¼‰
   - ä¸Šç•Œ2.0å€ï¼šæ¢ç´¢è¿‡æ‹Ÿåˆè¾¹ç•Œå’Œèƒ½è€—ä¸Šé™

**å‚è€ƒæ–‡çŒ®**:
- Bengio, Y. (2012). "Practical recommendations for gradient-based training of deep architectures"
- Smith, L. N. (2018). "A disciplined approach to neural network hyper-parameters"

---

### 2. Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰

#### æ¨èå˜å¼‚èŒƒå›´

**è¡¨è¾¾å¼**: `[default * 0.1, default * 10.0]`

**å˜å¼‚æ–¹å¼**: **å¯¹æ•°å‡åŒ€åˆ†å¸ƒ** (Log-Uniform Distribution) â­ **å¼ºçƒˆæ¨è**

**å…·ä½“å®ç°**:
```python
def mutate_learning_rate(default):
    """
    å¯¹æ•°å‡åŒ€åˆ†å¸ƒé‡‡æ ·å­¦ä¹ ç‡

    å­¦ä¹ ç‡åœ¨å¯¹æ•°ç©ºé—´å‡åŒ€åˆ†å¸ƒï¼Œåœ¨åŸå§‹ç©ºé—´å‘ˆç°æŒ‡æ•°åˆ†å¸ƒ
    è¿™æ˜¯å› ä¸ºå­¦ä¹ ç‡çš„å½±å“æ˜¯æŒ‡æ•°çº§çš„

    Args:
        default: é»˜è®¤å­¦ä¹ ç‡

    Returns:
        å˜å¼‚åçš„å­¦ä¹ ç‡
    """
    lower = default * 0.1
    upper = default * 10.0

    # å¯¹æ•°ç©ºé—´å‡åŒ€é‡‡æ ·
    log_lower = np.log10(lower)
    log_upper = np.log10(upper)
    log_value = np.random.uniform(log_lower, log_upper)

    return 10 ** log_value
```

**åˆ†æ¡£å»ºè®®**ï¼ˆç½‘æ ¼æœç´¢ï¼‰:
```python
def mutate_learning_rate_grid(default):
    """
    ç½‘æ ¼æœç´¢å¼é‡‡æ ·

    è¿”å›: default * {0.1, 0.3, 1.0, 3.0, 10.0} ä¹‹ä¸€
    """
    factors = [0.1, 0.3, 1.0, 3.0, 10.0]
    factor = np.random.choice(factors)
    return default * factor
```

#### å„æ¨¡å‹å…·ä½“å»ºè®®

| æ¨¡å‹ç»„ | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | å˜å¼‚æ–¹å¼ | è¯´æ˜ |
|--------|--------|---------|---------|------|
| **MRT-OAST** | 1e-4 | [1e-5, 1e-3] | å¯¹æ•°å‡åŒ€ | Adamä¼˜åŒ–å™¨ï¼Œè¾ƒå°å­¦ä¹ ç‡ |
| **pytorch_resnet** | 0.1 | [0.01, 1.0] | å¯¹æ•°å‡åŒ€ | SGD+Momentumï¼Œå¯ç”¨è¾ƒå¤§å­¦ä¹ ç‡ |
| **VulBERTa** | 3e-5 | [3e-6, 3e-4] | å¯¹æ•°å‡åŒ€ | BERTå¾®è°ƒï¼Œæå°å­¦ä¹ ç‡ |
| **Person_reID** | 0.05 | [0.005, 0.5] | å¯¹æ•°å‡åŒ€ | ç‰¹å¾æå–ä»»åŠ¡ |
| **examples** | 0.01 | [0.001, 0.1] | å¯¹æ•°å‡åŒ€ | MNISTç®€å•ä»»åŠ¡ |
| **bug-localization** | 1e-5 (alpha) | [1e-6, 1e-4] | å¯¹æ•°å‡åŒ€ | æ­£åˆ™åŒ–å‚æ•° |

#### åŸå› å’Œå‚è€ƒ

**ç†è®ºä¾æ®**:
1. **æŒ‡æ•°æ•æ„Ÿæ€§**: å­¦ä¹ ç‡å¯¹è®­ç»ƒçš„å½±å“æ˜¯æŒ‡æ•°çº§çš„ï¼ˆç›¸å·®10å€å¯èƒ½å¯¼è‡´å¤©å£¤ä¹‹åˆ«ï¼‰
2. **å¯¹æ•°é‡‡æ ·**: åœ¨å¯¹æ•°ç©ºé—´å‡åŒ€é‡‡æ ·ç¡®ä¿å°å€¼å’Œå¤§å€¼éƒ½æœ‰å……åˆ†æ¢ç´¢
3. **0.1-10å€èŒƒå›´**:
   - ä¸‹ç•Œ0.1å€ï¼šé¿å…æ”¶æ•›è¿‡æ…¢å¯¼è‡´æ¬ æ‹Ÿåˆ
   - ä¸Šç•Œ10å€ï¼šé¿å…æ¢¯åº¦çˆ†ç‚¸å’Œä¸æ”¶æ•›

**èƒ½è€—å½±å“**:
- è¿‡å°å­¦ä¹ ç‡ â†’ æ”¶æ•›æ…¢ â†’ éœ€è¦æ›´å¤šepoch â†’ èƒ½è€—å¢åŠ 
- è¿‡å¤§å­¦ä¹ ç‡ â†’ ä¸æ”¶æ•›/éœ‡è¡ â†’ æ— æ•ˆè®¡ç®— â†’ èƒ½è€—æµªè´¹
- æœ€ä¼˜å­¦ä¹ ç‡ â†’ å¿«é€Ÿæ”¶æ•› â†’ èƒ½è€—æœ€ä½

**å‚è€ƒæ–‡çŒ®**:
- Bengio, Y. (2012). "Practical recommendations for gradient-based training"
- Smith, L. N. (2017). "Cyclical Learning Rates for Training Neural Networks"
- You, Y., et al. (2017). "Large Batch Training of Convolutional Networks"

---

### 3. Weight Decayï¼ˆæƒé‡è¡°å‡ / L2æ­£åˆ™åŒ–ï¼‰

#### æ¨èå˜å¼‚èŒƒå›´

**è¡¨è¾¾å¼**: `[0.0, default * 100.0]`

**å˜å¼‚æ–¹å¼**: **å¯¹æ•°å‡åŒ€åˆ†å¸ƒ**ï¼ˆéé›¶æƒ…å†µï¼‰

**å…·ä½“å®ç°**:
```python
def mutate_weight_decay(default):
    """
    æƒé‡è¡°å‡å˜å¼‚

    å…è®¸0å€¼ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰+ å¯¹æ•°å‡åŒ€åˆ†å¸ƒ

    Args:
        default: é»˜è®¤weight decayå€¼

    Returns:
        å˜å¼‚åçš„weight decayå€¼
    """
    # 30% æ¦‚ç‡é‡‡æ ·ä¸º0ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
    if np.random.random() < 0.3:
        return 0.0

    # 70% æ¦‚ç‡åœ¨å¯¹æ•°ç©ºé—´é‡‡æ ·
    if default == 0.0:
        # å¦‚æœé»˜è®¤å€¼ä¸º0ï¼Œä½¿ç”¨å…¸å‹èŒƒå›´
        lower = 1e-6
        upper = 1e-2
    else:
        lower = default * 0.1
        upper = default * 100.0

    log_lower = np.log10(lower)
    log_upper = np.log10(upper)
    log_value = np.random.uniform(log_lower, log_upper)

    return 10 ** log_value
```

**åˆ†æ¡£å»ºè®®**:
```python
def mutate_weight_decay_discrete(default):
    """
    ç¦»æ•£é‡‡æ ·

    è¿”å›: {0.0, default*0.1, default, default*10, default*100} ä¹‹ä¸€
    """
    if default == 0.0:
        # é»˜è®¤æ— æ­£åˆ™åŒ–æ—¶çš„å€™é€‰å€¼
        candidates = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]
    else:
        candidates = [0.0, default*0.1, default, default*10, default*100]

    return np.random.choice(candidates)
```

#### å„æ¨¡å‹å…·ä½“å»ºè®®

| æ¨¡å‹ç»„ | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | å˜å¼‚æ–¹å¼ | è¯´æ˜ |
|--------|--------|---------|---------|------|
| **MRT-OAST** | 0.0 | [0.0, 1e-2] | æ··åˆ: 30%é›¶ + 70%å¯¹æ•° | å¯èƒ½ä¸éœ€è¦æ­£åˆ™åŒ– |
| **pytorch_resnet** | 1e-4 | [0.0, 1e-2] | æ··åˆ: 20%é›¶ + 80%å¯¹æ•° | CNNå¸¸ç”¨1e-4 |
| **VulBERTa** | 0.0 | [0.0, 1e-2] | æ··åˆ: 40%é›¶ + 60%å¯¹æ•° | é¢„è®­ç»ƒæ¨¡å‹å¯èƒ½ä¸éœ€è¦ |

#### åŸå› å’Œå‚è€ƒ

**ç†è®ºä¾æ®**:
1. **æ­£åˆ™åŒ–ä½œç”¨**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›
2. **å¯¹æ•°æ•æ„Ÿæ€§**: ç±»ä¼¼å­¦ä¹ ç‡ï¼Œå½±å“æ˜¯æŒ‡æ•°çº§çš„
3. **åŒ…å«é›¶å€¼**: å…è®¸æ¢ç´¢"æ— æ­£åˆ™åŒ–"çš„æƒ…å†µ

**èƒ½è€—å½±å“**:
- é€‚åº¦æ­£åˆ™åŒ– â†’ å‡å°‘è¿‡æ‹Ÿåˆ â†’ å¯èƒ½éœ€è¦æ›´å°‘epoch â†’ èƒ½è€—é™ä½
- è¿‡å¼ºæ­£åˆ™åŒ– â†’ æ¬ æ‹Ÿåˆ â†’ éœ€è¦æ›´å¤šepoch/è°ƒæ•´ â†’ èƒ½è€—å¢åŠ 

**å‚è€ƒæ–‡çŒ®**:
- Krogh, A., & Hertz, J. A. (1992). "A simple weight decay can improve generalization"
- Zhang, C., et al. (2018). "Three mechanisms of weight decay regularization"

---

### 4. Dropoutï¼ˆä¸¢å¼ƒç‡ï¼‰

#### æ¨èå˜å¼‚èŒƒå›´

**è¡¨è¾¾å¼**: `[0.0, 0.7]`

**å˜å¼‚æ–¹å¼**: **å‡åŒ€åˆ†å¸ƒ**ï¼ˆçº¿æ€§ç©ºé—´ï¼‰

**å…·ä½“å®ç°**:
```python
def mutate_dropout(default):
    """
    Dropoutç‡å˜å¼‚

    åœ¨[0.0, 0.7]èŒƒå›´å†…å‡åŒ€é‡‡æ ·

    Args:
        default: é»˜è®¤dropoutç‡

    Returns:
        å˜å¼‚åçš„dropoutç‡
    """
    # åœ¨åˆç†èŒƒå›´å†…å‡åŒ€é‡‡æ ·
    lower = 0.0
    upper = 0.7

    return np.random.uniform(lower, upper)
```

**åˆ†æ¡£å»ºè®®**:
```python
def mutate_dropout_discrete(default):
    """
    ç¦»æ•£é‡‡æ ·å¸¸ç”¨dropoutç‡

    è¿”å›: {0.0, 0.1, 0.2, 0.3, 0.5, 0.7} ä¹‹ä¸€
    """
    candidates = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7]
    return np.random.choice(candidates)
```

#### å„æ¨¡å‹å…·ä½“å»ºè®®

| æ¨¡å‹ç»„ | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | å˜å¼‚æ–¹å¼ | è¯´æ˜ |
|--------|--------|---------|---------|------|
| **MRT-OAST** | 0.2 | [0.0, 0.6] | å‡åŒ€åˆ†å¸ƒ | å…¨è¿æ¥å±‚dropout |
| **Person_reID** | 0.5 | [0.0, 0.7] | å‡åŒ€åˆ†å¸ƒ | ç‰¹å¾æå–ï¼Œå¯ç”¨è¾ƒå¤§dropout |

#### åŸå› å’Œå‚è€ƒ

**ç†è®ºä¾æ®**:
1. **ä¸Šç•Œ0.7**: è¶…è¿‡0.7ä¼šä¸¥é‡å½±å“ä¿¡æ¯æµåŠ¨ï¼ˆSrivastava et al., 2014ï¼‰
2. **ä¸‹ç•Œ0.0**: å…è®¸æ— dropoutï¼Œæ¢ç´¢æ˜¯å¦å¿…è¦
3. **å‡åŒ€åˆ†å¸ƒ**: Dropoutåœ¨çº¿æ€§ç©ºé—´çš„å½±å“è¾ƒä¸ºå‡åŒ€

**èƒ½è€—å½±å“**:
- Dropout â†’ å‡å°‘è¿‡æ‹Ÿåˆ â†’ å¯èƒ½æ›´å¿«æ”¶æ•› â†’ èƒ½è€—é™ä½
- ä½†Dropout â†’ è®­ç»ƒæ—¶é¢å¤–è®¡ç®— â†’ å•epochèƒ½è€—ç•¥å¢
- æ€»ä½“å½±å“ï¼šä»¥è®­ç»ƒæ•ˆç‡ä¸ºä¸»

**å‚è€ƒæ–‡çŒ®**:
- Srivastava, N., et al. (2014). "Dropout: A simple way to prevent neural networks from overfitting"
- Gal, Y., & Ghahramani, Z. (2016). "A theoretically grounded application of dropout"

---

### 5. Seedï¼ˆéšæœºç§å­ï¼‰

#### æ¨èå˜å¼‚èŒƒå›´

**è¡¨è¾¾å¼**: `[0, 9999]`

**å˜å¼‚æ–¹å¼**: **å‡åŒ€åˆ†å¸ƒ**ï¼ˆæ•´æ•°ï¼‰

**å…·ä½“å®ç°**:
```python
def mutate_seed():
    """
    éšæœºç§å­å˜å¼‚

    åœ¨[0, 9999]èŒƒå›´å†…å‡åŒ€é‡‡æ ·

    Returns:
        éšæœºç”Ÿæˆçš„ç§å­å€¼
    """
    return np.random.randint(0, 10000)
```

**è¯´æ˜**: Seedä¸å½±å“èƒ½è€—ï¼Œä½†å½±å“æ€§èƒ½æ–¹å·®ï¼Œç”¨äºè¯„ä¼°ç»“æœç¨³å®šæ€§ã€‚

#### åŸå› å’Œå‚è€ƒ

**ç†è®ºä¾æ®**:
1. **èŒƒå›´å……åˆ†**: 10000ä¸ªä¸åŒç§å­è¶³å¤Ÿè¯„ä¼°æ–¹å·®
2. **å‡åŒ€é‡‡æ ·**: æ— ååœ°æ¢ç´¢ä¸åŒåˆå§‹åŒ–

**å®éªŒè®¾è®¡**:
- å›ºå®šå…¶ä»–å‚æ•°ï¼Œå˜å¼‚seed â†’ è¯„ä¼°æ€§èƒ½æ–¹å·®
- å˜å¼‚å…¶ä»–å‚æ•°ï¼Œå›ºå®šseed â†’ è¯„ä¼°èƒ½è€—å½±å“

**å‚è€ƒæ–‡çŒ®**:
- Henderson, P., et al. (2018). "Deep reinforcement learning that matters"

---

### 6. ç‰¹æ®Šå‚æ•°

#### 6.1 max_iterï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•° - bug-localizationï¼‰

**æ¨èèŒƒå›´**: `[default * 0.5, default * 2.0]`
**å˜å¼‚æ–¹å¼**: å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
**é»˜è®¤å€¼**: 10000
**æ¨è**: [5000, 20000]

**åŸå› **: ç±»ä¼¼epochsï¼Œæ§åˆ¶è®­ç»ƒæ—¶é•¿å’Œèƒ½è€—ã€‚

#### 6.2 kfoldï¼ˆäº¤å‰éªŒè¯æŠ˜æ•° - bug-localizationï¼‰

**æ¨èèŒƒå›´**: `[2, 10]`
**å˜å¼‚æ–¹å¼**: å‡åŒ€æ•´æ•°åˆ†å¸ƒ
**é»˜è®¤å€¼**: 10
**æ¨è**: {2, 5, 10}ï¼ˆç¦»æ•£ï¼‰

**åŸå› **:
- å½±å“è®­ç»ƒæ—¶é—´ï¼ˆkfoldå€ï¼‰
- è¿‡å°(<3)ï¼šç»Ÿè®¡ä¸å¯é 
- è¿‡å¤§(>10)ï¼šè®¡ç®—æˆæœ¬é«˜ï¼Œæ”¶ç›Šé€’å‡

#### 6.3 alphaï¼ˆæ­£åˆ™åŒ–å‚æ•° - bug-localizationï¼‰

**æ¨èèŒƒå›´**: `[default * 0.1, default * 10.0]`
**å˜å¼‚æ–¹å¼**: å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
**é»˜è®¤å€¼**: 1e-5
**æ¨è**: [1e-6, 1e-4]

**åŸå› **: æœ¬è´¨æ˜¯å­¦ä¹ ç‡/æ­£åˆ™åŒ–å‚æ•°ï¼Œéµå¾ªå¯¹æ•°è§„å¾‹ã€‚

---

## ğŸ”¬ å˜å¼‚ç­–ç•¥å»ºè®®

### ç­–ç•¥1: å•å‚æ•°å˜å¼‚ï¼ˆæ¨èç”¨äºèƒ½è€—ç ”ç©¶ï¼‰

**ç›®çš„**: éš”ç¦»å•ä¸ªå‚æ•°å¯¹èƒ½è€—çš„å½±å“

**æ–¹æ³•**:
```python
# å›ºå®šå…¶ä»–å‚æ•°ï¼Œåªå˜å¼‚epochs
experiments = [
    {"epochs": 5, "lr": 0.1, "seed": 42},
    {"epochs": 10, "lr": 0.1, "seed": 42},
    {"epochs": 20, "lr": 0.1, "seed": 42},
]
```

**é€‚ç”¨**: ç†è§£å•ä¸ªè¶…å‚æ•°-èƒ½è€—å…³ç³»

---

### ç­–ç•¥2: é…å¯¹å˜å¼‚

**ç›®çš„**: æ¢ç´¢å‚æ•°é—´äº¤äº’ä½œç”¨

**æ–¹æ³•**:
```python
# epochså’Œlearning_rateé…å¯¹å˜å¼‚
experiments = [
    {"epochs": 10, "lr": 0.01},   # å°‘epochï¼Œå°lr
    {"epochs": 10, "lr": 0.1},    # å°‘epochï¼Œå¤§lr
    {"epochs": 100, "lr": 0.01},  # å¤šepochï¼Œå°lr
    {"epochs": 100, "lr": 0.1},   # å¤šepochï¼Œå¤§lr
]
```

**é€‚ç”¨**: ç ”ç©¶è¶…å‚æ•°äº¤äº’æ•ˆåº”

---

### ç­–ç•¥3: éšæœºå…¨å˜å¼‚

**ç›®çš„**: æ¢ç´¢å…¨å±€ç©ºé—´ï¼Œå‘ç°æ„å¤–æ¨¡å¼

**æ–¹æ³•**:
```python
# æ‰€æœ‰å‚æ•°åŒæ—¶éšæœºå˜å¼‚
for _ in range(n_experiments):
    experiment = {
        "epochs": mutate_epochs(default_epochs),
        "lr": mutate_learning_rate(default_lr),
        "weight_decay": mutate_weight_decay(default_wd),
        "seed": mutate_seed()
    }
```

**é€‚ç”¨**: å¤§è§„æ¨¡æ¢ç´¢æ€§ç ”ç©¶

---

## ğŸ“Š å®æ–½ä¼˜å…ˆçº§

### é«˜ä¼˜å…ˆçº§ï¼ˆå¼ºçƒˆæ¨èå˜å¼‚ï¼‰

1. **Epochs** - ç›´æ¥å½±å“èƒ½è€—ï¼ˆçº¿æ€§å…³ç³»ï¼‰
2. **Learning Rate** - å½±å“æ”¶æ•›é€Ÿåº¦å’Œèƒ½è€—æ•ˆç‡

### ä¸­ä¼˜å…ˆçº§

3. **Weight Decay** - å½±å“æ³›åŒ–å’Œè®­ç»ƒæ•ˆç‡
4. **Dropout** - å½±å“è®­ç»ƒæ—¶è®¡ç®—é‡

### ä½ä¼˜å…ˆçº§

5. **Seed** - ä¸»è¦ç”¨äºè¯„ä¼°æ–¹å·®ï¼Œä¸ç›´æ¥å½±å“èƒ½è€—

---

## ğŸ¯ æ¨¡å‹ç‰¹å®šå»ºè®®

### ResNet (CIFAR-10)

**é‡ç‚¹å˜å¼‚**: epochs, learning_rate
**æ¬¡è¦å˜å¼‚**: weight_decay
**åŸå› **: CNNè®­ç»ƒæ—¶é—´é•¿ï¼Œepochså’Œlrå½±å“æœ€å¤§

**æ¨èå®éªŒ**:
```python
{
    "epochs": [100, 150, 200, 300],  # ç¦»æ•£
    "learning_rate": log_uniform(0.01, 1.0),
    "weight_decay": [0.0, 1e-4, 1e-3],  # ç¦»æ•£
}
```

---

### VulBERTa (é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ)

**é‡ç‚¹å˜å¼‚**: learning_rate, epochs
**æ¬¡è¦å˜å¼‚**: weight_decay
**åŸå› **: é¢„è®­ç»ƒæ¨¡å‹å¯¹å­¦ä¹ ç‡ææ•æ„Ÿ

**æ¨èå®éªŒ**:
```python
{
    "learning_rate": log_uniform(1e-6, 1e-3),  # é‡ç‚¹
    "epochs": [5, 10, 15, 20],
    "weight_decay": [0.0, 1e-4, 1e-3],
}
```

---

### Person ReID

**é‡ç‚¹å˜å¼‚**: epochs, learning_rate, dropout
**æ¬¡è¦å˜å¼‚**: æ— 
**åŸå› **: æ£€ç´¢ä»»åŠ¡éœ€è¦å……åˆ†è®­ç»ƒï¼Œdropoutå½±å“ç‰¹å¾è´¨é‡

**æ¨èå®éªŒ**:
```python
{
    "epochs": log_uniform(30, 120),
    "learning_rate": log_uniform(0.005, 0.5),
    "dropout": uniform(0.0, 0.7),
}
```

---

## ğŸ“š å®Œæ•´é…ç½®å»ºè®®

### é…ç½®æ–‡ä»¶æ ¼å¼ï¼ˆæ–°å¢å­—æ®µï¼‰

```json
{
  "models": {
    "pytorch_resnet_cifar10": {
      "supported_hyperparams": {
        "epochs": {
          "flag": "-e",
          "type": "int",
          "default": 200,
          "range": [100, 400],
          "mutation_strategy": {
            "distribution": "log_uniform",
            "min_factor": 0.5,
            "max_factor": 2.0,
            "discrete_values": [100, 150, 200, 300, 400],
            "description": "å¯¹æ•°å‡åŒ€åˆ†å¸ƒæˆ–ç¦»æ•£é‡‡æ ·"
          }
        },
        "learning_rate": {
          "flag": "--lr",
          "type": "float",
          "default": 0.1,
          "range": [0.01, 1.0],
          "mutation_strategy": {
            "distribution": "log_uniform",
            "min_factor": 0.1,
            "max_factor": 10.0,
            "discrete_values": [0.01, 0.03, 0.1, 0.3, 1.0],
            "description": "å¯¹æ•°ç©ºé—´å‡åŒ€åˆ†å¸ƒï¼Œå­¦ä¹ ç‡æ•æ„Ÿ"
          }
        }
      }
    }
  }
}
```

---

## ğŸ” å‚è€ƒæ–‡çŒ®

### ç»¼åˆæ€§æ–‡çŒ®
1. Bengio, Y. (2012). "Practical recommendations for gradient-based training of deep architectures"
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). "Deep Learning" - Chapter 11: Practical Methodology

### å­¦ä¹ ç‡
3. Smith, L. N. (2017). "Cyclical Learning Rates for Training Neural Networks"
4. You, Y., et al. (2017). "Large Batch Training of Convolutional Networks"

### æ­£åˆ™åŒ–
5. Srivastava, N., et al. (2014). "Dropout: A simple way to prevent neural networks from overfitting"
6. Krogh, A., & Hertz, J. A. (1992). "A simple weight decay can improve generalization"

### å®éªŒè®¾è®¡
7. Henderson, P., et al. (2018). "Deep reinforcement learning that matters"
8. Liashchynskyi, P., & Liashchynskyi, P. (2019). "Grid search, random search, genetic algorithm"

---

**ç»´æŠ¤è€…**: Green
**é¡¹ç›®**: Mutation-Based Training Energy Profiler
**æœ€åæ›´æ–°**: 2025-11-09
