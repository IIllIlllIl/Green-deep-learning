#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡åˆ†æè„šæœ¬

åŠŸèƒ½:
1. åŠ è½½stage2_mediators.csv
2. å…¨é¢åˆ†ææ•°æ®è´¨é‡:
   - å®Œæ•´æ€§åˆ†æï¼ˆç¼ºå¤±å€¼ç»Ÿè®¡ï¼‰
   - åˆ†å¸ƒåˆ†æï¼ˆæ•°å€¼å˜é‡ï¼‰
   - å¼‚å¸¸å€¼æ£€æµ‹
   - å˜é‡é—´ç›¸å…³æ€§
   - åˆ†å±‚æ•°æ®è´¨é‡ï¼ˆæŒ‰repositoryã€modeï¼‰
   - å› æœåˆ†æé€‚ç”¨æ€§è¯„ä¼°
3. ç”Ÿæˆè¯¦ç»†çš„è´¨é‡åˆ†ææŠ¥å‘Š

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
PROCESSED_DIR = DATA_DIR / "processed"
INPUT_FILE = PROCESSED_DIR / "stage2_mediators.csv"
REPORT_FILE = PROCESSED_DIR / "data_quality_report.txt"


def load_data(filepath):
    """åŠ è½½CSVæ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {filepath}")
    df = pd.read_csv(filepath)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    return df


def analyze_completeness(df):
    """åˆ†ææ•°æ®å®Œæ•´æ€§"""
    print("\n" + "="*80)
    print("1. æ•°æ®å®Œæ•´æ€§åˆ†æ")
    print("="*80)

    # æŒ‰åˆ—ç±»å‹åˆ†ç»„
    column_groups = {
        'å…ƒä¿¡æ¯': [c for c in df.columns if c in ['experiment_id', 'timestamp', 'repository', 'model', 'mode', 'is_parallel']],
        'è¶…å‚æ•°': [c for c in df.columns if c.startswith('hyperparam_')],
        'è¶…å‚æ•°ç»Ÿä¸€': ['training_duration', 'l2_regularization'],
        'èƒ½è€—åŸå§‹': [c for c in df.columns if c.startswith('energy_')],
        'èƒ½è€—ä¸­ä»‹': ['gpu_util_avg', 'gpu_temp_max', 'cpu_pkg_ratio', 'gpu_power_fluctuation', 'gpu_temp_fluctuation'],
        'æ€§èƒ½': [c for c in df.columns if c.startswith('perf_')],
        'åå°ä»»åŠ¡': [c for c in df.columns if c.startswith('bg_')]
    }

    results = {}

    for group_name, cols in column_groups.items():
        cols = [c for c in cols if c in df.columns]
        if not cols:
            continue

        print(f"\n{group_name} ({len(cols)} åˆ—):")

        group_results = []
        for col in cols:
            filled = df[col].notna().sum()
            fill_rate = (filled / len(df)) * 100
            group_results.append({
                'column': col,
                'filled': filled,
                'fill_rate': fill_rate
            })

            if fill_rate < 50:
                status = "âŒ"
            elif fill_rate < 80:
                status = "âš ï¸ "
            else:
                status = "âœ…"

            print(f"  {status} {col:40s}: {filled:4d}/{len(df)} ({fill_rate:5.1f}%)")

        results[group_name] = group_results

    return results


def analyze_distributions(df):
    """åˆ†ææ•°å€¼å˜é‡åˆ†å¸ƒ"""
    print("\n" + "="*80)
    print("2. æ•°å€¼å˜é‡åˆ†å¸ƒåˆ†æ")
    print("="*80)

    # é‡ç‚¹åˆ†æçš„å˜é‡
    key_vars = {
        'è¶…å‚æ•°ç»Ÿä¸€': ['training_duration', 'l2_regularization'],
        'èƒ½è€—ä¸­ä»‹': ['gpu_util_avg', 'gpu_temp_max', 'cpu_pkg_ratio',
                     'gpu_power_fluctuation', 'gpu_temp_fluctuation'],
        'è¶…å‚æ•°': ['hyperparam_learning_rate', 'hyperparam_batch_size',
                   'hyperparam_epochs', 'hyperparam_dropout'],
        'èƒ½è€—': ['energy_cpu_total_joules', 'energy_gpu_total_joules']
    }

    results = {}

    for group_name, cols in key_vars.items():
        cols = [c for c in cols if c in df.columns]
        if not cols:
            continue

        print(f"\n{group_name}:")

        group_results = []
        for col in cols:
            data = df[col].dropna()
            if len(data) == 0:
                continue

            stats = {
                'column': col,
                'count': len(data),
                'mean': data.mean(),
                'std': data.std(),
                'min': data.min(),
                'q25': data.quantile(0.25),
                'median': data.median(),
                'q75': data.quantile(0.75),
                'max': data.max(),
                'unique': data.nunique()
            }

            group_results.append(stats)

            print(f"\n  {col}:")
            print(f"    N={stats['count']}, å”¯ä¸€å€¼={stats['unique']}")
            print(f"    å‡å€¼={stats['mean']:.4f}, æ ‡å‡†å·®={stats['std']:.4f}")
            print(f"    èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]")
            print(f"    å››åˆ†ä½: Q25={stats['q25']:.4f}, ä¸­ä½æ•°={stats['median']:.4f}, Q75={stats['q75']:.4f}")

        results[group_name] = group_results

    return results


def detect_outliers(df):
    """æ£€æµ‹å¼‚å¸¸å€¼"""
    print("\n" + "="*80)
    print("3. å¼‚å¸¸å€¼æ£€æµ‹")
    print("="*80)

    # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    key_vars = ['training_duration', 'l2_regularization', 'gpu_util_avg',
                'gpu_temp_max', 'cpu_pkg_ratio', 'gpu_power_fluctuation',
                'gpu_temp_fluctuation', 'energy_cpu_total_joules', 'energy_gpu_total_joules']

    key_vars = [c for c in key_vars if c in df.columns]

    results = {}

    for col in key_vars:
        data = df[col].dropna()
        if len(data) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®
            continue

        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = ((data < lower_bound) | (data > upper_bound)).sum()
        outlier_rate = (outliers / len(data)) * 100

        results[col] = {
            'outliers': outliers,
            'outlier_rate': outlier_rate,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }

        if outlier_rate > 10:
            status = "âš ï¸ "
        elif outlier_rate > 5:
            status = "â„¹ï¸ "
        else:
            status = "âœ…"

        print(f"{status} {col:30s}: {outliers:3d} å¼‚å¸¸å€¼ ({outlier_rate:4.1f}%)")
        if outliers > 0:
            print(f"   æ­£å¸¸èŒƒå›´: [{lower_bound:.4f}, {upper_bound:.4f}]")

    return results


def analyze_correlations(df):
    """åˆ†æå˜é‡é—´ç›¸å…³æ€§"""
    print("\n" + "="*80)
    print("4. å˜é‡é—´ç›¸å…³æ€§åˆ†æ")
    print("="*80)

    # é€‰æ‹©å…³é”®å˜é‡
    key_vars = [
        'training_duration', 'l2_regularization',
        'gpu_util_avg', 'gpu_temp_max', 'cpu_pkg_ratio',
        'gpu_power_fluctuation', 'gpu_temp_fluctuation',
        'energy_cpu_total_joules', 'energy_gpu_total_joules'
    ]

    key_vars = [c for c in key_vars if c in df.columns]

    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    corr_df = df[key_vars].corr()

    print("\né«˜ç›¸å…³æ€§å˜é‡å¯¹ (|r| > 0.7):")

    high_corr = []
    for i in range(len(corr_df.columns)):
        for j in range(i+1, len(corr_df.columns)):
            corr_val = corr_df.iloc[i, j]
            if abs(corr_val) > 0.7:
                var1 = corr_df.columns[i]
                var2 = corr_df.columns[j]
                high_corr.append({
                    'var1': var1,
                    'var2': var2,
                    'correlation': corr_val
                })
                print(f"  {var1:30s} <-> {var2:30s}: r={corr_val:6.3f}")

    if not high_corr:
        print("  âœ… æ— é«˜åº¦ç›¸å…³çš„å˜é‡å¯¹ï¼ˆè‰¯å¥½ï¼Œé¿å…å¤šé‡å…±çº¿æ€§ï¼‰")

    return high_corr, corr_df


def analyze_by_repository(df):
    """æŒ‰repositoryåˆ†ææ•°æ®è´¨é‡"""
    print("\n" + "="*80)
    print("5. åˆ†repositoryæ•°æ®è´¨é‡")
    print("="*80)

    if 'repository' not in df.columns:
        print("âš ï¸  repositoryåˆ—ä¸å­˜åœ¨")
        return None

    repos = df['repository'].value_counts()

    print(f"\nRepositoryåˆ†å¸ƒ:")
    for repo, count in repos.items():
        print(f"  {repo:40s}: {count:4d} ({count/len(df)*100:5.1f}%)")

    # æ£€æŸ¥æ¯ä¸ªrepositoryçš„æ•°æ®å®Œæ•´æ€§
    print(f"\nå„repositoryæ•°æ®å®Œæ•´æ€§:")

    key_vars = ['energy_cpu_total_joules', 'energy_gpu_total_joules',
                'training_duration', 'gpu_util_avg']
    key_vars = [c for c in key_vars if c in df.columns]

    results = {}

    for repo in repos.index:
        repo_df = df[df['repository'] == repo]
        print(f"\n  {repo}:")

        repo_results = {}
        for var in key_vars:
            filled = repo_df[var].notna().sum()
            fill_rate = (filled / len(repo_df)) * 100
            repo_results[var] = fill_rate

            if fill_rate < 70:
                status = "âš ï¸ "
            else:
                status = "âœ…"

            print(f"    {status} {var:30s}: {fill_rate:5.1f}%")

        results[repo] = repo_results

    return results


def analyze_by_mode(df):
    """æŒ‰modeåˆ†ææ•°æ®è´¨é‡"""
    print("\n" + "="*80)
    print("6. åˆ†modeæ•°æ®è´¨é‡")
    print("="*80)

    if 'is_parallel' not in df.columns:
        print("âš ï¸  is_parallelåˆ—ä¸å­˜åœ¨")
        return None

    parallel_count = (df['is_parallel'] == True).sum()
    nonparallel_count = (df['is_parallel'] == False).sum()

    print(f"\nModeåˆ†å¸ƒ:")
    print(f"  å¹¶è¡Œæ¨¡å¼: {parallel_count:4d} ({parallel_count/len(df)*100:5.1f}%)")
    print(f"  éå¹¶è¡Œæ¨¡å¼: {nonparallel_count:4d} ({nonparallel_count/len(df)*100:5.1f}%)")

    # æ£€æŸ¥ä¸¤ç§æ¨¡å¼çš„æ•°æ®å®Œæ•´æ€§
    key_vars = ['energy_cpu_total_joules', 'energy_gpu_total_joules',
                'training_duration', 'gpu_util_avg']
    key_vars = [c for c in key_vars if c in df.columns]

    print(f"\nå¹¶è¡Œ vs éå¹¶è¡Œæ•°æ®å®Œæ•´æ€§å¯¹æ¯”:")

    results = {}

    for var in key_vars:
        parallel_fill = df[df['is_parallel'] == True][var].notna().sum()
        parallel_rate = (parallel_fill / parallel_count) * 100 if parallel_count > 0 else 0

        nonparallel_fill = df[df['is_parallel'] == False][var].notna().sum()
        nonparallel_rate = (nonparallel_fill / nonparallel_count) * 100 if nonparallel_count > 0 else 0

        results[var] = {
            'parallel_rate': parallel_rate,
            'nonparallel_rate': nonparallel_rate
        }

        print(f"\n  {var}:")
        print(f"    å¹¶è¡Œ:   {parallel_rate:5.1f}%")
        print(f"    éå¹¶è¡Œ: {nonparallel_rate:5.1f}%")

    return results


def assess_causal_readiness(df):
    """è¯„ä¼°å› æœåˆ†æé€‚ç”¨æ€§"""
    print("\n" + "="*80)
    print("7. å› æœåˆ†æé€‚ç”¨æ€§è¯„ä¼°")
    print("="*80)

    issues = []

    # 1. æ ·æœ¬é‡æ£€æŸ¥
    print("\n1. æ ·æœ¬é‡æ£€æŸ¥:")
    total_samples = len(df)
    print(f"   æ€»æ ·æœ¬: {total_samples}")

    if total_samples < 50:
        issues.append("âŒ æ€»æ ·æœ¬é‡ < 50ï¼ˆDiBSæœ€ä½è¦æ±‚ï¼‰")
        print("   âŒ æ ·æœ¬é‡ä¸è¶³ï¼ˆDiBSéœ€è¦è‡³å°‘50ä¸ªæ ·æœ¬ï¼‰")
    elif total_samples < 100:
        issues.append("âš ï¸  æ€»æ ·æœ¬é‡ < 100ï¼ˆå»ºè®®è‡³å°‘100ä¸ªï¼‰")
        print("   âš ï¸  æ ·æœ¬é‡åå°‘ï¼ˆå»ºè®®è‡³å°‘100ä¸ªæ ·æœ¬ï¼‰")
    else:
        print("   âœ… æ ·æœ¬é‡å……è¶³")

    # 2. å˜é‡å®Œæ•´æ€§æ£€æŸ¥
    print("\n2. å…³é”®å˜é‡å®Œæ•´æ€§:")

    key_vars_groups = {
        'è¾“å…¥å˜é‡ï¼ˆè¶…å‚æ•°ï¼‰': ['training_duration', 'l2_regularization',
                            'hyperparam_learning_rate', 'hyperparam_batch_size'],
        'ä¸­ä»‹å˜é‡': ['gpu_util_avg', 'gpu_temp_max', 'cpu_pkg_ratio'],
        'è¾“å‡ºå˜é‡ï¼ˆèƒ½è€—ï¼‰': ['energy_cpu_total_joules', 'energy_gpu_total_joules']
    }

    for group_name, vars_list in key_vars_groups.items():
        print(f"\n   {group_name}:")
        vars_list = [v for v in vars_list if v in df.columns]

        for var in vars_list:
            fill_rate = (df[var].notna().sum() / len(df)) * 100

            if fill_rate < 50:
                status = "âŒ"
                issues.append(f"âŒ {var} å¡«å……ç‡ < 50% ({fill_rate:.1f}%)")
            elif fill_rate < 70:
                status = "âš ï¸ "
                issues.append(f"âš ï¸  {var} å¡«å……ç‡ < 70% ({fill_rate:.1f}%)")
            else:
                status = "âœ…"

            print(f"     {status} {var:30s}: {fill_rate:5.1f}%")

    # 3. æ•°æ®å˜å¼‚æ€§æ£€æŸ¥
    print("\n3. æ•°æ®å˜å¼‚æ€§æ£€æŸ¥:")

    key_numeric_vars = ['training_duration', 'gpu_util_avg', 'energy_gpu_total_joules']
    key_numeric_vars = [v for v in key_numeric_vars if v in df.columns]

    for var in key_numeric_vars:
        unique_count = df[var].nunique()

        if unique_count < 5:
            status = "âŒ"
            issues.append(f"âŒ {var} å”¯ä¸€å€¼ < 5 ({unique_count})")
        elif unique_count < 10:
            status = "âš ï¸ "
            issues.append(f"âš ï¸  {var} å”¯ä¸€å€¼ < 10 ({unique_count})")
        else:
            status = "âœ…"

        print(f"   {status} {var:30s}: {unique_count} ä¸ªå”¯ä¸€å€¼")

    # 4. åˆ†å±‚æ ·æœ¬é‡æ£€æŸ¥ï¼ˆå¦‚æœéœ€è¦åˆ†å±‚åˆ†æï¼‰
    print("\n4. åˆ†å±‚æ ·æœ¬é‡æ£€æŸ¥:")

    if 'repository' in df.columns:
        repos = df['repository'].value_counts()
        min_repo_samples = repos.min()

        print(f"   æœ€å°repositoryæ ·æœ¬é‡: {min_repo_samples}")

        if min_repo_samples < 20:
            issues.append(f"âš ï¸  æŸäº›repositoryæ ·æœ¬é‡ < 20")
            print("   âš ï¸  æŸäº›repositoryæ ·æœ¬é‡åå°‘ï¼ˆå»ºè®®è‡³å°‘20ä¸ªï¼‰")
        else:
            print("   âœ… æ‰€æœ‰repositoryæ ·æœ¬é‡å……è¶³")

    # æ€»ç»“
    print("\n" + "="*80)
    print("å› æœåˆ†æé€‚ç”¨æ€§æ€»ç»“:")
    print("="*80)

    if not issues:
        print("âœ… æ•°æ®å®Œå…¨æ»¡è¶³å› æœåˆ†æè¦æ±‚")
        readiness = "excellent"
    elif len([i for i in issues if i.startswith("âŒ")]) > 0:
        print(f"âŒ æ•°æ®å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œä¸å»ºè®®ç›´æ¥è¿›è¡Œå› æœåˆ†æ")
        print("\nä¸»è¦é—®é¢˜:")
        for issue in issues:
            if issue.startswith("âŒ"):
                print(f"  {issue}")
        readiness = "poor"
    else:
        print(f"âš ï¸  æ•°æ®åŸºæœ¬æ»¡è¶³è¦æ±‚ï¼Œä½†å­˜åœ¨ä»¥ä¸‹è­¦å‘Š:")
        for issue in issues:
            print(f"  {issue}")
        readiness = "good"

    return readiness, issues


def generate_quality_report(df, all_results):
    """ç”Ÿæˆå®Œæ•´çš„æ•°æ®è´¨é‡æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"æ•°æ®æ–‡ä»¶: {INPUT_FILE}")
    report_lines.append("")

    # åŸºæœ¬ä¿¡æ¯
    report_lines.append("=" * 80)
    report_lines.append("æ•°æ®åŸºæœ¬ä¿¡æ¯")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»è¡Œæ•°: {len(df):,}")
    report_lines.append(f"æ€»åˆ—æ•°: {len(df.columns)}")
    report_lines.append(f"æ•°æ®å¤§å°: {INPUT_FILE.stat().st_size / 1024:.1f} KB")
    report_lines.append("")

    # å®Œæ•´æ€§æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("æ•°æ®å®Œæ•´æ€§æ‘˜è¦")
    report_lines.append("=" * 80)

    if 'completeness' in all_results:
        for group_name, group_data in all_results['completeness'].items():
            avg_fill = np.mean([r['fill_rate'] for r in group_data])
            report_lines.append(f"{group_name}: å¹³å‡å¡«å……ç‡ {avg_fill:.1f}%")

    report_lines.append("")

    # å¼‚å¸¸å€¼æ‘˜è¦
    if 'outliers' in all_results:
        report_lines.append("=" * 80)
        report_lines.append("å¼‚å¸¸å€¼æ£€æµ‹æ‘˜è¦")
        report_lines.append("=" * 80)

        for var, stats in all_results['outliers'].items():
            if stats['outliers'] > 0:
                report_lines.append(f"{var}: {stats['outliers']} ä¸ªå¼‚å¸¸å€¼ ({stats['outlier_rate']:.1f}%)")

        report_lines.append("")

    # å› æœåˆ†æé€‚ç”¨æ€§
    if 'causal_readiness' in all_results:
        report_lines.append("=" * 80)
        report_lines.append("å› æœåˆ†æé€‚ç”¨æ€§")
        report_lines.append("=" * 80)

        readiness, issues = all_results['causal_readiness']

        if readiness == "excellent":
            report_lines.append("âœ… æ•°æ®å®Œå…¨æ»¡è¶³å› æœåˆ†æè¦æ±‚")
        elif readiness == "good":
            report_lines.append("âš ï¸  æ•°æ®åŸºæœ¬æ»¡è¶³è¦æ±‚ï¼Œå­˜åœ¨ä»¥ä¸‹è­¦å‘Š:")
            for issue in issues:
                report_lines.append(f"  {issue}")
        else:
            report_lines.append("âŒ æ•°æ®å­˜åœ¨ä¸¥é‡é—®é¢˜:")
            for issue in issues:
                report_lines.append(f"  {issue}")

        report_lines.append("")

    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("æ•°æ®è´¨é‡åˆ†æ")
    print("=" * 80)

    try:
        # åŠ è½½æ•°æ®
        df = load_data(INPUT_FILE)

        all_results = {}

        # 1. å®Œæ•´æ€§åˆ†æ
        all_results['completeness'] = analyze_completeness(df)

        # 2. åˆ†å¸ƒåˆ†æ
        all_results['distributions'] = analyze_distributions(df)

        # 3. å¼‚å¸¸å€¼æ£€æµ‹
        all_results['outliers'] = detect_outliers(df)

        # 4. ç›¸å…³æ€§åˆ†æ
        high_corr, corr_matrix = analyze_correlations(df)
        all_results['correlations'] = (high_corr, corr_matrix)

        # 5. åˆ†repositoryåˆ†æ
        all_results['by_repository'] = analyze_by_repository(df)

        # 6. åˆ†modeåˆ†æ
        all_results['by_mode'] = analyze_by_mode(df)

        # 7. å› æœåˆ†æé€‚ç”¨æ€§
        all_results['causal_readiness'] = assess_causal_readiness(df)

        # ç”ŸæˆæŠ¥å‘Š
        generate_quality_report(df, all_results)

        print("\n" + "=" * 80)
        print("âœ… æ•°æ®è´¨é‡åˆ†æå®Œæˆ")
        print("=" * 80)

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
