#!/usr/bin/env python3
"""
é˜¶æ®µ6: å½’ä¸€åŒ– (Normalization)

åŠŸèƒ½:
1. ä½¿ç”¨StandardScaleræ ‡å‡†åŒ–æ•°å€¼å˜é‡
2. ä¿ç•™å…ƒä¿¡æ¯åˆ—ï¼ˆä¸æ ‡å‡†åŒ–ï¼‰
3. ä¿ç•™One-Hotç¼–ç åˆ—ï¼ˆä¸æ ‡å‡†åŒ–ï¼‰
4. å¯¹è¶…å‚æ•°ã€èƒ½è€—ä¸­ä»‹å˜é‡ã€èƒ½è€—è¾“å‡ºã€æ€§èƒ½æŒ‡æ ‡è¿›è¡Œæ ‡å‡†åŒ–
5. ä¿å­˜æ ‡å‡†åŒ–å‚æ•°ï¼ˆmean, stdï¼‰
6. è¾“å‡º: 4ä¸ªä»»åŠ¡ç»„çš„æ ‡å‡†åŒ–CSVæ–‡ä»¶

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime
import pickle

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research" / "processed"
OUTPUT_DIR = DATA_DIR
SCALERS_DIR = DATA_DIR / "scalers"
SCALERS_DIR.mkdir(parents=True, exist_ok=True)

# ä»»åŠ¡ç»„å®šä¹‰
TASK_GROUPS = {
    'image_classification': 'å›¾åƒåˆ†ç±»',
    'person_reid': 'Person_reID',
    'vulberta': 'VulBERTa',
    'bug_localization': 'Bugå®šä½'
}


def identify_column_types(df):
    """
    è¯†åˆ«åˆ—çš„ç±»å‹ï¼ˆå…ƒä¿¡æ¯/One-Hot/æ•°å€¼ï¼‰

    Returns:
        dict: {
            'meta': [...],      # å…ƒä¿¡æ¯åˆ—ï¼ˆä¸æ ‡å‡†åŒ–ï¼‰
            'onehot': [...],    # One-Hotç¼–ç åˆ—ï¼ˆä¸æ ‡å‡†åŒ–ï¼‰
            'numeric': [...]    # æ•°å€¼åˆ—ï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼‰
        }
    """
    column_types = {
        'meta': [],
        'onehot': [],
        'numeric': []
    }

    # å…ƒä¿¡æ¯åˆ—ï¼ˆå›ºå®šï¼‰
    meta_keywords = ['experiment_id', 'timestamp', 'repository', 'model', 'mode']

    for col in df.columns:
        # å…ƒä¿¡æ¯åˆ—
        if any(keyword in col.lower() for keyword in meta_keywords):
            column_types['meta'].append(col)
        # One-Hotç¼–ç åˆ—
        elif col.startswith('is_'):
            column_types['onehot'].append(col)
        # æ•°å€¼åˆ—
        elif df[col].dtype in ['float64', 'int64', 'float32', 'int32']:
            column_types['numeric'].append(col)
        else:
            # æœªçŸ¥ç±»å‹ï¼Œæš‚æ—¶å½’ä¸ºå…ƒä¿¡æ¯
            column_types['meta'].append(col)

    return column_types


def standardize_numeric_columns(df, numeric_cols):
    """
    ä½¿ç”¨StandardScaleræ ‡å‡†åŒ–æ•°å€¼åˆ—

    Args:
        df: DataFrame
        numeric_cols: éœ€è¦æ ‡å‡†åŒ–çš„åˆ—ååˆ—è¡¨

    Returns:
        df_scaled: æ ‡å‡†åŒ–åçš„DataFrame
        scaler_params: æ ‡å‡†åŒ–å‚æ•° {col: {'mean': x, 'std': y}}
    """
    df_scaled = df.copy()
    scaler_params = {}

    for col in numeric_cols:
        # åªå¯¹éç©ºå€¼è¿›è¡Œæ ‡å‡†åŒ–
        non_null_mask = df[col].notna()

        if non_null_mask.sum() > 0:  # è‡³å°‘æœ‰ä¸€ä¸ªéç©ºå€¼
            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            mean = df.loc[non_null_mask, col].mean()
            std = df.loc[non_null_mask, col].std()

            # é¿å…é™¤ä»¥é›¶
            if std == 0 or pd.isna(std):
                print(f"  âš ï¸  è­¦å‘Š: {col} æ ‡å‡†å·®ä¸º0æˆ–NaNï¼Œè·³è¿‡æ ‡å‡†åŒ–ï¼ˆä¿ç•™åŸå€¼ï¼‰")
                scaler_params[col] = {'mean': mean, 'std': 1.0, 'skipped': True}
            else:
                # æ ‡å‡†åŒ–: (x - mean) / std
                df_scaled.loc[non_null_mask, col] = (
                    df.loc[non_null_mask, col] - mean
                ) / std

                scaler_params[col] = {'mean': mean, 'std': std, 'skipped': False}

                # éªŒè¯æ ‡å‡†åŒ–ç»“æœ
                scaled_mean = df_scaled.loc[non_null_mask, col].mean()
                scaled_std = df_scaled.loc[non_null_mask, col].std()

                if not (abs(scaled_mean) < 1e-6 and abs(scaled_std - 1.0) < 1e-6):
                    print(f"  âš ï¸  è­¦å‘Š: {col} æ ‡å‡†åŒ–åå‡å€¼={scaled_mean:.6f}, æ ‡å‡†å·®={scaled_std:.6f}")
        else:
            print(f"  âš ï¸  è­¦å‘Š: {col} å…¨ä¸ºNaNï¼Œè·³è¿‡æ ‡å‡†åŒ–")
            scaler_params[col] = {'mean': 0.0, 'std': 1.0, 'all_nan': True}

    return df_scaled, scaler_params


def normalize_task_group(task_name, task_display_name):
    """
    å½’ä¸€åŒ–å•ä¸ªä»»åŠ¡ç»„çš„æ•°æ®

    Args:
        task_name: ä»»åŠ¡ç»„åç§°ï¼ˆè‹±æ–‡ï¼‰
        task_display_name: ä»»åŠ¡ç»„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰

    Returns:
        bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    print(f"\n{'='*80}")
    print(f"ä»»åŠ¡ç»„: {task_display_name} ({task_name})")
    print(f"{'='*80}\n")

    # è¾“å…¥è¾“å‡ºæ–‡ä»¶
    input_file = OUTPUT_DIR / f"stage5_{task_name}.csv"
    output_file = OUTPUT_DIR / f"stage6_{task_name}.csv"
    scaler_file = SCALERS_DIR / f"scaler_{task_name}.pkl"

    # 1. åŠ è½½æ•°æ®
    if not input_file.exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return False

    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_file}")
    df = pd.read_csv(input_file)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

    # 2. è¯†åˆ«åˆ—ç±»å‹
    print(f"\nğŸ” è¯†åˆ«åˆ—ç±»å‹...")
    column_types = identify_column_types(df)

    print(f"\n  å…ƒä¿¡æ¯åˆ— ({len(column_types['meta'])} ä¸ª):")
    for col in column_types['meta']:
        print(f"    - {col}")

    print(f"\n  One-Hotç¼–ç åˆ— ({len(column_types['onehot'])} ä¸ª):")
    for col in column_types['onehot']:
        print(f"    - {col}")

    print(f"\n  æ•°å€¼åˆ— ({len(column_types['numeric'])} ä¸ª):")
    for col in column_types['numeric']:
        non_null = df[col].notna().sum()
        fill_rate = non_null / len(df) * 100
        print(f"    - {col}: {non_null}/{len(df)} ({fill_rate:.1f}%)")

    # 3. æ ‡å‡†åŒ–æ•°å€¼åˆ—
    print(f"\nğŸ“ æ ‡å‡†åŒ–æ•°å€¼åˆ—...")
    df_scaled, scaler_params = standardize_numeric_columns(df, column_types['numeric'])

    print(f"\nâœ… æ ‡å‡†åŒ–å®Œæˆ:")
    print(f"  - æ ‡å‡†åŒ–åˆ—æ•°: {len([p for p in scaler_params.values() if not p.get('skipped', False) and not p.get('all_nan', False)])}")
    print(f"  - è·³è¿‡åˆ—æ•°: {len([p for p in scaler_params.values() if p.get('skipped', False)])}")
    print(f"  - å…¨NaNåˆ—æ•°: {len([p for p in scaler_params.values() if p.get('all_nan', False)])}")

    # 4. éªŒè¯æ•°æ®èŒƒå›´
    print(f"\nğŸ” éªŒè¯æ ‡å‡†åŒ–åçš„æ•°æ®èŒƒå›´...")
    for col in column_types['numeric']:
        if col in scaler_params and not scaler_params[col].get('all_nan', False):
            non_null_mask = df_scaled[col].notna()
            if non_null_mask.sum() > 0:
                col_min = df_scaled.loc[non_null_mask, col].min()
                col_max = df_scaled.loc[non_null_mask, col].max()
                col_mean = df_scaled.loc[non_null_mask, col].mean()
                col_std = df_scaled.loc[non_null_mask, col].std()

                # æ£€æŸ¥æ˜¯å¦åˆç†
                if abs(col_mean) > 0.1:
                    print(f"  âš ï¸  {col}: å‡å€¼={col_mean:.3f} (åº”æ¥è¿‘0)")
                if abs(col_std - 1.0) > 0.1 and not scaler_params[col].get('skipped', False):
                    print(f"  âš ï¸  {col}: æ ‡å‡†å·®={col_std:.3f} (åº”æ¥è¿‘1)")

    print(f"âœ… æ•°æ®èŒƒå›´éªŒè¯å®Œæˆ")

    # 5. ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
    print(f"\nğŸ’¾ ä¿å­˜æ ‡å‡†åŒ–å‚æ•°...")
    scaler_info = {
        'task_name': task_name,
        'task_display_name': task_display_name,
        'scaler_params': scaler_params,
        'column_types': column_types,
        'num_samples': len(df),
        'num_features': len(column_types['numeric']),
        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(scaler_file, 'wb') as f:
        pickle.dump(scaler_info, f)

    print(f"âœ… æ ‡å‡†åŒ–å‚æ•°å·²ä¿å­˜: {scaler_file}")

    # 6. ä¿å­˜å½’ä¸€åŒ–æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜å½’ä¸€åŒ–æ•°æ®...")
    df_scaled.to_csv(output_file, index=False)

    file_size_kb = output_file.stat().st_size / 1024
    print(f"âœ… å½’ä¸€åŒ–æ•°æ®å·²ä¿å­˜: {output_file}")
    print(f"  - è¡Œæ•°: {len(df_scaled)}")
    print(f"  - åˆ—æ•°: {len(df_scaled.columns)}")
    print(f"  - æ–‡ä»¶å¤§å°: {file_size_kb:.1f} KB")

    return True


def generate_normalization_report(results):
    """ç”Ÿæˆå½’ä¸€åŒ–æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print(f"é˜¶æ®µ6: å½’ä¸€åŒ–æŠ¥å‘Š")
    print(f"{'='*80}\n")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ6: å½’ä¸€åŒ– (Normalization) æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")

    # æ±‡æ€»ç»Ÿè®¡
    report_lines.append("=" * 80)
    report_lines.append("1. å½’ä¸€åŒ–æ±‡æ€»")
    report_lines.append("=" * 80)

    total_tasks = len(results)
    success_tasks = sum(1 for r in results.values() if r['success'])

    report_lines.append(f"æ€»ä»»åŠ¡ç»„æ•°: {total_tasks}")
    report_lines.append(f"æˆåŠŸä»»åŠ¡ç»„: {success_tasks}")
    report_lines.append(f"å¤±è´¥ä»»åŠ¡ç»„: {total_tasks - success_tasks}")
    report_lines.append("")

    # å„ä»»åŠ¡ç»„è¯¦æƒ…
    report_lines.append("=" * 80)
    report_lines.append("2. å„ä»»åŠ¡ç»„è¯¦æƒ…")
    report_lines.append("=" * 80)

    for task_name, result in results.items():
        task_display = TASK_GROUPS[task_name]
        status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"

        report_lines.append(f"\n{task_display} ({task_name}): {status}")

        if result['success']:
            report_lines.append(f"  è¾“å‡ºæ–‡ä»¶: stage6_{task_name}.csv")
            report_lines.append(f"  æ ‡å‡†åŒ–å‚æ•°: scaler_{task_name}.pkl")
        else:
            report_lines.append(f"  é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("âœ… é˜¶æ®µ6å®Œæˆ")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    report_file = OUTPUT_DIR / "stage6_normalization_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(report_lines))

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n".join(report_lines))
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ6: å½’ä¸€åŒ– (Normalization)")
    print("=" * 80)

    results = {}

    # å¯¹æ¯ä¸ªä»»åŠ¡ç»„è¿›è¡Œå½’ä¸€åŒ–
    for task_name, task_display_name in TASK_GROUPS.items():
        try:
            success = normalize_task_group(task_name, task_display_name)
            results[task_name] = {
                'success': success,
                'task_display': task_display_name
            }
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: å¤„ç†ä»»åŠ¡ç»„ {task_name} æ—¶å‘ç”Ÿå¼‚å¸¸")
            print(f"  å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            import traceback
            traceback.print_exc()

            results[task_name] = {
                'success': False,
                'task_display': task_display_name,
                'error': str(e)
            }

    # ç”Ÿæˆå½’ä¸€åŒ–æŠ¥å‘Š
    generate_normalization_report(results)

    # è¿”å›çŠ¶æ€
    all_success = all(r['success'] for r in results.values())

    if all_success:
        print("\n" + "=" * 80)
        print("âœ… é˜¶æ®µ6å®Œæˆ: æ‰€æœ‰ä»»åŠ¡ç»„å½’ä¸€åŒ–æˆåŠŸ")
        print("=" * 80)
        return 0
    else:
        print("\n" + "=" * 80)
        print("âš ï¸  é˜¶æ®µ6å®Œæˆ: éƒ¨åˆ†ä»»åŠ¡ç»„å½’ä¸€åŒ–å¤±è´¥")
        print("=" * 80)
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
