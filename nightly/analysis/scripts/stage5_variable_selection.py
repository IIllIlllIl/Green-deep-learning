#!/usr/bin/env python3
"""
é˜¶æ®µ5: å˜é‡é€‰æ‹© (Variable Selection)

åŠŸèƒ½:
1. ä¸ºæ¯ä¸ªä»»åŠ¡ç»„é€‰æ‹©13-16ä¸ªå…³é”®å˜é‡
2. åŸºäºå¡«å……ç‡ã€ç›¸å…³æ€§å’Œå› æœåˆ†æéœ€æ±‚
3. åŒ…å«ï¼šå…ƒä¿¡æ¯ã€è¶…å‚æ•°ã€ä¸­ä»‹å˜é‡ã€èƒ½è€—è¾“å‡ºã€æ€§èƒ½æŒ‡æ ‡
4. è¾“å‡º: 4ä¸ªä»»åŠ¡ç»„çš„æœ€ç»ˆåˆ†ææ•°æ®

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
PROCESSED_DIR = DATA_DIR / "processed"
REPORT_FILE = PROCESSED_DIR / "stage5_variable_selection_report.txt"

# ä»»åŠ¡ç»„é…ç½®
TASK_CONFIGS = {
    'image_classification': {
        'input': PROCESSED_DIR / 'stage4_image_classification.csv',
        'output': PROCESSED_DIR / 'stage5_image_classification.csv',
        'name': 'å›¾åƒåˆ†ç±»',
        'variables': {
            # å…ƒä¿¡æ¯ (4)
            'metadata': [
                'experiment_id',
                'repository',
                'model',
                'timestamp'
            ],
            # è¶…å‚æ•° (4)
            'hyperparameters': [
                'hyperparam_learning_rate',
                'hyperparam_batch_size',
                'training_duration',
                'seed'
            ],
            # One-Hotç¼–ç  (2)
            'onehot': [
                'is_mnist',
                'is_cifar10'
            ],
            # èƒ½è€—ä¸­ä»‹å˜é‡ (5)
            'mediators': [
                'gpu_util_avg',
                'gpu_temp_max',
                'cpu_pkg_ratio',
                'gpu_power_fluctuation',
                'gpu_temp_fluctuation'
            ],
            # èƒ½è€—è¾“å‡º (2)
            'energy_outputs': [
                'energy_cpu_total_joules',
                'energy_gpu_total_joules'
            ],
            # æ€§èƒ½æŒ‡æ ‡ (1)
            'performance': [
                'perf_test_accuracy'
            ]
        }
    },
    'person_reid': {
        'input': PROCESSED_DIR / 'stage4_person_reid.csv',
        'output': PROCESSED_DIR / 'stage5_person_reid.csv',
        'name': 'Person_reIDæ£€ç´¢',
        'variables': {
            # å…ƒä¿¡æ¯ (4)
            'metadata': [
                'experiment_id',
                'repository',
                'model',
                'timestamp'
            ],
            # è¶…å‚æ•° (3)
            'hyperparameters': [
                'hyperparam_learning_rate',
                'hyperparam_dropout',
                'training_duration'
            ],
            # One-Hotç¼–ç  (3)
            'onehot': [
                'is_densenet121',
                'is_hrnet18',
                'is_pcb'
            ],
            # èƒ½è€—ä¸­ä»‹å˜é‡ (5)
            'mediators': [
                'gpu_util_avg',
                'gpu_temp_max',
                'cpu_pkg_ratio',
                'gpu_power_fluctuation',
                'gpu_temp_fluctuation'
            ],
            # èƒ½è€—è¾“å‡º (2)
            'energy_outputs': [
                'energy_cpu_total_joules',
                'energy_gpu_total_joules'
            ],
            # æ€§èƒ½æŒ‡æ ‡ (3)
            'performance': [
                'perf_map',
                'perf_rank1',
                'perf_rank5'
            ]
        }
    },
    'vulberta': {
        'input': PROCESSED_DIR / 'stage4_vulberta.csv',
        'output': PROCESSED_DIR / 'stage5_vulberta.csv',
        'name': 'VulBERTaæ¼æ´æ£€æµ‹',
        'variables': {
            # å…ƒä¿¡æ¯ (4)
            'metadata': [
                'experiment_id',
                'repository',
                'model',
                'timestamp'
            ],
            # è¶…å‚æ•° (2)
            'hyperparameters': [
                'hyperparam_learning_rate',
                'training_duration'
            ],
            # One-Hotç¼–ç  (0) - å•ä¸€æ¨¡å‹
            'onehot': [],
            # èƒ½è€—ä¸­ä»‹å˜é‡ (5)
            'mediators': [
                'gpu_util_avg',
                'gpu_temp_max',
                'cpu_pkg_ratio',
                'gpu_power_fluctuation',
                'gpu_temp_fluctuation'
            ],
            # èƒ½è€—è¾“å‡º (2)
            'energy_outputs': [
                'energy_cpu_total_joules',
                'energy_gpu_total_joules'
            ],
            # æ€§èƒ½æŒ‡æ ‡ (1)
            'performance': [
                'perf_eval_loss'
            ]
        }
    },
    'bug_localization': {
        'input': PROCESSED_DIR / 'stage4_bug_localization.csv',
        'output': PROCESSED_DIR / 'stage5_bug_localization.csv',
        'name': 'Bugå®šä½',
        'variables': {
            # å…ƒä¿¡æ¯ (4)
            'metadata': [
                'experiment_id',
                'repository',
                'model',
                'timestamp'
            ],
            # è¶…å‚æ•° (2)
            'hyperparameters': [
                'hyperparam_learning_rate',
                'training_duration'
            ],
            # One-Hotç¼–ç  (0) - å•ä¸€æ¨¡å‹
            'onehot': [],
            # èƒ½è€—ä¸­ä»‹å˜é‡ (5)
            'mediators': [
                'gpu_util_avg',
                'gpu_temp_max',
                'cpu_pkg_ratio',
                'gpu_power_fluctuation',
                'gpu_temp_fluctuation'
            ],
            # èƒ½è€—è¾“å‡º (2)
            'energy_outputs': [
                'energy_cpu_total_joules',
                'energy_gpu_total_joules'
            ],
            # æ€§èƒ½æŒ‡æ ‡ (2)
            'performance': [
                'perf_top1_accuracy',
                'perf_top5_accuracy'
            ]
        }
    }
}


def load_task_group(filepath, task_name):
    """åŠ è½½ä»»åŠ¡ç»„æ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½ {task_name}...")
    df = pd.read_csv(filepath)
    print(f"   åŸå§‹: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
    return df


def select_variables(df, var_config, task_name):
    """
    é€‰æ‹©å˜é‡å¹¶éªŒè¯

    å‚æ•°:
        df: DataFrame
        var_config: å˜é‡é…ç½®å­—å…¸
        task_name: ä»»åŠ¡ç»„åç§°

    è¿”å›:
        é€‰æ‹©åçš„DataFrame, é€‰æ‹©çš„åˆ—åˆ—è¡¨
    """
    print(f"\nğŸ”§ é€‰æ‹©å˜é‡ ({task_name})...")

    # æ”¶é›†æ‰€æœ‰é€‰æ‹©çš„å˜é‡
    all_vars = []
    for category, vars_list in var_config.items():
        all_vars.extend(vars_list)

    print(f"   ç›®æ ‡å˜é‡æ•°: {len(all_vars)}")
    print(f"   ç±»åˆ«æ•°: {len(var_config)}")

    # éªŒè¯å˜é‡å­˜åœ¨æ€§
    missing_vars = []
    existing_vars = []

    for var in all_vars:
        if var in df.columns:
            existing_vars.append(var)
        else:
            missing_vars.append(var)

    if missing_vars:
        print(f"\n   âš ï¸  ç¼ºå¤±å˜é‡ ({len(missing_vars)}):")
        for var in missing_vars:
            print(f"      - {var}")
    else:
        print(f"   âœ… æ‰€æœ‰å˜é‡å­˜åœ¨")

    # é€‰æ‹©åˆ—
    df_selected = df[existing_vars].copy()

    # ç»Ÿè®¡å„ç±»åˆ«å˜é‡æ•°
    print(f"\n   å˜é‡ç±»åˆ«åˆ†å¸ƒ:")
    for category, vars_list in var_config.items():
        existing_count = sum(1 for v in vars_list if v in existing_vars)
        total_count = len(vars_list)
        print(f"      {category:20s}: {existing_count}/{total_count}")

    return df_selected, existing_vars


def analyze_selected_variables(df, selected_vars, task_name):
    """
    åˆ†æé€‰æ‹©çš„å˜é‡è´¨é‡

    æ£€æŸ¥:
    1. å¡«å……ç‡
    2. å”¯ä¸€å€¼æ•°é‡ï¼ˆå˜å¼‚æ€§ï¼‰
    3. æ•°æ®ç±»å‹
    """
    print(f"\nğŸ“Š å˜é‡è´¨é‡åˆ†æ ({task_name})...")

    # å¡«å……ç‡åˆ†æ
    fill_rates = {}
    for var in selected_vars:
        if var in df.columns:
            fill_rate = df[var].notna().sum() / len(df) * 100
            fill_rates[var] = fill_rate

    # æŒ‰å¡«å……ç‡åˆ†ç±»
    high_fill = [v for v, r in fill_rates.items() if r >= 90]
    medium_fill = [v for v, r in fill_rates.items() if 50 <= r < 90]
    low_fill = [v for v, r in fill_rates.items() if r < 50]

    print(f"\n   å¡«å……ç‡åˆ†å¸ƒ:")
    print(f"      é«˜å¡«å…… (â‰¥90%): {len(high_fill)} ä¸ª")
    print(f"      ä¸­å¡«å…… (50-90%): {len(medium_fill)} ä¸ª")
    print(f"      ä½å¡«å…… (<50%): {len(low_fill)} ä¸ª")

    if low_fill:
        print(f"\n   âš ï¸  ä½å¡«å……ç‡å˜é‡:")
        for var in low_fill:
            print(f"      - {var}: {fill_rates[var]:.1f}%")

    # å”¯ä¸€å€¼åˆ†æï¼ˆæ•°å€¼å‹å˜é‡ï¼‰
    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_selected = [v for v in selected_vars if v in numeric_vars]

    low_variance = []
    for var in numeric_selected:
        unique_count = df[var].nunique()
        if unique_count < 5:
            low_variance.append((var, unique_count))

    if low_variance:
        print(f"\n   âš ï¸  ä½å˜å¼‚æ€§å˜é‡ (<5å”¯ä¸€å€¼):")
        for var, count in low_variance:
            print(f"      - {var}: {count} å”¯ä¸€å€¼")

    # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
    avg_fill_rate = np.mean(list(fill_rates.values()))
    quality_score = "ä¼˜ç§€" if avg_fill_rate >= 80 else "è‰¯å¥½" if avg_fill_rate >= 60 else "ä¸€èˆ¬"

    print(f"\n   æ•´ä½“è¯„ä¼°:")
    print(f"      å¹³å‡å¡«å……ç‡: {avg_fill_rate:.1f}%")
    print(f"      è´¨é‡è¯„çº§: {quality_score}")

    return fill_rates


def save_selected_data(df, output_file, task_name):
    """ä¿å­˜é€‰æ‹©åçš„æ•°æ®"""
    df.to_csv(output_file, index=False)
    file_size = output_file.stat().st_size / 1024

    print(f"\nğŸ’¾ ä¿å­˜ {task_name}:")
    print(f"   æ–‡ä»¶: {output_file.name}")
    print(f"   è¡Œæ•°: {len(df)}")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   å¤§å°: {file_size:.1f} KB")

    return {
        'task_name': task_name,
        'file_path': output_file,
        'row_count': len(df),
        'column_count': len(df.columns),
        'file_size_kb': file_size
    }


def generate_selection_report(results, all_fill_rates):
    """ç”Ÿæˆå˜é‡é€‰æ‹©æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆå˜é‡é€‰æ‹©æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ5: å˜é‡é€‰æ‹©æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")

    # ä»»åŠ¡ç»„æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("1. ä»»åŠ¡ç»„å˜é‡é€‰æ‹©æ‘˜è¦")
    report_lines.append("=" * 80)

    for result in results:
        task_id = result['task_id']
        task_config = TASK_CONFIGS[task_id]
        task_name = result['task_name']
        fill_rates = all_fill_rates[task_id]

        report_lines.append(f"\n{task_name}:")
        report_lines.append(f"  è¾“å‡ºæ–‡ä»¶: {result['file_path'].name}")
        report_lines.append(f"  æ ·æœ¬æ•°: {result['row_count']}")
        report_lines.append(f"  å˜é‡æ•°: {result['column_count']}")
        report_lines.append(f"  æ–‡ä»¶å¤§å°: {result['file_size_kb']:.1f} KB")

        # å˜é‡ç±»åˆ«ç»Ÿè®¡
        report_lines.append(f"  å˜é‡ç±»åˆ«:")
        for category, vars_list in task_config['variables'].items():
            report_lines.append(f"    - {category}: {len(vars_list)}ä¸ª")

        # å¡«å……ç‡ç»Ÿè®¡
        avg_fill = np.mean(list(fill_rates.values()))
        report_lines.append(f"  å¹³å‡å¡«å……ç‡: {avg_fill:.1f}%")

    report_lines.append("")

    # ç»Ÿè®¡æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("2. æ•´ä½“ç»Ÿè®¡")
    report_lines.append("=" * 80)

    total_samples = sum(r['row_count'] for r in results)
    avg_vars = np.mean([r['column_count'] for r in results])

    report_lines.append(f"ä»»åŠ¡ç»„æ€»æ•°: {len(results)}")
    report_lines.append(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    report_lines.append(f"å¹³å‡å˜é‡æ•°: {avg_vars:.1f}")
    report_lines.append(f"å˜é‡æ•°èŒƒå›´: {min(r['column_count'] for r in results)}-{max(r['column_count'] for r in results)}")

    # å˜é‡é€‰æ‹©è®¾è®¡è¯´æ˜
    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("3. å˜é‡é€‰æ‹©è®¾è®¡")
    report_lines.append("=" * 80)
    report_lines.append("")
    report_lines.append("æ‰€æœ‰ä»»åŠ¡ç»„åŒ…å«:")
    report_lines.append("  1. å…ƒä¿¡æ¯ (4): experiment_id, repository, model, timestamp")
    report_lines.append("  2. èƒ½è€—ä¸­ä»‹ (5): gpu_util_avg, gpu_temp_max, cpu_pkg_ratio, gpu_power_fluctuation, gpu_temp_fluctuation")
    report_lines.append("  3. èƒ½è€—è¾“å‡º (2): energy_cpu_total_joules, energy_gpu_total_joules")
    report_lines.append("")
    report_lines.append("ä»»åŠ¡ç‰¹å®šå˜é‡:")
    report_lines.append("  - å›¾åƒåˆ†ç±»: 4è¶…å‚æ•° + 2 One-Hot + 1æ€§èƒ½ (18å˜é‡)")
    report_lines.append("  - Person_reID: 3è¶…å‚æ•° + 3 One-Hot + 3æ€§èƒ½ (20å˜é‡)")
    report_lines.append("  - VulBERTa: 2è¶…å‚æ•° + 0 One-Hot + 1æ€§èƒ½ (14å˜é‡)")
    report_lines.append("  - Bugå®šä½: 2è¶…å‚æ•° + 0 One-Hot + 2æ€§èƒ½ (15å˜é‡)")

    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("âœ… é˜¶æ®µ5: å˜é‡é€‰æ‹©å®Œæˆ")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… å˜é‡é€‰æ‹©æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ5: å˜é‡é€‰æ‹© (Variable Selection)")
    print("=" * 80)

    try:
        results = []
        all_fill_rates = {}

        for task_id, task_config in TASK_CONFIGS.items():
            # 1. åŠ è½½æ•°æ®
            df = load_task_group(task_config['input'], task_config['name'])

            # 2. é€‰æ‹©å˜é‡
            df_selected, selected_vars = select_variables(
                df,
                task_config['variables'],
                task_config['name']
            )

            # 3. åˆ†æå˜é‡è´¨é‡
            fill_rates = analyze_selected_variables(
                df_selected,
                selected_vars,
                task_config['name']
            )

            all_fill_rates[task_id] = fill_rates

            # 4. ä¿å­˜æ•°æ®
            result = save_selected_data(
                df_selected,
                task_config['output'],
                task_config['name']
            )

            result['task_id'] = task_id
            results.append(result)

        # 5. ç”ŸæˆæŠ¥å‘Š
        generate_selection_report(results, all_fill_rates)

        print("\n" + "=" * 80)
        print("âœ… é˜¶æ®µ5å®Œæˆ: å˜é‡é€‰æ‹©æˆåŠŸ")
        print("=" * 80)
        print(f"\nç”Ÿæˆçš„æœ€ç»ˆåˆ†ææ–‡ä»¶:")
        for result in results:
            print(f"  - {result['file_path'].name} ({result['column_count']}å˜é‡, {result['row_count']}æ ·æœ¬)")

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
