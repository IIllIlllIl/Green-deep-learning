#!/usr/bin/env python3
"""
é˜¶æ®µ1: è¶…å‚æ•°ç»Ÿä¸€ (Hyperparameter Unification)

åŠŸèƒ½:
1. åŠ è½½é˜¶æ®µ0éªŒè¯æ•°æ®
2. åˆ›å»ºç»Ÿä¸€è¶…å‚æ•°:
   - training_duration = epochs (å¦‚æœæœ‰) æˆ– max_iter (å¦‚æœæœ‰)
   - l2_regularization = weight_decay (å¦‚æœæœ‰) æˆ– alpha (å¦‚æœæœ‰)
3. éªŒè¯äº’æ–¥æ€§
4. è¾“å‡º: stage1_unified.csv

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
PROCESSED_DIR = DATA_DIR / "processed"
INPUT_FILE = PROCESSED_DIR / "stage0_validated.csv"
OUTPUT_FILE = PROCESSED_DIR / "stage1_unified.csv"
REPORT_FILE = PROCESSED_DIR / "stage1_unification_report.txt"


def load_data(filepath):
    """åŠ è½½CSVæ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {filepath}")
    df = pd.read_csv(filepath)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    return df


def check_mutual_exclusivity(df):
    """æ£€æŸ¥epochså’Œmax_iterçš„äº’æ–¥æ€§"""
    print("\nğŸ” æ£€æŸ¥è¶…å‚æ•°äº’æ–¥æ€§...")

    # æ£€æŸ¥epochså’Œmax_iter
    has_epochs = df['hyperparam_epochs'].notna()
    has_max_iter = df['hyperparam_max_iter'].notna()

    both = (has_epochs & has_max_iter).sum()
    epochs_only = (has_epochs & ~has_max_iter).sum()
    max_iter_only = (~has_epochs & has_max_iter).sum()
    neither = (~has_epochs & ~has_max_iter).sum()

    print(f"\n  training_duration æºåˆ—åˆ†å¸ƒ:")
    print(f"    epochs only: {epochs_only} ({epochs_only/len(df)*100:.1f}%)")
    print(f"    max_iter only: {max_iter_only} ({max_iter_only/len(df)*100:.1f}%)")
    print(f"    both (å†²çª): {both} ({both/len(df)*100:.1f}%)")
    print(f"    neither: {neither} ({neither/len(df)*100:.1f}%)")

    # æ£€æŸ¥weight_decayå’Œalpha
    has_weight_decay = df['hyperparam_weight_decay'].notna()
    has_alpha = df['hyperparam_alpha'].notna()

    both_reg = (has_weight_decay & has_alpha).sum()
    wd_only = (has_weight_decay & ~has_alpha).sum()
    alpha_only = (~has_weight_decay & has_alpha).sum()
    neither_reg = (~has_weight_decay & ~has_alpha).sum()

    print(f"\n  l2_regularization æºåˆ—åˆ†å¸ƒ:")
    print(f"    weight_decay only: {wd_only} ({wd_only/len(df)*100:.1f}%)")
    print(f"    alpha only: {alpha_only} ({alpha_only/len(df)*100:.1f}%)")
    print(f"    both (å†²çª): {both_reg} ({both_reg/len(df)*100:.1f}%)")
    print(f"    neither: {neither_reg} ({neither_reg/len(df)*100:.1f}%)")

    issues = []
    if both > 0:
        issues.append(f"âš ï¸  {both} è¡ŒåŒæ—¶æœ‰epochså’Œmax_iter")
    if both_reg > 0:
        issues.append(f"âš ï¸  {both_reg} è¡ŒåŒæ—¶æœ‰weight_decayå’Œalpha")

    if not issues:
        print(f"\nâœ… äº’æ–¥æ€§éªŒè¯é€šè¿‡ï¼ˆæ— å†²çªï¼‰")

    return issues, {
        'epochs_only': epochs_only,
        'max_iter_only': max_iter_only,
        'both_duration': both,
        'wd_only': wd_only,
        'alpha_only': alpha_only,
        'both_reg': both_reg
    }


def create_training_duration(df):
    """åˆ›å»ºtraining_durationç»Ÿä¸€åˆ—"""
    print("\nğŸ”§ åˆ›å»º training_duration åˆ—...")

    # ä¼˜å…ˆä½¿ç”¨epochsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨max_iter
    df['training_duration'] = df['hyperparam_epochs'].fillna(df['hyperparam_max_iter'])

    filled = df['training_duration'].notna().sum()
    fill_rate = (filled / len(df)) * 100

    print(f"âœ… training_duration åˆ›å»ºæˆåŠŸ")
    print(f"  å¡«å……è¡Œæ•°: {filled}/{len(df)} ({fill_rate:.1f}%)")

    # ç»Ÿè®¡æ¥æº
    from_epochs = (df['hyperparam_epochs'].notna() & df['training_duration'].notna()).sum()
    from_max_iter = (df['hyperparam_epochs'].isna() & df['hyperparam_max_iter'].notna()).sum()

    print(f"  æ¥æºåˆ†å¸ƒ:")
    print(f"    ä» epochs: {from_epochs} ({from_epochs/filled*100:.1f}%)")
    print(f"    ä» max_iter: {from_max_iter} ({from_max_iter/filled*100:.1f}%)")

    # ç»Ÿè®¡èŒƒå›´
    if filled > 0:
        print(f"  æ•°å€¼èŒƒå›´: {df['training_duration'].min():.0f} - {df['training_duration'].max():.0f}")

    return filled, fill_rate


def create_l2_regularization(df):
    """åˆ›å»ºl2_regularizationç»Ÿä¸€åˆ—"""
    print("\nğŸ”§ åˆ›å»º l2_regularization åˆ—...")

    # ä¼˜å…ˆä½¿ç”¨weight_decayï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨alpha
    df['l2_regularization'] = df['hyperparam_weight_decay'].fillna(df['hyperparam_alpha'])

    filled = df['l2_regularization'].notna().sum()
    fill_rate = (filled / len(df)) * 100

    print(f"âœ… l2_regularization åˆ›å»ºæˆåŠŸ")
    print(f"  å¡«å……è¡Œæ•°: {filled}/{len(df)} ({fill_rate:.1f}%)")

    # ç»Ÿè®¡æ¥æº
    from_wd = (df['hyperparam_weight_decay'].notna() & df['l2_regularization'].notna()).sum()
    from_alpha = (df['hyperparam_weight_decay'].isna() & df['hyperparam_alpha'].notna()).sum()

    print(f"  æ¥æºåˆ†å¸ƒ:")
    print(f"    ä» weight_decay: {from_wd} ({from_wd/filled*100 if filled > 0 else 0:.1f}%)")
    print(f"    ä» alpha: {from_alpha} ({from_alpha/filled*100 if filled > 0 else 0:.1f}%)")

    # ç»Ÿè®¡èŒƒå›´
    if filled > 0:
        print(f"  æ•°å€¼èŒƒå›´: {df['l2_regularization'].min():.6f} - {df['l2_regularization'].max():.6f}")

    return filled, fill_rate


def verify_unification(df):
    """éªŒè¯ç»Ÿä¸€ç»“æœ"""
    print("\nğŸ” éªŒè¯ç»Ÿä¸€ç»“æœ...")

    issues = []

    # éªŒè¯training_duration
    td_notna = df['training_duration'].notna()
    epochs_notna = df['hyperparam_epochs'].notna()
    max_iter_notna = df['hyperparam_max_iter'].notna()

    # å¦‚æœæœ‰epochsæˆ–max_iterï¼Œåº”è¯¥æœ‰training_duration
    should_have_td = epochs_notna | max_iter_notna
    missing_td = should_have_td & df['training_duration'].isna()

    if missing_td.sum() > 0:
        issues.append(f"âŒ {missing_td.sum()} è¡Œåº”è¯¥æœ‰training_durationä½†ç¼ºå¤±")
        print(f"  âŒ training_duration ç¼ºå¤±: {missing_td.sum()} è¡Œ")
    else:
        print(f"  âœ… training_duration å®Œæ•´æ€§æ­£ç¡®")

    # éªŒè¯l2_regularization
    l2_notna = df['l2_regularization'].notna()
    wd_notna = df['hyperparam_weight_decay'].notna()
    alpha_notna = df['hyperparam_alpha'].notna()

    # å¦‚æœæœ‰weight_decayæˆ–alphaï¼Œåº”è¯¥æœ‰l2_regularization
    should_have_l2 = wd_notna | alpha_notna
    missing_l2 = should_have_l2 & df['l2_regularization'].isna()

    if missing_l2.sum() > 0:
        issues.append(f"âŒ {missing_l2.sum()} è¡Œåº”è¯¥æœ‰l2_regularizationä½†ç¼ºå¤±")
        print(f"  âŒ l2_regularization ç¼ºå¤±: {missing_l2.sum()} è¡Œ")
    else:
        print(f"  âœ… l2_regularization å®Œæ•´æ€§æ­£ç¡®")

    return issues


def generate_unification_report(df, stats, issues):
    """ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ1: è¶…å‚æ•°ç»Ÿä¸€æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
    report_lines.append(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    report_lines.append("")

    # æ•°æ®æ¦‚è§ˆ
    report_lines.append("=" * 80)
    report_lines.append("1. æ•°æ®æ¦‚è§ˆ")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»è¡Œæ•°: {len(df):,}")
    report_lines.append(f"åŸå§‹åˆ—æ•°: {len(df.columns) - 2}")  # å‡å»æ–°å¢çš„2åˆ—
    report_lines.append(f"æ–°å¢åˆ—æ•°: 2 (training_duration, l2_regularization)")
    report_lines.append(f"æœ€ç»ˆåˆ—æ•°: {len(df.columns)}")
    report_lines.append("")

    # ç»Ÿä¸€ç»“æœ
    report_lines.append("=" * 80)
    report_lines.append("2. è¶…å‚æ•°ç»Ÿä¸€ç»“æœ")
    report_lines.append("=" * 80)
    report_lines.append("")
    report_lines.append("2.1 training_duration:")
    report_lines.append(f"  å¡«å……ç‡: {df['training_duration'].notna().sum()/len(df)*100:.1f}%")
    report_lines.append(f"  æ¥æº: epochs ({stats['epochs_only']}), max_iter ({stats['max_iter_only']})")
    if stats['both_duration'] > 0:
        report_lines.append(f"  âš ï¸  å†²çª: {stats['both_duration']} è¡ŒåŒæ—¶æœ‰epochså’Œmax_iter")
    report_lines.append("")
    report_lines.append("2.2 l2_regularization:")
    report_lines.append(f"  å¡«å……ç‡: {df['l2_regularization'].notna().sum()/len(df)*100:.1f}%")
    report_lines.append(f"  æ¥æº: weight_decay ({stats['wd_only']}), alpha ({stats['alpha_only']})")
    if stats['both_reg'] > 0:
        report_lines.append(f"  âš ï¸  å†²çª: {stats['both_reg']} è¡ŒåŒæ—¶æœ‰weight_decayå’Œalpha")
    report_lines.append("")

    # é—®é¢˜æ±‡æ€»
    report_lines.append("=" * 80)
    report_lines.append("3. éªŒè¯é—®é¢˜æ±‡æ€»")
    report_lines.append("=" * 80)

    if issues:
        report_lines.append(f"å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        report_lines.append("")
        for i, issue in enumerate(issues, 1):
            report_lines.append(f"{i}. {issue}")
    else:
        report_lines.append("âœ… æœªå‘ç°é—®é¢˜ï¼Œè¶…å‚æ•°ç»Ÿä¸€æˆåŠŸï¼")

    report_lines.append("")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… ç»Ÿä¸€æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)

    return len(issues) == 0


def save_unified_data(df):
    """ä¿å­˜ç»Ÿä¸€åçš„æ•°æ®"""
    print(f"\nğŸ’¾ ä¿å­˜ç»Ÿä¸€æ•°æ®...")

    df.to_csv(OUTPUT_FILE, index=False)

    print(f"âœ… ç»Ÿä¸€æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")
    print(f"  è¡Œæ•°: {len(df):,}")
    print(f"  åˆ—æ•°: {len(df.columns)}")
    print(f"  æ–°å¢åˆ—: training_duration, l2_regularization")
    print(f"  æ–‡ä»¶å¤§å°: {OUTPUT_FILE.stat().st_size / 1024:.1f} KB")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ1: è¶…å‚æ•°ç»Ÿä¸€ (Hyperparameter Unification)")
    print("=" * 80)

    try:
        # 1. åŠ è½½æ•°æ®
        df = load_data(INPUT_FILE)

        # 2. æ£€æŸ¥äº’æ–¥æ€§
        exclusivity_issues, stats = check_mutual_exclusivity(df)
        all_issues = list(exclusivity_issues)

        # 3. åˆ›å»ºç»Ÿä¸€åˆ—
        td_filled, td_rate = create_training_duration(df)
        l2_filled, l2_rate = create_l2_regularization(df)

        # 4. éªŒè¯ç»Ÿä¸€ç»“æœ
        verification_issues = verify_unification(df)
        all_issues.extend(verification_issues)

        # 5. ç”ŸæˆæŠ¥å‘Š
        unification_passed = generate_unification_report(df, stats, all_issues)

        # 6. ä¿å­˜æ•°æ®
        save_unified_data(df)

        if unification_passed:
            print("\n" + "=" * 80)
            print("âœ… é˜¶æ®µ1å®Œæˆ: è¶…å‚æ•°ç»Ÿä¸€æˆåŠŸ")
            print("=" * 80)
            print(f"\næ–°å¢å˜é‡å¡«å……ç‡:")
            print(f"  training_duration: {td_rate:.1f}%")
            print(f"  l2_regularization: {l2_rate:.1f}%")
            return 0
        else:
            print("\n" + "=" * 80)
            print(f"âš ï¸  é˜¶æ®µ1å®Œæˆ: å‘ç° {len(all_issues)} ä¸ªé—®é¢˜")
            print("=" * 80)
            return 1

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
