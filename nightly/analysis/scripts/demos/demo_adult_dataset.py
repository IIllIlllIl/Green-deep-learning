"""
çœŸå®æ•°æ®é›†éªŒè¯è„šæœ¬ï¼šä½¿ç”¨Adultæ•°æ®é›†å¤ç°è®ºæ–‡å®éªŒ
å¯¹æ¯”è®ºæ–‡ä¸­çš„å‘ç°ï¼ŒéªŒè¯å¤ç°å‡†ç¡®æ€§
"""
import numpy as np
import pandas as pd
import sys
import os
import time
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.model import FFNN, ModelTrainer
from utils.metrics import MetricsCalculator, define_sign_functions
from utils.fairness_methods import get_fairness_method

# æ£€æŸ¥GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"è®¾å¤‡: {device}")
if device == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")

print("="*70)
print(" "*15 + "çœŸå®æ•°æ®é›†éªŒè¯ - Adultæ•°æ®é›†")
print("="*70)

start_time = time.time()

# ============================================================================
# æ­¥éª¤1: åŠ è½½Adultæ•°æ®é›†
# ============================================================================
print("\n" + "â–¶"*35)
print("æ­¥éª¤1: åŠ è½½Adultæ•°æ®é›†")
print("â–¶"*35)

try:
    from aif360.datasets import AdultDataset
    from aif360.algorithms.preprocessing import Reweighing as AIF360Reweighing

    print("\nä»AIF360åŠ è½½Adultæ•°æ®é›†...")

    # åŠ è½½æ•°æ®é›† (sexä¸ºæ•æ„Ÿå±æ€§ï¼Œä½¿ç”¨é»˜è®¤çš„ç‰¹å¾å¤„ç†)
    dataset = AdultDataset(
        protected_attribute_names=['sex'],
        privileged_classes=[['Male']],
        categorical_features=['workclass', 'education', 'marital-status',
                             'occupation', 'relationship', 'race', 'native-country'],
        features_to_drop=['fnlwgt']  # ç§»é™¤é‡‡æ ·æƒé‡
    )

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    X_full = dataset.features
    y_full = dataset.labels.ravel()
    sensitive_full = dataset.protected_attributes.ravel()

    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"  æ€»æ ·æœ¬æ•°: {len(X_full)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X_full.shape[1]}")
    print(f"  æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_full.astype(int))}")
    print(f"  æ•æ„Ÿå±æ€§åˆ†å¸ƒ: {np.bincount(sensitive_full.astype(int))}")

    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† (70/30)
    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
        X_full, y_full, sensitive_full,
        test_size=0.3,
        random_state=42,
        stratify=y_full
    )

    print(f"\nâœ“ æ•°æ®åˆ†å‰²å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"âœ“ ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")

    n_features = X_train.shape[1]

except Exception as e:
    print(f"\nâŒ åŠ è½½Adultæ•°æ®é›†å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…aif360: pip install aif360")
    sys.exit(1)

# ============================================================================
# æ­¥éª¤2: å®éªŒé…ç½®ï¼ˆæŒ‰è®ºæ–‡è®¾ç½®ï¼‰
# ============================================================================
print("\n" + "â–¶"*35)
print("æ­¥éª¤2: å®éªŒé…ç½®")
print("â–¶"*35)

# ä½¿ç”¨è®ºæ–‡ä¸­çš„é…ç½®
METHODS = ['Baseline', 'Reweighing']  # è®ºæ–‡ä¸­çš„ä¸¤ä¸ªä»£è¡¨æ€§æ–¹æ³•
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]  # è®ºæ–‡ä½¿ç”¨10ä¸ªï¼Œè¿™é‡Œç”¨5ä¸ª
EPOCHS = 50  # è®ºæ–‡ä½¿ç”¨50è½®
MODEL_WIDTH = 2  # è®ºæ–‡ä¸­çš„æ ‡å‡†å®½åº¦
DIBS_STEPS = 5000  # è®ºæ–‡ä½¿ç”¨10000ï¼Œè¿™é‡Œç”¨5000

print(f"\nå®éªŒé…ç½®:")
print(f"  æ–¹æ³•: {METHODS}")
print(f"  Alphaå€¼: {ALPHA_VALUES} ({len(ALPHA_VALUES)}ä¸ª)")
print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
print(f"  æ¨¡å‹å®½åº¦: {MODEL_WIDTH}")
print(f"  DiBSè¿­ä»£: {DIBS_STEPS}")
print(f"  æ€»é…ç½®æ•°: {len(METHODS)} Ã— {len(ALPHA_VALUES)} = {len(METHODS) * len(ALPHA_VALUES)}")

# ============================================================================
# æ­¥éª¤3: æ•°æ®æ”¶é›†
# ============================================================================
print("\n" + "â–¶"*35)
print("æ­¥éª¤3: æ•°æ®æ”¶é›†ï¼ˆå¤ç°è®ºæ–‡å®éªŒï¼‰")
print("â–¶"*35)

results = []
total_configs = len(METHODS) * len(ALPHA_VALUES)
current_config = 0

for method_name in METHODS:
    for alpha in ALPHA_VALUES:
        current_config += 1
        config_start = time.time()

        print(f"\n[{current_config}/{total_configs}] {method_name}, Î±={alpha:.2f}")

        try:
            # åº”ç”¨å…¬å¹³æ€§æ–¹æ³•
            method = get_fairness_method(method_name, alpha, sensitive_attr='sex')
            X_transformed, y_transformed = method.fit_transform(
                X_train, y_train, sensitive_train
            )

            # è®­ç»ƒæ¨¡å‹
            model = FFNN(input_dim=n_features, width=MODEL_WIDTH)
            trainer = ModelTrainer(model, device=device, lr=0.001)

            print(f"  è®­ç»ƒæ¨¡å‹ï¼ˆ{EPOCHS}è½®ï¼‰...", end=' ', flush=True)
            trainer.train(
                X_transformed, y_transformed,
                epochs=EPOCHS,
                batch_size=256,
                verbose=False
            )
            print(f"âœ“")

            # è®¡ç®—æŒ‡æ ‡
            calculator = MetricsCalculator(trainer, sensitive_attr='sex')

            print(f"  è®¡ç®—æŒ‡æ ‡...", end=' ', flush=True)

            # åŸå§‹æ•°æ®é›†æŒ‡æ ‡
            dataset_metrics = calculator.compute_all_metrics(
                X_train, y_train, sensitive_train, phase='D'
            )

            # è®­ç»ƒé›†æŒ‡æ ‡
            train_metrics = calculator.compute_all_metrics(
                X_transformed, y_transformed, sensitive_train, phase='Tr'
            )

            # æµ‹è¯•é›†æŒ‡æ ‡
            test_metrics = calculator.compute_all_metrics(
                X_test, y_test, sensitive_test, phase='Te'
            )
            print(f"âœ“")

            # åˆå¹¶ç»“æœ
            row = {
                'method': method_name,
                'alpha': alpha,
                'Width': MODEL_WIDTH
            }
            row.update(dataset_metrics)
            row.update(train_metrics)
            row.update(test_metrics)

            results.append(row)

            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            config_time = time.time() - config_start
            elapsed = time.time() - start_time
            remaining = (total_configs - current_config) * (elapsed / current_config)

            print(f"  ç»“æœ: Acc={test_metrics.get('Te_Acc', 0):.3f}, "
                  f"SPD={test_metrics.get('Te_SPD', 0):.3f}, "
                  f"DI={test_metrics.get('Te_DI', 0):.3f}")
            print(f"  â±  æœ¬æ¬¡: {config_time:.1f}ç§’ | "
                  f"å·²ç”¨: {elapsed/60:.1f}åˆ† | "
                  f"é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†")

        except Exception as e:
            print(f"\n  âœ— å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

# ä¿å­˜æ•°æ®
df = pd.DataFrame(results)
os.makedirs('data', exist_ok=True)
os.makedirs('results', exist_ok=True)
output_path = 'data/adult_training_data.csv'
df.to_csv(output_path, index=False)

print(f"\nâœ“ æ•°æ®æ”¶é›†å®Œæˆ")
print(f"  æ”¶é›†äº† {len(df)} ä¸ªæ•°æ®ç‚¹")
print(f"  ä¿å­˜åˆ°: {output_path}")

# æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
print(f"\næ•°æ®ç»Ÿè®¡:")
print(f"  Te_Acc: {df['Te_Acc'].min():.3f} ~ {df['Te_Acc'].max():.3f} (å‡å€¼={df['Te_Acc'].mean():.3f})")
if 'Te_SPD' in df.columns:
    print(f"  Te_SPD: {df['Te_SPD'].min():.3f} ~ {df['Te_SPD'].max():.3f} (å‡å€¼={df['Te_SPD'].mean():.3f})")
if 'Te_DI' in df.columns:
    print(f"  Te_DI:  {df['Te_DI'].min():.3f} ~ {df['Te_DI'].max():.3f} (å‡å€¼={df['Te_DI'].mean():.3f})")

# ============================================================================
# æ­¥éª¤4: DiBSå› æœå›¾å­¦ä¹ 
# ============================================================================
print("\n" + "â–¶"*35)
print("æ­¥éª¤4: DiBSå› æœå›¾å­¦ä¹ ")
print("â–¶"*35)

try:
    from utils.causal_discovery import CausalGraphLearner

    print(f"\nä½¿ç”¨DiBSå­¦ä¹ å› æœå›¾ï¼ˆ{DIBS_STEPS}æ­¥è¿­ä»£ï¼‰...")

    # å‡†å¤‡æ•°æ®
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'Width' in numeric_cols:
        numeric_cols.remove('Width')

    causal_data = df[numeric_cols]
    print(f"  å˜é‡æ•°: {len(numeric_cols)}")
    print(f"  æ•°æ®ç‚¹: {len(causal_data)}")

    # åˆ›å»ºå­¦ä¹ å™¨
    learner = CausalGraphLearner(
        n_vars=len(numeric_cols),
        n_steps=DIBS_STEPS,
        alpha=0.1,
        random_seed=42
    )

    # å­¦ä¹ å› æœå›¾
    print(f"\n  æ­£åœ¨è¿è¡ŒDiBSï¼ˆé¢„è®¡3-5åˆ†é’Ÿï¼‰...")
    causal_graph = learner.fit(causal_data, verbose=True)

    # åˆ†æç»“æœ
    edges = learner.get_edges(threshold=0.3)
    print(f"\nâœ“ DiBSå­¦ä¹ å®Œæˆ")
    print(f"  æ£€æµ‹åˆ° {len(edges)} æ¡å› æœè¾¹")

    # æ˜¾ç¤ºå…³é”®å› æœè¾¹
    if len(edges) > 0:
        print(f"\n  å‰10æ¡æœ€å¼ºå› æœè¾¹:")
        for i, (source, target, weight) in enumerate(edges[:10], 1):
            print(f"    {i}. {numeric_cols[source]} â†’ {numeric_cols[target]}: {weight:.3f}")

    # æ˜¾ç¤ºä¸alphaç›¸å…³çš„è¾¹
    alpha_idx = numeric_cols.index('alpha') if 'alpha' in numeric_cols else None
    if alpha_idx is not None:
        alpha_edges = [e for e in edges if e[0] == alpha_idx or e[1] == alpha_idx]
        if len(alpha_edges) > 0:
            print(f"\n  ä¸alphaç›¸å…³çš„å› æœè¾¹ (å…±{len(alpha_edges)}æ¡):")
            for i, (source, target, weight) in enumerate(alpha_edges[:10], 1):
                if source == alpha_idx:
                    print(f"    {i}. alpha â†’ {numeric_cols[target]}: {weight:.3f}")
                else:
                    print(f"    {i}. {numeric_cols[source]} â†’ alpha: {weight:.3f}")

    # ä¿å­˜å› æœå›¾
    graph_path = 'results/adult_causal_graph.npy'
    learner.save_graph(graph_path)
    print(f"\nâœ“ å› æœå›¾å·²ä¿å­˜åˆ°: {graph_path}")

except Exception as e:
    print(f"\nâš ï¸  DiBSå­¦ä¹ å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    causal_graph = None

# ============================================================================
# æ­¥éª¤5: DMLå› æœæ¨æ–­
# ============================================================================
if causal_graph is not None:
    print("\n" + "â–¶"*35)
    print("æ­¥éª¤5: DMLå› æœæ¨æ–­")
    print("â–¶"*35)

    try:
        from utils.causal_inference import CausalInferenceEngine

        print("\nä½¿ç”¨DMLä¼°è®¡å› æœæ•ˆåº”...")

        engine = CausalInferenceEngine(verbose=True)

        causal_effects = engine.analyze_all_edges(
            data=causal_data,
            causal_graph=causal_graph,
            var_names=numeric_cols,
            threshold=0.3
        )

        if causal_effects:
            effects_path = 'results/adult_causal_effects.csv'
            engine.save_results(effects_path)
            print(f"\nâœ“ å› æœæ•ˆåº”å·²ä¿å­˜åˆ°: {effects_path}")

            significant = engine.get_significant_effects()
            print(f"\nå› æœæ•ˆåº”ç»Ÿè®¡:")
            print(f"  æ€»è¾¹æ•°: {len(causal_effects)}")
            print(f"  ç»Ÿè®¡æ˜¾è‘—: {len(significant)}")

            if significant:
                print(f"\n  æ˜¾è‘—çš„å› æœæ•ˆåº” (å‰10ä¸ª):")
                for i, (edge, result) in enumerate(list(significant.items())[:10], 1):
                    print(f"    {i}. {edge}")
                    print(f"       ATE={result['ate']:.4f}, "
                          f"95% CI=[{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]")

    except Exception as e:
        print(f"\nâš ï¸  DMLå› æœæ¨æ–­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# æ­¥éª¤6: ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
# ============================================================================
print("\n" + "â–¶"*35)
print("æ­¥éª¤6: ä¸è®ºæ–‡ç»“æœå¯¹æ¯”")
print("â–¶"*35)

print("\nğŸ“Š æˆ‘ä»¬çš„å®éªŒç»“æœ:")
print("="*70)

for method_name in METHODS:
    method_df = df[df['method'] == method_name]
    if len(method_df) == 0:
        continue

    print(f"\n{method_name} æ–¹æ³•:")
    print(f"  AlphaèŒƒå›´: {method_df['alpha'].min():.2f} ~ {method_df['alpha'].max():.2f}")

    # åˆ†æalphaæ•ˆåº”
    alpha_0 = method_df[method_df['alpha'] == 0.0].iloc[0] if len(method_df[method_df['alpha'] == 0.0]) > 0 else None
    alpha_1 = method_df[method_df['alpha'] == 1.0].iloc[0] if len(method_df[method_df['alpha'] == 1.0]) > 0 else None

    if alpha_0 is not None and alpha_1 is not None:
        print(f"\n  Alphaæ•ˆåº”åˆ†æ (Î±: 0.0 â†’ 1.0):")

        metrics_to_check = [
            ('Te_Acc', 'æµ‹è¯•å‡†ç¡®ç‡'),
            ('Te_F1', 'æµ‹è¯•F1'),
            ('Te_SPD', 'ç»Ÿè®¡å¥‡å¶å·®'),
            ('Te_DI', 'Disparate Impact'),
            ('Te_AOD', 'å¹³å‡èµ”ç‡å·®')
        ]

        for metric_key, metric_name in metrics_to_check:
            if metric_key in alpha_0 and metric_key in alpha_1:
                val_0 = alpha_0[metric_key]
                val_1 = alpha_1[metric_key]
                change = val_1 - val_0
                pct_change = (change / val_0 * 100) if val_0 != 0 else 0

                print(f"    {metric_name:15s}: {val_0:6.3f} â†’ {val_1:6.3f} "
                      f"(Î”={change:+.3f}, {pct_change:+.1f}%)")

print("\n" + "="*70)
print("\nğŸ“š è®ºæ–‡ä¸­çš„å…¸å‹å‘ç°ï¼ˆå‚è€ƒï¼‰:")
print("="*70)
print("""
æ ¹æ®è®ºæ–‡ "Causality-Aided Trade-off Analysis for Machine Learning Fairness" (ASE 2023):

1. Reweighingæ–¹æ³•çš„å…¸å‹æ•ˆæœ:
   - å…¬å¹³æ€§æŒ‡æ ‡(SPD, DI)æ˜¾è‘—æ”¹å–„
   - å‡†ç¡®ç‡é€šå¸¸æœ‰è½»å¾®ä¸‹é™ (1-3%)
   - å­˜åœ¨accuracy vs fairnessçš„æƒè¡¡

2. å› æœå›¾å­¦ä¹ å‘ç°:
   - alphaå‚æ•° â†’ å…¬å¹³æ€§æŒ‡æ ‡ (ç›´æ¥å› æœè·¯å¾„)
   - æ–¹æ³•å‚æ•° â†’ æ•°æ®é›†æŒ‡æ ‡ â†’ è®­ç»ƒé›†æŒ‡æ ‡ â†’ æµ‹è¯•é›†æŒ‡æ ‡
   - è®­ç»ƒé›†æŒ‡æ ‡æ˜¯æƒè¡¡çš„ä¸»è¦åŸå›  (æ¯”æµ‹è¯•é›†æŒ‡æ ‡æ›´é¢‘ç¹)

3. æƒè¡¡æ£€æµ‹:
   - æ£€æµ‹åˆ°accuracy vs fairnessæƒè¡¡
   - æ£€æµ‹åˆ°fairness vs robustnessæƒè¡¡
   - ç®—æ³•1æˆåŠŸè¯†åˆ«æƒè¡¡æ¨¡å¼

å¯¹æ¯”è¦ç‚¹:
  âœ“ æ£€æŸ¥Reweighingæ˜¯å¦æ”¹å–„äº†å…¬å¹³æ€§æŒ‡æ ‡
  âœ“ æ£€æŸ¥æ˜¯å¦å­˜åœ¨accuracyä¸‹é™
  âœ“ æ£€æŸ¥alphaå‚æ•°æ˜¯å¦æœ‰æ˜¾è‘—å½±å“
  âœ“ æ£€æŸ¥å› æœå›¾æ˜¯å¦å‘ç°äº†é¢„æœŸçš„å› æœè·¯å¾„
""")

# ============================================================================
# æ€»ç»“
# ============================================================================
total_time = time.time() - start_time

print("\n" + "="*70)
print(" "*20 + "å®éªŒå®Œæˆï¼")
print("="*70)

print(f"\nâ±  æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")

print(f"\nâœ… å®Œæˆçš„ä»»åŠ¡:")
print(f"  1. âœ“ åŠ è½½AdultçœŸå®æ•°æ®é›†")
print(f"  2. âœ“ è®­ç»ƒ {len(df)} ä¸ªæ¨¡å‹é…ç½®")
print(f"  3. âœ“ DiBSå› æœå›¾å­¦ä¹ ")
print(f"  4. âœ“ DMLå› æœæ¨æ–­")
print(f"  5. âœ“ ä¸è®ºæ–‡ç»“æœå¯¹æ¯”")

print(f"\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
print(f"  - {output_path}")
if causal_graph is not None:
    print(f"  - results/adult_causal_graph.npy")
    if 'causal_effects' in locals() and causal_effects:
        print(f"  - results/adult_causal_effects.csv")

print(f"\nğŸ¯ éªŒè¯è¦ç‚¹:")
print(f"  1. æ•°æ®è§„æ¨¡: {len(df)} ä¸ªæ•°æ®ç‚¹ (è®ºæ–‡ä½¿ç”¨~60-70ä¸ª/æ•°æ®é›†)")
print(f"  2. æ–¹æ³•å®ç°: ä¸è®ºæ–‡ç›¸åŒçš„Reweighingå®ç°")
print(f"  3. æŒ‡æ ‡è®¡ç®—: ä¸è®ºæ–‡ç›¸åŒçš„å…¬å¹³æ€§å’Œæ€§èƒ½æŒ‡æ ‡")
print(f"  4. å› æœåˆ†æ: ä½¿ç”¨ç›¸åŒçš„DiBSå’ŒDMLæ–¹æ³•")

print("\n" + "="*70 + "\n")
