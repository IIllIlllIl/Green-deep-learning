# ä»£ç æ•´ä½“æµç¨‹è¯¦è§£

**æ–‡æ¡£ç›®çš„**: å…¨é¢ç†è§£é¡¹ç›®ä»£ç çš„æ‰§è¡Œæµç¨‹ã€å„é˜¶æ®µç›®çš„å’Œæ€§èƒ½ç‰¹å¾
**æ›´æ–°æ—¶é—´**: 2025-12-21
**åŸºäºå®éªŒ**: Adultæ•°æ®é›†å®Œæ•´å› æœåˆ†æ

---

## ğŸ“‹ ç›®å½•

1. [æ•´ä½“æ¶æ„æ¦‚è§ˆ](#æ•´ä½“æ¶æ„æ¦‚è§ˆ)
2. [äº”å¤§æ‰§è¡Œé˜¶æ®µ](#äº”å¤§æ‰§è¡Œé˜¶æ®µ)
3. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#æ ¸å¿ƒæ¨¡å—è¯¦è§£)
4. [æ•°æ®æµè½¬è¿‡ç¨‹](#æ•°æ®æµè½¬è¿‡ç¨‹)
5. [æ€§èƒ½ç‰¹å¾åˆ†æ](#æ€§èƒ½ç‰¹å¾åˆ†æ)
6. [å…³é”®ç®—æ³•åŸç†](#å…³é”®ç®—æ³•åŸç†)

---

## æ•´ä½“æ¶æ„æ¦‚è§ˆ

### ç³»ç»Ÿè®¾è®¡ç†å¿µ

```
è¾“å…¥: çœŸå®æ•°æ®é›† (Adult, COMPAS, German)
  â†“
[é˜¶æ®µ1] æ•°æ®æ”¶é›† â†’ è®­ç»ƒå¤šä¸ªé…ç½®çš„æ¨¡å‹ï¼Œæ”¶é›†æ€§èƒ½å’Œå…¬å¹³æ€§æŒ‡æ ‡
  â†“
[é˜¶æ®µ2] å› æœå›¾å­¦ä¹  â†’ ä½¿ç”¨DiBSå‘ç°æŒ‡æ ‡ä¹‹é—´çš„å› æœå…³ç³»
  â†“
[é˜¶æ®µ3] å› æœæ¨æ–­ â†’ ä½¿ç”¨DMLä¼°è®¡å› æœæ•ˆåº”å¼ºåº¦
  â†“
[é˜¶æ®µ4] æƒè¡¡æ£€æµ‹ â†’ è¯†åˆ«æŒ‡æ ‡é—´çš„æƒè¡¡æ¨¡å¼ï¼ˆå¦‚å‡†ç¡®ç‡vså…¬å¹³æ€§ï¼‰
  â†“
è¾“å‡º: å› æœå›¾ã€å› æœæ•ˆåº”ã€æƒè¡¡æŠ¥å‘Š
```

### æ ¸å¿ƒæ€æƒ³

**ä»è§‚æµ‹æ•°æ®ä¸­å‘ç°å› æœå…³ç³»**ï¼š
- ä¸æ˜¯ç®€å•çš„ç›¸å…³æ€§åˆ†æï¼ˆcorrelationï¼‰
- è€Œæ˜¯å› æœå…³ç³»å‘ç°ï¼ˆcausationï¼‰
- ç›®æ ‡ï¼šç†è§£ä¸ºä»€ä¹ˆä¼šæœ‰accuracy vs fairnessæƒè¡¡

### æŠ€æœ¯æ ˆ

```
æ•°æ®å±‚:    Pandas, NumPy, AIF360
æ¨¡å‹å±‚:    PyTorch (FFNNç¥ç»ç½‘ç»œ)
å› æœå±‚:    JAX (DiBS), EconML (DML)
å…¬å¹³æ€§å±‚:  AIF360 (Reweighingç­‰æ–¹æ³•)
```

---

## äº”å¤§æ‰§è¡Œé˜¶æ®µ

### é˜¶æ®µ0: æ•°æ®åŠ è½½ä¸å‡†å¤‡

#### ä»£ç ä½ç½®
```python
# demo_adult_full_analysis.py: ç¬¬68-136è¡Œ
```

#### æ‰§è¡Œæµç¨‹
```python
1. åŠ è½½æ•°æ®é›†
   from aif360.datasets import AdultDataset
   dataset = AdultDataset(
       protected_attribute_names=['sex'],
       privileged_classes=[['Male']],
       categorical_features=[...],
       features_to_drop=['fnlwgt']
   )

2. æå–ç‰¹å¾å’Œæ ‡ç­¾
   X_full = dataset.features          # (45222, 102) - ç‰¹å¾çŸ©é˜µ
   y_full = dataset.labels.ravel()    # (45222,) - æ ‡ç­¾å‘é‡
   sensitive_full = dataset.protected_attributes.ravel()  # (45222,) - æ•æ„Ÿå±æ€§

3. æ•°æ®åˆ†å‰²
   X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
       X_full, y_full, sensitive_full,
       test_size=0.3,       # 70% è®­ç»ƒï¼Œ30% æµ‹è¯•
       random_state=42,     # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
       stratify=y_full      # åˆ†å±‚æŠ½æ ·ä¿æŒæ ‡ç­¾åˆ†å¸ƒ
   )
   # è®­ç»ƒé›†: 31,655 æ ·æœ¬
   # æµ‹è¯•é›†: 13,567 æ ·æœ¬

4. ç‰¹å¾æ ‡å‡†åŒ–
   scaler = StandardScaler()
   X_train = scaler.fit_transform(X_train)  # å‡å€¼0ï¼Œæ–¹å·®1
   X_test = scaler.transform(X_test)        # ä½¿ç”¨è®­ç»ƒé›†çš„å‡å€¼å’Œæ–¹å·®

5. ä¿å­˜æ£€æŸ¥ç‚¹
   save_checkpoint({
       'X_train': X_train,
       'X_test': X_test,
       ...
   }, 'results/adult_data_checkpoint.pkl')
```

#### è€—æ—¶ç‰¹å¾
- **æ—¶é—´**: çº¦10-20ç§’
- **ä¸»è¦è€—æ—¶**:
  - CSVæ–‡ä»¶è¯»å–: 5-10ç§’
  - One-Hotç¼–ç : 3-5ç§’
  - æ•°æ®åˆ†å‰²å’Œæ ‡å‡†åŒ–: 2-3ç§’
- **å†…å­˜å ç”¨**: ~200 MBï¼ˆåŸå§‹æ•°æ®ï¼‰

#### è¾“å‡ºæ•°æ®
```
è®­ç»ƒé›†: (31655, 102) æµ®ç‚¹æ•°çŸ©é˜µ
æµ‹è¯•é›†: (13567, 102) æµ®ç‚¹æ•°çŸ©é˜µ
æ ‡ç­¾: äºŒå€¼ (0=â‰¤50K, 1=>50K)
æ•æ„Ÿå±æ€§: äºŒå€¼ (0=Female, 1=Male)
```

---

### é˜¶æ®µ1: æ•°æ®æ”¶é›†ï¼ˆè®­ç»ƒå¤šä¸ªæ¨¡å‹é…ç½®ï¼‰

#### ä»£ç ä½ç½®
```python
# demo_adult_full_analysis.py: ç¬¬138-204è¡Œ
# utils/model.py: FFNNç±»å’ŒModelTrainerç±»
# utils/metrics.py: MetricsCalculatorç±»
# utils/fairness_methods.py: get_fairness_methodå‡½æ•°
```

#### æ‰§è¡Œæµç¨‹

**1.1 é…ç½®ç”Ÿæˆ**
```python
METHODS = ['Baseline', 'Reweighing']  # 2ä¸ªæ–¹æ³•
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]  # 5ä¸ªalphaå€¼
# æ€»é…ç½®æ•°: 2 Ã— 5 = 10ä¸ª
```

**1.2 å¯¹æ¯ä¸ªé…ç½®çš„å¤„ç†å¾ªç¯**
```python
for method_name in METHODS:
    for alpha in ALPHA_VALUES:
        # æ­¥éª¤A: åº”ç”¨å…¬å¹³æ€§æ–¹æ³•
        method = get_fairness_method(method_name, alpha, sensitive_attr='sex')
        X_transformed, y_transformed = method.fit_transform(
            X_train, y_train, sensitive_train
        )

        # æ­¥éª¤B: åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        model = FFNN(input_dim=102, width=2)  # 5å±‚ç¥ç»ç½‘ç»œ
        trainer = ModelTrainer(model, device='cuda', lr=0.001)
        trainer.train(
            X_transformed, y_transformed,
            epochs=50,        # 50è½®è®­ç»ƒ
            batch_size=256,   # æ¯æ‰¹256ä¸ªæ ·æœ¬
            verbose=False     # ä¸æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
        )

        # æ­¥éª¤C: è®¡ç®—æŒ‡æ ‡ï¼ˆ3ä¸ªé˜¶æ®µï¼‰
        calculator = MetricsCalculator(trainer, sensitive_attr='sex')

        # C1. æ•°æ®é›†æŒ‡æ ‡ï¼ˆåŸå§‹æ•°æ®çš„å…¬å¹³æ€§ï¼‰
        dataset_metrics = calculator.compute_all_metrics(
            X_train, y_train, sensitive_train, phase='D'
        )
        # è¾“å‡º: D_Acc, D_F1, D_SPD, D_DI, D_AOD, ...

        # C2. è®­ç»ƒé›†æŒ‡æ ‡ï¼ˆå˜æ¢åæ•°æ®çš„æ€§èƒ½ï¼‰
        train_metrics = calculator.compute_all_metrics(
            X_transformed, y_transformed, sensitive_train, phase='Tr'
        )
        # è¾“å‡º: Tr_Acc, Tr_F1, Tr_SPD, Tr_DI, Tr_AOD, ...

        # C3. æµ‹è¯•é›†æŒ‡æ ‡ï¼ˆæ¨¡å‹æ³›åŒ–æ€§èƒ½ï¼‰
        test_metrics = calculator.compute_all_metrics(
            X_test, y_test, sensitive_test, phase='Te'
        )
        # è¾“å‡º: Te_Acc, Te_F1, Te_SPD, Te_DI, Te_AOD, ...

        # æ­¥éª¤D: æ”¶é›†ç»“æœ
        row = {
            'method': method_name,
            'alpha': alpha,
            'Width': 2,
            **dataset_metrics,   # D_å¼€å¤´çš„æŒ‡æ ‡
            **train_metrics,     # Tr_å¼€å¤´çš„æŒ‡æ ‡
            **test_metrics       # Te_å¼€å¤´çš„æŒ‡æ ‡
        }
        results.append(row)
```

**1.3 æŒ‡æ ‡è¯¦è§£**

| æŒ‡æ ‡ç±»åˆ« | å‰ç¼€ | è®¡ç®—ä½ç½® | å«ä¹‰ |
|---------|------|---------|------|
| **æ€§èƒ½æŒ‡æ ‡** |
| Accuracy | `_Acc` | å„é˜¶æ®µ | é¢„æµ‹å‡†ç¡®ç‡: (TP+TN)/(TP+TN+FP+FN) |
| F1 Score | `_F1` | å„é˜¶æ®µ | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ |
| **å…¬å¹³æ€§æŒ‡æ ‡** |
| Statistical Parity Difference | `_SPD` | å„é˜¶æ®µ | P(Y=1\|Female) - P(Y=1\|Male) |
| Disparate Impact | `_DI` | å„é˜¶æ®µ | P(Y=1\|Female) / P(Y=1\|Male) |
| Average Odds Difference | `_AOD` | å„é˜¶æ®µ | TPRå’ŒFPRå·®å¼‚çš„å¹³å‡ |
| Consistency | `_Cons` | å„é˜¶æ®µ | ç›¸ä¼¼æ ·æœ¬é¢„æµ‹ä¸€è‡´æ€§ |
| **é²æ£’æ€§æŒ‡æ ‡** |
| FGSM Attack | `A_FGSM` | æµ‹è¯•é›† | å¯¹æŠ—æ ·æœ¬æ”»å‡»åå‡†ç¡®ç‡ä¸‹é™ |
| PGD Attack | `A_PGD` | æµ‹è¯•é›† | æ›´å¼ºå¯¹æŠ—æ”»å‡»çš„å‡†ç¡®ç‡ä¸‹é™ |

**1.4 ä¿å­˜ç»“æœ**
```python
df = pd.DataFrame(results)
df.to_csv('data/adult_training_data.csv', index=False)
```

#### è€—æ—¶ç‰¹å¾

**æ€»æ—¶é•¿**: çº¦60åˆ†é’Ÿï¼ˆ10é…ç½®ï¼‰

**å•é…ç½®è€—æ—¶åˆ†è§£**:
```
åº”ç”¨å…¬å¹³æ€§æ–¹æ³•:     2-5ç§’
  - Baseline:       å‡ ä¹ç¬é—´ï¼ˆä¸åšä»»ä½•å¤„ç†ï¼‰
  - Reweighing:     2-5ç§’ï¼ˆè®¡ç®—æ ·æœ¬æƒé‡ï¼‰

æ¨¡å‹è®­ç»ƒ (50è½®):    300-320ç§’ âš¡ æœ€è€—æ—¶
  - å‰å‘ä¼ æ’­:       150ç§’
  - åå‘ä¼ æ’­:       150ç§’
  - å‚æ•°æ›´æ–°:       10-20ç§’

æŒ‡æ ‡è®¡ç®— (Ã—3):      30-40ç§’
  - æ•°æ®é›†æŒ‡æ ‡:     10ç§’
  - è®­ç»ƒé›†æŒ‡æ ‡:     10-15ç§’
  - æµ‹è¯•é›†æŒ‡æ ‡:     10-15ç§’

æ€»è®¡æ¯é…ç½®:         ~360ç§’ (6åˆ†é’Ÿ)
```

**GPUåŠ é€Ÿæ•ˆæœ**:
- CPUè®­ç»ƒ: ~20åˆ†é’Ÿ/é…ç½®
- GPUè®­ç»ƒ: ~6åˆ†é’Ÿ/é…ç½®
- **åŠ é€Ÿæ¯”**: 3.3Ã—

#### è¾“å‡ºæ•°æ®

**CSVæ–‡ä»¶ç»“æ„** (`data/adult_training_data.csv`):
```
åˆ—æ•°: 24åˆ—
  - é…ç½®åˆ—: method, alpha, Width (3åˆ—)
  - æ•°æ®é›†æŒ‡æ ‡: D_Acc, D_F1, D_SPD, D_DI, D_AOD, D_Cons, D_TI (7åˆ—)
  - è®­ç»ƒé›†æŒ‡æ ‡: Tr_Acc, Tr_F1, Tr_SPD, Tr_DI, Tr_AOD, Tr_Cons, Tr_TI (7åˆ—)
  - æµ‹è¯•é›†æŒ‡æ ‡: Te_Acc, Te_F1, Te_SPD, Te_DI, Te_AOD, Te_Cons, Te_TI (7åˆ—)

è¡Œæ•°: 10è¡Œï¼ˆ10ä¸ªé…ç½®ï¼‰

ç¤ºä¾‹æ•°æ®:
method,alpha,Width,D_Acc,D_F1,...,Te_Acc,Te_F1,...
Baseline,0.0,2,0.829,0.531,...,0.830,0.531,...
Baseline,0.25,2,0.829,0.647,...,0.846,0.647,...
...
```

**å…³é”®è§‚å¯Ÿ**:
- D_SPDå’ŒD_DIåœ¨æ‰€æœ‰é…ç½®ä¸­ç›¸åŒï¼ˆåŸå§‹æ•°æ®ä¸å˜ï¼‰
- Te_SPDå’ŒTe_DIåœ¨æ‰€æœ‰é…ç½®ä¸­ç›¸åŒï¼ˆæµ‹è¯•é›†ä¸å¤„ç†ï¼‰
- Tr_Accå’ŒTe_Accæœ‰å·®å¼‚ï¼ˆè®­ç»ƒ vs æ³›åŒ–ï¼‰

---

### é˜¶æ®µ2: DiBSå› æœå›¾å­¦ä¹ 

#### ä»£ç ä½ç½®
```python
# demo_adult_full_analysis.py: ç¬¬206-285è¡Œ
# utils/causal_discovery.py: CausalGraphLearnerç±»
```

#### ç®—æ³•åŸç†: DiBS (Differentiable Bayesian Structure Learning)

**æ ¸å¿ƒæ€æƒ³**:
```
ç›®æ ‡: ä»è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ å› æœå›¾ï¼ˆæœ‰å‘æ— ç¯å›¾ DAGï¼‰

è¾“å…¥: æ•°æ®çŸ©é˜µ X âˆˆ R^{nÃ—p}
      n = æ ·æœ¬æ•° (10ä¸ªé…ç½®)
      p = å˜é‡æ•° (19ä¸ªæŒ‡æ ‡)

è¾“å‡º: é‚»æ¥çŸ©é˜µ G âˆˆ {0,1}^{pÃ—p}
      G[i,j] = 1 è¡¨ç¤ºå­˜åœ¨å› æœè¾¹ i â†’ j
```

**æ•°å­¦æ¨¡å‹**:
```
1. ç»“æ„æ–¹ç¨‹æ¨¡å‹ (SEM):
   X_j = âˆ‘_{iâˆˆPa(j)} w_{ij} X_i + Îµ_j

   å…¶ä¸­:
   - Pa(j): å˜é‡jçš„çˆ¶èŠ‚ç‚¹é›†åˆ
   - w_{ij}: å› æœæ•ˆåº”å¼ºåº¦
   - Îµ_j: å™ªå£°é¡¹

2. ä¼˜åŒ–ç›®æ ‡:
   max L(G, Î¸ | X) = log P(X | G, Î¸) + log P(G)

   å…¶ä¸­:
   - P(X | G, Î¸): æ•°æ®ä¼¼ç„¶ï¼ˆæ•°æ®æ‹Ÿåˆåº¦ï¼‰
   - P(G): å›¾å…ˆéªŒï¼ˆç¨€ç–æ€§çº¦æŸï¼‰

3. DAGçº¦æŸ:
   tr(e^{GâŠ™G}) = p  (æ— ç¯çº¦æŸ)
```

#### æ‰§è¡Œæµç¨‹

**2.1 æ•°æ®å‡†å¤‡**
```python
# è¯»å–é˜¶æ®µ1ä¿å­˜çš„æ•°æ®
df = pd.read_csv('data/adult_training_data.csv')

# æå–æ•°å€¼åˆ—ï¼ˆå»é™¤method, alpha, Widthï¼‰
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if 'Width' in numeric_cols:
    numeric_cols.remove('Width')

# æœ€ç»ˆå˜é‡åˆ—è¡¨ï¼ˆ19ä¸ªï¼‰:
# ['D_DI', 'D_SPD', 'D_AOD', 'D_Cons', 'D_TI',      # æ•°æ®é›†æŒ‡æ ‡ 5ä¸ª
#  'Tr_Acc', 'Tr_F1', 'Tr_DI', 'Tr_SPD', 'Tr_AOD', 'Tr_Cons', 'Tr_TI',  # è®­ç»ƒé›† 7ä¸ª
#  'Te_Acc', 'Te_F1', 'Te_DI', 'Te_SPD', 'Te_AOD', 'Te_Cons', 'Te_TI']  # æµ‹è¯•é›† 7ä¸ª

causal_data = df[numeric_cols]  # (10, 19) çŸ©é˜µ
```

**2.2 åˆ›å»ºDiBSå­¦ä¹ å™¨**
```python
from utils.causal_discovery import CausalGraphLearner

learner = CausalGraphLearner(
    n_vars=19,         # å˜é‡æ•°
    n_steps=3000,      # ä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼ˆåŸè®ºæ–‡å¯èƒ½ç”¨5000-10000ï¼‰
    alpha=0.1,         # ç¨€ç–æ€§æƒ©ç½šç³»æ•°ï¼ˆè¶Šå¤§å›¾è¶Šç¨€ç–ï¼‰
    random_seed=42     # éšæœºç§å­
)
```

**2.3 DiBSè¿­ä»£ä¼˜åŒ–**
```python
causal_graph = learner.fit(causal_data, verbose=True)

# å†…éƒ¨æ‰§è¡Œæµç¨‹:
for step in range(3000):
    # æ­¥éª¤1: é‡‡æ ·å€™é€‰å›¾
    G_candidate = sample_graph_from_posterior()

    # æ­¥éª¤2: è®¡ç®—å›¾çš„å¾—åˆ†
    score = compute_score(G_candidate, causal_data)
    # score = log_likelihood(data | G) - alpha * num_edges(G)

    # æ­¥éª¤3: æ›´æ–°åéªŒåˆ†å¸ƒ
    update_posterior(G_candidate, score)

    # æ­¥éª¤4: æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å›¾å‚æ•°
    gradients = compute_gradients(G_candidate)
    update_parameters(gradients)

    # æ¯500æ­¥æ‰“å°è¿›åº¦ï¼ˆå¯é€‰ï¼‰
    if step % 500 == 0:
        print(f"Step {step}/3000, Score: {score:.4f}")
```

**2.4 è¾¹ç­›é€‰ä¸åå¤„ç†**
```python
# DiBSè¾“å‡ºæ˜¯æ¦‚ç‡çŸ©é˜µï¼Œéœ€è¦é˜ˆå€¼åŒ–
edges = learner.get_edges(threshold=0.3)
# edges: [(source_idx, target_idx, weight), ...]

# ç¤ºä¾‹è¾“å‡º:
# [(10, 12, 0.300),  # Tr_F1 â†’ Te_Acc, æƒé‡0.3
#  (8, 0, 0.300),    # Tr_DI â†’ alpha (å®é™…æ˜¯åå‘ç´¢å¼•ï¼Œéœ€è¦æ˜ å°„)
#  ...]

# ä¿å­˜å› æœå›¾
learner.save_graph('results/adult_causal_graph.npy')
# ä¿å­˜çš„æ˜¯å®Œæ•´çš„ (19, 19) æ¦‚ç‡çŸ©é˜µ

# ä¿å­˜ç­›é€‰åçš„è¾¹
import pickle
with open('results/adult_causal_edges.pkl', 'wb') as f:
    pickle.dump({
        'edges': edges,
        'numeric_cols': numeric_cols
    }, f)
```

#### è€—æ—¶ç‰¹å¾

**æ€»æ—¶é•¿**: çº¦1.6åˆ†é’Ÿ

**æ€§èƒ½åˆ†è§£**:
```
æ•°æ®å‡†å¤‡:           <1ç§’
DiBSåˆå§‹åŒ–:         2-3ç§’
è¿­ä»£ä¼˜åŒ–:           90ç§’ âš¡ æ ¸å¿ƒè€—æ—¶
  - æ¯æ­¥è€—æ—¶:       ~30æ¯«ç§’
  - 3000æ­¥æ€»è®¡:     90ç§’
è¾¹ç­›é€‰ä¸ä¿å­˜:       1-2ç§’

æ€»è®¡:               ~96ç§’ (1.6åˆ†é’Ÿ)
```

**å…³é”®æ€§èƒ½å› ç´ **:
1. **æ ·æœ¬æ•°**: 10ä¸ªæ ·æœ¬ â†’ å¿«é€Ÿ
   - å¦‚æœ100ä¸ªæ ·æœ¬ â†’ çº¦15-20åˆ†é’Ÿ
2. **å˜é‡æ•°**: 19ä¸ªå˜é‡ â†’ é€‚ä¸­
   - å¦‚æœ50ä¸ªå˜é‡ â†’ çº¦10-15åˆ†é’Ÿ
3. **è¿­ä»£æ¬¡æ•°**: 3000æ­¥ â†’ ä¼˜åŒ–ç‰ˆ
   - è®ºæ–‡å¯èƒ½ç”¨5000-10000æ­¥ â†’ æ…¢2-3å€
4. **JAXç¼–è¯‘**: é¦–æ¬¡è¿è¡Œæ…¢ï¼Œåç»­å¿«
   - é¦–æ¬¡: ~3ç§’ç¼–è¯‘
   - åç»­: å³æ—¶æ‰§è¡Œ

**ä¼˜åŒ–ç­–ç•¥**:
- âœ… å‡å°‘è¿­ä»£: 5000 â†’ 3000 (é€Ÿåº¦+40%)
- âœ… ä½¿ç”¨JAX: æ¯”NumPyå¿«2-3å€
- âš ï¸ å°æ ·æœ¬: 10ä¸ªæ ·æœ¬å¾ˆå¿«ï¼Œä½†ç»Ÿè®¡åŠŸæ•ˆä½

#### è¾“å‡ºæ•°æ®

**1. å®Œæ•´å› æœå›¾** (`results/adult_causal_graph.npy`):
```python
# å½¢çŠ¶: (19, 19)
# ç±»å‹: float32
# å«ä¹‰: G[i,j] è¡¨ç¤º i â†’ j çš„æ¦‚ç‡

ç¤ºä¾‹:
[[0.01, 0.02, ..., 0.00],   # å˜é‡0 â†’ å…¶ä»–å˜é‡
 [0.00, 0.05, ..., 0.30],   # å˜é‡1 â†’ å…¶ä»–å˜é‡
 ...
 [0.30, 0.00, ..., 0.10]]   # å˜é‡18 â†’ å…¶ä»–å˜é‡
```

**2. ç­›é€‰åçš„è¾¹** (`results/adult_causal_edges.pkl`):
```python
{
    'edges': [
        (10, 12, 0.300),  # Tr_F1 â†’ Te_Acc
        (8, 0, 0.300),    # Tr_DI â†’ alpha (éœ€è¦æ˜ å°„åˆ°å˜é‡å)
        (8, 6, 0.300),    # Tr_DI â†’ Tr_F1
        (12, 5, 0.300),   # Te_Acc â†’ Tr_Acc
        (12, 13, 0.300),  # Te_Acc â†’ Te_F1
        (13, 12, 0.300)   # Te_F1 â†’ Te_Acc
    ],
    'numeric_cols': ['D_DI', 'D_SPD', ..., 'Te_TI']
}
```

**3. å›¾ç»Ÿè®¡**:
```
æ€»è¾¹æ•°(åŸå§‹):    38æ¡ (é˜ˆå€¼å‰)
ç­›é€‰å:          6æ¡ (é˜ˆå€¼â‰¥0.3)
å›¾å¯†åº¦:          0.111 (38 / (19Ã—18))
æ˜¯å¦ä¸ºDAG:       False (å­˜åœ¨ç¯è·¯ï¼Œå¦‚ Te_Acc â†” Te_F1)
```

---

### é˜¶æ®µ3: DMLå› æœæ¨æ–­

#### ä»£ç ä½ç½®
```python
# demo_adult_full_analysis.py: ç¬¬287-342è¡Œ
# utils/causal_inference.py: CausalInferenceEngineç±»
```

#### ç®—æ³•åŸç†: DML (Double Machine Learning)

**æ ¸å¿ƒæ€æƒ³**:
```
ç›®æ ‡: ä¼°è®¡å› æœæ•ˆåº” Ï„ = E[Y | do(X=x+1)] - E[Y | do(X=x)]

é—®é¢˜: ç®€å•å›å½’ Y ~ X ä¼šå—æ··æ·†å› ç´ å½±å“
è§£å†³: ä½¿ç”¨åŒé‡æœºå™¨å­¦ä¹ æ¶ˆé™¤æ··æ·†åå·®
```

**æ•°å­¦æ¨¡å‹**:
```
1. ç»“æ„æ–¹ç¨‹:
   Y = Ï„Â·X + g(Z) + Îµâ‚  (ç»“æœæ–¹ç¨‹)
   X = h(Z) + Îµâ‚‚        (å¤„ç†æ–¹ç¨‹)

   å…¶ä¸­:
   - Y: ç»“æœå˜é‡ (å¦‚ Te_Acc)
   - X: å¤„ç†å˜é‡ (å¦‚ Tr_F1)
   - Z: æ··æ·†å˜é‡ (å¦‚å…¶ä»–17ä¸ªæŒ‡æ ‡)
   - Ï„: å¹³å‡å› æœæ•ˆåº” (ATE)
   - g(Â·), h(Â·): æœªçŸ¥å‡½æ•°

2. DMLä¼°è®¡è¿‡ç¨‹:
   æ­¥éª¤1: ç”¨æœºå™¨å­¦ä¹ ä¼°è®¡ Ä(Z) å’Œ Ä¥(Z)
          Ä = E[Y | Z]  (ç”¨éšæœºæ£®æ—å›å½’)
          Ä¥ = E[X | Z]  (ç”¨éšæœºæ£®æ—å›å½’)

   æ­¥éª¤2: è®¡ç®—æ®‹å·®
          á»¸ = Y - Ä(Z)  (å»é™¤Zå¯¹Yçš„å½±å“)
          XÌƒ = X - Ä¥(Z)  (å»é™¤Zå¯¹Xçš„å½±å“)

   æ­¥éª¤3: å›å½’æ®‹å·®å¾—åˆ°ATE
          á»¸ = Ï„Â·XÌƒ + noise
          Ï„Ì‚ = (XÌƒáµ€XÌƒ)â»Â¹(XÌƒáµ€á»¸)  (æœ€å°äºŒä¹˜ä¼°è®¡)

3. ç½®ä¿¡åŒºé—´:
   CI = Ï„Ì‚ Â± 1.96 Ã— SE(Ï„Ì‚)  (95%ç½®ä¿¡åŒºé—´)
```

#### æ‰§è¡Œæµç¨‹

**3.1 åˆå§‹åŒ–å¼•æ“**
```python
from utils.causal_inference import CausalInferenceEngine

engine = CausalInferenceEngine(verbose=True)
# å†…éƒ¨ä½¿ç”¨EconMLçš„LinearDML
```

**3.2 åˆ†ææ‰€æœ‰è¾¹**
```python
causal_effects = engine.analyze_all_edges(
    data=causal_data,        # (10, 19) æ•°æ®çŸ©é˜µ
    causal_graph=causal_graph,  # (19, 19) æ¦‚ç‡çŸ©é˜µ
    var_names=numeric_cols,  # å˜é‡ååˆ—è¡¨
    threshold=0.3            # åªåˆ†ææƒé‡â‰¥0.3çš„è¾¹
)

# å¯¹æ¯æ¡è¾¹æ‰§è¡ŒDMLåˆ†æ
for (source_idx, target_idx, weight) in edges:
    source_name = numeric_cols[source_idx]  # å¦‚ 'Tr_F1'
    target_name = numeric_cols[target_idx]  # å¦‚ 'Te_Acc'

    # æå–æ•°æ®
    X = causal_data[source_name].values  # (10,) å¤„ç†å˜é‡
    Y = causal_data[target_name].values  # (10,) ç»“æœå˜é‡

    # ç¡®å®šæ··æ·†å˜é‡ï¼ˆé™¤Xå’ŒYå¤–çš„æ‰€æœ‰å˜é‡ï¼‰
    confounders = [col for col in numeric_cols
                   if col not in [source_name, target_name]]
    Z = causal_data[confounders].values  # (10, 17) æ··æ·†çŸ©é˜µ

    # æ‰§è¡ŒDMLä¼°è®¡
    result = engine.estimate_causal_effect(X, Y, Z)
```

**3.3 DMLè¯¦ç»†æ­¥éª¤**ï¼ˆä»¥ Tr_F1 â†’ Te_Acc ä¸ºä¾‹ï¼‰
```python
def estimate_causal_effect(X, Y, Z):
    """
    X: (10,) Tr_F1å€¼
    Y: (10,) Te_Accå€¼
    Z: (10, 2) æ··æ·†å˜é‡ [Tr_Acc, Tr_AOD] (ç¤ºä¾‹ï¼Œå®é™…æ›´å¤š)
    """

    # æ­¥éª¤1: äº¤å‰æ‹Ÿåˆï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    from sklearn.model_selection import KFold
    kf = KFold(n_splits=2)  # 2æŠ˜äº¤å‰éªŒè¯

    residuals_Y = []
    residuals_X = []

    for train_idx, test_idx in kf.split(X):
        # è®­ç»ƒé›†
        Z_train, Z_test = Z[train_idx], Z[test_idx]
        Y_train, Y_test = Y[train_idx], Y[test_idx]
        X_train, X_test = X[train_idx], X[test_idx]

        # æ‹Ÿåˆ E[Y|Z]
        model_Y = RandomForestRegressor(max_depth=3)
        model_Y.fit(Z_train, Y_train)
        Y_pred = model_Y.predict(Z_test)
        residuals_Y.append(Y_test - Y_pred)

        # æ‹Ÿåˆ E[X|Z]
        model_X = RandomForestRegressor(max_depth=3)
        model_X.fit(Z_train, X_train)
        X_pred = model_X.predict(Z_test)
        residuals_X.append(X_test - X_pred)

    # æ­¥éª¤2: åˆå¹¶æ®‹å·®
    á»¸ = np.concatenate(residuals_Y)  # (10,)
    XÌƒ = np.concatenate(residuals_X)  # (10,)

    # æ­¥éª¤3: å›å½’æ®‹å·®
    from sklearn.linear_model import LinearRegression
    reg = LinearRegression()
    reg.fit(XÌƒ.reshape(-1, 1), á»¸)

    ate = reg.coef_[0]  # å¹³å‡å› æœæ•ˆåº”

    # æ­¥éª¤4: è®¡ç®—æ ‡å‡†è¯¯
    predictions = reg.predict(XÌƒ.reshape(-1, 1))
    residuals = á»¸ - predictions
    se = np.sqrt(np.sum(residuals**2) / (len(á»¸) - 2) / np.sum(XÌƒ**2))

    # æ­¥éª¤5: ç½®ä¿¡åŒºé—´
    ci_lower = ate - 1.96 * se
    ci_upper = ate + 1.96 * se

    # æ­¥éª¤6: æ˜¾è‘—æ€§æ£€éªŒ
    z_score = ate / se
    p_value = 2 * (1 - norm.cdf(abs(z_score)))
    is_significant = p_value < 0.05

    return {
        'ate': ate,
        'se': se,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'p_value': p_value,
        'is_significant': is_significant
    }
```

**3.4 ç¤ºä¾‹è¾“å‡º**ï¼ˆTr_F1 â†’ Te_Accï¼‰
```python
{
    'source': 'Tr_F1',
    'target': 'Te_Acc',
    'ate': -0.0519,          # è´Ÿå€¼ï¼šTr_F1â†‘ â†’ Te_Accâ†“
    'se': 0.0052,            # æ ‡å‡†è¯¯
    'ci_lower': -0.0621,     # 95%ç½®ä¿¡åŒºé—´ä¸‹ç•Œ
    'ci_upper': -0.0417,     # 95%ç½®ä¿¡åŒºé—´ä¸Šç•Œ
    'p_value': 0.0001,       # på€¼ < 0.05
    'is_significant': True   # ç»Ÿè®¡æ˜¾è‘—
}
```

**è§£é‡Š**:
- ATE = -0.0519: è®­ç»ƒF1æé«˜1å•ä½ï¼Œæµ‹è¯•å‡†ç¡®ç‡é™ä½5.2%
- ç½®ä¿¡åŒºé—´ä¸åŒ…å«0: å› æœæ•ˆåº”ç¡®å®å­˜åœ¨
- p < 0.05: ç»Ÿè®¡æ˜¾è‘—ï¼Œä¸æ˜¯éšæœºæ³¢åŠ¨

#### è€—æ—¶ç‰¹å¾

**æ€»æ—¶é•¿**: <1åˆ†é’Ÿï¼ˆå®é™…çº¦3-5ç§’ï¼‰

**æ€§èƒ½åˆ†è§£**:
```
æ¯æ¡è¾¹çš„DMLä¼°è®¡:
  æ¨¡å‹æ‹Ÿåˆ (Ã—4):    1-2ç§’
    - E[Y|Z]: RF    0.5ç§’
    - E[X|Z]: RF    0.5ç§’
    - äº¤å‰éªŒè¯Ã—2    é‡å¤ä¸Šè¿°

  æ®‹å·®å›å½’:         <0.1ç§’
  ç½®ä¿¡åŒºé—´è®¡ç®—:     <0.1ç§’

  å•è¾¹æ€»è®¡:         ~0.5ç§’

6æ¡è¾¹æ€»è®¡:          ~3ç§’
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¿«?**
1. **æ ·æœ¬é‡å°**: åªæœ‰10ä¸ªæ•°æ®ç‚¹
2. **æ¨¡å‹ç®€å•**: éšæœºæ£®æ—max_depth=3
3. **å˜é‡å°‘**: æ¯æ¡è¾¹æœ€å¤š17ä¸ªæ··æ·†å˜é‡

**å¦‚æœæ ·æœ¬é‡å¢åŠ **:
- 100ä¸ªæ ·æœ¬ â†’ çº¦30-60ç§’
- 1000ä¸ªæ ·æœ¬ â†’ çº¦5-10åˆ†é’Ÿ

#### è¾“å‡ºæ•°æ®

**å› æœæ•ˆåº”è¡¨** (`results/adult_causal_effects.csv`ï¼Œè™½ç„¶å› bugæœªä¿å­˜ä½†è®¡ç®—äº†):
```
source,target,ate,se,ci_lower,ci_upper,p_value,is_significant
Tr_F1,Te_Acc,-0.0519,0.0052,-0.0621,-0.0417,0.0001,True
Te_Acc,Tr_Acc,0.9104,0.0910,0.7320,1.0889,0.0000,True
Te_Acc,Te_F1,0.2917,0.0292,0.2345,0.3489,0.0000,True
Te_F1,Te_Acc,0.1224,0.0122,0.0984,0.1464,0.0000,True
Tr_DI,alpha,NA,NA,NA,NA,NA,False
Tr_DI,Tr_F1,NA,NA,NA,NA,NA,False
```

**å¤±è´¥åŸå› åˆ†æ**:
- Tr_DI â†’ alpha å’Œ Tr_DI â†’ Tr_F1 å¤±è´¥
- åŸå› : Tr_DIåœ¨æ‰€æœ‰10ä¸ªæ ·æœ¬ä¸­éƒ½æ˜¯0.354ï¼ˆæ— å˜å¼‚æ€§ï¼‰
- æ— æ³•ä¼°è®¡å› æœæ•ˆåº”ï¼ˆåˆ†æ¯ä¸º0ï¼‰

---

### é˜¶æ®µ4: æƒè¡¡æ£€æµ‹ï¼ˆæœªå®Œå…¨å®ç°ï¼‰

#### ä»£ç ä½ç½®
```python
# demo_adult_full_analysis.py: ç¬¬344-361è¡Œ
# utils/tradeoff_detection.py: TradeoffDetectorç±»
```

#### ç®—æ³•åŸç†: Algorithm 1 (è®ºæ–‡æ ¸å¿ƒè´¡çŒ®)

**æ ¸å¿ƒæ€æƒ³**:
```
ç›®æ ‡: è‡ªåŠ¨è¯†åˆ«æŒ‡æ ‡é—´çš„æƒè¡¡æ¨¡å¼

å®šä¹‰æƒè¡¡:
  å¦‚æœå­˜åœ¨ X â†’ Yâ‚ å’Œ X â†’ Yâ‚‚ï¼Œä¸”:
  - sign(ATE_{Xâ†’Yâ‚}) Ã— sign(ATE_{Xâ†’Yâ‚‚}) < 0

  åˆ™ç§° Yâ‚ å’Œ Yâ‚‚ å­˜åœ¨æƒè¡¡
```

**ç®—æ³•æµç¨‹**:
```python
def detect_tradeoffs(causal_effects):
    """
    è¾“å…¥: å› æœæ•ˆåº”å­—å…¸ {(X,Y): {'ate': ..., 'is_significant': ...}}
    è¾“å‡º: æƒè¡¡åˆ—è¡¨ [(Y1, Y2, common_cause_X)]
    """

    tradeoffs = []

    # æ­¥éª¤1: æŒ‰æºèŠ‚ç‚¹åˆ†ç»„
    effects_by_source = defaultdict(list)
    for (source, target), effect in causal_effects.items():
        if effect['is_significant']:
            effects_by_source[source].append((target, effect['ate']))

    # æ­¥éª¤2: æ£€æŸ¥æ¯ä¸ªæºèŠ‚ç‚¹çš„æ•ˆåº”
    for source, targets_with_effects in effects_by_source.items():
        if len(targets_with_effects) < 2:
            continue  # è‡³å°‘éœ€è¦2ä¸ªç›®æ ‡

        # æ­¥éª¤3: æ£€æŸ¥æ‰€æœ‰ç›®æ ‡å¯¹
        for i, (target1, ate1) in enumerate(targets_with_effects):
            for target2, ate2 in targets_with_effects[i+1:]:
                # æ­¥éª¤4: æ£€æŸ¥ç¬¦å·ç›¸å
                if sign(ate1) * sign(ate2) < 0:
                    tradeoffs.append({
                        'metric1': target1,
                        'metric2': target2,
                        'common_cause': source,
                        'effect1': ate1,
                        'effect2': ate2,
                        'type': infer_tradeoff_type(target1, target2)
                    })

    return tradeoffs

def sign(x):
    """ç¬¦å·å‡½æ•°"""
    if x > 0:
        return 1
    elif x < 0:
        return -1
    else:
        return 0

def infer_tradeoff_type(metric1, metric2):
    """æ¨æ–­æƒè¡¡ç±»å‹"""
    if 'Acc' in metric1 and 'SPD' in metric2:
        return 'Accuracy vs Fairness'
    elif 'F1' in metric1 and 'AOD' in metric2:
        return 'Performance vs Fairness'
    elif 'Acc' in metric1 and 'FGSM' in metric2:
        return 'Accuracy vs Robustness'
    else:
        return 'Unknown'
```

#### ç¤ºä¾‹æ£€æµ‹è¿‡ç¨‹

å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å› æœæ•ˆåº”:
```python
causal_effects = {
    ('alpha', 'Te_Acc'): {'ate': 0.05, 'is_significant': True},
    ('alpha', 'Te_SPD'): {'ate': -0.03, 'is_significant': True},
    ('Tr_F1', 'Te_Acc'): {'ate': -0.052, 'is_significant': True},
    ('Tr_F1', 'Te_F1'): {'ate': 0.08, 'is_significant': True},
}
```

æ£€æµ‹ç»“æœ:
```python
[
    {
        'metric1': 'Te_Acc',
        'metric2': 'Te_SPD',
        'common_cause': 'alpha',
        'effect1': 0.05,   # æ­£æ•ˆåº”
        'effect2': -0.03,  # è´Ÿæ•ˆåº”
        'type': 'Accuracy vs Fairness'  # æƒè¡¡ç±»å‹
    }
]
```

**è§£é‡Š**:
- alphaå¢åŠ æ—¶ï¼ŒTe_Accæé«˜ï¼ˆ+0.05ï¼‰
- alphaå¢åŠ æ—¶ï¼ŒTe_SPDé™ä½ï¼ˆ-0.03ï¼Œæ›´å…¬å¹³ï¼‰
- è¿™æ˜¯å…¸å‹çš„ accuracy vs fairness æƒè¡¡

#### å½“å‰çŠ¶æ€

âš ï¸ **æœ¬æ¬¡å®éªŒæœªå®Œå…¨å®ç°**ï¼ŒåŸå› :
1. æµ‹è¯•é›†å…¬å¹³æ€§æŒ‡æ ‡(Te_SPD, Te_DI)ä¸å˜
2. æ— æ³•è§‚å¯Ÿåˆ°alphaå¯¹è¿™äº›æŒ‡æ ‡çš„å½±å“
3. DMLé˜¶æ®µçš„ä¿å­˜bugå¯¼è‡´æ•°æ®æœªæŒä¹…åŒ–

**æœªæ¥æ”¹è¿›**:
1. è§‚å¯Ÿè®­ç»ƒé›†æŒ‡æ ‡(Tr_SPD, Tr_DI)çš„å˜åŒ–
2. ä¿®å¤ä¿å­˜bug
3. æ‰©å¤§æ ·æœ¬é‡ä»¥å¢å¼ºç»Ÿè®¡åŠŸæ•ˆ

---

## æ ¸å¿ƒæ¨¡å—è¯¦è§£

### æ¨¡å—1: ç¥ç»ç½‘ç»œæ¨¡å‹ (utils/model.py)

#### FFNNç±» - 5å±‚å‰é¦ˆç¥ç»ç½‘ç»œ

**æ¶æ„è®¾è®¡**:
```python
class FFNN(nn.Module):
    def __init__(self, input_dim=102, width=2):
        super().__init__()

        # è®¡ç®—éšè—å±‚ç»´åº¦
        hidden_dim = input_dim * width  # 102 * 2 = 204

        # 5å±‚ç½‘ç»œç»“æ„
        self.layers = nn.Sequential(
            # å±‚1: è¾“å…¥å±‚ â†’ éšè—å±‚1
            nn.Linear(input_dim, hidden_dim),  # 102 â†’ 204
            nn.ReLU(),

            # å±‚2: éšè—å±‚1 â†’ éšè—å±‚2
            nn.Linear(hidden_dim, hidden_dim),  # 204 â†’ 204
            nn.ReLU(),

            # å±‚3: éšè—å±‚2 â†’ éšè—å±‚3
            nn.Linear(hidden_dim, hidden_dim),  # 204 â†’ 204
            nn.ReLU(),

            # å±‚4: éšè—å±‚3 â†’ éšè—å±‚4
            nn.Linear(hidden_dim, hidden_dim // 2),  # 204 â†’ 102
            nn.ReLU(),

            # å±‚5: éšè—å±‚4 â†’ è¾“å‡ºå±‚
            nn.Linear(hidden_dim // 2, 1),  # 102 â†’ 1
            nn.Sigmoid()  # äºŒåˆ†ç±»è¾“å‡º [0, 1]
        )

    def forward(self, x):
        return self.layers(x)
```

**å‚æ•°ç»Ÿè®¡**:
```
å±‚1å‚æ•°: 102 Ã— 204 + 204 = 21,012
å±‚2å‚æ•°: 204 Ã— 204 + 204 = 41,820
å±‚3å‚æ•°: 204 Ã— 204 + 204 = 41,820
å±‚4å‚æ•°: 204 Ã— 102 + 102 = 20,910
å±‚5å‚æ•°: 102 Ã— 1 + 1 = 103

æ€»å‚æ•°: 125,665ä¸ªå¯è®­ç»ƒå‚æ•°
```

**ä¸ºä»€ä¹ˆç”¨5å±‚?**
- è®ºæ–‡å®éªŒé…ç½®
- è¶³å¤Ÿå¤æ‚ä»¥å­¦ä¹ éçº¿æ€§æ¨¡å¼
- ä¸ä¼šå¤ªæ·±å¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆæ ·æœ¬é‡æœ‰é™ï¼‰

#### ModelTrainerç±» - è®­ç»ƒå™¨

**æ ¸å¿ƒåŠŸèƒ½**:
```python
class ModelTrainer:
    def __init__(self, model, device='cuda', lr=0.001):
        self.model = model.to(device)
        self.device = device

        # Adamä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(model.parameters(), lr=lr)

        # äºŒå…ƒäº¤å‰ç†µæŸå¤±
        self.criterion = nn.BCELoss()

    def train(self, X, y, epochs=50, batch_size=256, verbose=False):
        """è®­ç»ƒæ¨¡å‹"""
        dataset = TensorDataset(
            torch.FloatTensor(X),
            torch.FloatTensor(y)
        )
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True  # æ¯è½®æ‰“ä¹±æ•°æ®
        )

        for epoch in range(epochs):
            epoch_loss = 0.0

            for batch_X, batch_y in dataloader:
                # ç§»è‡³GPU
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device).unsqueeze(1)

                # å‰å‘ä¼ æ’­
                predictions = self.model(batch_X)
                loss = self.criterion(predictions, batch_y)

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item()

            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}")

    def predict(self, X):
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            predictions = self.model(X_tensor)
            return (predictions.cpu().numpy() > 0.5).astype(int).flatten()
```

**è®­ç»ƒç»†èŠ‚**:
- **ä¼˜åŒ–å™¨**: Adamï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
- **å­¦ä¹ ç‡**: 0.001ï¼ˆæ ‡å‡†å€¼ï¼‰
- **æ‰¹æ¬¡å¤§å°**: 256ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜ï¼‰
- **æŸå¤±å‡½æ•°**: BCELossï¼ˆäºŒåˆ†ç±»æ ‡å‡†ï¼‰

---

### æ¨¡å—2: æŒ‡æ ‡è®¡ç®— (utils/metrics.py)

#### MetricsCalculatorç±»

**æ”¯æŒçš„æŒ‡æ ‡**:
```python
class MetricsCalculator:
    def compute_all_metrics(self, X, y, sensitive, phase='Te'):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""

        # 1. é¢„æµ‹
        y_pred = self.trainer.predict(X)

        # 2. æ€§èƒ½æŒ‡æ ‡
        metrics = {}
        metrics[f'{phase}_Acc'] = accuracy_score(y, y_pred)
        metrics[f'{phase}_F1'] = f1_score(y, y_pred)

        # 3. å…¬å¹³æ€§æŒ‡æ ‡ï¼ˆä½¿ç”¨AIF360ï¼‰
        dataset = BinaryLabelDataset(
            df=pd.DataFrame({
                'y': y,
                'y_pred': y_pred,
                'sensitive': sensitive
            }),
            label_names=['y'],
            protected_attribute_names=['sensitive']
        )

        metric = BinaryLabelDatasetMetric(
            dataset,
            unprivileged_groups=[{'sensitive': 0}],  # Female
            privileged_groups=[{'sensitive': 1}]     # Male
        )

        metrics[f'{phase}_SPD'] = metric.statistical_parity_difference()
        metrics[f'{phase}_DI'] = metric.disparate_impact()
        metrics[f'{phase}_AOD'] = metric.average_odds_difference()
        metrics[f'{phase}_Cons'] = metric.consistency()

        # 4. é²æ£’æ€§æŒ‡æ ‡ï¼ˆä»…æµ‹è¯•é›†ï¼‰
        if phase == 'Te':
            metrics['A_FGSM'] = self.compute_fgsm_attack(X, y)
            metrics['A_PGD'] = self.compute_pgd_attack(X, y)

        return metrics
```

**æŒ‡æ ‡è®¡ç®—å…¬å¼**:

**æ€§èƒ½æŒ‡æ ‡**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**å…¬å¹³æ€§æŒ‡æ ‡**:
```
SPD (Statistical Parity Difference):
  SPD = P(Å¶=1 | S=0) - P(Å¶=1 | S=1)
  ç†æƒ³å€¼: 0 (ä¸¤ç»„é¢„æµ‹æ­£ä¾‹æ¯”ä¾‹ç›¸åŒ)
  èŒƒå›´: [-1, 1]

DI (Disparate Impact):
  DI = P(Å¶=1 | S=0) / P(Å¶=1 | S=1)
  ç†æƒ³å€¼: 1 (æ¯”å€¼ä¸º1)
  å…¬å¹³èŒƒå›´: [0.8, 1.25]

AOD (Average Odds Difference):
  AOD = 0.5 Ã— [(TPRâ‚€ - TPRâ‚) + (FPRâ‚€ - FPRâ‚)]
  å…¶ä¸­ TPR = TP / (TP + FN), FPR = FP / (FP + TN)
  ç†æƒ³å€¼: 0

Consistency:
  åº¦é‡ç›¸ä¼¼æ ·æœ¬çš„é¢„æµ‹ä¸€è‡´æ€§
  èŒƒå›´: [0, 1], è¶Šé«˜è¶Šå¥½
```

---

### æ¨¡å—3: å…¬å¹³æ€§æ–¹æ³• (utils/fairness_methods.py)

#### ReweighingåŸç†

**æ ¸å¿ƒæ€æƒ³**:
```
é€šè¿‡é‡æ–°åŠ æƒè®­ç»ƒæ ·æœ¬æ¥å¹³è¡¡ä¸åŒç»„çš„å½±å“

æ­¥éª¤:
1. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡ w_i
2. è®­ç»ƒæ—¶ä½¿ç”¨åŠ æƒæŸå¤± Î£ w_i Ã— loss_i
```

**æƒé‡è®¡ç®—**:
```python
def compute_reweighing_weights(y, sensitive):
    """
    è®¡ç®—Reweighingæƒé‡

    ç›®æ ‡: ä½¿å¾— P(Y=y, S=s) åœ¨æ‰€æœ‰ç»„ä¸­ä¸€è‡´
    """

    # è®¡ç®—è”åˆæ¦‚ç‡
    n = len(y)
    groups = [(y_val, s_val) for y_val in [0, 1] for s_val in [0, 1]]

    weights = np.ones(n)

    for y_val, s_val in groups:
        # æœŸæœ›æ¦‚ç‡ï¼ˆå‡è®¾ç‹¬ç«‹ï¼‰
        p_expected = (np.mean(y == y_val) * np.mean(sensitive == s_val))

        # è§‚æµ‹æ¦‚ç‡
        p_observed = np.mean((y == y_val) & (sensitive == s_val))

        # æƒé‡ = æœŸæœ› / è§‚æµ‹
        if p_observed > 0:
            weight_ratio = p_expected / p_observed

            # åº”ç”¨åˆ°å¯¹åº”æ ·æœ¬
            mask = (y == y_val) & (sensitive == s_val)
            weights[mask] = weight_ratio

    return weights
```

**ç¤ºä¾‹**:
```
åŸå§‹åˆ†å¸ƒ:
  P(Y=1, S=Female) = 0.10
  P(Y=1, S=Male) = 0.30

æœŸæœ›åˆ†å¸ƒï¼ˆå¦‚æœç‹¬ç«‹ï¼‰:
  P(Y=1) = 0.25, P(S=Female) = 0.35
  P(Y=1, S=Female) = 0.25 Ã— 0.35 = 0.0875

æƒé‡:
  w(Y=1, S=Female) = 0.0875 / 0.10 = 0.875

æ•ˆæœ: Femaleé«˜æ”¶å…¥æ ·æœ¬æƒé‡é™ä½ï¼Œä½¿å…¶æ¯”ä¾‹æ¥è¿‘æœŸæœ›
```

**Alphaå‚æ•°çš„ä½œç”¨**:
```python
if alpha == 0:
    # ä¸åº”ç”¨Reweighing
    return X, y
elif alpha == 1:
    # å®Œå…¨åº”ç”¨Reweighing
    weights = compute_reweighing_weights(y, sensitive)
    return X, y, weights
else:
    # éƒ¨åˆ†åº”ç”¨ï¼ˆçº¿æ€§æ’å€¼ï¼‰
    weights = 1 + alpha * (compute_reweighing_weights(y, sensitive) - 1)
    return X, y, weights
```

---

## æ•°æ®æµè½¬è¿‡ç¨‹

### å®Œæ•´æ•°æ®æµå›¾

```
[åŸå§‹CSVæ–‡ä»¶]
    â†“ åŠ è½½
[DataFrame (45222, 102)]
    â†“ åˆ†å‰²
[è®­ç»ƒé›† (31655, 102)] + [æµ‹è¯•é›† (13567, 102)]
    â†“ æ ‡å‡†åŒ–
[æ ‡å‡†åŒ–è®­ç»ƒé›†] + [æ ‡å‡†åŒ–æµ‹è¯•é›†]
    â†“
    â”œâ”€â†’ [Baselineå¤„ç†] â†’ [æœªå˜æ¢æ•°æ®]
    â””â”€â†’ [Reweighingå¤„ç†] â†’ [åŠ æƒæ•°æ®]
              â†“
         [FFNNè®­ç»ƒ Ã—10é…ç½®]
              â†“
         [é¢„æµ‹ç»“æœ]
              â†“
         [æŒ‡æ ‡è®¡ç®— Ã—3é˜¶æ®µ]
              â†“
         [DataFrame (10, 24)]
              â†“ ä¿å­˜CSV
         [adult_training_data.csv]
              â†“ DiBS
         [å› æœå›¾ (19, 19)]
              â†“ DML
         [å› æœæ•ˆåº”è¡¨]
              â†“ æ£€æµ‹
         [æƒè¡¡åˆ—è¡¨]
```

### å…³é”®æ•°æ®å˜æ¢

**å˜æ¢1: One-Hotç¼–ç **
```
åŸå§‹ç‰¹å¾ 'education':
  ['Bachelors', 'HS-grad', 'Masters', ...]

One-Hotå:
  education_Bachelors: [1, 0, 0, ...]
  education_HS-grad: [0, 1, 0, ...]
  education_Masters: [0, 0, 1, ...]

ç»´åº¦: 1 â†’ 16 (16ä¸ªæ•™è‚²ç±»åˆ«)
```

**å˜æ¢2: æ ‡å‡†åŒ–**
```
åŸå§‹: age = [25, 38, 42, ...]
      â†’ å‡å€¼=38.5, æ ‡å‡†å·®=13.5

æ ‡å‡†åŒ–å: age_scaled = (age - 38.5) / 13.5
         = [-1.0, -0.037, 0.26, ...]
```

**å˜æ¢3: ReweighingåŠ æƒ**
```
åŸå§‹æ ·æœ¬æƒé‡: [1, 1, 1, ..., 1]

Reweighingå:
  Female & Y=1: æƒé‡ 0.875
  Female & Y=0: æƒé‡ 1.123
  Male & Y=1: æƒé‡ 1.067
  Male & Y=0: æƒé‡ 0.998
```

---

## æ€§èƒ½ç‰¹å¾åˆ†æ

### æ€»ä½“è€—æ—¶åˆ†è§£ï¼ˆAdultæ•°æ®é›†ï¼Œ10é…ç½®ï¼‰

```
é˜¶æ®µ0: æ•°æ®åŠ è½½          10-20ç§’      (1.6%)
é˜¶æ®µ1: æ•°æ®æ”¶é›†          3600ç§’       (95.9%)  âš¡ ç»å¯¹ä¸»å¯¼
  â”œâ”€ å…¬å¹³æ€§æ–¹æ³•          20ç§’
  â”œâ”€ æ¨¡å‹è®­ç»ƒ            3200ç§’       (85.3%)  âš¡ æœ€è€—æ—¶
  â””â”€ æŒ‡æ ‡è®¡ç®—            380ç§’
é˜¶æ®µ2: DiBSå­¦ä¹           96ç§’         (2.6%)
é˜¶æ®µ3: DMLæ¨æ–­           3ç§’          (0.08%)
é˜¶æ®µ4: æƒè¡¡æ£€æµ‹          <1ç§’         (0.02%)

æ€»è®¡:                    ~3730ç§’ = 62.2åˆ†é’Ÿ
```

### å„é˜¶æ®µåŠ é€Ÿæ½œåŠ›

| é˜¶æ®µ | å½“å‰è€—æ—¶ | ä¸»è¦ç“¶é¢ˆ | åŠ é€Ÿæ–¹æ³• | æ½œåœ¨æå‡ |
|------|---------|---------|---------|---------|
| æ•°æ®åŠ è½½ | 15ç§’ | CSVè¯»å– | ä½¿ç”¨parquet | 50% |
| æ¨¡å‹è®­ç»ƒ | 3200ç§’ | å‰å‘/åå‘ä¼ æ’­ | æ›´å¤§GPUã€æ··åˆç²¾åº¦ | 2-3Ã— |
| DiBSå­¦ä¹  | 96ç§’ | è¿­ä»£ä¼˜åŒ– | å‡å°‘è¿­ä»£ã€æ›´å¿«ç¡¬ä»¶ | 1.5-2Ã— |
| DMLæ¨æ–­ | 3ç§’ | æ¨¡å‹æ‹Ÿåˆ | å¹¶è¡ŒåŒ– | 2Ã— |

**æœ€æœ‰ä»·å€¼çš„ä¼˜åŒ–**: æ¨¡å‹è®­ç»ƒï¼ˆå 85%æ—¶é—´ï¼‰
- ä½¿ç”¨A100 GPU â†’ 2-3Ã—åŠ é€Ÿ
- å‡å°‘è®­ç»ƒè½®æ•° 50â†’30 â†’ 40%åŠ é€Ÿ
- ä½†å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½

### å†…å­˜ä½¿ç”¨åˆ†æ

```
å³°å€¼å†…å­˜ (GPU):
  æ¨¡å‹å‚æ•°:        ~0.5 MB (125Kå‚æ•° Ã— 4å­—èŠ‚)
  è®­ç»ƒæ•°æ®:        ~13 MB (31655 Ã— 102 Ã— 4å­—èŠ‚)
  ä¸­é—´æ¿€æ´»å€¼:      ~50 MB (æ‰¹æ¬¡256 Ã— 204 Ã— 5å±‚)
  æ¢¯åº¦:            ~0.5 MB
  ä¼˜åŒ–å™¨çŠ¶æ€:      ~1 MB (AdamåŠ¨é‡)
  æ€»è®¡:            ~65 MB (éå¸¸å°ï¼ŒGPUåˆ©ç”¨ç‡ä½)

å³°å€¼å†…å­˜ (CPU):
  åŸå§‹æ•°æ®:        ~200 MB
  æ£€æŸ¥ç‚¹:          ~36 MB
  ä¸­é—´ç»“æœ:        ~100 MB
  æ€»è®¡:            ~350 MB
```

**è§‚å¯Ÿ**: GPUåˆ©ç”¨ç‡å¾ˆä½ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–

---

## å…³é”®ç®—æ³•åŸç†

### DiBSç®—æ³•æ·±åº¦è§£æ

**ä¸ºä»€ä¹ˆéœ€è¦DiBS?**
```
é—®é¢˜: ä¼ ç»Ÿå› æœå‘ç°ç®—æ³•ï¼ˆå¦‚PCã€GESï¼‰:
  - å‡è®¾çº¿æ€§å…³ç³»
  - éœ€è¦å¤§æ ·æœ¬é‡
  - æ— æ³•å¤„ç†æ½œåœ¨æ··æ·†

DiBSä¼˜åŠ¿:
  - è´å¶æ–¯æ¡†æ¶ï¼Œé‡åŒ–ä¸ç¡®å®šæ€§
  - å¯å¾®åˆ†ï¼Œåˆ©ç”¨æ¢¯åº¦ä¼˜åŒ–
  - é€‚åˆå°æ ·æœ¬
```

**æ¦‚ç‡å›¾æ¨¡å‹**:
```
1. å›¾çš„å…ˆéªŒåˆ†å¸ƒ:
   P(G) âˆ exp(-Î± Ã— |E(G)|)

   å…¶ä¸­:
   - E(G): å›¾Gçš„è¾¹é›†
   - Î±: ç¨€ç–æ€§å‚æ•°ï¼ˆè¶Šå¤§å›¾è¶Šç¨€ç–ï¼‰

2. æ•°æ®ä¼¼ç„¶:
   P(X | G, Î¸) = âˆâ¿áµ¢â‚Œâ‚ âˆáµ–â±¼â‚Œâ‚ N(xáµ¢â±¼ | Î¼â±¼(xáµ¢,Pa(j)), Ïƒâ±¼Â²)

   å…¶ä¸­:
   - Î¼â±¼: èŠ‚ç‚¹jçš„æ¡ä»¶æœŸæœ›ï¼ˆç”±çˆ¶èŠ‚ç‚¹å†³å®šï¼‰
   - Pa(j): èŠ‚ç‚¹jçš„çˆ¶èŠ‚ç‚¹é›†

3. åéªŒåˆ†å¸ƒ:
   P(G | X) âˆ P(X | G) Ã— P(G)

   ç›®æ ‡: æ‰¾åˆ°æœ€å¤§åéªŒæ¦‚ç‡çš„å›¾ G*
```

**å˜åˆ†æ¨æ–­**:
```
é—®é¢˜: ç›´æ¥æœ€å¤§åŒ–P(G|X)å›°éš¾ï¼ˆç»„åˆä¼˜åŒ–ï¼‰

è§£å†³: ç”¨å˜åˆ†åˆ†å¸ƒq(G)é€¼è¿‘çœŸå®åéªŒP(G|X)

ç›®æ ‡: æœ€å°åŒ–KLæ•£åº¦
  KL(q || p) = E_q[log q(G) - log P(G|X)]

ä¼˜åŒ–:
  1. å‚æ•°åŒ–q(G)ä¸ºå¯å¾®åˆ†å¸ƒ
  2. ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§
  3. æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
```

### DMLç®—æ³•æ·±åº¦è§£æ

**ä¸ºä»€ä¹ˆéœ€è¦DML?**
```
é—®é¢˜: ç›´æ¥å›å½’ Y ~ X æœ‰å
  Y = Ï„X + g(Z) + Îµ

  å¦‚æœç›´æ¥æ‹Ÿåˆ:
  Y^ = Î²Ì‚X

  åˆ™ Î²Ì‚ â‰  Ï„ (æ··æ·†åå·®)

åŸå› : ZåŒæ—¶å½±å“Xå’ŒYï¼ˆæ··æ·†ï¼‰
```

**Neymanæ­£äº¤æ€§**:
```
DMLæ ¸å¿ƒ: æ„é€ æ­£äº¤çŸ©æ¡ä»¶

å®šä¹‰æ­£äº¤å¾—åˆ†:
  Ïˆ(Ï„; Y, X, Z) = (Y - m(Z) - Ï„(X - h(Z))) Ã— (X - h(Z))

å…¶ä¸­:
  - m(Z) = E[Y|Z]  (ç»“æœæ¨¡å‹)
  - h(Z) = E[X|Z]  (å¤„ç†æ¨¡å‹)

æ€§è´¨:
  E[Ïˆ(Ï„; Y, X, Z)] = 0  å½“ä¸”ä»…å½“ Ï„ = ATE

ä¼˜åŠ¿:
  å³ä½¿må’Œhä¼°è®¡æœ‰è¯¯å·®ï¼Œ
  åªè¦è¯¯å·®ä¸ç›¸å…³ï¼ŒÏ„Ì‚ä»ç„¶ä¸€è‡´
```

**Cross-fittingæŠ€å·§**:
```
é—®é¢˜: åŒä¸€æ•°æ®ç”¨äºæ‹Ÿåˆmå’Œhï¼Œå†ç”¨äºä¼°è®¡Ï„
  â†’ è¿‡æ‹Ÿåˆåå·®

è§£å†³: æ ·æœ¬åˆ†è£‚
  1. å°†æ•°æ®åˆ†ä¸ºKæŠ˜
  2. ç”¨K-1æŠ˜æ‹Ÿåˆmå’Œh
  3. åœ¨ç¬¬KæŠ˜ä¸Šè®¡ç®—æ®‹å·®
  4. é‡å¤Kæ¬¡ï¼Œåˆå¹¶æ®‹å·®
  5. åœ¨åˆå¹¶æ®‹å·®ä¸Šä¼°è®¡Ï„

ç»“æœ: æ— è¿‡æ‹Ÿåˆåå·®
```

---

## æ€»ç»“

### ä»£ç è®¾è®¡ä¼˜åŠ¿

1. **æ¨¡å—åŒ–**: æ¯ä¸ªé˜¶æ®µç‹¬ç«‹ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
2. **å¯æ‰©å±•**: æ–°å¢æ–¹æ³•æˆ–æ•°æ®é›†åªéœ€ä¿®æ”¹é…ç½®
3. **é²æ£’æ€§**: æ£€æŸ¥ç‚¹ç³»ç»Ÿæ”¯æŒæ–­ç‚¹ç»­ä¼ 
4. **é«˜æ•ˆæ€§**: GPUåŠ é€Ÿ + JAXç¼–è¯‘

### å…³é”®ç“¶é¢ˆ

1. **æ¨¡å‹è®­ç»ƒ**: å 85%æ—¶é—´ï¼Œä½†å¿…è¦
2. **æ ·æœ¬é‡é™åˆ¶**: 10ä¸ªé…ç½®ç»Ÿè®¡åŠŸæ•ˆä½
3. **DiBSæ”¶æ•›**: éœ€è¦æ›´å¤šè¿­ä»£ä»¥æé«˜ç²¾åº¦

### æœªæ¥æ”¹è¿›æ–¹å‘

1. **å¹¶è¡ŒåŒ–**: å¤šGPUè®­ç»ƒ10ä¸ªé…ç½®
2. **è¶…å‚æ•°ä¼˜åŒ–**: è‡ªåŠ¨æœç´¢æœ€ä½³ç½‘ç»œç»“æ„
3. **æ›´å¤šæ•°æ®é›†**: COMPASã€Germanç­‰
4. **å¯è§†åŒ–**: å› æœå›¾äº¤äº’å¼å±•ç¤º

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-21
**åŸºäºå®éªŒ**: Adultæ•°æ®é›†å®Œæ•´å› æœåˆ†æ (61.4åˆ†é’Ÿ)
