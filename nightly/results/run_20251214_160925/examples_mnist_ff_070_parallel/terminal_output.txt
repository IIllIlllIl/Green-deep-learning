================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_070_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_070_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff --seed 486
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3834878
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff --seed 486
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 486
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 17:01:06...

training layer:  0
Loss:  1.1267644166946411
Loss:  0.7548906803131104
Loss:  0.7028068900108337
Loss:  0.7050988674163818
Loss:  0.6946011185646057
Loss:  0.6919228434562683
Loss:  0.6902201175689697
Loss:  0.6879358887672424
Loss:  0.6848011612892151
Loss:  0.6804323792457581
Loss:  0.6745247840881348
Loss:  0.6669521331787109
Loss:  0.6577726602554321
Loss:  0.6472395062446594
Loss:  0.6358033418655396
Loss:  0.6240101456642151
Loss:  0.6122755408287048
Loss:  0.6007879376411438
Loss:  0.5896264314651489
Loss:  0.5788204073905945
Loss:  0.5683740973472595
Loss:  0.5582831501960754
Loss:  0.5485426187515259
Loss:  0.5391454696655273
Loss:  0.5300858616828918
Loss:  0.5213643908500671
Loss:  0.5129781365394592
Loss:  0.5049183964729309
Loss:  0.4971715807914734
Loss:  0.4897245466709137
Loss:  0.48256540298461914
Loss:  0.4756791293621063
Loss:  0.4690479338169098
Loss:  0.46265703439712524
Loss:  0.4564935564994812
Loss:  0.45054617524147034
Loss:  0.4448050558567047
Loss:  0.4392586946487427
Loss:  0.4338954985141754
Loss:  0.4287058413028717
Loss:  0.4236810803413391
Loss:  0.4188132584095001
Loss:  0.4140963852405548
Loss:  0.40952345728874207
Loss:  0.40508729219436646
Loss:  0.40078070759773254
Loss:  0.39659667015075684
Loss:  0.3925282955169678
Loss:  0.38856953382492065
Loss:  0.38471564650535583
Loss:  0.38096293807029724
Loss:  0.3773079514503479
Loss:  0.3737468719482422
Loss:  0.37027519941329956
Loss:  0.36688855290412903
Loss:  0.36358267068862915
Loss:  0.3603535294532776
Loss:  0.3571973443031311
Loss:  0.3541104793548584
Loss:  0.35108938813209534
Loss:  0.34813085198402405
Loss:  0.34523218870162964
Loss:  0.34239131212234497
Loss:  0.33960628509521484
Loss:  0.3368750810623169
Loss:  0.33419540524482727
Loss:  0.3315649926662445
Loss:  0.3289816677570343
Loss:  0.3264438211917877
Loss:  0.32395026087760925
Loss:  0.3214997947216034
Loss:  0.31909114122390747
Loss:  0.3167227506637573
Loss:  0.3143927752971649
Loss:  0.31209954619407654
Loss:  0.30984145402908325
Loss:  0.3076171278953552
Loss:  0.30542540550231934
Loss:  0.30326521396636963
Loss:  0.3011355996131897
Loss:  0.2990354001522064
Loss:  0.2969633936882019
Loss:  0.2949185073375702
Loss:  0.2928997874259949
Loss:  0.2909066677093506
Loss:  0.2889386713504791
Loss:  0.28699541091918945
Loss:  0.2850765883922577
Loss:  0.2831818163394928
Loss:  0.2813105881214142
Loss:  0.27946239709854126
Loss:  0.2776366174221039
Loss:  0.2758328318595886
Loss:  0.2740504741668701
Loss:  0.27228930592536926
Loss:  0.2705490291118622
Loss:  0.268829345703125
Loss:  0.2671300172805786
Loss:  0.2654506266117096
Loss:  0.2637908160686493
training layer:  1
Loss:  1.1266499757766724
Loss:  0.6307439208030701
Loss:  0.5537406802177429
Loss:  0.5447326302528381
Loss:  0.5391901135444641
Loss:  0.5277764797210693
Loss:  0.5199283957481384
Loss:  0.5094886422157288
Loss:  0.49708670377731323
Loss:  0.48242825269699097
Loss:  0.4668489992618561
Loss:  0.45155301690101624
Loss:  0.43722155690193176
Loss:  0.42391619086265564
Loss:  0.41165798902511597
Loss:  0.4005524218082428
Loss:  0.3904987573623657
Loss:  0.3813225030899048
Loss:  0.37290358543395996
Loss:  0.36515653133392334
Loss:  0.35739666223526
Loss:  0.34962698817253113
Loss:  0.34238776564598083
Loss:  0.33571118116378784
Loss:  0.32955071330070496
Loss:  0.32384568452835083
Loss:  0.3185405731201172
Loss:  0.3136031925678253
Loss:  0.30900657176971436
Loss:  0.3047012984752655
Loss:  0.30065473914146423
Loss:  0.29683759808540344
Loss:  0.2932251989841461
Loss:  0.2897956967353821
Loss:  0.28652915358543396
Loss:  0.28340384364128113
Loss:  0.2804120182991028
Loss:  0.2775494158267975
Loss:  0.27480342984199524
Loss:  0.2721640467643738
Loss:  0.2696225941181183
Loss:  0.2671704888343811
Loss:  0.2647990882396698
Loss:  0.2624986171722412
Loss:  0.2602635622024536
Loss:  0.2580936551094055
Loss:  0.2559821605682373
Loss:  0.253925085067749
Loss:  0.2519201636314392
Loss:  0.24996596574783325
Loss:  0.24806274473667145
Loss:  0.24620966613292694
Loss:  0.24440576136112213
Loss:  0.24264982342720032
Loss:  0.24094079434871674
Loss:  0.23927739262580872
Loss:  0.23765745759010315
Loss:  0.23607875406742096
Loss:  0.2345396727323532
Loss:  0.2330390363931656
Loss:  0.23157545924186707
Loss:  0.23014740645885468
Loss:  0.22875314950942993
Loss:  0.22739073634147644
Loss:  0.22605817019939423
Loss:  0.22475460171699524
Loss:  0.223480224609375
Loss:  0.2222343236207962
Loss:  0.22101542353630066
Loss:  0.2198224514722824
Loss:  0.21865448355674744
Loss:  0.21751053631305695
Loss:  0.21638964116573334
Loss:  0.2152908593416214
Loss:  0.214213564991951
Loss:  0.2131570726633072
Loss:  0.2121206521987915
Loss:  0.21110373735427856
Loss:  0.21010570228099823
Loss:  0.20912602543830872
Loss:  0.20816411077976227
Loss:  0.20721937716007233
Loss:  0.2062913030385971
Loss:  0.2053793966770172
Loss:  0.20448310673236847
Loss:  0.2036018818616867
Loss:  0.20273526012897491
Loss:  0.20188279449939728
Loss:  0.20104409754276276
Loss:  0.20021876692771912
Loss:  0.1994064450263977
Loss:  0.19860680401325226
Loss:  0.19781948626041412
Loss:  0.19704417884349823
Loss:  0.19628050923347473
Loss:  0.19552816450595856
Loss:  0.19478683173656464
Loss:  0.19405622780323029
Loss:  0.19333603978157043
Loss:  0.1926259994506836
train error: 0.06870001554489136
test error: 0.07960003614425659

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 17:01:06
[0;34m[INFO][0m Training End: 2025-12-15 17:01:24
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 92.03999638557434100%
[0;34m[INFO][0m Test Error: 0.07960003614425659
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.06870001554489136
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_070_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_070_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
