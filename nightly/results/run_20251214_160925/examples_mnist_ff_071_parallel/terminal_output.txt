================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_071_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_071_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff --seed 3122
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3835480
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff --seed 3122
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 3122
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 17:02:58...

training layer:  0
Loss:  1.126756191253662
Loss:  0.754428505897522
Loss:  0.7024412751197815
Loss:  0.7045706510543823
Loss:  0.6939793825149536
Loss:  0.6911713480949402
Loss:  0.6893947720527649
Loss:  0.6870717406272888
Loss:  0.6839426755905151
Loss:  0.6796661019325256
Loss:  0.6739626526832581
Loss:  0.6667171716690063
Loss:  0.6579521298408508
Loss:  0.6478572487831116
Loss:  0.6368438601493835
Loss:  0.6254340410232544
Loss:  0.6140406131744385
Loss:  0.6028589010238647
Loss:  0.5919395685195923
Loss:  0.5812945365905762
Loss:  0.5709248185157776
Loss:  0.5608313083648682
Loss:  0.5510183572769165
Loss:  0.5414886474609375
Loss:  0.5322443246841431
Loss:  0.5232922434806824
Loss:  0.5146344900131226
Loss:  0.5062739253044128
Loss:  0.49821004271507263
Loss:  0.4904405176639557
Loss:  0.4829638600349426
Loss:  0.47577333450317383
Loss:  0.46885818243026733
Loss:  0.46220624446868896
Loss:  0.45580580830574036
Loss:  0.4496441185474396
Loss:  0.44370967149734497
Loss:  0.4379907548427582
Loss:  0.43247634172439575
Loss:  0.42715632915496826
Loss:  0.42201995849609375
Loss:  0.41705653071403503
Loss:  0.4122569262981415
Loss:  0.40761250257492065
Loss:  0.40311408042907715
Loss:  0.39875203371047974
Loss:  0.394517183303833
Loss:  0.39040154218673706
Loss:  0.38639897108078003
Loss:  0.38250336050987244
Loss:  0.3787093758583069
Loss:  0.37501260638237
Loss:  0.37140971422195435
Loss:  0.3678975999355316
Loss:  0.36447280645370483
Loss:  0.3611311912536621
Loss:  0.35786953568458557
Loss:  0.3546845614910126
Loss:  0.3515734374523163
Loss:  0.34853339195251465
Loss:  0.3455617129802704
Loss:  0.34265565872192383
Loss:  0.3398127853870392
Loss:  0.33703088760375977
Loss:  0.3343076705932617
Loss:  0.33164089918136597
Loss:  0.3290281295776367
Loss:  0.32646694779396057
Loss:  0.3239547312259674
Loss:  0.3214890658855438
Loss:  0.31906771659851074
Loss:  0.3166888952255249
Loss:  0.3143511712551117
Loss:  0.3120531141757965
Loss:  0.30979329347610474
Loss:  0.307570219039917
Loss:  0.30538225173950195
Loss:  0.30322781205177307
Loss:  0.3011057674884796
Loss:  0.29901543259620667
Loss:  0.296956330537796
Loss:  0.2949279248714447
Loss:  0.2929292619228363
Loss:  0.29095932841300964
Loss:  0.28901708126068115
Loss:  0.28710150718688965
Loss:  0.28521162271499634
Loss:  0.28334635496139526
Loss:  0.28150492906570435
Loss:  0.27968648076057434
Loss:  0.2778901755809784
Loss:  0.2761152386665344
Loss:  0.27436113357543945
Loss:  0.2726273238658905
Loss:  0.270913302898407
Loss:  0.26921868324279785
Loss:  0.2675429582595825
Loss:  0.2658858001232147
Loss:  0.2642466127872467
Loss:  0.26262494921684265
training layer:  1
Loss:  1.1266778707504272
Loss:  0.6344679594039917
Loss:  0.5532018542289734
Loss:  0.5393230319023132
Loss:  0.5342268943786621
Loss:  0.5222586989402771
Loss:  0.5121917724609375
Loss:  0.4997040927410126
Loss:  0.48507797718048096
Loss:  0.4687991142272949
Loss:  0.45206591486930847
Loss:  0.43607357144355774
Loss:  0.42154133319854736
Loss:  0.40858978033065796
Loss:  0.39699551463127136
Loss:  0.38654935359954834
Loss:  0.37712419033050537
Loss:  0.36863765120506287
Loss:  0.36094310879707336
Loss:  0.3539092242717743
Loss:  0.3474160134792328
Loss:  0.3413926661014557
Loss:  0.33579710125923157
Loss:  0.33055976033210754
Loss:  0.32564985752105713
Loss:  0.3210269808769226
Loss:  0.3166486322879791
Loss:  0.31247997283935547
Loss:  0.3085017502307892
Loss:  0.3047035038471222
Loss:  0.3010751008987427
Loss:  0.29760342836380005
Loss:  0.2942736744880676
Loss:  0.2910701036453247
Loss:  0.28797996044158936
Loss:  0.28499388694763184
Loss:  0.2821028530597687
Loss:  0.2793044447898865
Loss:  0.27659791707992554
Loss:  0.27397969365119934
Loss:  0.27144619822502136
Loss:  0.26899370551109314
Loss:  0.2666192650794983
Loss:  0.2643192410469055
Loss:  0.26208895444869995
Loss:  0.2599242627620697
Loss:  0.25782278180122375
Loss:  0.2557835280895233
Loss:  0.25380486249923706
Loss:  0.2518833875656128
Loss:  0.2500150203704834
Loss:  0.24819721281528473
Loss:  0.24642902612686157
Loss:  0.24470853805541992
Loss:  0.24303321540355682
Loss:  0.24140043556690216
Loss:  0.23980771005153656
Loss:  0.2382526844739914
Loss:  0.23673295974731445
Loss:  0.23524624109268188
Loss:  0.23378998041152954
Loss:  0.23236264288425446
Loss:  0.23096372187137604
Loss:  0.22959120571613312
Loss:  0.22824223339557648
Loss:  0.2269146889448166
Loss:  0.22560957074165344
Loss:  0.22432784736156464
Loss:  0.22306840121746063
Loss:  0.2218300998210907
Loss:  0.2206123173236847
Loss:  0.21941451728343964
Loss:  0.2182365208864212
Loss:  0.21708060801029205
Loss:  0.21594850718975067
Loss:  0.21484042704105377
Loss:  0.2137553095817566
Loss:  0.2126917988061905
Loss:  0.21164840459823608
Loss:  0.21062421798706055
Loss:  0.20961894094944
Loss:  0.20863234996795654
Loss:  0.2076636105775833
Loss:  0.20671197772026062
Loss:  0.20577660202980042
Loss:  0.2048567831516266
Loss:  0.2039518803358078
Loss:  0.20306168496608734
Loss:  0.2021857649087906
Loss:  0.20132331550121307
Loss:  0.20047327876091003
Loss:  0.19963480532169342
Loss:  0.19880804419517517
Loss:  0.1979934573173523
Loss:  0.19719095528125763
Loss:  0.1964002102613449
Loss:  0.19562087953090668
Loss:  0.19485290348529816
Loss:  0.19409632682800293
Loss:  0.19335101544857025
train error: 0.06710004806518555
test error: 0.07730001211166382

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 17:02:58
[0;34m[INFO][0m Training End: 2025-12-15 17:03:16
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 92.26999878883361800%
[0;34m[INFO][0m Test Error: 0.07730001211166382
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.06710004806518555
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_071_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_071_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
