================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_061_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_061_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -l 0.008294
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3830492
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -l 0.008294
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.008294
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:45:04...

training layer:  0
Loss:  1.1267664432525635
Loss:  0.7948498129844666
Loss:  0.7410545945167542
Loss:  0.7238782048225403
Loss:  0.7080198526382446
Loss:  0.701276421546936
Loss:  0.6982362270355225
Loss:  0.696590781211853
Loss:  0.6955369710922241
Loss:  0.6947717666625977
Loss:  0.6941593289375305
Loss:  0.6936370134353638
Loss:  0.693159818649292
Loss:  0.6926880478858948
Loss:  0.6921883225440979
Loss:  0.6916370987892151
Loss:  0.6910151243209839
Loss:  0.6903064846992493
Loss:  0.6894963383674622
Loss:  0.6885688900947571
Loss:  0.6875060796737671
Loss:  0.6862879991531372
Loss:  0.6848939657211304
Loss:  0.6833060383796692
Loss:  0.6815117597579956
Loss:  0.6795046329498291
Loss:  0.6772834062576294
Loss:  0.674850583076477
Loss:  0.6722107529640198
Loss:  0.6693720817565918
Loss:  0.6663457751274109
Loss:  0.6631463170051575
Loss:  0.6597922444343567
Loss:  0.6563054323196411
Loss:  0.6527093648910522
Loss:  0.6490275263786316
Loss:  0.6452834010124207
Loss:  0.6414992809295654
Loss:  0.6376956105232239
Loss:  0.633888840675354
Loss:  0.6300911903381348
Loss:  0.6263116002082825
Loss:  0.6225557327270508
Loss:  0.6188274025917053
Loss:  0.6151294708251953
Loss:  0.611464262008667
Loss:  0.6078329682350159
Loss:  0.6042360663414001
Loss:  0.6006730198860168
Loss:  0.5971423387527466
Loss:  0.5936425924301147
Loss:  0.5901728272438049
Loss:  0.5867326259613037
Loss:  0.5833211541175842
Loss:  0.5799380540847778
Loss:  0.576583206653595
Loss:  0.5732568502426147
Loss:  0.5699589848518372
Loss:  0.5666898488998413
Loss:  0.5634498596191406
Loss:  0.5602405071258545
Loss:  0.5570630431175232
Loss:  0.5539189577102661
Loss:  0.5508090853691101
Loss:  0.5477345585823059
Loss:  0.5446962714195251
Loss:  0.5416951179504395
Loss:  0.5387313365936279
Loss:  0.5358052849769592
Loss:  0.5329166054725647
Loss:  0.5300651788711548
Loss:  0.5272504687309265
Loss:  0.524472177028656
Loss:  0.5217297673225403
Loss:  0.519023060798645
Loss:  0.5163516998291016
Loss:  0.5137155055999756
Loss:  0.5111144185066223
Loss:  0.5085480809211731
Loss:  0.506016194820404
Loss:  0.5035184025764465
Loss:  0.5010539293289185
Loss:  0.4986222982406616
Loss:  0.4962227940559387
Loss:  0.4938548803329468
Loss:  0.49151811003685
Loss:  0.48921194672584534
Loss:  0.48693612217903137
Loss:  0.4846901297569275
Loss:  0.4824737310409546
Loss:  0.4802864193916321
Loss:  0.4781278073787689
Loss:  0.4759974479675293
Loss:  0.47389477491378784
Loss:  0.4718192219734192
Loss:  0.4697701036930084
Loss:  0.4677468538284302
Loss:  0.4657486081123352
Loss:  0.4637746810913086
Loss:  0.4618242084980011
training layer:  1
Loss:  1.1266942024230957
Loss:  0.8079404234886169
Loss:  0.7478575706481934
Loss:  0.729053258895874
Loss:  0.7145987153053284
Loss:  0.7090662121772766
Loss:  0.7065634727478027
Loss:  0.7045544385910034
Loss:  0.7021819353103638
Loss:  0.6985472440719604
Loss:  0.6930840611457825
Loss:  0.685536801815033
Loss:  0.6763956546783447
Loss:  0.6664997935295105
Loss:  0.6565800905227661
Loss:  0.64702969789505
Loss:  0.638159453868866
Loss:  0.6300552487373352
Loss:  0.6226821541786194
Loss:  0.6160350441932678
Loss:  0.610010027885437
Loss:  0.6044943928718567
Loss:  0.5994343757629395
Loss:  0.5947675704956055
Loss:  0.5904281139373779
Loss:  0.5863672494888306
Loss:  0.5825436115264893
Loss:  0.5789403319358826
Loss:  0.5755356550216675
Loss:  0.5722889304161072
Loss:  0.5691920518875122
Loss:  0.5662460923194885
Loss:  0.5634278059005737
Loss:  0.5607113242149353
Loss:  0.5580801367759705
Loss:  0.5555227994918823
Loss:  0.553031325340271
Loss:  0.5506041049957275
Loss:  0.5482450127601624
Loss:  0.5459548830986023
Loss:  0.5437273979187012
Loss:  0.5415540933609009
Loss:  0.5394327640533447
Loss:  0.5373691320419312
Loss:  0.5353654026985168
Loss:  0.5334182381629944
Loss:  0.531522810459137
Loss:  0.5296747088432312
Loss:  0.5278710126876831
Loss:  0.5261104106903076
Loss:  0.5243925452232361
Loss:  0.522716760635376
Loss:  0.5210815072059631
Loss:  0.5194846391677856
Loss:  0.5179233551025391
Loss:  0.5163935422897339
Loss:  0.5148927569389343
Loss:  0.5134197473526001
Loss:  0.511972963809967
Loss:  0.5105498433113098
Loss:  0.5091502666473389
Loss:  0.5077770352363586
Loss:  0.5064295530319214
Loss:  0.5051066875457764
Loss:  0.5038071274757385
Loss:  0.5025293827056885
Loss:  0.5012714862823486
Loss:  0.5000323057174683
Loss:  0.49881091713905334
Loss:  0.4976061284542084
Loss:  0.496417373418808
Loss:  0.4952443838119507
Loss:  0.4940864145755768
Loss:  0.49294236302375793
Loss:  0.4918111562728882
Loss:  0.4906917214393616
Loss:  0.48958221077919006
Loss:  0.48848089575767517
Loss:  0.487385630607605
Loss:  0.4862941801548004
Loss:  0.48520466685295105
Loss:  0.4841158092021942
Loss:  0.4830271303653717
Loss:  0.4819384515285492
Loss:  0.4808492958545685
Loss:  0.4797598421573639
Loss:  0.47867298126220703
Loss:  0.47759509086608887
Loss:  0.47653231024742126
Loss:  0.4754854738712311
Loss:  0.4744538962841034
Loss:  0.4734361171722412
Loss:  0.4724307358264923
Loss:  0.47143661975860596
Loss:  0.47045281529426575
Loss:  0.4694786071777344
Loss:  0.468513548374176
Loss:  0.46755722165107727
Loss:  0.4666094183921814
Loss:  0.46566981077194214
train error: 0.1291000247001648
test error: 0.13270002603530884

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:45:04
[0;34m[INFO][0m Training End: 2025-12-15 16:45:22
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 86.72999739646911600%
[0;34m[INFO][0m Test Error: 0.13270002603530884
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.1291000247001648
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_061_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_061_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
