================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_069_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_069_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff --seed 2251
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3834269
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff --seed 2251
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 2251
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:59:15...

training layer:  0
Loss:  1.126763105392456
Loss:  0.754343569278717
Loss:  0.7029100656509399
Loss:  0.7050544619560242
Loss:  0.6947089433670044
Loss:  0.6921295523643494
Loss:  0.6904791593551636
Loss:  0.6882182359695435
Loss:  0.6851108074188232
Loss:  0.6808057427406311
Loss:  0.675026535987854
Loss:  0.6676554083824158
Loss:  0.6586911678314209
Loss:  0.6483193039894104
Loss:  0.6370698809623718
Loss:  0.6254855990409851
Loss:  0.6139480471611023
Loss:  0.6026473045349121
Loss:  0.5916417837142944
Loss:  0.5809395909309387
Loss:  0.5705381631851196
Loss:  0.5604273676872253
Loss:  0.550614058971405
Loss:  0.5411105155944824
Loss:  0.5319297909736633
Loss:  0.5230754613876343
Loss:  0.5145425200462341
Loss:  0.5063258409500122
Loss:  0.4984188973903656
Loss:  0.49081340432167053
Loss:  0.48350009322166443
Loss:  0.47646844387054443
Loss:  0.4697049558162689
Loss:  0.4631958305835724
Loss:  0.45692816376686096
Loss:  0.4508892595767975
Loss:  0.44506579637527466
Loss:  0.43944451212882996
Loss:  0.4340146780014038
Loss:  0.42876601219177246
Loss:  0.4236878752708435
Loss:  0.41877058148384094
Loss:  0.4140055477619171
Loss:  0.409384548664093
Loss:  0.4048997163772583
Loss:  0.40054407715797424
Loss:  0.3963126242160797
Loss:  0.3922003507614136
Loss:  0.38820183277130127
Loss:  0.38431137800216675
Loss:  0.380524218082428
Loss:  0.3768359422683716
Loss:  0.37324222922325134
Loss:  0.36973831057548523
Loss:  0.3663196563720703
Loss:  0.3629820644855499
Loss:  0.35972169041633606
Loss:  0.3565349578857422
Loss:  0.3534185588359833
Loss:  0.35036906599998474
Loss:  0.3473830223083496
Loss:  0.34445732831954956
Loss:  0.34158894419670105
Loss:  0.33877578377723694
Loss:  0.33601677417755127
Loss:  0.3333105444908142
Loss:  0.33065566420555115
Loss:  0.328050822019577
Loss:  0.3254943788051605
Loss:  0.322984516620636
Loss:  0.3205193877220154
Loss:  0.31809717416763306
Loss:  0.31571629643440247
Loss:  0.31337523460388184
Loss:  0.3110727071762085
Loss:  0.3088074326515198
Loss:  0.3065778911113739
Loss:  0.3043823838233948
Loss:  0.30221909284591675
Loss:  0.30008628964424133
Loss:  0.29798275232315063
Loss:  0.2959077060222626
Loss:  0.2938607931137085
Loss:  0.29184192419052124
Loss:  0.2898506820201874
Loss:  0.2878859341144562
Loss:  0.2859467566013336
Loss:  0.2840322256088257
Loss:  0.28214147686958313
Loss:  0.28027358651161194
Loss:  0.278427392244339
Loss:  0.2766019105911255
Loss:  0.27479663491249084
Loss:  0.2730112671852112
Loss:  0.2712458372116089
Loss:  0.26950016617774963
Loss:  0.2677738070487976
Loss:  0.2660663425922394
Loss:  0.26437732577323914
Loss:  0.2627063989639282
training layer:  1
Loss:  1.1266638040542603
Loss:  0.6370396614074707
Loss:  0.5519753694534302
Loss:  0.5394443273544312
Loss:  0.5354522466659546
Loss:  0.5244740843772888
Loss:  0.5165860056877136
Loss:  0.5073491334915161
Loss:  0.4963902235031128
Loss:  0.4833647310733795
Loss:  0.46856486797332764
Loss:  0.4527595639228821
Loss:  0.43715789914131165
Loss:  0.4225447177886963
Loss:  0.409201979637146
Loss:  0.39723125100135803
Loss:  0.3865189254283905
Loss:  0.37692123651504517
Loss:  0.3682517111301422
Loss:  0.3603528141975403
Loss:  0.3531191349029541
Loss:  0.3464618921279907
Loss:  0.3403216600418091
Loss:  0.3346387445926666
Loss:  0.32933518290519714
Loss:  0.3243592083454132
Loss:  0.3196622133255005
Loss:  0.3152051270008087
Loss:  0.31096377968788147
Loss:  0.30691099166870117
Loss:  0.3030261695384979
Loss:  0.2993031442165375
Loss:  0.2957383096218109
Loss:  0.2923128306865692
Loss:  0.28899407386779785
Loss:  0.28578001260757446
Loss:  0.2826954126358032
Loss:  0.27973470091819763
Loss:  0.2768881618976593
Loss:  0.27414676547050476
Loss:  0.27150020003318787
Loss:  0.2689392566680908
Loss:  0.2664584517478943
Loss:  0.26405271887779236
Loss:  0.26172149181365967
Loss:  0.25946497917175293
Loss:  0.25727981328964233
Loss:  0.25516292452812195
Loss:  0.25311118364334106
Loss:  0.2511208653450012
Loss:  0.24918803572654724
Loss:  0.24730952084064484
Loss:  0.2454838752746582
Loss:  0.24371051788330078
Loss:  0.2419876903295517
Loss:  0.24031297862529755
Loss:  0.23868398368358612
Loss:  0.2370983362197876
Loss:  0.23555395007133484
Loss:  0.2340490221977234
Loss:  0.23258168995380402
Loss:  0.23115016520023346
Loss:  0.2297525852918625
Loss:  0.22838717699050903
Loss:  0.22705240547657013
Loss:  0.2257469743490219
Loss:  0.2244701087474823
Loss:  0.22322101891040802
Loss:  0.22199872136116028
Loss:  0.2208021879196167
Loss:  0.2196304202079773
Loss:  0.21848264336585999
Loss:  0.21735797822475433
Loss:  0.21625560522079468
Loss:  0.21517466008663177
Loss:  0.214114248752594
Loss:  0.21307359635829926
Loss:  0.21205194294452667
Loss:  0.21104863286018372
Loss:  0.21006301045417786
Loss:  0.20909430086612701
Loss:  0.2081420123577118
Loss:  0.20720557868480682
Loss:  0.20628461241722107
Loss:  0.20537863671779633
Loss:  0.20448726415634155
Loss:  0.2036101222038269
Loss:  0.20274698734283447
Loss:  0.20189762115478516
Loss:  0.20106162130832672
Loss:  0.20023854076862335
Loss:  0.19942797720432281
Loss:  0.19862955808639526
Loss:  0.19784313440322876
Loss:  0.19706857204437256
Loss:  0.19630570709705353
Loss:  0.19555428624153137
Loss:  0.19481399655342102
Loss:  0.1940845400094986
Loss:  0.19336558878421783
train error: 0.06710004806518555
test error: 0.08160001039505005

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:59:15
[0;34m[INFO][0m Training End: 2025-12-15 16:59:32
[0;34m[INFO][0m Total Duration: 0h 0m 17s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 91.83999896049499500%
[0;34m[INFO][0m Test Error: 0.08160001039505005
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.06710004806518555
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_069_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_069_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
