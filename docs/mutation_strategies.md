# è¶…å‚æ•°å˜å¼‚ç­–ç•¥å®ç°æ–‡æ¡£

**æ—¥æœŸ**: 2025-11-10
**ç‰ˆæœ¬**: v2.0 - é«˜çº§åˆ†å¸ƒç­–ç•¥
**çŠ¶æ€**: âœ… å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡

---

## æ¦‚è¿°

å®ç°äº†åŸºäºåˆ†å¸ƒçš„é«˜çº§è¶…å‚æ•°å˜å¼‚ç­–ç•¥ï¼Œæ”¯æŒå¯¹æ•°å‡åŒ€åˆ†å¸ƒã€é›¶å€¼æ¦‚ç‡å’Œæ ‡å‡†å‡åŒ€åˆ†å¸ƒã€‚

---

## å˜å¼‚ç­–ç•¥è®¾è®¡

### å‚æ•°åˆ†ç±»ä¸å˜å¼‚èŒƒå›´

| å‚æ•° | å˜å¼‚èŒƒå›´ | åˆ†å¸ƒæ–¹å¼ | é›¶å€¼æ¦‚ç‡ | è®¾è®¡åŸå›  |
|------|---------|---------|---------|---------|
| **Epochs** | `[defaultÃ—0.5, defaultÃ—2.0]` | å¯¹æ•°å‡åŒ€ | 0% | ç›´æ¥å½±å“èƒ½è€—(çº¿æ€§)ï¼Œé¿å…æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ |
| **Learning Rate** | `[defaultÃ—0.1, defaultÃ—10.0]` | å¯¹æ•°å‡åŒ€ â­ | 0% | æŒ‡æ•°æ•æ„Ÿæ€§ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦å’Œèƒ½è€—æ•ˆç‡ |
| **Weight Decay** | `[0.0, defaultÃ—100]` | å¯¹æ•°å‡åŒ€ | 30% | é˜²è¿‡æ‹Ÿåˆï¼Œå¯¹æ•°æ•æ„Ÿï¼Œå…è®¸æ— æ­£åˆ™åŒ– |
| **Dropout** | `[0.0, 0.7]` | å‡åŒ€åˆ†å¸ƒ | 0% | çº¿æ€§å½±å“ï¼Œè¶…è¿‡0.7ä¸¥é‡é˜»ç¢ä¿¡æ¯æµåŠ¨ |
| **Seed** | `[0, 9999]` | å‡åŒ€æ•´æ•° | 0% | è¯„ä¼°ç¨³å®šæ€§ï¼Œä¸ç›´æ¥å½±å“èƒ½è€— |

---

## å®ç°ç»†èŠ‚

### 1. å¯¹æ•°å‡åŒ€åˆ†å¸ƒ (Log-Uniform)

**ç”¨é€”**: Epochs, Learning Rate, Weight Decay (éé›¶å€¼)

**æ•°å­¦åŸç†**:
```
log_min = log(min_val)
log_max = log(max_val)
log_value = uniform(log_min, log_max)
value = exp(log_value)
```

**ç‰¹ç‚¹**:
- åœ¨å¯¹æ•°ç©ºé—´ä¸­å‡åŒ€é‡‡æ ·
- é€‚åˆæŒ‡æ•°æ•æ„Ÿçš„å‚æ•°
- è‡ªåŠ¨å€¾å‘äºè¾ƒå°å€¼ï¼ˆç¬¦åˆå¤§å¤šæ•°æœ€ä¼˜è¶…å‚æ•°çš„ç‰¹æ€§ï¼‰

**å®é™…æ•ˆæœ** (æµ‹è¯•ç»“æœ):
- Learning Rate: åœ¨ [0.001, 0.1] èŒƒå›´å†…ï¼Œlog10å€¼å‡åŒ€åˆ†å¸ƒåœ¨ [-3.0, -1.0]
- 30% å€¼è½åœ¨ 0.001-0.003, 31% åœ¨ 0.003-0.01, 19% åœ¨ 0.01-0.03, 20% åœ¨ 0.03-0.1
- Epochs: 61% å€¼ < 12, 39% å€¼ â‰¥ 12 (èŒƒå›´ [5, 20])

### 2. é›¶å€¼æ¦‚ç‡ (Zero Probability)

**ç”¨é€”**: Weight Decay

**å®ç°**:
```python
if zero_probability > 0 and random.random() < zero_probability:
    return 0.0
```

**ç‰¹ç‚¹**:
- 30% æ¦‚ç‡è¿”å› 0.0 (æ— æ­£åˆ™åŒ–)
- 70% æ¦‚ç‡ä½¿ç”¨å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
- å…è®¸è¯„ä¼°æ­£åˆ™åŒ–çš„çœŸå®å½±å“

**å®é™…æ•ˆæœ** (æµ‹è¯•ç»“æœ):
- 1000æ¬¡é‡‡æ ·: 304æ¬¡é›¶å€¼ (30.4%)
- éé›¶å€¼: å‡å€¼ 0.001577, ä¸­ä½æ•° 0.000376
- å®Œç¾ç¬¦åˆ30%é›¶å€¼çš„è®¾è®¡ç›®æ ‡

### 3. æ ‡å‡†å‡åŒ€åˆ†å¸ƒ (Uniform)

**ç”¨é€”**: Dropout, Seed

**å®ç°**:
```python
# Float
value = random.uniform(min_val, max_val)

# Integer
value = random.randint(min_val, max_val)
```

**ç‰¹ç‚¹**:
- çº¿æ€§ç©ºé—´ä¸­å‡åŒ€é‡‡æ ·
- é€‚åˆçº¿æ€§å½±å“çš„å‚æ•°

**å®é™…æ•ˆæœ** (æµ‹è¯•ç»“æœ):
- Dropout [0.0, 0.7]: å‡å€¼ 0.356 (æ¥è¿‘0.35), å„åŒºé—´åˆ†å¸ƒå‡åŒ€
- Seed [0, 9999]: å‡å€¼ 4811.6 (æ¥è¿‘5000), å››åˆ†ä½æ•°å¤§è‡´å¹³è¡¡

---

## ä»£ç å®ç°

### ä¿®æ”¹æ–‡ä»¶: `mutation.py`

#### 1. å¯¼å…¥mathæ¨¡å—
```python
import math
```

#### 2. æ›´æ–°`mutate_hyperparameter`æ–¹æ³•

```python
def mutate_hyperparameter(self, param_config: Dict, param_name: str = "") -> Any:
    """Mutate a single hyperparameter with advanced strategies

    Implements parameter-specific mutation strategies:
    - Epochs: Log-uniform distribution [defaultÃ—0.5, defaultÃ—2.0]
    - Learning Rate: Log-uniform distribution [defaultÃ—0.1, defaultÃ—10.0]
    - Weight Decay: 30% zero + 70% log-uniform [0.0, defaultÃ—100]
    - Dropout: Uniform distribution [0.0, 0.7]
    - Seed: Uniform integer [0, 9999]
    """
    param_type = param_config["type"]
    param_range = param_config["range"]

    # Get distribution strategy from config
    distribution = param_config.get("distribution", "uniform")
    zero_probability = param_config.get("zero_probability", 0.0)

    # Handle zero probability
    if zero_probability > 0 and random.random() < zero_probability:
        return 0.0 if param_type == "float" else 0

    min_val, max_val = param_range[0], param_range[1]

    # Log-uniform distribution
    if distribution == "log_uniform":
        if min_val <= 0:
            raise ValueError(f"Log-uniform requires min_val > 0")

        log_min = math.log(min_val)
        log_max = math.log(max_val)
        log_value = random.uniform(log_min, log_max)
        value = math.exp(log_value)

        if param_type == "int":
            return max(min_val, min(max_val, int(round(value))))
        else:
            return max(min_val, min(max_val, value))

    # Standard uniform distribution
    elif distribution == "uniform":
        if param_type == "int":
            return random.randint(min_val, max_val)
        else:
            return random.uniform(min_val, max_val)
```

#### 3. æ›´æ–°`generate_mutations`è°ƒç”¨

```python
# Pass parameter name to mutate_hyperparameter
mutation[param] = self.mutate_hyperparameter(param_config, param)
```

---

## é…ç½®æ–‡ä»¶æ ¼å¼

### è¶…å‚æ•°é…ç½®ç¤ºä¾‹

```json
{
  "supported_hyperparams": {
    "epochs": {
      "type": "int",
      "default": 10,
      "range": [5, 20],
      "flag": "--epochs",
      "distribution": "log_uniform"
    },
    "learning_rate": {
      "type": "float",
      "default": 0.01,
      "range": [0.001, 0.1],
      "flag": "--lr",
      "distribution": "log_uniform"
    },
    "weight_decay": {
      "type": "float",
      "default": 0.0001,
      "range": [0.00001, 0.01],
      "flag": "--weight-decay",
      "distribution": "log_uniform",
      "zero_probability": 0.3
    },
    "dropout": {
      "type": "float",
      "default": 0.5,
      "range": [0.0, 0.7],
      "flag": "--dropout",
      "distribution": "uniform"
    },
    "seed": {
      "type": "int",
      "default": 42,
      "range": [0, 9999],
      "flag": "--seed",
      "distribution": "uniform"
    }
  }
}
```

### æ–°å¢å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `distribution` | string | å¦ | åˆ†å¸ƒç±»å‹: "log_uniform" æˆ– "uniform" (é»˜è®¤) |
| `zero_probability` | float | å¦ | é›¶å€¼æ¦‚ç‡ (0.0-1.0)ï¼Œé»˜è®¤0.0 |

---

## æµ‹è¯•éªŒè¯

### æµ‹è¯•æ–‡ä»¶: `test/test_mutation_strategies.py`

è¿è¡Œæµ‹è¯•:
```bash
python3 test/test_mutation_strategies.py
```

### æµ‹è¯•ç»“æœæ‘˜è¦

| æµ‹è¯•é¡¹ | çŠ¶æ€ | å…³é”®æŒ‡æ ‡ |
|--------|------|---------|
| Log-Uniform (Epochs) | âœ… | 61% < ä¸­ä½æ•°, ç¬¦åˆå¯¹æ•°åˆ†å¸ƒç‰¹å¾ |
| Log-Uniform (Learning Rate) | âœ… | Logç©ºé—´å‡åŒ€åˆ†å¸ƒ, å‡å€¼-2.13 |
| Zero Probability (Weight Decay) | âœ… | 30.4% é›¶å€¼, è¯¯å·®<1% |
| Uniform (Dropout) | âœ… | å‡å€¼0.356, æ¥è¿‘ç†è®ºå€¼0.35 |
| Uniform (Seed) | âœ… | å‡å€¼4811.6, æ¥è¿‘ç†è®ºå€¼5000 |
| Mutation Uniqueness | âœ… | 20/20 å˜å¼‚å…¨éƒ¨å”¯ä¸€ |

### å®Œæ•´æµ‹è¯•è¾“å‡º

```
âœ… ALL TESTS COMPLETED SUCCESSFULLY

Summary:
- Log-uniform distribution working for epochs and learning_rate
- Zero probability working for weight_decay (30% zeros)
- Uniform distribution working for dropout and seed
- Mutation uniqueness guaranteed

âœ¨ Mutation strategies are ready for use!
```

---

## ä½¿ç”¨ç¤ºä¾‹

### 1. å‘½ä»¤è¡Œæ¨¡å¼

```bash
# å˜å¼‚epochså’Œlearning_rate (è‡ªåŠ¨ä½¿ç”¨é…ç½®ä¸­çš„åˆ†å¸ƒç­–ç•¥)
python3 mutation.py -r pytorch_resnet_cifar10 -m resnet20 \
                    -mt epochs,learning_rate -n 10

# å˜å¼‚æ‰€æœ‰å‚æ•°
python3 mutation.py -r VulBERTa -m mlp -mt all -n 20
```

### 2. é…ç½®æ–‡ä»¶æ¨¡å¼

åˆ›å»ºå®éªŒé…ç½® `settings/mutation_test.json`:

```json
{
  "experiment_name": "mutation_strategy_test",
  "description": "Test advanced mutation strategies",
  "mode": "mutation",
  "runs_per_config": 10,
  "experiments": [
    {
      "repo": "pytorch_resnet_cifar10",
      "model": "resnet20",
      "mutate": ["epochs", "learning_rate", "weight_decay"]
    }
  ]
}
```

è¿è¡Œ:
```bash
python3 mutation.py -ec settings/mutation_test.json
```

### 3. Python API

```python
from mutation import MutationRunner

runner = MutationRunner(random_seed=42)

# ç”Ÿæˆ10ä¸ªå˜å¼‚
mutations = runner.generate_mutations(
    repo="pytorch_resnet_cifar10",
    model="resnet20",
    mutate_params=["epochs", "learning_rate", "weight_decay"],
    num_mutations=10
)

# æ¯ä¸ªmutationéƒ½æ˜¯å”¯ä¸€çš„è¶…å‚æ•°ç»„åˆ
for i, m in enumerate(mutations, 1):
    print(f"Mutation {i}: {m}")
```

---

## å˜å¼‚ç­–ç•¥ä¼˜åŠ¿

### 1. æ›´æ™ºèƒ½çš„é‡‡æ ·

| ç­–ç•¥ | ä¼ ç»Ÿæ–¹æ³• | æ–°æ–¹æ³• | ä¼˜åŠ¿ |
|------|---------|--------|------|
| Learning Rate | å‡åŒ€åˆ†å¸ƒ [0.001, 0.1] | å¯¹æ•°å‡åŒ€ | æ›´å¤šé‡‡æ ·ç‚¹åœ¨æœ‰æ•ˆèŒƒå›´(0.001-0.01) |
| Weight Decay | å‡åŒ€åˆ†å¸ƒ | 30%é›¶å€¼ + å¯¹æ•°å‡åŒ€ | æ˜¾å¼è¯„ä¼°æ— æ­£åˆ™åŒ–æ•ˆæœ |
| Epochs | å‡åŒ€åˆ†å¸ƒ | å¯¹æ•°å‡åŒ€ | å€¾å‘è¾ƒå°‘epoch(èŠ‚èƒ½) |

### 2. èƒ½è€—-æ€§èƒ½æƒè¡¡

- **å¯¹æ•°åˆ†å¸ƒ**å€¾å‘è¾ƒå°å€¼ï¼šè‡ªç„¶å‡å°‘è®­ç»ƒæ—¶é—´å’Œèƒ½è€—
- **é›¶å€¼æ¦‚ç‡**å…è®¸è¯„ä¼°æç«¯é…ç½®(æ— æ­£åˆ™åŒ–)
- **å‡åŒ€åˆ†å¸ƒ**ç¡®ä¿å……åˆ†æ¢ç´¢dropoutç©ºé—´

### 3. ç§‘å­¦æ€§å’Œå¯é‡å¤æ€§

- åŸºäºæ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦åŸç†
- æ”¯æŒéšæœºç§å­è®¾ç½®
- ç¡®ä¿å˜å¼‚å”¯ä¸€æ€§
- æ‰€æœ‰ç­–ç•¥ç»è¿‡æµ‹è¯•éªŒè¯

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„åˆ†å¸ƒ

| å‚æ•°ç‰¹å¾ | æ¨èåˆ†å¸ƒ | ç¤ºä¾‹å‚æ•° |
|---------|---------|---------|
| æŒ‡æ•°æ•æ„Ÿ | log_uniform | learning_rate, weight_decay |
| çº¿æ€§å½±å“ | uniform | dropout, temperature |
| è®¡æ•°/æ ‡è¯†ç¬¦ | uniform (int) | epochs, seed, batch_size |

### 2. è®¾ç½®å˜å¼‚èŒƒå›´

```python
# åŸºäºé»˜è®¤å€¼çš„å€æ•°
"range": [default * 0.5, default * 2.0]  # Epochs
"range": [default * 0.1, default * 10.0]  # Learning Rate

# ç»å¯¹èŒƒå›´
"range": [0.0, 0.7]  # Dropout (ç‰©ç†çº¦æŸ)
"range": [0, 9999]   # Seed (ä»»æ„æœ‰æ•ˆèŒƒå›´)
```

### 3. ä½¿ç”¨é›¶å€¼æ¦‚ç‡

é€‚ç”¨åœºæ™¯:
- Weight Decay: è¯„ä¼°æ— æ­£åˆ™åŒ–
- Dropout: è¯„ä¼°æ— dropout
- ä»»ä½•å¯ä»¥"å…³é—­"çš„æŠ€æœ¯

å»ºè®®å€¼: 0.2-0.3 (20-30%)

---

## åç»­æ‰©å±•

### è®¡åˆ’æ”¯æŒçš„åˆ†å¸ƒ

1. **æ­£æ€åˆ†å¸ƒ** (Normal): å›´ç»•é»˜è®¤å€¼é‡‡æ ·
2. **Betaåˆ†å¸ƒ**: æ›´çµæ´»çš„æœ‰ç•Œåˆ†å¸ƒ
3. **ç¦»æ•£é›†åˆ**: ä»é¢„å®šä¹‰å€¼ä¸­é€‰æ‹©
4. **æ¡ä»¶åˆ†å¸ƒ**: åŸºäºå…¶ä»–å‚æ•°çš„ä¾èµ–é‡‡æ ·

### é…ç½®æ–‡ä»¶å¢å¼º

```json
{
  "learning_rate": {
    "distribution": "normal",
    "mean": 0.01,
    "std": 0.003,
    "range": [0.001, 0.1]
  },
  "optimizer": {
    "distribution": "discrete",
    "values": ["adam", "sgd", "rmsprop"]
  }
}
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: Log-uniformæŠ¥é”™ "min_val > 0"

**åŸå› **: å¯¹æ•°å‡½æ•°è¦æ±‚è¾“å…¥ > 0

**è§£å†³**: ç¡®ä¿ range[0] > 0
```json
// âŒ é”™è¯¯
"range": [0.0, 0.1]

// âœ… æ­£ç¡®
"range": [0.0001, 0.1]
```

### é—®é¢˜2: é›¶å€¼å¤ªå¤š/å¤ªå°‘

**åŸå› **: éšæœºæ³¢åŠ¨

**è§£å†³**: å¢åŠ é‡‡æ ·æ•°é‡ (ç»Ÿè®¡è§„å¾‹åœ¨å¤§æ ·æœ¬ä¸‹æ›´ç¨³å®š)

### é—®é¢˜3: å˜å¼‚ä¸å¤Ÿå¤šæ ·

**æ£€æŸ¥**:
1. æ˜¯å¦èŒƒå›´å¤ªçª„?
2. æ˜¯å¦ç²¾åº¦å¤ªä½ (int vs float)?
3. æ˜¯å¦å‚æ•°æ•°é‡å¤ªå°‘?

**è§£å†³**: æ‰©å¤§èŒƒå›´æˆ–å¢åŠ å˜å¼‚å‚æ•°

---

## æ€»ç»“

### âœ… å·²å®Œæˆ

1. âœ… å®ç°å¯¹æ•°å‡åŒ€åˆ†å¸ƒ (log_uniform)
2. âœ… å®ç°é›¶å€¼æ¦‚ç‡æœºåˆ¶ (zero_probability)
3. âœ… ä¿ç•™æ ‡å‡†å‡åŒ€åˆ†å¸ƒ (uniform)
4. âœ… æ”¯æŒæ•´æ•°å’Œæµ®ç‚¹æ•°
5. âœ… ç¡®ä¿å˜å¼‚å”¯ä¸€æ€§
6. âœ… å®Œæ•´æµ‹è¯•å¥—ä»¶
7. âœ… æ–‡æ¡£å’Œç¤ºä¾‹

### ğŸ“ˆ æ€§èƒ½å½±å“

- **ä»£ç å¤æ‚åº¦**: ä½ (ä»…å¢åŠ ~40è¡Œæ ¸å¿ƒä»£ç )
- **è¿è¡Œæ€§èƒ½**: æ— å½±å“ (O(1)é‡‡æ ·ç®—æ³•)
- **å¯ç»´æŠ¤æ€§**: é«˜ (æ¸…æ™°çš„åˆ†å¸ƒæŠ½è±¡)
- **å¯æ‰©å±•æ€§**: å¼º (æ˜“äºæ·»åŠ æ–°åˆ†å¸ƒ)

### ğŸ¯ æ¨èé…ç½®

å¯¹äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹:

```json
{
  "epochs": {"distribution": "log_uniform", "range": "[default*0.5, default*2]"},
  "learning_rate": {"distribution": "log_uniform", "range": "[default*0.1, default*10]"},
  "weight_decay": {"distribution": "log_uniform", "zero_probability": 0.3},
  "dropout": {"distribution": "uniform", "range": "[0.0, 0.7]"},
  "seed": {"distribution": "uniform", "range": "[0, 9999]"}
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0
**æœ€åæ›´æ–°**: 2025-11-10
**æµ‹è¯•çŠ¶æ€**: å…¨éƒ¨é€šè¿‡ âœ…
