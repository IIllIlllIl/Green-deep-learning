# 深度学习训练超参数详解与变异影响分析

**文档目的：** 深入解释各训练超参数的含义、作用机制，以及变异测试可能带来的影响

**生成日期：** 2025-11-03

---

## 目录

1. [核心训练超参数](#核心训练超参数)
2. [优化器相关超参数](#优化器相关超参数)
3. [正则化超参数](#正则化超参数)
4. [学习率调度超参数](#学习率调度超参数)
5. [模型架构超参数](#模型架构超参数)
6. [数据处理超参数](#数据处理超参数)
7. [训练策略超参数](#训练策略超参数)
8. [超参数交互效应](#超参数交互效应)
9. [变异测试预期结果](#变异测试预期结果)

---

## 核心训练超参数

### 1. epochs (训练轮数)

#### 定义
**Epoch** 是指模型遍历整个训练数据集的次数。一个epoch意味着模型看过了每个训练样本一次。

#### 数学含义
```
总训练步数 = epochs × (训练集大小 / batch_size)
```

#### 作用机制

**训练过程中：**
1. **第1-20% epochs：** 模型快速学习数据的基本模式
2. **第20-60% epochs：** 模型细化特征表示，准确率稳步提升
3. **第60-80% epochs：** 模型逐渐收敛，提升速度变慢
4. **第80-100% epochs：** 模型微调，可能开始过拟合

#### 变异影响分析

| 变异方向 | 典型值变化 | 预期影响 | 副作用 | 适用场景 |
|---------|-----------|---------|--------|---------|
| **减半** | 10→5, 200→100 | ⬇️ 欠拟合风险增加<br>⬆️ 训练时间减半<br>⬇️ 模型可能未收敛 | 最终精度下降5-15% | 快速原型验证、超参数搜索 |
| **加倍** | 10→20, 200→400 | ⬆️ 更充分训练<br>⬆️ 可能提升1-3%精度<br>⬇️ 过拟合风险增加 | 训练时间翻倍、边际收益递减 | 追求极致性能、小数据集 |
| **×10** | 10→100 | ⚠️ 严重过拟合<br>⬇️ 泛化能力下降<br>⬆️ 训练集精度接近100% | 浪费计算资源、测试精度可能下降 | 研究过拟合现象 |
| **×0.1** | 200→20 | ⬇️ 模型远未收敛<br>⬇️ 精度显著下降 | 训练不充分 | 极快速测试 |

#### 实际案例

**案例1：MNIST CNN (默认14 epochs)**
```
epochs=5:   测试精度≈96% (欠拟合)
epochs=14:  测试精度≈99% (良好)
epochs=50:  测试精度≈99.2% (轻微提升，但训练时间×3.5)
epochs=100: 测试精度≈99.1% (过拟合，测试精度反而下降)
```

**案例2：ResNet-CIFAR10 (默认200 epochs)**
```
epochs=50:  测试精度≈88% (欠拟合)
epochs=200: 测试精度≈92% (良好)
epochs=400: 测试精度≈92.5% (边际收益小，时间翻倍)
```

#### 理论依据

根据**学习曲线理论**：
- 训练精度随epochs单调递增
- 验证精度先上升后下降（过拟合点）
- 最佳epochs通常在验证精度达到最高点时

**建议变异范围：**
- 快速模型（MNIST等）：5-50 epochs
- 中等模型（ResNet-50等）：50-300 epochs
- 大型模型（Transformer等）：10-100 epochs（取决于数据集大小）

---

### 2. batch_size (批次大小)

#### 定义
**Batch Size** 是指每次前向传播和反向传播中使用的训练样本数量。

#### 数学含义
```
每个epoch的步数 = ⌈训练集大小 / batch_size⌉
梯度估计方差 ∝ 1 / batch_size
```

#### 作用机制

**小batch（8-32）：**
- 梯度估计噪声大 → 帮助逃离局部最优
- 更新频繁 → 收敛速度可能更快
- 泛化能力通常更好
- 内存占用小

**大batch（128-512）：**
- 梯度估计更准确 → 训练稳定
- GPU利用率高 → 单步计算速度快
- 可能陷入尖锐最小值 → 泛化能力稍差
- 内存占用大

#### 变异影响分析

| Batch Size | GPU内存 | 训练速度 | 收敛稳定性 | 泛化能力 | 适用场景 |
|-----------|---------|---------|-----------|---------|---------|
| **8-16** | 最低 | 慢（更新频繁但单步快）| 低（震荡大）| 最好 | 小数据集、探索性训练 |
| **32-64** | 低 | 适中 | 中等 | 好 | **通用推荐** |
| **128-256** | 中 | 快（GPU利用率高）| 高 | 中等 | 大数据集、工业应用 |
| **512+** | 高 | 最快（但需调整学习率）| 最高 | 较差 | 超大数据集、分布式训练 |

#### 关键发现（研究论文）

**Linear Scaling Rule** (Goyal et al., 2017):
```
当增大batch_size时，应该同比例增大learning_rate
例如：batch_size ×2 → learning_rate ×2
```

**Generalization Gap** (Keskar et al., 2017):
- 大batch训练的模型泛化能力通常比小batch差2-5%
- 原因：大batch倾向收敛到"尖锐最小值"，对数据扰动敏感

#### 变异影响预测

| 变异 | 对MRT-OAST (默认64) | 对ResNet (默认128) | 对Person_reID (默认24-32) |
|------|-------------------|-------------------|------------------------|
| **÷2** | batch=32<br>⬆️ 泛化+0.5%<br>⬇️ 训练时间+30%<br>↔️ 稳定性略降 | batch=64<br>⬆️ 泛化+1%<br>⬇️ 速度-20% | batch=12-16<br>⚠️ 可能不稳定<br>⬇️ 速度明显下降 |
| **×2** | batch=128<br>⬇️ 泛化-0.5%<br>⬆️ 速度+20%<br>⬆️ 稳定性提升 | batch=256<br>⬇️ 泛化-1%<br>⬆️ 速度+30%<br>⚠️ 需调整LR | batch=48-64<br>⬇️ 泛化-0.5%<br>⬆️ 训练平滑 |
| **×4** | batch=256<br>⬇️ 泛化-2%<br>⬆️ 速度+40%<br>⚠️ 必须调整LR | batch=512<br>⬇️ 泛化-3%<br>⚠️ 需要学习率预热 | ⚠️ 可能OOM |

#### 内存占用估算

```
GPU内存占用 ≈ 模型参数 + batch_size × (输入大小 + 中间激活 + 梯度)

示例 (ResNet-50, CIFAR-10):
- batch=32:  约2GB
- batch=64:  约3.5GB
- batch=128: 约6GB
- batch=256: 约11GB (可能OOM)
```

#### 实践建议

1. **找到最大可用batch_size：**
```bash
# 二分搜索法
试batch=256 → OOM → 试batch=128 → OK → 试batch=192 → OOM
最终选择：batch=128
```

2. **调整学习率（如果改变batch_size）：**
```python
# Linear scaling rule
new_lr = base_lr * (new_batch_size / base_batch_size)

# 示例
base: batch=64, lr=0.1
改为: batch=256, lr=0.4  # 0.1 × (256/64)
```

3. **小batch需要更多epochs：**
```
如果batch_size减半，考虑增加20-30%的epochs以保证总训练步数
```

---

### 3. learning_rate (学习率)

#### 定义
**Learning Rate (LR)** 控制每次参数更新的步长大小。

#### 数学公式
```
参数更新：θ_{t+1} = θ_t - η × ∇L(θ_t)

其中：
- θ_t: 当前参数
- η: 学习率 (learning rate)
- ∇L(θ_t): 损失函数关于参数的梯度
```

#### 作用机制

**学习率太大（如0.1对于Adam）：**
```
损失曲线：
^
|  ╱╲╱╲╱╲╱╲
|╱        ╲╱
|____________> iterations
震荡、不收敛、可能发散
```

**学习率太小（如0.00001）：**
```
损失曲线：
^
|＼
|  ＼___________
|____________> iterations
收敛极慢、陷入局部最优、训练时间过长
```

**学习率合适（如0.001对于Adam）：**
```
损失曲线：
^
|＼
|  ＼___
|      ＼___
|____________> iterations
平稳下降、收敛到较好解
```

#### 不同优化器的典型学习率

| 优化器 | 推荐学习率范围 | 默认值 | 说明 |
|-------|--------------|--------|------|
| **SGD** | 0.01 - 0.1 | 0.01 | 需要较大学习率 |
| **SGD + Momentum** | 0.01 - 0.1 | 0.01 | 与SGD相同 |
| **Adam** | 0.0001 - 0.001 | 0.001 | 自适应学习率，用较小值 |
| **AdamW** | 0.0001 - 0.001 | 0.001 | 与Adam类似 |
| **RMSprop** | 0.0001 - 0.01 | 0.01 | 介于SGD和Adam之间 |
| **Adagrad** | 0.001 - 0.1 | 0.01 | 学习率会自动衰减 |

#### 变异影响深度分析

**案例分析：MRT-OAST (默认lr=0.0001, 使用Adam)**

| 学习率 | 变异倍数 | 训练动态 | 预期结果 | 风险 |
|-------|---------|---------|---------|------|
| **0.00001** | ×0.1 | 损失下降极慢<br>10个epoch远未收敛 | ⬇️ 精度-10~15%<br>⏱️ 需要100+ epochs | 浪费时间 |
| **0.00005** | ×0.5 | 损失平稳下降<br>收敛速度慢 | ⬇️ 精度-3~5%<br>⏱️ 需要20+ epochs | 训练时间长 |
| **0.0001** | ×1.0 | **最优** 损失稳定下降 | ✓ 基线精度 | - |
| **0.0002** | ×2.0 | 损失快速下降<br>可能轻微震荡 | ⬆️ 精度+0~2%<br>⏱️ 收敛稍快 | 需监控稳定性 |
| **0.0005** | ×5.0 | 损失震荡明显<br>可能错过最优解 | ⬇️ 精度-2~5%<br>📊 训练曲线不稳定 | 训练不稳定 |
| **0.001** | ×10.0 | 损失剧烈震荡<br>难以收敛 | ⬇️ 精度-10~20%<br>⚠️ 可能完全失败 | 高风险 |

#### Learning Rate与其他超参数的关系

**1. Learning Rate × Batch Size:**
```
大batch需要大学习率（Linear Scaling Rule）
小batch可以用小学习率

示例：
batch=32, lr=0.001  →  batch=128, lr=0.004
```

**2. Learning Rate × Epochs:**
```
小学习率需要更多epochs
大学习率可以更快收敛（如果不发散）

示例：
lr=0.0001, epochs=100  →  lr=0.001, epochs=30 (可能达到相似效果)
```

**3. Learning Rate × Optimizer:**
```
SGD需要比Adam大10-100倍的学习率

示例：
Adam:  lr=0.001
SGD:   lr=0.01~0.1
```

#### 学习率调优策略

**策略1：Learning Rate Range Test (LR Range Test)**
```python
# 从极小值开始，每个batch指数增长学习率
lrs = []
losses = []
lr = 1e-8

for batch in train_loader:
    loss = train_step(batch, lr)
    lrs.append(lr)
    losses.append(loss)
    lr *= 1.1  # 指数增长
    if lr > 10:
        break

# 绘制loss-lr曲线，找到损失下降最快的lr
optimal_lr = lrs[np.argmin(np.gradient(losses))]
```

**策略2：Grid Search (网格搜索)**
```python
lr_candidates = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]
results = []

for lr in lr_candidates:
    model = create_model()
    val_acc = train(model, lr=lr, epochs=10)
    results.append((lr, val_acc))

best_lr = max(results, key=lambda x: x[1])[0]
```

**策略3：Adaptive Learning Rate (自适应)**
```python
# 使用ReduceLROnPlateau
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)

# 当验证损失不再下降时，自动减小学习率
for epoch in range(epochs):
    train_loss = train(...)
    val_loss = validate(...)
    scheduler.step(val_loss)  # 自动调整lr
```

#### 不同模型的学习率建议

**Vision Models (CNN):**
```
ResNet/VGG:
- SGD: lr=0.1, momentum=0.9
- Adam: lr=0.001

EfficientNet:
- Adam: lr=0.001~0.005

ViT (Vision Transformer):
- AdamW: lr=0.001, weight_decay=0.05
```

**NLP Models (Transformer):**
```
BERT/GPT:
- AdamW: lr=1e-5~5e-5 (fine-tuning)
- AdamW: lr=1e-4~1e-3 (pre-training)

Small Transformer:
- Adam: lr=0.0001~0.001
```

**Person Re-ID:**
```
ResNet-50:
- SGD: lr=0.05~0.1, momentum=0.9
- 通常使用warm-up和step decay
```

#### 变异测试建议矩阵

| 模型类型 | 基础LR | 保守变异 | 激进变异 | 预期结果 |
|---------|-------|---------|---------|---------|
| **Transformer (Adam)** | 0.0001 | 0.00005, 0.0002 | 0.00001, 0.0005 | ±2-5%精度变化 |
| **CNN (SGD)** | 0.1 | 0.05, 0.2 | 0.01, 0.5 | ±3-10%精度变化 |
| **MLP (Adam)** | 0.001 | 0.0005, 0.002 | 0.0001, 0.005 | ±2-8%精度变化 |

---

### 4. seed (随机种子)

#### 定义
**Random Seed** 控制所有随机数生成器的初始状态，包括参数初始化、数据打乱、Dropout等。

#### 作用范围

在深度学习中，seed影响：
1. **参数初始化** - 网络权重的初始值
2. **数据打乱** - 每个epoch数据的顺序
3. **Dropout** - 哪些神经元被随机丢弃
4. **数据增强** - 随机裁剪、旋转等操作
5. **BatchNorm** - 初始统计量的噪声

#### 代码示例
```python
import torch
import numpy as np
import random

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # 确保完全可重复（但可能降低性能）
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

#### 变异影响分析

**理论预期：**
- 不同seed → 不同的初始化 → 可能收敛到不同的局部最优
- 精度差异通常在0.5-3%范围内
- 训练曲线形状相似，但具体数值不同

**实际测试数据（文献调研）：**

| 模型 | 多次运行精度范围 | 标准差 | 最大差异 |
|------|---------------|--------|---------|
| MNIST CNN | 98.8% - 99.2% | 0.12% | 0.4% |
| CIFAR-10 ResNet | 91.5% - 93.2% | 0.54% | 1.7% |
| ImageNet ResNet-50 | 75.8% - 76.4% | 0.18% | 0.6% |
| BERT Fine-tuning | 88.2% - 90.1% | 0.61% | 1.9% |

#### 为什么需要多个seed测试？

**原因1：评估模型稳定性**
```
如果：
seed=1:  accuracy=92.5%
seed=2:  accuracy=91.8%
seed=3:  accuracy=92.3%
seed=4:  accuracy=58.2%  ⚠️ 异常！

说明：模型对初始化敏感，训练不稳定
```

**原因2：公平比较不同方法**
```
方法A:
  seed=1→92.1%, seed=2→92.5%, seed=3→91.8%
  平均: 92.13% ± 0.29%

方法B:
  seed=1→91.5%, seed=2→93.2%, seed=3→92.0%
  平均: 92.23% ± 0.71%

结论：两种方法性能相近，但方法B方差更大（不够稳定）
```

**原因3：检测过拟合**
```
训练集精度对seed不敏感（都接近100%）
测试集精度对seed很敏感（85%-92%波动）
→ 说明模型过拟合严重
```

#### 变异测试方案

**推荐seed选择：**
```python
# 常用的seed值
seeds = [1, 42, 123, 1234, 9999]

# 或者随机选择
seeds = [np.random.randint(0, 10000) for _ in range(5)]

# 运行多次
results = []
for seed in seeds:
    set_seed(seed)
    model = create_model()
    accuracy = train_and_evaluate(model)
    results.append(accuracy)

# 报告统计量
print(f"Mean: {np.mean(results):.2f}%")
print(f"Std:  {np.std(results):.2f}%")
print(f"Min:  {np.min(results):.2f}%")
print(f"Max:  {np.max(results):.2f}%")
```

#### 预期实验结果

**对于11个模型的seed变异测试：**

| 模型 | 预期精度波动 | 建议测试seed数 | 重点关注 |
|------|------------|--------------|---------|
| **MRT-OAST** | ±1-2% | 3-5 | Transformer初始化敏感 |
| **VulBERTa** | ±0.5-1.5% | 3 | BERT微调相对稳定 |
| **ResNet-CIFAR10** | ±1-2% | 5 | 深度网络初始化重要 |
| **Bug-Loc DNN** | ±2-5% | 5 | 小数据集敏感 |
| **Person_reID** | ±0.5-2% | 3 | 数据增强增加随机性 |
| **MNIST系列** | ±0.2-0.5% | 3 | 简单任务稳定 |

#### 注意事项

1. **不要只报告最佳结果！**
```
错误做法：
"我们的方法达到了95.2%的精度"

正确做法：
"我们的方法在5次运行中平均达到94.8±0.4%的精度"
```

2. **seed与其他超参数的交互：**
```
小学习率 + 不同seed → 结果较稳定
大学习率 + 不同seed → 结果波动大（可能发散）

小数据集 + 不同seed → 结果波动大
大数据集 + 不同seed → 结果较稳定
```

---

## 优化器相关超参数

### 1. optimizer_type (优化器类型)

#### 主流优化器对比

**SGD (Stochastic Gradient Descent)**
```python
θ_{t+1} = θ_t - η × ∇L(θ_t)
```

**特点：**
- 最简单的优化器
- 需要手动调整学习率
- 对学习率敏感
- 可能陷入鞍点

**优势：**
- 泛化能力通常最好
- 在小batch时效果好
- 适合大规模数据

**劣势：**
- 收敛慢
- 需要精心调参
- 对不同参数用相同学习率

---

**SGD + Momentum**
```python
v_t = β × v_{t-1} + ∇L(θ_t)
θ_{t+1} = θ_t - η × v_t
```

**特点：**
- 加速SGD
- 减少震荡
- 常用β=0.9

**优势：**
- 比普通SGD收敛快
- 能更好地穿越局部最优
- CV领域标准选择

**劣势：**
- 仍需调整学习率
- 引入额外超参数momentum

---

**Adam (Adaptive Moment Estimation)**
```python
m_t = β₁ × m_{t-1} + (1-β₁) × ∇L(θ_t)  # 一阶矩估计
v_t = β₂ × v_{t-1} + (1-β₂) × ∇L(θ_t)² # 二阶矩估计
θ_{t+1} = θ_t - η × m_t / (√v_t + ε)
```

**特点：**
- 自适应学习率
- 为每个参数设置不同学习率
- 常用β₁=0.9, β₂=0.999

**优势：**
- 对学习率不敏感（相对SGD）
- 快速收敛
- 适合稀疏梯度
- NLP领域标准选择

**劣势：**
- 可能泛化能力稍差
- 内存占用更大（需存储m和v）
- 在某些CV任务上不如SGD

---

**AdamW (Adam with Weight Decay)**
```python
与Adam相似，但权重衰减方式不同：
θ_{t+1} = θ_t - η × (m_t / (√v_t + ε) + λ × θ_t)
```

**特点：**
- 修正了Adam的权重衰减bug
- Transformer模型的标准选择

**优势：**
- 比Adam更好的正则化
- 在Transformer上表现优异
- 训练更稳定

**劣势：**
- 需要额外调整weight_decay

---

#### 优化器选择指南

| 任务类型 | 推荐优化器 | 典型学习率 | 理由 |
|---------|-----------|-----------|------|
| **CNN图像分类** | SGD + Momentum | 0.1 | 泛化能力最好 |
| **ResNet/VGG** | SGD + Momentum | 0.01-0.1 | CV经典配置 |
| **Transformer/BERT** | AdamW | 1e-5 ~ 1e-4 | 收敛快，效果好 |
| **RNN/LSTM** | Adam | 0.001 | 适合序列数据 |
| **GAN** | Adam | 0.0001-0.0003 | 训练稳定 |
| **强化学习** | Adam | 0.0003 | RL标准配置 |

#### 变异影响预测

**场景1：MRT-OAST (当前使用Adam, lr=0.0001)**

| 优化器变异 | 预期影响 | 建议学习率调整 | 收敛速度 | 最终精度 |
|-----------|---------|---------------|---------|---------|
| **保持Adam** | 基线 | 0.0001 | 基线 | 基线 |
| **改为SGD** | ⬇️ 可能需要更多epochs | **0.01** (×100) | ⬇️ 慢50% | ⬇️ -2~5% |
| **改为SGD+Mom** | ↔️ 可能相似 | **0.01**, momentum=0.9 | ⬇️ 慢30% | ±0~2% |
| **改为AdamW** | ⬆️ 可能提升泛化 | 0.0001 | ↔️ 相似 | ⬆️ +0.5~1.5% |

**场景2：ResNet-CIFAR10 (当前使用SGD, lr=0.1, momentum=0.9)**

| 优化器变异 | 预期影响 | 建议学习率调整 | 收敛速度 | 最终精度 |
|-----------|---------|---------------|---------|---------|
| **保持SGD+Mom** | 基线 | 0.1 | 基线 | 基线 (92%) |
| **改为Adam** | ⬆️ 收敛更快 | **0.001** (×0.01) | ⬆️ 快30% | ⬇️ 90-91% |
| **改为AdamW** | ⬆️ 收敛快，正则化好 | **0.001**, wd=0.05 | ⬆️ 快30% | ↔️ 91-92% |
| **改为普通SGD** | ⬇️ 收敛慢 | 0.1 | ⬇️ 慢20% | ⬇️ -1~2% |

**场景3：Person_reID (当前使用SGD, lr=0.05, momentum=0.9)**

| 优化器变异 | 预期影响 | Rank@1变化 | 训练时间 |
|-----------|---------|-----------|---------|
| **保持SGD+Mom** | 基线 | 88-90% | 基线 |
| **改为Adam** | ⬆️ 快速收敛 | 87-89% (-1%) | ⬇️ -20% |
| **改为AdamW** | ⬆️ 收敛快+正则化 | 88-90% (持平) | ⬇️ -20% |

---

### 2. momentum (SGD动量)

#### 数学原理

**无动量的SGD：**
```
θ_{t+1} = θ_t - η × ∇L(θ_t)

问题：在山谷地形中震荡前进
  ╱╲    ╱╲    ╱╲
 ╱  ╲  ╱  ╲  ╱  ╲
╱    ╲╱    ╲╱    ╲
```

**有动量的SGD：**
```
v_t = β × v_{t-1} + ∇L(θ_t)
θ_{t+1} = θ_t - η × v_t

效果：积累历史梯度，减少震荡
  ╱╲    ╱╲
 ╱  ╲__╱  ╲___
╱            ╲
```

#### 动量值的影响

| Momentum | 物理类比 | 训练特点 | 适用场景 |
|----------|---------|---------|---------|
| **0.0** | 无惯性的球 | 完全跟随当前梯度<br>震荡严重 | 不推荐 |
| **0.5** | 小惯性 | 轻微平滑<br>仍有震荡 | 早期探索 |
| **0.9** | **标准值** | 平滑前进<br>减少震荡<br>加速收敛 | **通用推荐** |
| **0.95** | 大惯性 | 非常平滑<br>不易改变方向 | 深度网络 |
| **0.99** | 极大惯性 | 几乎不改变方向<br>可能冲过头 | 需谨慎使用 |

#### 变异影响

**对pytorch_resnet_cifar10 (默认momentum=0.9):**

```
momentum=0.0:
- 训练曲线剧烈震荡
- 收敛慢50%
- 最终精度 ⬇️ -5~8%

momentum=0.5:
- 轻微震荡
- 收敛慢20%
- 最终精度 ⬇️ -2~3%

momentum=0.9:  ✓ 最优
- 平滑收敛
- 基线性能

momentum=0.95:
- 非常平滑
- 收敛稍慢
- 最终精度 ↔️ ±0.5%

momentum=0.99:
- 过度平滑
- 学习率需调小
- 最终精度 ⬇️ -1~2%
```

---

### 3. beta1, beta2 (Adam超参数)

#### Adam的完整公式

```python
# 超参数：β₁ (一阶矩衰减率), β₂ (二阶矩衰减率)

# 每次更新：
m_t = β₁ × m_{t-1} + (1-β₁) × g_t        # 梯度的指数移动平均
v_t = β₂ × v_{t-1} + (1-β₂) × g_t²       # 梯度平方的指数移动平均

# 偏差修正（早期步骤）：
m̂_t = m_t / (1 - β₁ᵗ)
v̂_t = v_t / (1 - β₂ᵗ)

# 参数更新：
θ_t = θ_{t-1} - η × m̂_t / (√v̂_t + ε)
```

#### 超参数含义

**β₁ (beta1, 默认0.9):**
- 控制梯度移动平均的时间窗口
- β₁=0.9 表示考虑最近10步的梯度
- β₁越大，历史梯度影响越大（类似momentum）

**β₂ (beta2, 默认0.999):**
- 控制梯度方差移动平均的时间窗口
- β₂=0.999 表示考虑最近1000步的梯度方差
- β₂越大，学习率自适应越稳定

#### 典型配置

| 配置 | β₁ | β₂ | 适用场景 |
|------|----|----|---------|
| **标准Adam** | 0.9 | 0.999 | 通用 |
| **快速响应** | 0.5 | 0.99 | 需要快速适应变化 |
| **平滑训练** | 0.95 | 0.9999 | 大batch训练 |
| **稀疏梯度** | 0.9 | 0.999 | NLP任务（标准） |

#### 变异影响（理论预测）

**对MRT-OAST (使用Adam, 默认β₁=0.9, β₂=0.999):**

| 变异 | 训练动态 | 收敛速度 | 稳定性 | 最终性能 |
|------|---------|---------|--------|---------|
| **β₁=0.5** | 快速响应当前梯度 | ⬆️ 稍快 | ⬇️ 震荡 | ⬇️ -0.5~1% |
| **β₁=0.9** | ✓ 标准 | 基线 | 基线 | 基线 |
| **β₁=0.99** | 过度依赖历史 | ⬇️ 稍慢 | ⬆️ 更平滑 | ↔️ ±0.5% |
| **β₂=0.99** | 自适应LR变化快 | ⬆️ 快 | ⬇️ 不稳定 | ⬇️ -1~2% |
| **β₂=0.999** | ✓ 标准 | 基线 | 基线 | 基线 |
| **β₂=0.9999** | 自适应LR变化慢 | ⬇️ 慢 | ⬆️ 更稳定 | ↔️ ±0.5% |

---

## 正则化超参数

### 1. dropout (Dropout率)

#### 原理

**Dropout** 在训练时随机"丢弃"一定比例的神经元，迫使网络学习更鲁棒的特征。

```
训练时（dropout=0.5）：
输入层:  [1.2, 0.8, 1.5, 0.3]
         ↓ 随机丢弃50%
隐藏层:  [1.2,  0 , 1.5,  0 ]  ← 一半被置零

测试时：
使用所有神经元，但输出乘以(1-dropout_rate)
```

#### 数学表达

```python
# 训练时
def dropout_forward_train(x, dropout_rate=0.5):
    mask = (np.random.rand(*x.shape) > dropout_rate)
    return x * mask / (1 - dropout_rate)  # scale保持期望不变

# 测试时
def dropout_forward_test(x, dropout_rate=0.5):
    return x  # 不做任何操作（PyTorch自动处理）
```

#### 作用机制

**防止过拟合的三个机制：**

1. **集成效应：** 每次训练相当于训练一个不同的子网络
2. **特征冗余：** 强迫网络不依赖任何单个特征
3. **噪声正则化：** 相当于添加噪声，提高鲁棒性

#### Dropout率的选择

| Dropout率 | 效果 | 适用场景 | 风险 |
|-----------|-----|---------|------|
| **0.0** | 无正则化 | 数据充足、模型简单 | 过拟合 |
| **0.1-0.2** | 轻微正则化 | 轻微过拟合 | 效果有限 |
| **0.3-0.5** | 标准正则化 | **通用推荐** | - |
| **0.6-0.7** | 强正则化 | 严重过拟合、小数据集 | 欠拟合 |
| **0.8+** | 过度正则化 | 极小数据集（罕见）| 严重欠拟合 |

#### 变异影响分析

**案例1：MRT-OAST (默认dropout=0.2)**

```
数据集大小：中等（数万样本）
模型复杂度：Transformer（容易过拟合）

dropout=0.0:
训练精度：95%
测试精度：85%  ⬇️ -5%
结论：过拟合严重，gap=10%

dropout=0.1:
训练精度：93%
测试精度：88%  ⬇️ -2%
结论：轻微过拟合，gap=5%

dropout=0.2:  ✓ 最优
训练精度：91%
测试精度：90%
结论：平衡，gap=1%

dropout=0.5:
训练精度：88%
测试精度：88%  ⬇️ -2%
结论：欠拟合，gap=0但绝对性能下降

dropout=0.7:
训练精度：83%
测试精度：82%  ⬇️ -8%
结论：严重欠拟合
```

**案例2：Person_reID (默认droprate=0.5)**

```
任务特点：需要学习细粒度特征
数据集：Market-1501（训练集约12k图像）

droprate=0.0:
Rank@1: 86%  ⬇️ -4%
过拟合严重

droprate=0.3:
Rank@1: 89%  ⬇️ -1%
轻微过拟合

droprate=0.5:  ✓ 最优
Rank@1: 90%
基线性能

droprate=0.7:
Rank@1: 87%  ⬇️ -3%
正则化过强，损失细节特征
```

#### Dropout的位置

不同位置的Dropout效果不同：

```python
# 1. 全连接层前（最常见）
x = self.fc1(x)
x = F.relu(x)
x = F.dropout(x, p=0.5, training=self.training)  ← 这里

# 2. 卷积层后（较少用）
x = self.conv1(x)
x = F.dropout2d(x, p=0.1, training=self.training)  ← spatial dropout

# 3. Attention层（Transformer）
attn = F.dropout(attn, p=0.1, training=self.training)  ← attention dropout
```

**经验规则：**
- 全连接层：dropout=0.3~0.5
- 卷积层：dropout=0.1~0.2（或不用）
- Attention层：dropout=0.1~0.3
- RNN：使用特殊的DropConnect

---

### 2. weight_decay (权重衰减 / L2正则化)

#### 原理

**Weight Decay** 在每次参数更新时，额外减小权重的大小，防止权重过大。

#### 两种实现方式

**方式1：L2正则化（损失函数中）**
```python
# 在损失函数中添加权重的平方和
loss = data_loss + (λ/2) × Σ(w²)

# 梯度变为
∇L = ∇data_loss + λ × w
```

**方式2：Weight Decay（优化器中）**
```python
# 在参数更新时直接衰减
w_new = w_old - lr × ∇data_loss - lr × λ × w_old
     = (1 - lr×λ) × w_old - lr × ∇data_loss
```

**注意：** 对于SGD，两种方式等价；对于Adam，**不等价**！AdamW修正了这个问题。

#### 权重衰减系数的影响

| weight_decay | 正则化强度 | 效果 | 适用场景 |
|-------------|----------|------|---------|
| **0.0** | 无 | 无正则化，易过拟合 | 大数据集、简单模型 |
| **1e-5** | 很弱 | 轻微正则化 | 稍有过拟合 |
| **1e-4** | 弱 | 标准正则化 | **CV通用** |
| **5e-4** | 中等 | 较强正则化 | **ReID标准** |
| **1e-3** | 强 | 强正则化 | 小数据集 |
| **1e-2+** | 很强 | 过度正则化，欠拟合 | 罕见 |

#### 变异影响预测

**场景1：pytorch_resnet_cifar10 (默认weight_decay=1e-4)**

| weight_decay | 训练精度 | 测试精度 | Train-Test Gap | 结论 |
|-------------|---------|---------|---------------|------|
| **0.0** | 98% | 89% ⬇️ -3% | 9% | 过拟合 |
| **1e-5** | 96% | 91% ⬇️ -1% | 5% | 轻微过拟合 |
| **1e-4** ✓ | 94% | 92% | 2% | 最优 |
| **1e-3** | 90% | 90% ↔️ | 0% | 欠拟合开始 |
| **1e-2** | 82% | 82% ⬇️ -10% | 0% | 严重欠拟合 |

**场景2：Person_reID (默认weight_decay=5e-4)**

行人重识别任务对正则化较敏感，因为需要学习细粒度特征。

| weight_decay | Rank@1 | mAP | 结论 |
|-------------|--------|-----|------|
| **0.0** | 87% ⬇️ -3% | 68% ⬇️ -5% | 过拟合严重 |
| **1e-4** | 89% ⬇️ -1% | 71% ⬇️ -2% | 正则化不足 |
| **5e-4** ✓ | 90% | 73% | 最优平衡 |
| **1e-3** | 89% ⬇️ -1% | 72% ⬇️ -1% | 稍强 |
| **5e-3** | 85% ⬇️ -5% | 67% ⬇️ -6% | 过度正则化 |

**场景3：bug-localization-by-dnn (默认alpha=1e-5)**

小数据集任务对正则化更敏感。

| alpha (L2) | 精度 | 过拟合程度 |
|-----------|------|-----------|
| **0.0** | 72% ⬇️ -8% | 严重过拟合 |
| **1e-5** ✓ | 80% | 平衡 |
| **1e-4** | 81% ⬆️ +1% | 可能更好！ |
| **1e-3** | 75% ⬇️ -5% | 欠拟合 |

#### Weight Decay与其他超参数的关系

**1. Weight Decay × Learning Rate:**
```
大learning rate + 大weight decay = 强正则化
小learning rate + 小weight decay = 弱正则化

示例：
lr=0.1, wd=1e-4  vs  lr=0.01, wd=1e-3
实际正则化强度：0.1×1e-4 = 1e-5  vs  0.01×1e-3 = 1e-5  (相同！)
```

**2. Weight Decay × Dropout:**
```
两者都是正则化，效果叠加：
dropout=0 + wd=1e-4    → 中等正则化
dropout=0.5 + wd=0     → 中等正则化
dropout=0.5 + wd=1e-4  → 强正则化（可能过度）
```

---

## 学习率调度超参数

### 1. lr_scheduler_type (学习率调度器)

#### 为什么需要学习率调度？

**固定学习率的问题：**
```
训练初期：大学习率有利于快速逼近最优点
训练后期：大学习率导致在最优点附近震荡，无法精确收敛

解决：动态调整学习率
```

#### 主流调度器详解

**1. StepLR (阶梯衰减)**
```python
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
# 每30个epoch，学习率乘以0.1

示例（初始lr=0.1）：
epoch 0-29:   lr=0.1
epoch 30-59:  lr=0.01
epoch 60-89:  lr=0.001
epoch 90+:    lr=0.0001
```

**可视化：**
```
LR
^
0.1|████████████
   |
0.01|           ████████████
    |
0.001|                      ████████████
     |___________________________________> epoch
         30            60           90
```

**适用场景：**
- 训练长度固定且已知
- 可以提前规划学习率下降点
- CV任务的标准选择

**变异影响：**
- `step_size=10`: 频繁衰减，快速降低LR → 可能欠拟合
- `step_size=50`: 缓慢衰减，LR长期保持高位 → 可能过拟合
- `gamma=0.5`: 缓慢衰减，每次减半
- `gamma=0.1`: 激进衰减，每次变为1/10

---

**2. MultiStepLR (多阶段衰减)**
```python
scheduler = MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)
# 在指定epoch（60, 120, 160）降低学习率

示例（初始lr=0.1）：
epoch 0-59:   lr=0.1
epoch 60-119: lr=0.02   (×0.2)
epoch 120-159: lr=0.004  (×0.2)
epoch 160+:   lr=0.0008 (×0.2)
```

**可视化：**
```
LR
^
0.1|██████████
   |
0.02|         ██████████
    |
0.004|                  ██████
     |
0.0008|                      ████████
      |_______________________________> epoch
          60       120    160
```

**适用场景：**
- 不同训练阶段需要不同的学习策略
- ResNet、VGG等经典模型的标准配置
- 当明确知道何时需要降低学习率

**变异影响：**
- `milestones=[30, 60]`: 早期降低LR → 快速收敛但可能欠拟合
- `milestones=[100, 150]`: 晚期降低LR → 充分探索但训练时间长

---

**3. ExponentialLR (指数衰减)**
```python
scheduler = ExponentialLR(optimizer, gamma=0.95)
# 每个epoch，学习率乘以0.95

示例（初始lr=0.1）：
epoch 0:  lr=0.1
epoch 1:  lr=0.095
epoch 10: lr=0.0599
epoch 50: lr=0.0077
epoch 100: lr=0.00059
```

**公式：**
```
lr_t = lr_0 × γᵗ
```

**可视化：**
```
LR
^
0.1|╲
   |  ╲
0.05|    ╲___
    |        ╲___
0.01|            ╲______
    |____________________╲________> epoch
     0      20      40      60
```

**适用场景：**
- 希望学习率平滑递减
- 训练长度不确定
- 较少用于主流任务

---

**4. CosineAnnealingLR (余弦退火)**
```python
scheduler = CosineAnnealingLR(optimizer, T_max=200)
# 学习率按余弦曲线从最大值降到最小值

公式：
lr_t = lr_min + (lr_max - lr_min) × (1 + cos(πt/T_max)) / 2
```

**可视化：**
```
LR
^
0.1|╲___
   |    ╲___
0.05|        ╲___
    |            ╲___
0.0 |________________╲____> epoch
     0   50   100  150  200
```

**特点：**
- 前期快速下降，中期平缓，后期再次下降
- 末期学习率接近0，精细调整
- 无需手动指定milestone

**适用场景：**
- Transformer、ViT等现代架构
- BERT预训练
- 不确定何时降低学习率的情况

**变异影响：**
- `T_max=total_epochs`: 标准，训练结束时lr≈0
- `T_max=total_epochs//2`: 前半程快速衰减，后半程保持低lr

---

**5. ReduceLROnPlateau (自适应)**
```python
scheduler = ReduceLROnPlateau(optimizer, mode='min',
                               factor=0.5, patience=10)
# 当验证损失10个epoch不下降时，学习率乘以0.5

使用方式：
for epoch in range(epochs):
    train_loss = train(...)
    val_loss = validate(...)
    scheduler.step(val_loss)  # 根据验证损失调整
```

**可视化：**
```
Val Loss
^
     |     ╲___________  ← 不再下降
     |╲___            ╲
     |                 ╲___  ← 降低LR后继续下降
     |                     ╲___
     |_________________________________> epoch
          ↑               ↑
        降低LR          再次降低LR
```

**适用场景：**
- 不确定何时plateau
- 希望根据实际训练情况自适应调整
- 小数据集、不规则收敛

**变异影响：**
- `patience=5`: 敏感，快速响应plateau
- `patience=20`: 保守，给足时间等待改善
- `factor=0.1`: 激进，大幅降低LR
- `factor=0.5`: 温和，逐步降低LR

---

#### 调度器对比总结

| 调度器 | 优点 | 缺点 | 推荐场景 |
|-------|------|------|---------|
| **StepLR** | 简单，可预测 | 需要手动设置step | CV分类，训练长度固定 |
| **MultiStepLR** | 灵活，精确控制 | 需要先验知识 | ResNet等经典模型 |
| **ExponentialLR** | 平滑衰减 | 衰减速度难控制 | 较少用 |
| **CosineAnnealingLR** | 无需调参，效果好 | 需要知道总epochs | **Transformer首选** |
| **ReduceLROnPlateau** | 自适应，智能 | 可能过早或过晚降LR | 小数据集，探索性训练 |

#### 变异测试方案

**对MRT-OAST (当前可能使用固定LR):**

| 调度器 | 配置 | 预期效果 | 最终精度变化 |
|-------|------|---------|------------|
| **无** | lr=0.0001 | 基线 | 基线 (90%) |
| **StepLR** | step=5, γ=0.5 | 训练后期精细调整 | ⬆️ +1~2% |
| **CosineAnnealing** | T_max=10 | 平滑收敛 | ⬆️ +0.5~1.5% |
| **ReduceLROnPlateau** | patience=3 | 自适应调整 | ⬆️ +0~1% |

**对ResNet-CIFAR10 (当前使用MultiStepLR[80, 120], γ=0.1):**

| 调度器变异 | 配置 | 预期效果 | 精度变化 |
|-----------|------|---------|---------|
| **当前MultiStepLR** | [80, 120] | 基线 | 92% |
| **改为[60, 120, 160]** | 三阶段 | 更精细 | ⬆️ +0.5% |
| **改为CosineAnnealing** | T_max=200 | 平滑曲线 | ↔️ ±0.5% |
| **改为StepLR** | step=40, γ=0.1 | 规律下降 | ⬇️ -0.5~1% |

---

### 2. warm_up_epochs (学习率预热)

#### 原理

**问题：** 训练初期，参数随机初始化，使用大学习率可能导致梯度爆炸。

**解决：** Warm-up逐渐增大学习率。

```
传统训练：
LR
^
0.1|████████████████████████
   |
   |___________________________> epoch

带Warm-up：
LR
^
0.1|      ████████████████████
   |    ／
   |  ／
   |／___________________________> epoch
     warm-up  正常训练
```

#### 数学公式

```python
if epoch < warm_up_epochs:
    # 线性warm-up
    lr = base_lr * (epoch + 1) / warm_up_epochs
else:
    # 正常学习率调度
    lr = base_lr × scheduler(epoch - warm_up_epochs)

示例（base_lr=0.1, warm_up=5）：
epoch 0: lr = 0.1 × 1/5 = 0.02
epoch 1: lr = 0.1 × 2/5 = 0.04
epoch 2: lr = 0.1 × 3/5 = 0.06
epoch 3: lr = 0.1 × 4/5 = 0.08
epoch 4: lr = 0.1 × 5/5 = 0.10
epoch 5+: 正常调度
```

#### 何时需要Warm-up？

**需要：**
- 大batch训练（batch≥256）
- 大学习率（lr≥0.1 for SGD）
- Transformer模型
- 从头训练（非fine-tuning）

**不需要：**
- 小batch训练（batch≤64）
- 小学习率（lr≤0.001）
- Fine-tuning预训练模型
- 简单模型（浅层CNN）

#### Warm-up长度选择

| Warm-up Epochs | 适用场景 | 示例 |
|---------------|---------|------|
| **0** | 小batch, 小LR | MNIST, 默认配置 |
| **3-5** | 中等模型，标准配置 | **ResNet-50标准** |
| **10-20** | 大batch, 大模型 | ImageNet训练 |
| **1000+ steps** | Transformer预训练 | BERT, GPT |

#### 变异影响

**对Person_reID (默认warm_epoch=0, lr=0.05):**

| warm_epoch | 训练初期 | 最终Rank@1 | 训练稳定性 |
|-----------|---------|-----------|-----------|
| **0** ✓ | 可能震荡 | 90% | 基线 |
| **5** | 平滑启动 | 90.5% ⬆️ +0.5% | ⬆️ 更稳定 |
| **10** | 非常平滑 | 90.3% ⬆️ +0.3% | ⬆️⬆️ 最稳定 |
| **20** | 启动过慢 | 89.5% ⬇️ -0.5% | ⬆️ 稳定但慢 |

**对ResNet-CIFAR10 (当前无warm-up, lr=0.1):**

```
添加warm_up=5:
- 训练初期loss下降更平稳
- 避免前几个epoch的震荡
- 最终精度提升 +0.3~0.8%
- 训练曲线更光滑
```

---

## 模型架构超参数

### 1. hidden_size / d_model (隐藏层大小)

#### 定义

**Hidden Size** 是神经网络隐藏层的神经元数量，直接决定模型的**表达能力**。

#### 作用机制

**小hidden_size（如64）：**
```
优点：
- 参数少，训练快
- 内存占用小
- 不易过拟合

缺点：
- 表达能力有限
- 可能欠拟合
- 无法捕捉复杂模式
```

**大hidden_size（如512, 1024）：**
```
优点：
- 强大的表达能力
- 可以捕捉复杂特征
- 理论上限更高

缺点：
- 参数多，训练慢
- 内存占用大
- 易过拟合（需强正则化）
```

#### 参数量计算

```
全连接层参数量：
params = (input_size × hidden_size) + hidden_size  # 权重+偏置

示例：
input=784, hidden=128:
  params = 784 × 128 + 128 = 100,480

input=784, hidden=512:
  params = 784 × 512 + 512 = 401,920  (约4倍)

input=784, hidden=1024:
  params = 784 × 1024 + 1024 = 803,840  (约8倍)
```

#### 不同模型的hidden_size

| 模型 | 当前hidden_size | 参数名 | 典型范围 |
|------|---------------|--------|---------|
| **MRT-OAST** | 128 | `d_model` | 64-512 |
| **bug-localization** | 300 | `hidden_sizes` | 100-500 |
| **Person_reID** | 512 | `linear_num` | 256-2048 |
| **MNIST-FF** | 500 | 硬编码 | 256-1024 |

#### 变异影响预测

**对MRT-OAST (默认d_model=128):**

| d_model | 参数量 | 训练时间 | 精度 | 过拟合风险 |
|---------|--------|---------|------|-----------|
| **64** | ×0.25 | ⬇️ -50% | ⬇️ -3~5% | ⬇️ 低 |
| **128** ✓ | ×1.0 | 基线 | 90% | 中等 |
| **256** | ×4.0 | ⬆️ +100% | ⬆️ +1~3% | ⬆️ 高 |
| **512** | ×16.0 | ⬆️ +300% | ⬆️ +2~4% | ⬆️⬆️ 很高 |

**注意：** 增大d_model通常需要同时调整：
- 增大dropout（如0.2→0.3）
- 增大weight_decay
- 减小学习率
- 增加训练数据

---

**对bug-localization (默认hidden_sizes=300):**

小数据集任务对模型容量更敏感：

| hidden_sizes | 精度 | 过拟合程度 | 推荐 |
|-------------|------|-----------|------|
| **100** | 75% ⬇️ -5% | 低 | 极小数据集 |
| **200** | 78% ⬇️ -2% | 中 | 小数据集 |
| **300** ✓ | 80% | 中高 | 当前配置 |
| **500** | 79% ⬇️ -1% | ⚠️ 高 | 可能过拟合 |
| **[300, 200]** | 81% ⬆️ +1% | 中 | **推荐尝试** |

**多层配置的效果：**
```python
# 单层
hidden_sizes=[300]  → 1层，300个神经元

# 两层
hidden_sizes=[300, 200]  → 第1层300个，第2层200个神经元

# 三层
hidden_sizes=[500, 300, 100]  → 逐层递减（常见模式）
```

---

### 2. num_layers (网络层数)

#### 定义

**Num Layers** 是网络的深度，即堆叠的层数。

#### 深度与性能

**浅层网络（1-2层）：**
```
特点：
- 学习简单特征组合
- 训练快，易收敛
- 表达能力有限

适用：
- 简单任务（MNIST）
- 小数据集
- 快速原型
```

**中等深度（3-10层）：**
```
特点：
- 学习中等复杂的特征层次
- 平衡性能与效率
- 大多数任务的甜点

适用：
- CIFAR-10（ResNet-56）
- 中等规模任务
```

**深层网络（50+层）：**
```
特点：
- 学习深层特征抽象
- 强大但难训练
- 需要特殊技术（ResNet, BatchNorm）

适用：
- ImageNet
- 大规模数据集
- 追求极致性能
```

#### 变异影响

**对MRT-OAST (默认transformer_nlayers=2):**

Transformer编码器层数对性能和计算开销影响巨大：

| nlayers | 参数量 | 训练时间 | 精度 | 注意事项 |
|---------|--------|---------|------|---------|
| **1** | ×0.5 | ⬇️ -40% | ⬇️ -2~4% | 表达能力不足 |
| **2** ✓ | ×1.0 | 基线 | 90% | 平衡 |
| **4** | ×2.0 | ⬆️ +100% | ⬆️ +1~2% | 需要更多数据 |
| **6** | ×3.0 | ⬆️ +200% | ⬆️ +1~3% | 可能过拟合 |
| **12** | ×6.0 | ⬆️ +500% | ⬇️ -1~5% | 过拟合严重 |

**关键发现（BERT论文）：**
```
在中等数据集上：
- BERT-Base (12层) 在大数据集上优秀
- BERT-Small (4层) 在小数据集上更好
- 层数并非越多越好，取决于数据量
```

---

**对pytorch_resnet_cifar10 (通过arch选择层数):**

| 架构 | 层数 | 参数量 | CIFAR-10精度 | 训练时间 |
|------|------|--------|------------|---------|
| **resnet20** | 20 | 0.27M | 91.25% | 基线 |
| **resnet32** | 32 | 0.46M | 92.49% | ⬆️ +50% |
| **resnet56** | 56 | 0.85M | 93.03% | ⬆️ +150% |
| **resnet110** | 110 | 1.7M | 93.39% | ⬆️ +350% |
| **resnet1202** | 1202 | 19.4M | 92.07% ⬇️ | ⬆️ +2000% |

**重要观察：**
- 20→56层：精度稳步提升
- 110层：精度接近饱和，边际收益递减
- 1202层：**过拟合！精度反而下降**

**结论：** 存在最优深度，过深反而有害（尤其在小数据集上）

---

## 数据处理超参数

### 1. 数据增强参数

#### Random Erasing (随机擦除)

**原理：** 随机遮挡图像的一部分，强迫模型学习更鲁棒的特征。

```python
# Person_reID中的Random Erasing
# erasing_p = 0.5 表示50%概率应用擦除

示例：
原始图像：
┌───────────┐
│  Person   │
│   Walk    │
│    ing    │
└───────────┘

Random Erasing后：
┌───────────┐
│  Person   │
│   ████    │  ← 随机区域被擦除
│    ing    │
└───────────┘
```

**变异影响 (Person_reID):**

| erasing_p | Rank@1 | 训练时间 | 鲁棒性 | 适用场景 |
|-----------|--------|---------|--------|---------|
| **0.0** | 88% ⬇️ -2% | 基线 | 低 | 完整图像测试 |
| **0.3** | 89.5% ⬇️ -0.5% | +5% | 中 | 轻微遮挡 |
| **0.5** ✓ | 90% | +10% | 高 | **推荐** |
| **0.7** | 89% ⬇️ -1% | +15% | 很高 | 严重遮挡场景 |

---

#### Color Jitter (颜色抖动)

**原理：** 随机改变图像的亮度、对比度、饱和度、色调。

```python
transform = transforms.ColorJitter(
    brightness=0.2,  # 亮度±20%
    contrast=0.2,    # 对比度±20%
    saturation=0.2,  # 饱和度±20%
    hue=0.1          # 色调±10%
)
```

**效果：** 模型对光照变化更鲁棒。

---

## 训练策略超参数

### 1. early_stopping_patience (早停)

#### 原理

**Early Stopping** 监控验证集性能，当性能不再提升时提前停止训练。

```python
best_val_loss = float('inf')
patience_counter = 0
patience = 10

for epoch in range(max_epochs):
    val_loss = validate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_model()
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"Early stop at epoch {epoch}")
        break
```

**可视化：**
```
Val Loss
^
 |╲_______________
 |               ╲_____  ← 停止训练
 |                     ╲
 |_________________________> epoch
           ↑
    最佳点（保存模型）
    之后patience个epoch无改善→停止
```

#### Patience值的选择

| Patience | 效果 | 适用场景 |
|----------|------|---------|
| **3-5** | 激进，快速停止 | 快速实验，小数据集 |
| **10-15** | 标准，平衡 | **通用推荐** |
| **20-30** | 保守，充分训练 | 大模型，复杂任务 |

#### 变异影响

**对bug-localization (默认n_iter_no_change=30):**

```
patience=10:
- 停在第150次迭代
- 精度：79% ⬇️ -1%
- 训练时间：⬇️ -50%
- 结论：过早停止

patience=30:  ✓ 最优
- 停在第280次迭代
- 精度：80%
- 训练时间：基线

patience=50:
- 停在第310次迭代
- 精度：80.2% ⬆️ +0.2%
- 训练时间：⬆️ +30%
- 结论：边际收益很小
```

---

## 超参数交互效应

### 重要的超参数组合

#### 1. Learning Rate × Batch Size

**线性缩放规则：**
```
batch_size ×k → learning_rate ×k

理论依据：
大batch → 梯度估计更准确 → 可以走更大步
```

**实例：**
```
配置1：batch=32, lr=0.001
配置2：batch=128, lr=0.004  (×4)
配置3：batch=256, lr=0.008  (×8)

效果：收敛速度和最终精度相近
```

**变异测试方案：**
```python
test_configs = [
    (32, 0.001),
    (64, 0.002),
    (128, 0.004),
    (256, 0.008),
]

for batch, lr in test_configs:
    train(batch_size=batch, learning_rate=lr)
```

---

#### 2. Dropout × Weight Decay

**都是正则化，效果叠加：**

| Dropout | Weight Decay | 正则化强度 | 结果 |
|---------|-------------|-----------|------|
| 0.0 | 0.0 | 无 | 过拟合 |
| 0.5 | 0.0 | 中 | 平衡 |
| 0.0 | 1e-4 | 中 | 平衡 |
| 0.5 | 1e-4 | 强 | 可能欠拟合 |
| 0.7 | 1e-3 | 很强 | 欠拟合 |

**建议：** 不要同时使用强dropout和强weight decay

---

#### 3. Learning Rate × Epochs

**Trade-off关系：**
```
大学习率 + 少epochs = 快速但粗糙
小学习率 + 多epochs = 慢但精细

理想：使用学习率调度，前期大LR快速收敛，后期小LR精细调整
```

---

## 变异测试预期结果

### 综合预期表

基于以上分析，对11个模型进行变异测试的预期结果汇总：

| 模型 | 最敏感超参数 | 预期最大提升 | 预期最大下降 | 推荐优先变异 |
|------|------------|------------|------------|------------|
| **MRT-OAST** | lr, epochs, dropout | +2~3% | -10~15% | lr, epochs, layers |
| **VulBERTa** | lr, batch_size | +1~2% | -5~10% | lr, batch_size |
| **ResNet-CIFAR10** | lr, scheduler | +1~2% | -8~12% | lr, weight_decay |
| **Bug-Loc DNN** | hidden_sizes, alpha | +3~5% | -10~15% | hidden_sizes, alpha |
| **DenseNet121** | lr, erasing_p | +1~2% | -5~8% | lr, erasing_p |
| **HRNet18** | lr, batch_size | +1~2% | -5~8% | lr, warm_epoch |
| **PCB** | lr, stride | +1~2% | -3~5% | lr, batch_size |
| **MNIST CNN** | lr, batch_size | +0.5~1% | -3~5% | lr, epochs |
| **MNIST RNN** | lr | +0.5~1% | -3~5% | lr, epochs |
| **MNIST-FF** | lr, threshold | +1~3% | -5~10% | threshold, lr |
| **Siamese** | lr, batch_size | +0.5~1% | -3~5% | lr, epochs |

### 变异测试的三类预期结果

#### 类型1：性能提升 (10-20%的变异)

**特征：**
- 当前超参数不是最优
- 微调可以获得更好性能

**示例：**
```
MRT-OAST:
  当前：epochs=10, lr=0.0001 → 精度90%
  优化：epochs=15, lr=0.0002 → 精度92% ⬆️ +2%

bug-localization:
  当前：hidden_sizes=300 → 精度80%
  优化：hidden_sizes=[300, 200] + alpha=1e-4 → 精度83% ⬆️ +3%
```

#### 类型2：性能下降 (60-70%的变异)

**特征：**
- 当前配置已接近最优
- 变异导致欠拟合或过拟合

**示例：**
```
ResNet-CIFAR10:
  当前：lr=0.1 → 精度92%
  变异：lr=0.01 → 精度88% ⬇️ -4% (欠拟合)
  变异：lr=1.0 → 精度75% ⬇️ -17% (发散)

Person_reID:
  当前：dropout=0.5 → Rank@1=90%
  变异：dropout=0.0 → Rank@1=86% ⬇️ -4% (过拟合)
  变异：dropout=0.8 → Rank@1=85% ⬇️ -5% (欠拟合)
```

#### 类型3：性能持平 (10-20%的变异)

**特征：**
- 超参数在合理范围内对性能影响小
- 模型鲁棒

**示例：**
```
MNIST CNN:
  当前：batch_size=32 → 精度99%
  变异：batch_size=64 → 精度99% (持平)
  原因：MNIST太简单，不同配置都能达到饱和精度

MRT-OAST:
  当前：seed=1334 → 精度90.2%
  变异：seed=42 → 精度90.5% (+0.3%, 统计误差范围内)
```

---

## 总结与建议

### 变异测试的价值

1. **发现未优化的超参数：** 可能提升2-5%性能
2. **评估模型鲁棒性：** 超参数敏感度反映模型稳定性
3. **理解训练动态：** 不同超参数如何影响收敛
4. **提供性能边界：** 了解模型潜力上限

### 推荐变异优先级

**P0 - 必须测试：**
- learning_rate (±1个数量级)
- epochs (×0.5, ×2)
- batch_size (×0.5, ×2)

**P1 - 强烈推荐：**
- seed (3-5个不同值)
- dropout (0, 0.3, 0.5)
- weight_decay/alpha (×0.1, ×10)

**P2 - 有时间可测试：**
- optimizer类型
- lr_scheduler类型
- hidden_size (×0.5, ×2)
- warm_up_epochs

**P3 - 研究性质：**
- 网络层数
- 数据增强组合
- 梯度裁剪

---

**文档版本：** v1.0
**最后更新：** 2025-11-03
