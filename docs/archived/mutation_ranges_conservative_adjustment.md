# 变异范围保守调整方案
# Conservative Adjustment of Mutation Ranges

**调整时间**: 2025-11-11
**依据**: 边界测试结果分析 (docs/boundary_test_analysis_report.md)

---

## 📋 调整原则

基于边界测试结果，采用保守调整策略：
1. 缩小导致显著性能下降（>10%）的超参数范围
2. 保持跨模型统一的倍数表达式
3. 在"统一性"和"性能保证"之间取得平衡

---

## 🔄 调整对比

### Epochs（训练轮数）

| 模型 | 原范围 | 原表达式 | 新范围 | 新表达式 | 变化 |
|------|-------|---------|-------|---------|------|
| MNIST | [5, 20] | [0.5×, 2.0×] | **[5, 15]** | **[0.5×, 1.5×]** | 📉 上界缩小 |
| ResNet20 | [100, 400] | [0.5×, 2.0×] | **[100, 300]** | **[0.5×, 1.5×]** | 📉 上界缩小 |
| DenseNet121 | [30, 120] | [0.5×, 2.0×] | **[30, 90]** | **[0.5×, 1.5×]** | 📉 上界缩小 |
| MRT-OAST | [5, 20] | [0.5×, 2.0×] | **[5, 15]** | **[0.5×, 1.5×]** | 📉 上界缩小 |

**统一表达式**: `[default×0.5, default×1.5]`
**理由**: 虽然epochs边界测试表现优秀，但适当缩小上界可减少训练时间，提高搜索效率

---

### Learning Rate（学习率）⭐ 重要调整

| 模型 | 原范围 | 原表达式 | 新范围 | 新表达式 | 变化 |
|------|-------|---------|-------|---------|------|
| MNIST | [0.001, 0.1] | [0.1×, 10×] | **[0.002, 0.05]** | **[0.2×, 5×]** | 📉 显著缩小 |
| ResNet20 | [0.01, 1.0] | [0.1×, 10×] | **[0.02, 0.5]** | **[0.2×, 5×]** | 📉 显著缩小 |
| DenseNet121 | [0.005, 0.5] | [0.1×, 10×] | **[0.01, 0.25]** | **[0.2×, 5×]** | 📉 显著缩小 |
| MRT-OAST | [0.00001, 0.001] | [0.1×, 10×] | **[0.00002, 0.0005]** | **[0.2×, 5×]** | 📉 显著缩小 |

**统一表达式**: `[default×0.2, default×5.0]`

**关键问题和解决**:
- ❌ **原问题**: `[0.1×, 10×]`范围导致2/3模型边界不可接受
  - DenseNet121 @ lr=0.005 (0.1×): mAP下降15.12%
  - DenseNet121 @ lr=0.5 (10×): mAP崩溃99.75%
  - MRT-OAST @ lr=0.00001 (0.1×): accuracy下降18.35%

- ✅ **新方案**: `[0.2×, 5×]`
  - 下界从0.1×提升到0.2×，避免过小学习率导致训练不充分
  - 上界从10×降低到5×，避免过大学习率导致训练不收敛
  - 预期所有模型边界性能下降 < 10%

---

### Dropout（随机失活）

| 模型 | 原范围 | 新范围 | 变化 |
|------|-------|-------|------|
| DenseNet121 | [0.0, 0.7] | **[0.0, 0.5]** | 📉 上界降低 |
| MRT-OAST | [0.0, 0.7] | **[0.0, 0.5]** | 📉 上界降低 |

**统一绝对值范围**: `[0.0, 0.5]`

**关键问题和解决**:
- ❌ **原问题**: dropout=0.7导致严重性能下降
  - MRT-OAST @ dropout=0.7: accuracy下降23.38%

- ✅ **新方案**: 上界降至0.5
  - 0.5是深度学习模型常用的dropout上界
  - DenseNet121测试显示dropout=0.7仅下降2.78%，但为统一性降至0.5
  - 避免过度dropout阻碍信息流动

---

### Weight Decay（权重衰减）

**保持不变**: `[0.0, default×100]`

**理由**:
- MRT-OAST测试显示最大下降9.72% < 10%阈值
- 当前范围合理，无需调整

---

### Seed（随机种子）

**保持不变**: `[0, 9999]` (统一整数)

**理由**: 不影响性能，仅用于评估稳定性

---

## 📊 调整后的标准化范围总结

| 超参数 | 调整后范围 | 分布类型 | 说明 |
|--------|-----------|----------|------|
| **Epochs** | `[default×0.5, default×1.5]` | 对数均匀 | 缩小上界，提高搜索效率 |
| **Learning Rate** | `[default×0.2, default×5.0]` | 对数均匀 ⭐ | 显著缩小，避免极端边界 |
| **Weight Decay** | `[0.0, default×100]` | 30%零值+70%对数 | 保持不变 |
| **Dropout** | `[0.0, 0.5]` | 均匀分布 | 降低上界，避免过度失活 |
| **Seed** | `[0, 9999]` | 均匀整数 | 保持不变 |

---

## 🎯 预期效果

### MNIST
- **Learning Rate**: 下界性能下降预计从6.25%降至~3%
- **所有边界**: 预计都 < 5%性能下降

### ResNet20
- **需重新测试**: 之前因技术问题全部失败
- **预期**: Learning Rate新范围应表现良好

### DenseNet121
- **Learning Rate下界**: 从15.12%下降预计降至 < 5%
- **Learning Rate上界**: 从99.75%崩溃预计降至 < 10%
- **Dropout**: 保持优秀表现

### MRT-OAST
- **Learning Rate下界**: 从18.35%下降预计降至 < 10%
- **Dropout上界**: 从23.38%下降预计降至 < 10%
- **所有边界**: 预计都在可接受范围内

---

## 📝 具体数值对比

### MNIST (default: epochs=10, lr=0.01)
```
epochs:        [5, 20]  →  [5, 15]        (缩小25%)
learning_rate: [0.001, 0.1]  →  [0.002, 0.05]  (缩小60%)
```

### ResNet20 (default: epochs=200, lr=0.1)
```
epochs:        [100, 400]  →  [100, 300]     (缩小25%)
learning_rate: [0.01, 1.0]  →  [0.02, 0.5]   (缩小60%)
```

### DenseNet121 (default: epochs=60, lr=0.05, dropout=0.5)
```
epochs:        [30, 120]  →  [30, 90]       (缩小25%)
learning_rate: [0.005, 0.5]  →  [0.01, 0.25] (缩小60%)
dropout:       [0.0, 0.7]  →  [0.0, 0.5]    (缩小29%)
```

### MRT-OAST (default: epochs=10, lr=0.0001, dropout=0.2)
```
epochs:        [5, 20]  →  [5, 15]            (缩小25%)
learning_rate: [0.00001, 0.001]  →  [0.00002, 0.0005]  (缩小60%)
dropout:       [0.0, 0.7]  →  [0.0, 0.5]      (缩小29%)
```

---

## 🔬 验证计划

为验证调整效果，需要对Learning Rate和Dropout进行聚焦边界测试：

### 测试配置
- **测试超参数**: Learning Rate, Dropout
- **测试模型**: MNIST, ResNet20, DenseNet121, MRT-OAST
- **测试方法**: 每个模型的每个超参数测试其边界值（SFAT原则）

### 预期测试项
- 每个模型: 1 baseline + 2 lr边界 + 2 dropout边界 (如有) = 3-5个配置
- 总配置数: ~15-18个

详见: `settings/boundary_test_lr_dropout_focused.json`

---

## ✅ 变更记录

- **2025-11-11**: 初次保守调整
  - Epochs上界: 2.0× → 1.5×
  - Learning Rate: [0.1×, 10×] → [0.2×, 5×]
  - Dropout上界: 0.7 → 0.5
  - 依据: docs/boundary_test_analysis_report.md

---

**报告生成**: 2025-11-11
**配置文件**: config/models_config.json
**验证测试**: settings/boundary_test_lr_dropout_focused.json (待生成)
