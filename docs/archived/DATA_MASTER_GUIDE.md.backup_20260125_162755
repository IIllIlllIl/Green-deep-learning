# èƒ½è€—DLé¡¹ç›® - æ•°æ®ä½¿ç”¨ä¸»æŒ‡å—

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-01-15
**æœ€åæ›´æ–°**: 2026-01-15
**çŠ¶æ€**: âœ… å½“å‰æœ‰æ•ˆ

> **é‡è¦**: è¿™æ˜¯é¡¹ç›®æ•°æ®ä½¿ç”¨çš„**å”¯ä¸€æƒå¨æŒ‡å—**ã€‚æ‰€æœ‰æ•°æ®å¤„ç†å·¥ä½œåº”é¦–å…ˆå‚è€ƒæœ¬æ–‡æ¡£ã€‚

---

## ğŸ“‘ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹](#å…³é”®æ³¨æ„äº‹é¡¹)
3. [æ•°æ®æ–‡ä»¶é€‰æ‹©](#æ•°æ®æ–‡ä»¶é€‰æ‹©)
4. [æ•°æ®è´¨é‡ç°çŠ¶](#æ•°æ®è´¨é‡ç°çŠ¶)
5. [æ•°æ®å¤„ç†æœ€ä½³å®è·µ](#æ•°æ®å¤„ç†æœ€ä½³å®è·µ)
6. [å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ)
7. [å‚è€ƒæ–‡æ¡£](#å‚è€ƒæ–‡æ¡£)

---

## å¿«é€Ÿå¼€å§‹

### æ¨èä½¿ç”¨æ–¹æ¡ˆ

| ä½¿ç”¨åœºæ™¯ | æ¨èæ–‡ä»¶ | ç†ç”± |
|---------|---------|------|
| **âœ… å›å½’åˆ†æ** | `data/data.csv` | ç»Ÿä¸€æ ¼å¼ï¼Œæ— éœ€å¤„ç†å¹¶è¡Œ/éå¹¶è¡Œå·®å¼‚ |
| **âœ… å› æœåˆ†æ** | `data/data.csv` | ç»Ÿä¸€æ ¼å¼ï¼Œé€‚åˆDiBSç­‰å› æœå·¥å…· |
| **âœ… æ€§èƒ½åˆ†æ** | `data/data.csv` | å­—æ®µæ¸…æ™°ï¼Œæ˜“äºå¤„ç† |
| **âš ï¸ æœ€å¤§å®Œæ•´æ€§éœ€æ±‚** | `data/raw_data.csv` | 970è¡Œï¼ˆdata.csvæœ‰726è¡Œï¼‰ï¼Œä½†éœ€ç‰¹æ®Šå¤„ç† |
| **âš ï¸ è°ƒè¯•å’ŒéªŒè¯** | `data/raw_data.csv` | åŒ…å«æ‰€æœ‰åŸå§‹å­—æ®µï¼Œä¾¿äºé—®é¢˜æ’æŸ¥ |

### å¿«é€ŸåŠ è½½ä»£ç 

#### æ¨èï¼šä½¿ç”¨ data.csv

```python
import pandas as pd

# è¯»å–æ•°æ®ï¼ˆæ¨èï¼‰
df = pd.read_csv('data/data.csv')

# åŸºæœ¬æ£€æŸ¥
print(f"æ€»è®°å½•æ•°: {len(df)}")
print(f"åˆ—æ•°: {len(df.columns)}")
print(f"å”¯ä¸€timestamp: {df['timestamp'].nunique()}")

# éªŒè¯å”¯ä¸€æ€§
assert df['timestamp'].nunique() == len(df), "timestamp åº”è¯¥æ˜¯å”¯ä¸€çš„ï¼"

# ä½¿ç”¨æ•°æ®
# ç›´æ¥è®¿é—®å­—æ®µï¼Œæ— éœ€è€ƒè™‘ fg_ å‰ç¼€
learning_rate = df['learning_rate']
batch_size = df['batch_size']
energy = df['energy_gpu_avg']
```

#### å¤‡é€‰ï¼šä½¿ç”¨ raw_data.csv

```python
import pandas as pd

# è¯»å–æ•°æ®ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
df = pd.read_csv('data/raw_data.csv')

# âš ï¸ å¿…è¯»ï¼šå‚è€ƒ RAW_DATA_CSV_USAGE_GUIDE.md è·å–å®Œæ•´å¤„ç†æ–¹æ³•

def get_field(row, field_name):
    """
    æ™ºèƒ½è·å–å­—æ®µå€¼ï¼Œè‡ªåŠ¨å¤„ç†å¹¶è¡Œ/éå¹¶è¡Œæ¨¡å¼

    Args:
        row: DataFrameè¡Œ
        field_name: å­—æ®µåï¼ˆä¸å¸¦fg_å‰ç¼€ï¼‰

    Returns:
        å­—æ®µå€¼
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¶è¡Œæ¨¡å¼
    if pd.notna(row.get('fg_learning_rate', None)):
        # å¹¶è¡Œæ¨¡å¼ï¼šä½¿ç”¨ fg_ å‰ç¼€
        return row.get(f'fg_{field_name}', row.get(field_name, ''))
    else:
        # éå¹¶è¡Œæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨å­—æ®µå
        return row.get(field_name, '')

# åº”ç”¨åˆ°æ‰€æœ‰è¡Œ
df['learning_rate_unified'] = df.apply(lambda row: get_field(row, 'learning_rate'), axis=1)
```

---

## âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹

### ğŸš¨ æå…¶é‡è¦ï¼šå”¯ä¸€æ ‡è¯†ç¬¦

**é”™è¯¯è®¤è¯†** âŒ:
```python
# âŒ é”™è¯¯ï¼experiment_id ä¸æ˜¯å”¯ä¸€çš„
df_unique = df.drop_duplicates(subset=['experiment_id'])
# è¿™ä¼šä¸¢å¤±æ•°æ®ï¼
```

**æ­£ç¡®è®¤è¯†** âœ…:
```python
# âœ… æ­£ç¡®ï¼timestamp æ˜¯å”¯ä¸€é”®
df_unique = df.drop_duplicates(subset=['timestamp'])

# âœ… æˆ–ä½¿ç”¨å¤åˆé”®ï¼ˆraw_data.csvä¸­ï¼‰
df_unique = df.drop_duplicates(subset=['experiment_id', 'timestamp'])
```

**åŸå› è¯´æ˜**:
- **experiment_id**: ä»£è¡¨å®éªŒ**é…ç½®**ï¼ˆå¯ä»¥è¿è¡Œå¤šæ¬¡ï¼‰
  - ä¾‹å¦‚ï¼š`default__mnist_default_001` å¯ä»¥åœ¨ä¸åŒæ—¶é—´è¿è¡Œå¤šæ¬¡
  - ä¸åŒè½®æ¬¡ï¼ˆrunsï¼‰ä¼šå¤ç”¨ç›¸åŒçš„ experiment_id

- **timestamp**: ä»£è¡¨å®éªŒ**è¿è¡Œå®ä¾‹**ï¼ˆåº”è¯¥å”¯ä¸€ï¼‰
  - ä¾‹å¦‚ï¼š`2025-11-18T20:37:37.187907`
  - æ¯æ¬¡è¿è¡Œäº§ç”Ÿå”¯ä¸€çš„æ—¶é—´æˆ³

**è¯¦ç»†è¯´æ˜**: å‚è€ƒ [analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md](../analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md) â­â­â­

### ğŸš¨ é‡è¦ï¼šå¹¶è¡Œæ¨¡å¼æ•°æ®å¤„ç†

**data.csv** âœ…:
- å·²è‡ªåŠ¨åˆå¹¶å¹¶è¡Œ/éå¹¶è¡Œå­—æ®µ
- æ·»åŠ äº† `is_parallel` åˆ—æ ‡è¯†æ¨¡å¼
- **ç›´æ¥ä½¿ç”¨å­—æ®µå**ï¼Œæ— éœ€è€ƒè™‘ `fg_` å‰ç¼€

**raw_data.csv** âš ï¸:
- å¹¶è¡Œæ¨¡å¼æ•°æ®åœ¨ `fg_` å‰ç¼€å­—æ®µä¸­
- éå¹¶è¡Œæ¨¡å¼æ•°æ®åœ¨é¡¶å±‚å­—æ®µä¸­
- **å¿…é¡»**æ ¹æ®æ¨¡å¼é€‰æ‹©æ­£ç¡®çš„å­—æ®µ
- **å¼ºçƒˆæ¨è**é˜…è¯»: [RAW_DATA_CSV_USAGE_GUIDE.md](RAW_DATA_CSV_USAGE_GUIDE.md) â­â­â­â­â­

### ğŸš¨ é‡è¦ï¼šç©ºå­—ç¬¦ä¸² vs ç¼ºå¤±å€¼

åœ¨ raw_data.csv ä¸­ï¼š
- **ç©ºå­—ç¬¦ä¸² `""`**: è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼æˆ–ä¸é€‚ç”¨ï¼ˆä¸æ˜¯æ•°æ®ç¼ºå¤±ï¼‰
- **NaN/ç©º**: è¡¨ç¤ºçœŸæ­£çš„æ•°æ®ç¼ºå¤±

```python
# é”™è¯¯çš„åˆ¤æ–­æ–¹å¼ âŒ
missing = df[df['learning_rate'] == '']  # è¿™å¯èƒ½ä¸æ˜¯ç¼ºå¤±ï¼

# æ­£ç¡®çš„åˆ¤æ–­æ–¹å¼ âœ…
import pandas as pd
missing = df[df['learning_rate'].isna()]  # çœŸæ­£çš„ç¼ºå¤±å€¼
has_default = df[df['learning_rate'] == '']  # ä½¿ç”¨é»˜è®¤å€¼
```

---

## æ•°æ®æ–‡ä»¶é€‰æ‹©

### data.csv vs raw_data.csv è¯¦ç»†å¯¹æ¯”

| ç‰¹å¾ | data.csv âœ… æ¨è | raw_data.csv âš ï¸ å¤‡é€‰ |
|------|-----------------|---------------------|
| **è®°å½•æ•°** | 726è¡Œ | 970è¡Œ |
| **å¯ç”¨è®°å½•** | 692è¡Œ (95.3%) | 577è¡Œ (59.5%) |
| **å­—æ®µæ•°** | 56åˆ— | 87åˆ— |
| **æ ¼å¼ç»Ÿä¸€æ€§** | âœ… å®Œå…¨ç»Ÿä¸€ | âš ï¸ éœ€è¦ç‰¹æ®Šå¤„ç† |
| **å¹¶è¡Œæ¨¡å¼å¤„ç†** | âœ… å·²è‡ªåŠ¨åˆå¹¶ | âŒ éœ€è¦æ‰‹åŠ¨å¤„ç† |
| **is_parallelæ ‡è¯†** | âœ… æœ‰ | âŒ æ—  |
| **timestampå”¯ä¸€æ€§** | âœ… 100%å”¯ä¸€ | âœ… 100%å”¯ä¸€ (å»é‡å) |
| **é€‚ç”¨åœºæ™¯** | å›å½’åˆ†æã€å› æœåˆ†æã€é€šç”¨åˆ†æ | è°ƒè¯•ã€æœ€å¤§å®Œæ•´æ€§éœ€æ±‚ |
| **ä¸Šæ‰‹éš¾åº¦** | â­ ç®€å• | â­â­â­â­ å¤æ‚ |
| **æ¨èåº¦** | â­â­â­â­â­ | â­â­ |

**è¯¦ç»†å¯¹æ¯”**: å‚è€ƒ [analysis/docs/DATA_FILES_COMPARISON.md](../analysis/docs/DATA_FILES_COMPARISON.md) â­â­â­

### ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ raw_data.csvï¼Ÿ

**ä»…åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨**:
1. éœ€è¦**æœ€å¤§æ•°æ®å®Œæ•´æ€§**ï¼ˆ970è¡Œ vs 726è¡Œï¼‰
2. éœ€è¦**æ‰€æœ‰åŸå§‹å­—æ®µ**ï¼ˆ87åˆ— vs 56åˆ—ï¼‰
3. éœ€è¦**è°ƒè¯•æ•°æ®é—®é¢˜**æˆ–éªŒè¯æ•°æ®å¤„ç†æµç¨‹
4. éœ€è¦è®¿é—®**ç‰¹æ®Šå­—æ®µ**ï¼ˆå¦‚æŸäº›æœªåˆå¹¶çš„å­—æ®µï¼‰

**ä½¿ç”¨å‰å¿…è¯»**: [RAW_DATA_CSV_USAGE_GUIDE.md](RAW_DATA_CSV_USAGE_GUIDE.md) â­â­â­â­â­

---

## æ•°æ®è´¨é‡ç°çŠ¶

### æœ€æ–°ç»Ÿè®¡ï¼ˆ2026-01-15æ›´æ–°ï¼‰

#### data.csv æ•°æ®è´¨é‡ âœ… ä¼˜ç§€

| æŒ‡æ ‡ | æ•°å€¼ | ç™¾åˆ†æ¯” |
|------|------|--------|
| **æ€»è®°å½•æ•°** | 726 | 100% |
| **âœ… å¯ç”¨è®°å½•** | 692 | **95.3%** |
| **è®­ç»ƒæˆåŠŸ** | 726 | **100%** |
| **æœ‰èƒ½è€—æ•°æ®** | 692 | **95.3%** |
| **æœ‰æ€§èƒ½æŒ‡æ ‡** | 726 | **100%** |
| **timestampå”¯ä¸€** | 726 | **100%** |

**ç»“è®º**: data.csv æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œ**å¼ºçƒˆæ¨èç”¨äºæ‰€æœ‰åˆ†æå·¥ä½œ** âœ…

#### raw_data.csv æ•°æ®è´¨é‡ âš ï¸ æ··åˆ

| æŒ‡æ ‡ | æ•°å€¼ | ç™¾åˆ†æ¯” |
|------|------|--------|
| **æ€»è®°å½•æ•°** | 970 | 100% |
| **âœ… å¯ç”¨è®°å½•** | 577 | **59.5%** |
| **âŒ ä¸å¯ç”¨è®°å½•** | 393 | **40.5%** |
| **è®­ç»ƒæˆåŠŸ** | 854 | 88.0% |
| **æœ‰èƒ½è€—æ•°æ®** | 828 | 85.4% |
| **æœ‰æ€§èƒ½æŒ‡æ ‡** | 577 | 59.5% |
| **timestampå”¯ä¸€** | 970 | **100%** (å»é‡å) |

**ä¸å¯ç”¨åŸå› åˆ†æ**:
- ğŸš¨ **æ€§èƒ½æŒ‡æ ‡ç¼ºå¤±**: 393æ¡ (100%ä¸å¯ç”¨è®°å½•)
- âš¡ **èƒ½è€—æ•°æ®ç¼ºå¤±**: 142æ¡ (36.1%ä¸å¯ç”¨è®°å½•)
- âŒ **è®­ç»ƒå¤±è´¥**: 116æ¡ (29.5%ä¸å¯ç”¨è®°å½•)

**ç»“è®º**: raw_data.csv æœ‰æ›´å¤šè®°å½•ä½†è´¨é‡è¾ƒä½ï¼Œä»…åœ¨éœ€è¦æœ€å¤§å®Œæ•´æ€§æ—¶ä½¿ç”¨ âš ï¸

### æ¨èä½¿ç”¨çš„é«˜è´¨é‡æ•°æ®

**8ä¸ª100%å¯ç”¨æ¨¡å‹** (487æ¡è®°å½•ï¼Œæ¥è‡ªdata.csv):
- `pytorch_resnet_cifar10/resnet20` (53æ¡)
- `Person_reID_baseline_pytorch/densenet121` (53æ¡)
- `Person_reID_baseline_pytorch/hrnet18` (53æ¡)
- `Person_reID_baseline_pytorch/pcb` (53æ¡)
- `examples/mnist` (69æ¡)
- `examples/mnist_rnn` (69æ¡)
- `examples/siamese` (69æ¡)
- `examples/mnist_ff` (68æ¡)

**è¯¦ç»†åˆ†æ**: å‚è€ƒ [DATA_USABILITY_FOR_REGRESSION_20260114.md](DATA_USABILITY_FOR_REGRESSION_20260114.md) â­

### 6åˆ†ç»„å›å½’åˆ†ææ•°æ®å¯ç”¨æ€§

é’ˆå¯¹ç ”ç©¶é—®é¢˜1ï¼ˆè¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“ï¼‰ï¼Œæ•°æ®æŒ‰æ¨¡å‹ç‰¹å¾åˆ†ä¸º6ç»„ï¼š

| ç»„åˆ« | æ¨¡å‹ | å¯ç”¨è®°å½• | å æ¯” |
|------|------|---------|------|
| **Group 1** | Image Classification (3æ¨¡å‹) | 159 | 100% |
| **Group 2** | Image-based Matching (3æ¨¡å‹) | 207 | 100% |
| **Group 3** | Text-based Tasks (2æ¨¡å‹) | 0 | 0% âš ï¸ |
| **Group 4** | Defect Localization (1æ¨¡å‹) | 25 | 100% |
| **Group 5** | Bug Localization (1æ¨¡å‹) | 0 | 0% âš ï¸ |
| **Group 6** | Vulnerability Detection (1æ¨¡å‹) | 0 | 0% âš ï¸ |

**å¯ç”¨ç»„åˆ«**: Group 1, 2, 4 **(3/6ç»„ï¼Œ391æ¡è®°å½•)**

**è¯¦ç»†æŠ¥å‘Š**: [DATA_USABILITY_FOR_REGRESSION_20260114.md](DATA_USABILITY_FOR_REGRESSION_20260114.md)

---

## æ•°æ®å¤„ç†æœ€ä½³å®è·µ

### 1. å»é‡å¤„ç†

```python
import pandas as pd

# è¯»å–æ•°æ®
df = pd.read_csv('data/data.csv')

# æ–¹æ³•1: åŸºäº timestamp å»é‡ï¼ˆæ¨èï¼‰
df_unique = df.drop_duplicates(subset=['timestamp'], keep='first')

# éªŒè¯
print(f"åŸå§‹è¡Œæ•°: {len(df)}")
print(f"å»é‡åè¡Œæ•°: {len(df_unique)}")
print(f"ç§»é™¤è¡Œæ•°: {len(df) - len(df_unique)}")

# æ£€æŸ¥å”¯ä¸€æ€§
assert df_unique['timestamp'].nunique() == len(df_unique), "å»é‡å¤±è´¥ï¼"
```

**å·¥å…·è„šæœ¬**: `tools/data_management/deduplicate_by_timestamp.py`

### 2. ç¼ºå¤±å€¼å¤„ç†

```python
import pandas as pd
import numpy as np

df = pd.read_csv('data/data.csv')

# æ£€æŸ¥ç¼ºå¤±å€¼
print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isnull().sum())

# èƒ½è€—æ•°æ®ç¼ºå¤±æ£€æŸ¥
energy_cols = [col for col in df.columns if col.startswith('energy_')]
energy_missing = df[energy_cols].isnull().all(axis=1)
print(f"ç¼ºå¤±æ‰€æœ‰èƒ½è€—æ•°æ®çš„è®°å½•: {energy_missing.sum()}")

# æ€§èƒ½æŒ‡æ ‡ç¼ºå¤±æ£€æŸ¥
perf_cols = [col for col in df.columns if col.startswith('perf_')]
perf_missing = df[perf_cols].isnull().all(axis=1)
print(f"ç¼ºå¤±æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡çš„è®°å½•: {perf_missing.sum()}")

# ç­›é€‰å¯ç”¨æ•°æ®ï¼ˆæ¨èï¼‰
df_usable = df[
    (df['status'] == 'success') &  # è®­ç»ƒæˆåŠŸ
    (~energy_missing) &             # æœ‰èƒ½è€—æ•°æ®
    (~perf_missing)                 # æœ‰æ€§èƒ½æŒ‡æ ‡
].copy()

print(f"å¯ç”¨è®°å½•æ•°: {len(df_usable)} / {len(df)} ({len(df_usable)/len(df)*100:.1f}%)")
```

### 3. æ•°æ®éªŒè¯æµç¨‹

```python
def validate_data(df, file_name='unknown'):
    """
    éªŒè¯æ•°æ®è´¨é‡

    Args:
        df: DataFrame
        file_name: æ–‡ä»¶åï¼ˆç”¨äºæŠ¥å‘Šï¼‰

    Returns:
        dict: éªŒè¯ç»“æœ
    """
    results = {
        'file': file_name,
        'total_rows': len(df),
        'unique_timestamps': df['timestamp'].nunique(),
        'has_duplicates': df['timestamp'].nunique() < len(df),
        'training_success': (df['status'] == 'success').sum(),
        'has_energy': (~df[[col for col in df.columns if col.startswith('energy_')]].isnull().all(axis=1)).sum(),
        'has_performance': (~df[[col for col in df.columns if col.startswith('perf_')]].isnull().all(axis=1)).sum(),
    }

    # è®¡ç®—å¯ç”¨è®°å½•
    results['usable'] = len(df[
        (df['status'] == 'success') &
        (~df[[col for col in df.columns if col.startswith('energy_')]].isnull().all(axis=1)) &
        (~df[[col for col in df.columns if col.startswith('perf_')]].isnull().all(axis=1))
    ])

    # è¾“å‡ºæŠ¥å‘Š
    print(f"\næ•°æ®éªŒè¯æŠ¥å‘Š - {file_name}")
    print("=" * 60)
    print(f"æ€»è®°å½•æ•°: {results['total_rows']}")
    print(f"å”¯ä¸€timestamp: {results['unique_timestamps']}")
    print(f"æœ‰é‡å¤: {'æ˜¯' if results['has_duplicates'] else 'å¦'}")
    print(f"è®­ç»ƒæˆåŠŸ: {results['training_success']} ({results['training_success']/results['total_rows']*100:.1f}%)")
    print(f"æœ‰èƒ½è€—æ•°æ®: {results['has_energy']} ({results['has_energy']/results['total_rows']*100:.1f}%)")
    print(f"æœ‰æ€§èƒ½æŒ‡æ ‡: {results['has_performance']} ({results['has_performance']/results['total_rows']*100:.1f}%)")
    print(f"âœ… å¯ç”¨è®°å½•: {results['usable']} ({results['usable']/results['total_rows']*100:.1f}%)")

    return results

# ä½¿ç”¨ç¤ºä¾‹
df = pd.read_csv('data/data.csv')
results = validate_data(df, 'data.csv')
```

**å·¥å…·è„šæœ¬**: `tools/data_management/validate_raw_data.py`

### 4. æ•°æ®è¿½åŠ æµç¨‹

å½“æœ‰æ–°å®éªŒç»“æœæ—¶ï¼Œä½¿ç”¨æ ‡å‡†è¿½åŠ æµç¨‹ï¼š

```bash
# è¿½åŠ æ–°å®éªŒç»“æœåˆ° raw_data.csv
python3 tools/data_management/append_session_to_raw_data.py results/run_YYYYMMDD_HHMMSS

# é‡æ–°ç”Ÿæˆ data.csvï¼ˆå¦‚æœéœ€è¦ï¼‰
python3 tools/data_management/create_unified_data_csv.py
```

**è¯¦ç»†æŒ‡å—**: [APPEND_SESSION_TO_RAW_DATA_GUIDE.md](APPEND_SESSION_TO_RAW_DATA_GUIDE.md)

---

## å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### âŒ é”™è¯¯1: ä½¿ç”¨ experiment_id ä½œä¸ºå”¯ä¸€é”®

**é”™è¯¯ä»£ç **:
```python
df_unique = df.drop_duplicates(subset=['experiment_id'])
# ğŸš¨ è¿™ä¼šä¸¢å¤±å¤§é‡æœ‰æ•ˆæ•°æ®ï¼
```

**æ­£ç¡®æ–¹æ³•**:
```python
df_unique = df.drop_duplicates(subset=['timestamp'])
```

**åŸå› **: experiment_id ä»£è¡¨é…ç½®ï¼ˆå¯é‡å¤ï¼‰ï¼Œtimestamp ä»£è¡¨è¿è¡Œå®ä¾‹ï¼ˆå”¯ä¸€ï¼‰

**å‚è€ƒ**: [analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md](../analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md)

### âŒ é”™è¯¯2: ç›´æ¥è¯»å– raw_data.csv ä¸å¤„ç†å¹¶è¡Œæ¨¡å¼

**é”™è¯¯ä»£ç **:
```python
df = pd.read_csv('data/raw_data.csv')
learning_rate = df['learning_rate']  # ğŸš¨ å¹¶è¡Œæ¨¡å¼æ•°æ®ä¼šä¸¢å¤±ï¼
```

**æ­£ç¡®æ–¹æ³•**:
- **æ–¹æ¡ˆA**: ä½¿ç”¨ data.csvï¼ˆæ¨èï¼‰
```python
df = pd.read_csv('data/data.csv')
learning_rate = df['learning_rate']  # âœ… è‡ªåŠ¨å¤„ç†
```

- **æ–¹æ¡ˆB**: ä½¿ç”¨ raw_data.csv + ç‰¹æ®Šå¤„ç†
```python
df = pd.read_csv('data/raw_data.csv')
# å‚è€ƒ RAW_DATA_CSV_USAGE_GUIDE.md ä¸­çš„ get_field() å‡½æ•°
```

**å‚è€ƒ**: [RAW_DATA_CSV_USAGE_GUIDE.md](RAW_DATA_CSV_USAGE_GUIDE.md)

### âŒ é”™è¯¯3: è¯¯åˆ¤ç©ºå­—ç¬¦ä¸²ä¸ºç¼ºå¤±å€¼

**é”™è¯¯ä»£ç **:
```python
# åœ¨ raw_data.csv ä¸­
missing = df[df['learning_rate'] == '']  # ğŸš¨ å¯èƒ½ä¸æ˜¯ç¼ºå¤±ï¼
```

**æ­£ç¡®æ–¹æ³•**:
```python
# ç©ºå­—ç¬¦ä¸²é€šå¸¸è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼ï¼Œä¸æ˜¯ç¼ºå¤±
truly_missing = df[df['learning_rate'].isna()]

# æˆ–ä½¿ç”¨ data.csvï¼ˆå·²å¤„ç†å¥½ï¼‰
df = pd.read_csv('data/data.csv')
```

### âŒ é”™è¯¯4: å¿½ç•¥è®­ç»ƒå¤±è´¥çš„è®°å½•

**é”™è¯¯ä»£ç **:
```python
# ä¸æ£€æŸ¥ status å­—æ®µ
df_analysis = df[~df['energy_gpu_avg'].isna()]  # ğŸš¨ å¯èƒ½åŒ…å«å¤±è´¥è®°å½•
```

**æ­£ç¡®æ–¹æ³•**:
```python
# æ˜ç¡®ç­›é€‰è®­ç»ƒæˆåŠŸçš„è®°å½•
df_analysis = df[
    (df['status'] == 'success') &
    (~df['energy_gpu_avg'].isna())
]
```

### âŒ é”™è¯¯5: æ··æ·† data.csv å’Œ raw_data.csv çš„è¡Œæ•°

**é”™è¯¯è®¤è¯†**:
- "æˆ‘çš„åˆ†æåªæœ‰726è¡Œï¼Œä½†åº”è¯¥æœ‰970è¡Œï¼Œæ•°æ®ä¸¢å¤±äº†ï¼"

**æ­£ç¡®è®¤è¯†**:
- data.csv: 726è¡Œï¼ˆç²¾é€‰çš„é«˜è´¨é‡æ•°æ®ï¼‰
- raw_data.csv: 970è¡Œï¼ˆåŒ…å«æ‰€æœ‰è®°å½•ï¼Œå«ä½è´¨é‡æ•°æ®ï¼‰
- ä¸¤è€…éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©

### âŒ é”™è¯¯6: ä¸éªŒè¯æ•°æ®è´¨é‡å°±å¼€å§‹åˆ†æ

**é”™è¯¯åšæ³•**:
```python
df = pd.read_csv('data/data.csv')
# ç›´æ¥å¼€å§‹åˆ†æï¼Œä¸æ£€æŸ¥æ•°æ®è´¨é‡
model = LinearRegression()
model.fit(X, y)  # ğŸš¨ å¯èƒ½åŒ…å«å¼‚å¸¸å€¼ã€ç¼ºå¤±å€¼ç­‰
```

**æ­£ç¡®æ–¹æ³•**:
```python
df = pd.read_csv('data/data.csv')

# 1. éªŒè¯æ•°æ®è´¨é‡
results = validate_data(df, 'data.csv')

# 2. ç­›é€‰å¯ç”¨æ•°æ®
df_usable = df[
    (df['status'] == 'success') &
    (~df[[col for col in df.columns if col.startswith('energy_')]].isnull().all(axis=1)) &
    (~df[[col for col in df.columns if col.startswith('perf_')]].isnull().all(axis=1))
].copy()

# 3. æ£€æŸ¥å¼‚å¸¸å€¼
# ... (æ ¹æ®å…·ä½“æƒ…å†µ)

# 4. å¼€å§‹åˆ†æ
model = LinearRegression()
model.fit(X, y)
```

---

## å‚è€ƒæ–‡æ¡£

### æ ¸å¿ƒæ–‡æ¡£ â­â­â­â­â­

| æ–‡æ¡£ | ç”¨é€” |
|------|------|
| **æœ¬æ–‡æ¡£** | æ•°æ®ä½¿ç”¨ä¸»æŒ‡å—ï¼ˆå•ä¸€çœŸå®æ¥æºï¼‰ |
| [RAW_DATA_CSV_USAGE_GUIDE.md](RAW_DATA_CSV_USAGE_GUIDE.md) | raw_data.csv è¯¦ç»†ä½¿ç”¨æŒ‡å— â­â­â­â­â­ |
| [analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md](../analysis/docs/DATA_UNIQUENESS_CLARIFICATION_20251228.md) | å”¯ä¸€æ ‡è¯†è¯´æ˜ â­â­â­ |
| [analysis/docs/DATA_UNDERSTANDING_CORRECTION_20251228.md](../analysis/docs/DATA_UNDERSTANDING_CORRECTION_20251228.md) | æ•°æ®ç†è§£å…³é”®æ›´æ­£ â­â­â­ |

### æ•°æ®è´¨é‡æŠ¥å‘Š

| æ–‡æ¡£ | ç”¨é€” |
|------|------|
| [DATA_USABILITY_FOR_REGRESSION_20260114.md](DATA_USABILITY_FOR_REGRESSION_20260114.md) | 6åˆ†ç»„å›å½’åˆ†ææ•°æ®å¯ç”¨æ€§ |
| [DATA_REPAIR_FINAL_SUMMARY_20260113.md](DATA_REPAIR_FINAL_SUMMARY_20260113.md) | æ•°æ®ä¿®å¤æœ€ç»ˆæ€»ç»“ |
| [analysis/data/energy_research/DATA_STATUS_REPORT_20260114.md](../analysis/data/energy_research/DATA_STATUS_REPORT_20260114.md) | æ•°æ®ç°çŠ¶å®Œæ•´æŠ¥å‘Š |

### æ–‡ä»¶å¯¹æ¯”åˆ†æ

| æ–‡æ¡£ | ç”¨é€” |
|------|------|
| [analysis/docs/DATA_FILES_COMPARISON.md](../analysis/docs/DATA_FILES_COMPARISON.md) | data.csv vs raw_data.csv è¯¦ç»†å¯¹æ¯” |
| [analysis/data/energy_research/RAW_DATA_VS_DATA_CSV_COMPARISON.md](../analysis/data/energy_research/RAW_DATA_VS_DATA_CSV_COMPARISON.md) | ä¸¤æ–‡ä»¶å¯¹æ¯”åˆ†æ |

### æ•°æ®å¤„ç†å·¥å…·

| è„šæœ¬ | ç”¨é€” |
|------|------|
| `tools/data_management/validate_raw_data.py` | æ•°æ®éªŒè¯ |
| `tools/data_management/deduplicate_by_timestamp.py` | å»é‡å¤„ç† |
| `tools/data_management/append_session_to_raw_data.py` | è¿½åŠ æ–°æ•°æ® |
| `tools/data_management/create_unified_data_csv.py` | ç”Ÿæˆç»Ÿä¸€data.csv |
| `tools/data_management/analyze_experiment_status.py` | å®éªŒçŠ¶æ€åˆ†æ |

### å†å²æ–‡æ¡£å½’æ¡£

æ‰€æœ‰å†å²æ•°æ®æŠ¥å‘Šï¼ˆ2025-12åˆ°2026-01ï¼‰å·²å½’æ¡£åˆ°:
- `docs/archived/data_reports_archive_20260115/`

å½’æ¡£æ–‡æ¡£ä»å¯è®¿é—®ï¼Œä½†ä¸å†ç»´æŠ¤æ›´æ–°ã€‚

---

## ğŸ“ è·å–å¸®åŠ©

### é‡åˆ°é—®é¢˜æ—¶

1. **é¦–å…ˆ**: æ£€æŸ¥æœ¬æ–‡æ¡£çš„"å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ"ç« èŠ‚
2. **å…¶æ¬¡**: æŸ¥é˜…ç›¸å…³çš„å‚è€ƒæ–‡æ¡£
3. **å·¥å…·**: ä½¿ç”¨ `tools/data_management/` ä¸­çš„éªŒè¯è„šæœ¬
4. **æ–‡æ¡£**: æŸ¥çœ‹ CLAUDE.md æˆ– analysis/docs/INDEX.md

### æŠ¥å‘Šé—®é¢˜

å¦‚æœå‘ç°æ–‡æ¡£é”™è¯¯æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š
1. åˆ›å»ºè¯¦ç»†çš„é—®é¢˜æè¿°
2. åŒ…å«å¤ç°æ­¥éª¤å’Œæ•°æ®ç¤ºä¾‹
3. æåŠç›¸å…³çš„æ–‡æ¡£ç‰ˆæœ¬å’Œæ—¥æœŸ

---

## ğŸ“ ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´å†…å®¹ |
|------|------|---------|
| 1.0 | 2026-01-15 | åˆå§‹ç‰ˆæœ¬ï¼Œæ•´åˆ78ä¸ªæ•°æ®æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ |

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Claude Assistant
**ä¸‹æ¬¡å®¡æŸ¥**: 2026-02-15
**åé¦ˆ**: è¯·é€šè¿‡é¡¹ç›®ä¸»æ–‡æ¡£ CLAUDE.md è”ç³»
