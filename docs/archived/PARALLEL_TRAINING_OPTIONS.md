# å¹¶è¡Œè®­ç»ƒæ–¹æ¡ˆå¯¹æ¯”ä¸æ¨è

## æ–‡æ¡£ä¿¡æ¯

- **ç‰ˆæœ¬**: v1.0
- **æ—¥æœŸ**: 2025-11-12
- **çŠ¶æ€**: æ–¹æ¡ˆå¯¹æ¯”
- **ä½œè€…**: Claude Code

---

## æ ¸å¿ƒéœ€æ±‚å›é¡¾

1. âœ… å¹¶è¡Œè¿è¡Œä¸¤ä¸ªæ¨¡å‹ï¼šå˜å¼‚æ¨¡å‹Aï¼ˆå‰æ™¯ï¼‰+ èƒŒæ™¯æ¨¡å‹B
2. âœ… Aæ‰§è¡Œå˜å¼‚è®­ç»ƒå¹¶å®Œæ•´ç›‘æ§ï¼ŒBä½¿ç”¨é»˜è®¤å‚æ•°ä»…ä½œä¸ºGPUè´Ÿè½½
3. âœ… BæŒç»­å¾ªç¯è®­ç»ƒï¼Œç›´åˆ°Aå®Œæˆ
4. âœ… èµ„æºç”±OS/GPUè‡ªåŠ¨è°ƒåº¦
5. âœ… å®Œå…¨å‘åå…¼å®¹ï¼Œä¸ä¿®æ”¹configå’Œç°æœ‰settings
6. âœ… é»˜è®¤æ‰§è¡ŒåŸæœ‰å•æ¨¡å‹è®­ç»ƒ

---

## æ–¹æ¡ˆä¸€è§ˆè¡¨

| æ–¹æ¡ˆ | å®ç°æ–¹å¼ | å¤æ‚åº¦ | å‘åå…¼å®¹ | æ¨èåº¦ |
|------|---------|--------|---------|--------|
| **æ–¹æ¡ˆ1** | subprocess.Popenåå°è¿›ç¨‹ + Shellå¾ªç¯è„šæœ¬ | â­â­â­â­â­ ç®€å• | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â­ **å¼ºçƒˆæ¨è** |
| **æ–¹æ¡ˆ2** | Python threadingå¤šçº¿ç¨‹ | â­â­â­â­â˜† ä¸­ç­‰ | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â˜† æ¨è |
| **æ–¹æ¡ˆ3** | Python multiprocessingå¤šè¿›ç¨‹ | â­â­â­â˜†â˜† å¤æ‚ | â­â­â­â­â˜† è‰¯å¥½ | â­â­â­â˜†â˜† å¤‡é€‰ |
| **æ–¹æ¡ˆ4** | ä¿®æ”¹run.shæ”¯æŒåå°æ¨¡å¼ | â­â­â˜†â˜†â˜† å¤æ‚ | â­â­â­â˜†â˜† ä¸€èˆ¬ | â­â­â˜†â˜†â˜† ä¸æ¨è |

---

## æ–¹æ¡ˆ1: subprocess.Popen + Shellå¾ªç¯è„šæœ¬ â­ æ¨è

### æ ¸å¿ƒæ€æƒ³

ä½¿ç”¨Pythonçš„`subprocess.Popen`å¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„Shellè„šæœ¬ï¼Œè¯¥è„šæœ¬åŒ…å«æ— é™å¾ªç¯é€»è¾‘ï¼ŒæŒç»­è¿è¡ŒèƒŒæ™¯è®­ç»ƒï¼Œç›´åˆ°è¢«å‰æ™¯è®­ç»ƒå®Œæˆåç»ˆæ­¢ã€‚

### æ¶æ„å›¾

```
mutation.py (ä¸»è¿›ç¨‹)
    â”‚
    â”œâ”€ å¯åŠ¨ background_training.sh (ç‹¬ç«‹è¿›ç¨‹)
    â”‚    â””â”€ while true; do train.sh; done
    â”‚
    â”œâ”€ è¿è¡Œ run_experiment() (å‰æ™¯è®­ç»ƒ)
    â”‚    â””â”€ å®Œæ•´ç›‘æ§ + èƒ½è€—æµ‹é‡
    â”‚
    â””â”€ ç»ˆæ­¢ background_training.sh (killpg)
```

### å®ç°è¦ç‚¹

#### 1. ç”Ÿæˆåå°è®­ç»ƒè„šæœ¬

```python
def _start_background_training(self, repo, model, hyperparams, experiment_id):
    """åˆ›å»ºå¹¶å¯åŠ¨èƒŒæ™¯è®­ç»ƒShellè„šæœ¬"""

    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd_args = self._build_training_args(repo, model, hyperparams)

    # ç”ŸæˆShellè„šæœ¬
    script_path = self.results_dir / f"background_training_{experiment_id}.sh"
    script_content = f"""#!/bin/bash
# èƒŒæ™¯è®­ç»ƒå¾ªç¯è„šæœ¬
REPO_PATH="{self.project_root / repo_config['path']}"
TRAIN_SCRIPT="{repo_config['train_script']}"
TRAIN_ARGS="{cmd_args}"
LOG_DIR="{self.results_dir}/background_logs_{experiment_id}"

mkdir -p "$LOG_DIR"
cd "$REPO_PATH"

echo "[Background] Starting training loop at $(date)"

run_count=0
while true; do
    run_count=$((run_count + 1))
    echo "[Background] Run #$run_count starting at $(date)"

    # è¿è¡Œè®­ç»ƒï¼Œè¾“å‡ºåˆ°ç‹¬ç«‹æ—¥å¿—
    $TRAIN_SCRIPT $TRAIN_ARGS > "$LOG_DIR/run_$run_count.log" 2>&1

    exit_code=$?
    echo "[Background] Run #$run_count finished with exit code $exit_code"

    # çŸ­æš‚ä¼‘çœ é¿å…è¿‡äºé¢‘ç¹é‡å¯
    sleep 2
done
"""

    # å†™å…¥æ–‡ä»¶å¹¶è®¾ç½®å¯æ‰§è¡Œæƒé™
    with open(script_path, 'w') as f:
        f.write(script_content)
    os.chmod(script_path, 0o755)

    # å¯åŠ¨åå°è¿›ç¨‹ï¼ˆåˆ›å»ºæ–°è¿›ç¨‹ç»„ï¼‰
    process = subprocess.Popen(
        [str(script_path)],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
        preexec_fn=os.setsid  # å…³é”®ï¼šåˆ›å»ºæ–°è¿›ç¨‹ç»„
    )

    return process
```

#### 2. åè°ƒå¹¶è¡Œè®­ç»ƒ

```python
def run_parallel_experiment(self, fg_repo, fg_model, fg_mutation,
                           bg_repo, bg_model, bg_hyperparams, max_retries=2):
    """è¿è¡Œå¹¶è¡Œå®éªŒ"""

    experiment_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{fg_repo}_{fg_model}_parallel"
    background_process = None

    try:
        # 1. å¯åŠ¨èƒŒæ™¯è®­ç»ƒ
        background_process = self._start_background_training(
            bg_repo, bg_model, bg_hyperparams, experiment_id
        )
        print(f"âœ“ Background training started (PID: {background_process.pid})")

        # 2. ç­‰å¾…èƒŒæ™¯è¿›ç¨‹å¯åŠ¨
        time.sleep(5)

        # 3. è¿è¡Œå‰æ™¯è®­ç»ƒï¼ˆæ­£å¸¸ç›‘æ§ï¼‰
        print(f"\nğŸš€ Starting foreground training...")
        foreground_result = self.run_experiment(
            fg_repo, fg_model, fg_mutation, max_retries
        )

        print(f"\nâœ… Foreground training completed")

    finally:
        # 4. ç¡®ä¿åœæ­¢èƒŒæ™¯è®­ç»ƒ
        if background_process and background_process.poll() is None:
            print(f"\nğŸ›‘ Stopping background training...")
            self._stop_background_training(background_process)

    # 5. è¿”å›ç»“æœï¼ˆä»…å‰æ™¯ï¼‰
    return {
        "experiment_id": experiment_id,
        "mode": "parallel",
        "foreground_result": foreground_result,
        "background_info": {
            "repo": bg_repo,
            "model": bg_model,
            "hyperparameters": bg_hyperparams,
            "note": "Background training served as GPU load only"
        }
    }
```

#### 3. åœæ­¢èƒŒæ™¯è®­ç»ƒ

```python
def _stop_background_training(self, process):
    """åœæ­¢èƒŒæ™¯è®­ç»ƒè¿›ç¨‹ç»„"""
    try:
        # å‘æ•´ä¸ªè¿›ç¨‹ç»„å‘é€SIGTERM
        os.killpg(os.getpgid(process.pid), signal.SIGTERM)

        # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
        process.wait(timeout=10)
        print("âœ“ Background training stopped gracefully")

    except subprocess.TimeoutExpired:
        # è¶…æ—¶å¼ºåˆ¶ç»ˆæ­¢
        os.killpg(os.getpgid(process.pid), signal.SIGKILL)
        process.wait()
        print("âš ï¸ Background training force killed")

    except ProcessLookupError:
        print("âœ“ Background training already stopped")
```

### ä¼˜ç‚¹

1. âœ… **æå…¶ç®€å•**ï¼šä»…çº¦160è¡Œä»£ç 
2. âœ… **å®Œå…¨éš”ç¦»**ï¼šèƒŒæ™¯è®­ç»ƒåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œ
3. âœ… **å¯é æ¸…ç†**ï¼šè¿›ç¨‹ç»„ç®¡ç†ç¡®ä¿æ‰€æœ‰å­è¿›ç¨‹è¢«ç»ˆæ­¢
4. âœ… **æ—¥å¿—éš”ç¦»**ï¼šèƒŒæ™¯æ—¥å¿—ä¸æ±¡æŸ“å‰æ™¯æ—¥å¿—
5. âœ… **æ˜“äºè°ƒè¯•**ï¼šå¯ä»¥ç›´æ¥æŸ¥çœ‹Shellè„šæœ¬å’Œæ—¥å¿—
6. âœ… **èµ„æºè‡ªåŠ¨è°ƒåº¦**ï¼šOS/GPUè‡ªåŠ¨åˆ†é…èµ„æº

### ç¼ºç‚¹

1. âš ï¸ **éœ€è¦åˆ›å»ºä¸´æ—¶è„šæœ¬**ï¼šæ¯æ¬¡å®éªŒç”Ÿæˆä¸€ä¸ª.shæ–‡ä»¶
2. âš ï¸ **è·¨å¹³å°é™åˆ¶**ï¼šä¾èµ–Unix/Linuxçš„è¿›ç¨‹ç»„æ¦‚å¿µ

### ä»£ç é‡ä¼°è®¡

- æ–°å¢æ–¹æ³•ï¼š3ä¸ªï¼ˆçº¦160è¡Œï¼‰
- ä¿®æ”¹ç°æœ‰æ–¹æ³•ï¼š1ä¸ªï¼ˆçº¦30è¡Œï¼‰
- æ€»è®¡ï¼š**çº¦190è¡Œ**

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```json
{
  "mode": "parallel",
  "experiments": [
    {
      "foreground": {
        "repo": "pytorch_resnet_cifar10",
        "model": "resnet20",
        "mutate": ["learning_rate"]
      },
      "background": {
        "repo": "Person_reID_baseline_pytorch",
        "model": "densenet121",
        "hyperparameters": {
          "epochs": 60,
          "learning_rate": 0.05,
          "dropout": 0.5
        }
      }
    }
  ]
}
```

---

## æ–¹æ¡ˆ2: Python threadingå¤šçº¿ç¨‹

### æ ¸å¿ƒæ€æƒ³

ä½¿ç”¨Pythonæ ‡å‡†åº“çš„`threading`æ¨¡å—ï¼Œåœ¨ä¸»è¿›ç¨‹ä¸­å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹æŒç»­è¿è¡ŒèƒŒæ™¯è®­ç»ƒã€‚

### æ¶æ„å›¾

```
mutation.py (ä¸»è¿›ç¨‹)
    â”‚
    â”œâ”€ Thread 1 (background_worker)
    â”‚    â””â”€ while not stop_event: train()
    â”‚
    â””â”€ Main Thread
         â””â”€ run_experiment() â†’ set stop_event
```

### å®ç°è¦ç‚¹

```python
import threading
import queue

def _background_training_worker(self, repo, model, hyperparams,
                                stop_event, results_queue, experiment_id):
    """åå°çº¿ç¨‹å·¥ä½œå‡½æ•°"""
    run_count = 0

    while not stop_event.is_set():
        run_count += 1
        print(f"[Background] Starting run #{run_count}")

        try:
            # è¿è¡Œè®­ç»ƒ
            result = self._run_training_without_monitoring(
                repo, model, hyperparams,
                log_file=f"background_logs_{experiment_id}/run_{run_count}.log"
            )
            results_queue.put(result)

        except Exception as e:
            print(f"[Background] Run #{run_count} failed: {e}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if stop_event.wait(timeout=2):  # 2ç§’é—´éš”
            break

def run_parallel_experiment(self, ...):
    """è¿è¡Œå¹¶è¡Œå®éªŒ"""

    # åˆ›å»ºåœæ­¢äº‹ä»¶å’Œç»“æœé˜Ÿåˆ—
    stop_event = threading.Event()
    results_queue = queue.Queue()

    # å¯åŠ¨åå°çº¿ç¨‹
    background_thread = threading.Thread(
        target=self._background_training_worker,
        args=(bg_repo, bg_model, bg_hyperparams,
              stop_event, results_queue, experiment_id),
        daemon=True
    )
    background_thread.start()

    # è¿è¡Œå‰æ™¯è®­ç»ƒ
    foreground_result = self.run_experiment(fg_repo, fg_model, fg_mutation, max_retries)

    # åœæ­¢åå°çº¿ç¨‹
    stop_event.set()
    background_thread.join(timeout=60)

    # è¿”å›ç»“æœ
    return {...}
```

### ä¼˜ç‚¹

1. âœ… **æ ‡å‡†åº“æ”¯æŒ**ï¼šæ— éœ€å¤–éƒ¨ä¾èµ–
2. âœ… **å…±äº«å†…å­˜**ï¼šçº¿ç¨‹é—´é€šä¿¡ç®€å•
3. âœ… **èµ„æºå¼€é”€å°**ï¼šæ¯”å¤šè¿›ç¨‹è½»é‡
4. âœ… **æ˜“äºåŒæ­¥**ï¼šä½¿ç”¨Eventå’ŒQueue

### ç¼ºç‚¹

1. âš ï¸ **GILé™åˆ¶**ï¼šPythonå…¨å±€è§£é‡Šå™¨é”ï¼Œå¯èƒ½å½±å“æ€§èƒ½
2. âš ï¸ **éœ€è¦æ–°å¢è®­ç»ƒå‡½æ•°**ï¼š`_run_training_without_monitoring()`
3. âš ï¸ **å¤æ‚åº¦ç•¥é«˜**ï¼šéœ€è¦å¤„ç†çº¿ç¨‹åŒæ­¥

### ä»£ç é‡ä¼°è®¡

- æ–°å¢æ–¹æ³•ï¼š4ä¸ªï¼ˆçº¦220è¡Œï¼‰
- ä¿®æ”¹ç°æœ‰æ–¹æ³•ï¼š1ä¸ªï¼ˆçº¦30è¡Œï¼‰
- æ€»è®¡ï¼š**çº¦250è¡Œ**

---

## æ–¹æ¡ˆ3: Python multiprocessingå¤šè¿›ç¨‹

### æ ¸å¿ƒæ€æƒ³

ä½¿ç”¨`multiprocessing`æ¨¡å—å¯åŠ¨ç‹¬ç«‹çš„Pythonè¿›ç¨‹è¿è¡ŒèƒŒæ™¯è®­ç»ƒã€‚

### æ¶æ„å›¾

```
mutation.py (ä¸»è¿›ç¨‹)
    â”‚
    â”œâ”€ Process 1 (background_process)
    â”‚    â””â”€ while not stop_event: train()
    â”‚
    â””â”€ Main Process
         â””â”€ run_experiment() â†’ set stop_event
```

### å®ç°è¦ç‚¹

```python
import multiprocessing

def run_parallel_experiment(self, ...):
    """è¿è¡Œå¹¶è¡Œå®éªŒ"""

    # åˆ›å»ºè¿›ç¨‹é—´å…±äº«å¯¹è±¡
    stop_event = multiprocessing.Event()
    results_queue = multiprocessing.Queue()

    # å¯åŠ¨åå°è¿›ç¨‹
    background_process = multiprocessing.Process(
        target=self._background_training_worker,
        args=(bg_repo, bg_model, bg_hyperparams,
              stop_event, results_queue, experiment_id)
    )
    background_process.start()

    # è¿è¡Œå‰æ™¯è®­ç»ƒ
    foreground_result = self.run_experiment(...)

    # åœæ­¢åå°è¿›ç¨‹
    stop_event.set()
    background_process.join(timeout=60)
    background_process.terminate()

    return {...}
```

### ä¼˜ç‚¹

1. âœ… **çœŸæ­£å¹¶è¡Œ**ï¼šä¸å—GILé™åˆ¶
2. âœ… **è¿›ç¨‹éš”ç¦»**ï¼šæ•…éšœä¸ä¼šç›¸äº’å½±å“

### ç¼ºç‚¹

1. âŒ **å†…å­˜å¼€é”€å¤§**ï¼šæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹å†…å­˜ç©ºé—´
2. âŒ **è¿›ç¨‹é—´é€šä¿¡å¤æ‚**ï¼šéœ€è¦åºåˆ—åŒ–æ•°æ®
3. âŒ **ä»£ç å¤æ‚**ï¼šéœ€è¦å¤„ç†è¿›ç¨‹åŒæ­¥å’Œå…±äº«çŠ¶æ€

### ä»£ç é‡ä¼°è®¡

- æ–°å¢æ–¹æ³•ï¼š4ä¸ªï¼ˆçº¦250è¡Œï¼‰
- ä¿®æ”¹ç°æœ‰æ–¹æ³•ï¼š1ä¸ªï¼ˆçº¦30è¡Œï¼‰
- æ€»è®¡ï¼š**çº¦280è¡Œ**

---

## æ–¹æ¡ˆ4: ä¿®æ”¹run.shæ”¯æŒåå°æ¨¡å¼

### æ ¸å¿ƒæ€æƒ³

ä¿®æ”¹`scripts/run.sh`ï¼Œæ·»åŠ åå°è¿è¡Œæ¨¡å¼ï¼Œé€šè¿‡å‚æ•°æ§åˆ¶æ˜¯å¦åœ¨åå°å¾ªç¯è¿è¡Œã€‚

### å®ç°è¦ç‚¹

```bash
# run.sh æ–°å¢å‚æ•°
BACKGROUND_MODE=$5  # æ–°å¢å‚æ•°

if [ "$BACKGROUND_MODE" = "background" ]; then
    # åå°å¾ªç¯æ¨¡å¼
    while true; do
        $TRAIN_SCRIPT $TRAIN_ARGS
        sleep 2
    done
else
    # æ­£å¸¸æ¨¡å¼
    $TRAIN_SCRIPT $TRAIN_ARGS
fi
```

### ä¼˜ç‚¹

1. âœ… **å¤ç”¨ç°æœ‰è„šæœ¬**ï¼šä¸éœ€è¦ç”Ÿæˆä¸´æ—¶æ–‡ä»¶

### ç¼ºç‚¹

1. âŒ **ä¿®æ”¹æ ¸å¿ƒè„šæœ¬**ï¼šå½±å“ç°æœ‰åŠŸèƒ½
2. âŒ **å‘åå…¼å®¹é£é™©**ï¼šå¯èƒ½ç ´åç°æœ‰è°ƒç”¨
3. âŒ **å¤æ‚åº¦é«˜**ï¼šéœ€è¦ä¿®æ”¹å¤šå¤„ä»£ç 
4. âŒ **æµ‹è¯•æˆæœ¬é«˜**ï¼šéœ€è¦å›å½’æµ‹è¯•æ‰€æœ‰åŠŸèƒ½

### ä»£ç é‡ä¼°è®¡

- ä¿®æ”¹run.shï¼šçº¦50è¡Œ
- ä¿®æ”¹mutation.pyï¼šçº¦200è¡Œ
- æ€»è®¡ï¼š**çº¦250è¡Œ**

---

## æ–¹æ¡ˆå¯¹æ¯”è¯¦è¡¨

| ç»´åº¦ | æ–¹æ¡ˆ1: Shellè„šæœ¬ | æ–¹æ¡ˆ2: çº¿ç¨‹ | æ–¹æ¡ˆ3: å¤šè¿›ç¨‹ | æ–¹æ¡ˆ4: ä¿®æ”¹run.sh |
|------|---------------|----------|-----------|---------------|
| **å®ç°å¤æ‚åº¦** | â­â­â­â­â­ éå¸¸ç®€å• | â­â­â­â­â˜† ç®€å• | â­â­â­â˜†â˜† ä¸­ç­‰ | â­â­â˜†â˜†â˜† å¤æ‚ |
| **ä»£ç é‡** | 190è¡Œ | 250è¡Œ | 280è¡Œ | 250è¡Œ |
| **å‘åå…¼å®¹** | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â˜† è‰¯å¥½ | â­â­â­â˜†â˜† ä¸€èˆ¬ |
| **èµ„æºç®¡ç†** | â­â­â­â­â­ å®Œå…¨éš”ç¦» | â­â­â­â­â˜† å…±äº«å†…å­˜ | â­â­â­â­â­ å®Œå…¨éš”ç¦» | â­â­â­â˜†â˜† ä¾èµ–ä¿®æ”¹ |
| **è¿›ç¨‹æ¸…ç†** | â­â­â­â­â­ è¿›ç¨‹ç»„ | â­â­â­â­â˜† çº¿ç¨‹join | â­â­â­â­â˜† è¿›ç¨‹terminate | â­â­â­â˜†â˜† æ‰‹åŠ¨ç®¡ç† |
| **æ—¥å¿—éš”ç¦»** | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â˜† éœ€è¦å¤„ç† | â­â­â­â­â˜† éœ€è¦å¤„ç† | â­â­â­â˜†â˜† éœ€è¦ä¿®æ”¹ |
| **è°ƒè¯•éš¾åº¦** | â­â­â­â­â­ å¾ˆå®¹æ˜“ | â­â­â­â˜†â˜† ä¸­ç­‰ | â­â­â˜†â˜†â˜† è¾ƒéš¾ | â­â­â˜†â˜†â˜† è¾ƒéš¾ |
| **è·¨å¹³å°** | â­â­â­â˜†â˜† Linux/Mac | â­â­â­â­â­ å…¨å¹³å° | â­â­â­â­â­ å…¨å¹³å° | â­â­â­â˜†â˜† Linux/Mac |
| **æµ‹è¯•æˆæœ¬** | â­â­â­â­â­ å¾ˆä½ | â­â­â­â­â˜† ä½ | â­â­â­â˜†â˜† ä¸­ç­‰ | â­â­â˜†â˜†â˜† é«˜ |
| **ç»´æŠ¤æˆæœ¬** | â­â­â­â­â­ å¾ˆä½ | â­â­â­â­â˜† ä½ | â­â­â­â˜†â˜† ä¸­ç­‰ | â­â­â˜†â˜†â˜† é«˜ |

---

## æ¨èå†³ç­–

### ğŸ¥‡ **é¦–é€‰æ–¹æ¡ˆï¼šæ–¹æ¡ˆ1 - subprocess.Popen + Shellè„šæœ¬**

**ç†ç”±**ï¼š
1. âœ… **æœ€ç®€å•**ï¼šå®ç°æ¸…æ™°ï¼Œä»£ç é‡æœ€å°‘
2. âœ… **æœ€å¯é **ï¼šè¿›ç¨‹ç»„ç®¡ç†ï¼Œæ¸…ç†å½»åº•
3. âœ… **æœ€æ˜“ç»´æŠ¤**ï¼šé€»è¾‘ç‹¬ç«‹ï¼Œä¸å½±å“ç°æœ‰ä»£ç 
4. âœ… **å®Œå…¨å…¼å®¹**ï¼šä¸ä¿®æ”¹ä»»ä½•ç°æœ‰æ–‡ä»¶
5. âœ… **æ˜“äºè°ƒè¯•**ï¼šå¯ä»¥ç›´æ¥æŸ¥çœ‹ç”Ÿæˆçš„Shellè„šæœ¬

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… Linux/Macç¯å¢ƒï¼ˆé¡¹ç›®å½“å‰ç¯å¢ƒï¼‰
- âœ… ä¸éœ€è¦è·¨å¹³å°æ”¯æŒ
- âœ… è¿½æ±‚ç®€æ´å’Œå¯é æ€§

### ğŸ¥ˆ **å¤‡é€‰æ–¹æ¡ˆï¼šæ–¹æ¡ˆ2 - Python threading**

**ç†ç”±**ï¼š
1. âœ… æ ‡å‡†åº“æ”¯æŒï¼Œè·¨å¹³å°
2. âœ… å…±äº«å†…å­˜ï¼Œé€šä¿¡ç®€å•
3. âš ï¸ ä½†ä»£ç å¤æ‚åº¦ç•¥é«˜
4. âš ï¸ GILå¯èƒ½å½±å“æ€§èƒ½ï¼ˆå®é™…å½±å“ä¸å¤§ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦è·¨å¹³å°æ”¯æŒï¼ˆWindowsï¼‰
- âœ… éœ€è¦æ›´ç»†ç²’åº¦çš„æ§åˆ¶
- âš ï¸ æ„¿æ„æ¥å—ç•¥é«˜çš„å¤æ‚åº¦

### âŒ **ä¸æ¨èæ–¹æ¡ˆ**

- **æ–¹æ¡ˆ3ï¼ˆå¤šè¿›ç¨‹ï¼‰**ï¼šå¤æ‚åº¦é«˜ï¼Œæ”¶ç›Šä¸æ˜æ˜¾
- **æ–¹æ¡ˆ4ï¼ˆä¿®æ”¹run.shï¼‰**ï¼šé£é™©é«˜ï¼Œç ´åç°æœ‰åŠŸèƒ½

---

## å®ç°å»ºè®®

### æ¨èå®æ–½è·¯å¾„

**Phase 1: å¿«é€ŸåŸå‹ï¼ˆ1-2å°æ—¶ï¼‰**
1. å®ç°æ–¹æ¡ˆ1çš„æ ¸å¿ƒåŠŸèƒ½
2. åˆ›å»ºç®€å•æµ‹è¯•é…ç½®
3. éªŒè¯åŸºæœ¬åŠŸèƒ½

**Phase 2: å®Œå–„åŠŸèƒ½ï¼ˆ2-3å°æ—¶ï¼‰**
1. æ·»åŠ é”™è¯¯å¤„ç†
2. ä¼˜åŒ–æ—¥å¿—è¾“å‡º
3. æ·»åŠ é…ç½®éªŒè¯

**Phase 3: æ–‡æ¡£å’Œæµ‹è¯•ï¼ˆ1-2å°æ—¶ï¼‰**
1. æ›´æ–°æ–‡æ¡£
2. åˆ›å»ºç¤ºä¾‹é…ç½®
3. ç¼–å†™æµ‹è¯•è„šæœ¬

### éªŒæ”¶æ ‡å‡†

1. âœ… å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶è¿è¡Œå¹¶è¡Œè®­ç»ƒ
2. âœ… å‰æ™¯è®­ç»ƒæ­£å¸¸ç›‘æ§ï¼Œæ•°æ®å‡†ç¡®
3. âœ… èƒŒæ™¯è®­ç»ƒæŒç»­è¿è¡Œï¼Œå‰æ™¯å®Œæˆååœæ­¢
4. âœ… è¿›ç¨‹æ¸…ç†å¹²å‡€ï¼Œæ— åƒµå°¸è¿›ç¨‹
5. âœ… åŸæœ‰åŠŸèƒ½å®Œå…¨ä¸å—å½±å“

---

## åç»­æ‰©å±•

### å¯é€‰åŠŸèƒ½ï¼ˆPhase 3+ï¼‰

1. **GPUå†…å­˜é™åˆ¶**
   ```python
   os.environ['CUDA_VISIBLE_DEVICES'] = '0'
   # åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½®memory_fraction
   ```

2. **å¤šä¸ªèƒŒæ™¯æ¨¡å‹**
   ```json
   "background": [
       {"repo": "...", "model": "..."},
       {"repo": "...", "model": "..."}
   ]
   ```

3. **èƒŒæ™¯è®­ç»ƒç»Ÿè®¡**
   - è®°å½•èƒŒæ™¯è®­ç»ƒå®Œæˆçš„è½®æ•°
   - ç»Ÿè®¡èƒŒæ™¯è®­ç»ƒæ€»æ—¶é—´

4. **åŠ¨æ€è°ƒæ•´**
   - æ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
   - æ ¹æ®GPUè´Ÿè½½åŠ¨æ€è°ƒæ•´èƒŒæ™¯è®­ç»ƒå¼ºåº¦

---

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæ¨èæ–¹æ¡ˆ1è€Œä¸æ˜¯å¤šçº¿ç¨‹ï¼Ÿ

**A**:
1. **ç®€å•æ€§**ï¼šShellè„šæœ¬æ–¹æ¡ˆä»£ç é‡æ›´å°‘ï¼Œé€»è¾‘æ›´æ¸…æ™°
2. **éš”ç¦»æ€§**ï¼šå®Œå…¨ç‹¬ç«‹çš„è¿›ç¨‹ï¼Œä¸ä¼šç›¸äº’å¹²æ‰°
3. **å¯é æ€§**ï¼šè¿›ç¨‹ç»„ç®¡ç†ç¡®ä¿æ¸…ç†å½»åº•
4. **è°ƒè¯•å‹å¥½**ï¼šå¯ä»¥ç›´æ¥æŸ¥çœ‹Shellè„šæœ¬å’Œæ‰§è¡Œè¿‡ç¨‹

### Q: æ–¹æ¡ˆ1åœ¨Windowsä¸Šèƒ½ç”¨å—ï¼Ÿ

**A**: ä¸èƒ½ã€‚æ–¹æ¡ˆ1ä¾èµ–Unix/Linuxçš„è¿›ç¨‹ç»„æ¦‚å¿µã€‚å¦‚æœéœ€è¦Windowsæ”¯æŒï¼Œåº”è¯¥é€‰æ‹©æ–¹æ¡ˆ2ï¼ˆthreadingï¼‰ã€‚ä½†é¡¹ç›®å½“å‰ç¯å¢ƒæ˜¯Linuxï¼Œæ–¹æ¡ˆ1å®Œå…¨é€‚ç”¨ã€‚

### Q: èƒ½è€—æ•°æ®å¦‚ä½•å¤„ç†ï¼Ÿ

**A**: æ‰€æœ‰æ–¹æ¡ˆçš„èƒ½è€—æ•°æ®éƒ½æ˜¯å‰æ™¯+èƒŒæ™¯çš„æ€»å’Œï¼Œæ— æ³•ç²¾ç¡®åˆ†ç¦»ï¼ˆç¡¬ä»¶é™åˆ¶ï¼‰ã€‚åœ¨ç»“æœJSONä¸­ä¼šæ˜ç¡®æ ‡æ³¨ï¼š
```json
{
  "energy_metrics": {
    "attribution": "combined",
    "note": "Energy includes both foreground and background training"
  }
}
```

### Q: å¦‚æœèƒŒæ™¯è®­ç»ƒå´©æºƒæ€ä¹ˆåŠï¼Ÿ

**A**: Shellè„šæœ¬ä¸­çš„whileå¾ªç¯ä¼šè‡ªåŠ¨é‡å¯èƒŒæ™¯è®­ç»ƒï¼Œä¸å½±å“å‰æ™¯è®­ç»ƒã€‚å´©æºƒä¿¡æ¯ä¼šè®°å½•åœ¨èƒŒæ™¯æ—¥å¿—ä¸­ã€‚

---

## å†³ç­–å»ºè®®

**å¼ºçƒˆæ¨èä½¿ç”¨æ–¹æ¡ˆ1** - subprocess.Popen + Shellè„šæœ¬

**ç†ç”±æ€»ç»“**ï¼š
1. â­â­â­â­â­ æœ€ç®€å•
2. â­â­â­â­â­ æœ€å¯é 
3. â­â­â­â­â­ æœ€æ˜“ç»´æŠ¤
4. â­â­â­â­â­ å®Œå…¨å‘åå…¼å®¹
5. â­â­â­â­â­ é€‚åˆå½“å‰ç¯å¢ƒ

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼š
1. ç¡®è®¤æ–¹æ¡ˆé€‰æ‹©
2. å¼€å§‹å®æ–½Phase 1
3. åˆ›å»ºæµ‹è¯•é…ç½®éªŒè¯åŠŸèƒ½

---

**æœ€åæ›´æ–°**: 2025-11-12
**è¯¦ç»†è®¾è®¡**: [PARALLEL_TRAINING_DESIGN.md](PARALLEL_TRAINING_DESIGN.md)
