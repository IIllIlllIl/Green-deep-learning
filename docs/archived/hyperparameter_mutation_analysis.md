# è¶…å‚æ•°å˜å¼‚å¯è¡Œæ€§åˆ†æ

## æ¦‚è¿°
æœ¬æ–‡æ¡£åˆ†æäº†6ä¸ªä»“åº“ä¸­10ä¸ªæ¨¡å‹å¯¹è¶…å‚æ•°å˜å¼‚çš„æ”¯æŒæƒ…å†µï¼Œè¯„ä¼°æ˜¯å¦éœ€è¦ä¿®æ”¹æºä»£ç ä»¥åŠä¿®æ”¹çš„å¯è¡Œæ€§ã€‚

## éœ€è¦å˜å¼‚çš„è¶…å‚æ•°
1. **epochs** - è®­ç»ƒè½®æ•°
2. **learning_rate** - å­¦ä¹ ç‡
3. **seed** - éšæœºç§å­
4. **precision** - ç²¾åº¦ (fp16/bf16/fp32)
5. **dropout** - Dropoutæ¯”ç‡
6. **weight_decay** - æƒé‡è¡°å‡

---

## å„ä»“åº“è¶…å‚æ•°æ”¯æŒæƒ…å†µ

### 1. MRT-OAST (1ä¸ªæ¨¡å‹)

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `--epochs` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `--lr` | å¦ |
| seed | âœ… å®Œå…¨æ”¯æŒ | `--seed` | å¦ |
| precision | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| dropout | âœ… å®Œå…¨æ”¯æŒ | `--dropout` | å¦ |
| weight_decay | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |

**ä¿®æ”¹éš¾åº¦**: ğŸŸ¡ ä¸­ç­‰
- éœ€è¦åœ¨ `main_batch.py` æ·»åŠ  precision å’Œ weight_decay å‚æ•°
- éœ€è¦ä¿®æ”¹ä¼˜åŒ–å™¨é…ç½®ï¼ˆæ·»åŠ weight_decayå‚æ•°ï¼‰
- éœ€è¦æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒï¼ˆtorch.cuda.ampï¼‰

---

### 2. bug-localization-by-dnn-and-rvsm (1ä¸ªæ¨¡å‹)

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âš ï¸ éƒ¨åˆ†æ”¯æŒ | `--max_iter` | å¦ (ä½¿ç”¨max_iter) |
| learning_rate | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (sklearné™åˆ¶) |
| seed | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| precision | N/A | - | N/A (sklearnä¸é€‚ç”¨) |
| dropout | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (sklearné™åˆ¶) |
| weight_decay | âœ… å®Œå…¨æ”¯æŒ | `--alpha` | å¦ (ä½¿ç”¨alpha) |

**ä¿®æ”¹éš¾åº¦**: ğŸ”´ å›°éš¾
- åŸºäºsklearnçš„MLPClassifierï¼Œè®¸å¤šå‚æ•°å›ºå®šåœ¨æ¨¡å‹ä¸­
- learning_rate éœ€è¦ä¿®æ”¹ `train_wrapper.py` æ·»åŠ  `learning_rate_init` å‚æ•°
- seed éœ€è¦æ·»åŠ  `random_state` å‚æ•°
- dropout ä¸æ˜“å®ç°ï¼ˆsklearnçš„MLPClassifieræ²¡æœ‰ç›´æ¥çš„dropoutå‚æ•°ï¼‰

**å»ºè®®**: æ­¤æ¨¡å‹å¯èƒ½ä¸é€‚åˆè¿›è¡Œå®Œæ•´çš„è¶…å‚æ•°å˜å¼‚å®éªŒ

---

### 3. pytorch_resnet_cifar10 (1ä¸ªæ¨¡å‹)

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `--epochs` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `--lr` | å¦ |
| seed | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| precision | âš ï¸ éƒ¨åˆ†æ”¯æŒ | `--half` (ä»…fp16) | **æ˜¯** (æ·»åŠ bf16) |
| dropout | N/A | - | N/A (ResNetæ— dropout) |
| weight_decay | âœ… å®Œå…¨æ”¯æŒ | `--weight-decay` | å¦ |

**ä¿®æ”¹éš¾åº¦**: ğŸŸ¢ ç®€å•
- seed: åœ¨ `trainer.py` æ·»åŠ  seed å‚æ•°ï¼Œè®¾ç½® random seed
- bf16: æ·»åŠ  `--bf16` å‚æ•°ï¼Œä½¿ç”¨ torch.autocast(dtype=torch.bfloat16)

---

### 4. VulBERTa (2ä¸ªæ¨¡å‹: mlp, cnn)

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `--epochs` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `--learning_rate` | å¦ |
| seed | âœ… å®Œå…¨æ”¯æŒ | `--seed` | å¦ |
| precision | âš ï¸ éƒ¨åˆ†æ”¯æŒ | `--fp16` (ä»…fp16) | **æ˜¯** (æ·»åŠ bf16) |
| dropout | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| weight_decay | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |

**ä¿®æ”¹éš¾åº¦**: ğŸŸ¡ ä¸­ç­‰
- dropout: éœ€è¦åœ¨æ¨¡å‹å®šä¹‰ä¸­æ·»åŠ dropoutå±‚ï¼ˆä¿®æ”¹æ¨¡å‹æ¶æ„ï¼‰
- weight_decay: åœ¨ä¼˜åŒ–å™¨é…ç½®ä¸­æ·»åŠ  weight_decay å‚æ•°
- bf16: æ·»åŠ  `--bf16` å‚æ•°æ”¯æŒ

---

### 5. Person_reID_baseline_pytorch (3ä¸ªæ¨¡å‹: densenet121, hrnet18, pcb)

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `--total_epoch` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `--lr` | å¦ |
| seed | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| precision | âœ… å®Œå…¨æ”¯æŒ | `--fp16`, `--bf16` | å¦ |
| dropout | âœ… å®Œå…¨æ”¯æŒ | `--droprate` | å¦ |
| weight_decay | âœ… å®Œå…¨æ”¯æŒ | `--weight_decay` | å¦ |

**ä¿®æ”¹éš¾åº¦**: ğŸŸ¢ ç®€å•
- seed: ä»…éœ€æ·»åŠ  seed å‚æ•°å¹¶è®¾ç½®éšæœºç§å­

**æœ€ä½³å®è·µæ¡ˆä¾‹ï¼** è¿™ä¸ªä»“åº“å¯¹è¶…å‚æ•°æ”¯æŒæœ€å®Œå–„ã€‚

---

### 6. examples (4ä¸ªæ¨¡å‹)

#### MNIST CNN, MNIST RNN, Siamese Network

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `-e` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `-l` | å¦ |
| seed | âœ… å®Œå…¨æ”¯æŒ | `--seed` | å¦ |
| precision | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| dropout | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| weight_decay | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |

#### MNIST Forward-Forward

| è¶…å‚æ•° | æ”¯æŒæƒ…å†µ | å‘½ä»¤è¡Œå‚æ•° | æ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç  |
|--------|----------|-----------|----------------|
| epochs | âœ… å®Œå…¨æ”¯æŒ | `-e` | å¦ |
| learning_rate | âœ… å®Œå…¨æ”¯æŒ | `-l` | å¦ |
| seed | âœ… å®Œå…¨æ”¯æŒ | `--seed` | å¦ |
| precision | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| dropout | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |
| weight_decay | âŒ ä¸æ”¯æŒ | - | **æ˜¯** (éœ€æ·»åŠ ) |

**ä¿®æ”¹éš¾åº¦**: ğŸŸ¡ ä¸­ç­‰
- æ¯ä¸ªç¤ºä¾‹æ¨¡å‹éƒ½éœ€è¦ç‹¬ç«‹ä¿®æ”¹
- precision: æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
- dropout: åœ¨æ¨¡å‹ä¸­æ·»åŠ dropoutå±‚
- weight_decay: åœ¨ä¼˜åŒ–å™¨ä¸­æ·»åŠ å‚æ•°

---

## æ€»ä½“è¯„ä¼°

### ğŸ“Š æ”¯æŒæƒ…å†µç»Ÿè®¡

| è¶…å‚æ•° | å®Œå…¨æ”¯æŒ | éƒ¨åˆ†æ”¯æŒ | ä¸æ”¯æŒ | æ”¯æŒç‡ |
|--------|---------|---------|--------|-------|
| epochs | 9/10 | 1/10 | 0/10 | **100%** |
| learning_rate | 8/10 | 0/10 | 2/10 | **80%** |
| seed | 6/10 | 0/10 | 4/10 | **60%** |
| precision | 1/10 | 3/10 | 6/10 | **40%** |
| dropout | 3/10 | 0/10 | 7/10 | **30%** |
| weight_decay | 3/10 | 1/10 | 6/10 | **40%** |

### ğŸ¯ ä¿®æ”¹ç­–ç•¥å»ºè®®

#### **ç­–ç•¥1: ä»…ä½¿ç”¨å·²æ”¯æŒçš„å‚æ•°ï¼ˆæ¨è - å¿«é€Ÿå¯åŠ¨ï¼‰**
ä¼˜ç‚¹ï¼š
- âœ… æ— éœ€ä¿®æ”¹ä»£ç ï¼Œç«‹å³å¯ç”¨
- âœ… ä¸å½±å“æ¨¡å‹åŸå§‹è¡Œä¸º
- âœ… é™ä½å¼•å…¥bugçš„é£é™©

é™åˆ¶ï¼š
- âš ï¸ åªèƒ½å˜å¼‚éƒ¨åˆ†è¶…å‚æ•°
- âš ï¸ ä¸åŒæ¨¡å‹å¯å˜å¼‚çš„å‚æ•°ä¸ä¸€è‡´

**é€‚ç”¨åœºæ™¯**ï¼šå¿«é€ŸéªŒè¯å®éªŒæ¡†æ¶ï¼Œåˆæ­¥æ•°æ®æ”¶é›†

---

#### **ç­–ç•¥2: ä¿®æ”¹ä»£ç æ·»åŠ ç¼ºå¤±å‚æ•°ï¼ˆæ¨è - å®Œæ•´å®éªŒï¼‰**
éœ€è¦ä¿®æ”¹çš„ä»“åº“ï¼š
- ğŸ”´ **é«˜ä¼˜å…ˆçº§** (æ˜“ä¿®æ”¹ï¼Œå½±å“å¤§):
  - `pytorch_resnet_cifar10`: æ·»åŠ  seed, bf16
  - `Person_reID_baseline_pytorch`: æ·»åŠ  seed

- ğŸŸ¡ **ä¸­ä¼˜å…ˆçº§** (ä¸­ç­‰éš¾åº¦):
  - `MRT-OAST`: æ·»åŠ  precision, weight_decay
  - `VulBERTa`: æ·»åŠ  dropout, weight_decay, bf16
  - `examples`: æ·»åŠ  precision, dropout, weight_decay

- ğŸ”´ **ä½ä¼˜å…ˆçº§** (å›°éš¾/ä¸å»ºè®®):
  - `bug-localization-by-dnn-and-rvsm`: sklearné™åˆ¶å¤š

**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å®Œæ•´å¯¹æ¯”æ‰€æœ‰è¶…å‚æ•°å½±å“

---

#### **ç­–ç•¥3: æ··åˆç­–ç•¥ï¼ˆæ¨è - å¹³è¡¡æ–¹æ¡ˆï¼‰**
```
é˜¶æ®µ1: ä½¿ç”¨å·²æ”¯æŒå‚æ•°è¿›è¡Œåˆæ­¥å®éªŒï¼ˆ1-2å‘¨ï¼‰
  â”œâ”€ æ”¶é›†åŸºå‡†æ•°æ®
  â””â”€ éªŒè¯å®éªŒæ¡†æ¶

é˜¶æ®µ2: é€æ­¥æ·»åŠ å…³é”®å‚æ•°ï¼ˆ2-3å‘¨ï¼‰
  â”œâ”€ ä¼˜å…ˆä¿®æ”¹ç®€å•çš„ï¼ˆseed, bf16ï¼‰
  â”œâ”€ é€æ­¥æ·»åŠ ä¸­ç­‰éš¾åº¦çš„ï¼ˆweight_decay, dropoutï¼‰
  â””â”€ è·³è¿‡å›°éš¾çš„æˆ–ä¸é€‚ç”¨çš„
```

---

## ğŸ› ï¸ ä»£ç ä¿®æ”¹æŒ‡å—

### 1. æ·»åŠ  Seed å‚æ•°ï¼ˆéš¾åº¦: â­ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**: `train.py` æˆ– `trainer.py`

```python
# 1. æ·»åŠ argparseå‚æ•°
parser.add_argument('--seed', type=int, default=42, help='random seed')

# 2. åœ¨è®­ç»ƒå¼€å§‹å‰è®¾ç½®
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

args = parser.parse_args()
set_seed(args.seed)
```

**å½±å“**: æœ€å°ï¼Œä¸æ”¹å˜æ¨¡å‹è¡Œä¸ºï¼Œåªå½±å“éšæœºæ€§

---

### 2. æ·»åŠ  Weight Decayï¼ˆéš¾åº¦: ï¿½ï¿½ï¿½ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**: ä¼˜åŒ–å™¨é…ç½®éƒ¨åˆ†

```python
# 1. æ·»åŠ argparseå‚æ•°
parser.add_argument('--weight_decay', type=float, default=5e-4,
                    help='weight decay (L2 penalty)')

# 2. åœ¨ä¼˜åŒ–å™¨ä¸­ä½¿ç”¨
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=args.lr,
    momentum=0.9,
    weight_decay=args.weight_decay  # æ·»åŠ è¿™ä¸€è¡Œ
)
```

**å½±å“**: æœ€å°ï¼ŒPyTorchåŸç”Ÿæ”¯æŒ

---

### 3. æ·»åŠ  BF16 ç²¾åº¦ï¼ˆéš¾åº¦: â­â­ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**: è®­ç»ƒå¾ªç¯

```python
# 1. æ·»åŠ argparseå‚æ•°
parser.add_argument('--bf16', action='store_true',
                    help='use bfloat16 precision')

# 2. ä¿®æ”¹è®­ç»ƒå¾ªç¯
if args.bf16:
    scaler = torch.cuda.amp.GradScaler(enabled=True)

for data, target in train_loader:
    if args.bf16:
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            output = model(data)
            loss = criterion(output, target)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    else:
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

**å½±å“**: ä¸­ç­‰ï¼Œéœ€è¦æµ‹è¯•æ•°å€¼ç¨³å®šæ€§

---

### 4. æ·»åŠ  Dropoutï¼ˆéš¾åº¦: â­â­â­ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**: æ¨¡å‹å®šä¹‰æ–‡ä»¶

```python
# 1. æ·»åŠ argparseå‚æ•°
parser.add_argument('--dropout', type=float, default=0.5,
                    help='dropout probability')

# 2. ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼ˆç¤ºä¾‹ï¼‰
class Net(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 512)
        self.dropout1 = nn.Dropout(dropout_rate)  # æ·»åŠ 
        self.fc2 = nn.Linear(512, 10)
        self.dropout2 = nn.Dropout(dropout_rate)  # æ·»åŠ 

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)  # æ·»åŠ 
        x = self.fc2(x)
        return x
```

**å½±å“**: è¾ƒå¤§ï¼Œæ”¹å˜æ¨¡å‹æ¶æ„ï¼Œéœ€è¦é‡æ–°éªŒè¯æ€§èƒ½

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. **æ¨¡å‹æ¶æ„ç›¸å…³çš„å‚æ•°ï¼ˆDropoutï¼‰**
- ä¿®æ”¹åå¯èƒ½æ˜¾è‘—æ”¹å˜æ¨¡å‹æ€§èƒ½åŸºå‡†
- å»ºè®®ï¼šå…ˆç”¨åŸå§‹æ¨¡å‹è®­ç»ƒä½œä¸ºbaselineï¼Œå†è¿›è¡Œå˜å¼‚å®éªŒ

### 2. **ç²¾åº¦ç›¸å…³å‚æ•°ï¼ˆFP16/BF16ï¼‰**
- å¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§å’Œæ”¶æ•›æ€§
- æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡
- å»ºè®®ï¼šé€ä¸ªæ¨¡å‹æµ‹è¯•ï¼Œç¡®ä¿è®­ç»ƒæ­£å¸¸æ”¶æ•›

### 3. **sklearnæ¨¡å‹é™åˆ¶**
- `bug-localization-by-dnn-and-rvsm` ä½¿ç”¨sklearnï¼Œå‚æ•°çµæ´»æ€§æœ‰é™
- å»ºè®®ï¼šå¯ä»¥è€ƒè™‘è·³è¿‡æ­¤æ¨¡å‹ï¼Œæˆ–ä»…å˜å¼‚æœ‰é™çš„å‚æ•°

### 4. **éšæœºç§å­çš„å½±å“**
- æ·»åŠ seedå¯ä»¥æé«˜å®éªŒå¯é‡å¤æ€§
- ä½†æŸäº›æ¨¡å‹ï¼ˆå¦‚CUDAæ“ä½œï¼‰å¯èƒ½æ— æ³•å®Œå…¨ç¡®å®šæ€§

---

## ğŸ“‹ è¡ŒåŠ¨å»ºè®®

### æ–¹æ¡ˆA: æœ€å°ä¿®æ”¹ï¼ˆ1-2å¤©å·¥ä½œé‡ï¼‰
ä»…ä¿®æ”¹å®¹æ˜“çš„å‚æ•°ï¼š
1. âœ… `pytorch_resnet_cifar10`: æ·»åŠ  seed
2. âœ… `Person_reID_baseline_pytorch`: æ·»åŠ  seed

**ç»“æœ**: 6ä¸ªä»“åº“ï¼Œ10ä¸ªæ¨¡å‹ï¼Œå¯å˜å¼‚4-5ä¸ªè¶…å‚æ•°

---

### æ–¹æ¡ˆB: é€‚åº¦ä¿®æ”¹ï¼ˆ1å‘¨å·¥ä½œé‡ï¼‰
æ·»åŠ å…³é”®å‚æ•°ï¼š
1. âœ… æ‰€æœ‰ç¼ºå¤±seedçš„ä»“åº“ï¼šæ·»åŠ  seed
2. âœ… `MRT-OAST`: æ·»åŠ  weight_decay
3. âœ… `VulBERTa`: æ·»åŠ  weight_decay
4. âœ… `pytorch_resnet_cifar10`: æ·»åŠ  bf16
5. âœ… `Person_reID_baseline_pytorch`: å·²å®Œæ•´æ”¯æŒ

**ç»“æœ**: å¤§éƒ¨åˆ†æ¨¡å‹å¯å˜å¼‚5-6ä¸ªè¶…å‚æ•°

---

### æ–¹æ¡ˆC: å®Œæ•´ä¿®æ”¹ï¼ˆ2-3å‘¨å·¥ä½œé‡ï¼‰
æ·»åŠ æ‰€æœ‰å‚æ•°ï¼š
- ä¿®æ”¹æ‰€æœ‰æ¨¡å‹æ¶æ„ä»¥æ”¯æŒdropout
- ä¸ºæ‰€æœ‰æ¨¡å‹æ·»åŠ æ··åˆç²¾åº¦æ”¯æŒ
- ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ”¯æŒæ‰€æœ‰6ä¸ªè¶…å‚æ•°

**ç»“æœ**: æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€æ”¯æŒ6ä¸ªè¶…å‚æ•°ï¼ˆé™¤sklearnæ¨¡å‹ï¼‰

---

## ğŸ“ æœ€ç»ˆå»ºè®®

**æ¨èé‡‡ç”¨æ–¹æ¡ˆBï¼ˆé€‚åº¦ä¿®æ”¹ï¼‰**

ç†ç”±ï¼š
1. âœ… å¹³è¡¡å·¥ä½œé‡ä¸å®éªŒå®Œæ•´æ€§
2. âœ… å¤§éƒ¨åˆ†æ¨¡å‹å¯ä»¥è¿›è¡Œæœ‰æ„ä¹‰çš„è¶…å‚æ•°å˜å¼‚
3. âœ… é¿å…è¿‡åº¦ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼ˆä¿æŒåŸå§‹ç‰¹æ€§ï¼‰
4. âœ… å¯ä»¥åœ¨å®éªŒè¿‡ç¨‹ä¸­é€æ­¥æ‰©å±•

**åˆ†é˜¶æ®µæ‰§è¡Œ**ï¼š
```
Week 1: ä¿®æ”¹æ‰€æœ‰ç¼ºå¤±seedçš„ä»“åº“ + pytorch_resnet_cifar10çš„bf16
Week 2: ä¸ºMRT-OASTå’ŒVulBERTaæ·»åŠ weight_decay
Week 3: æ ¹æ®åˆæ­¥å®éªŒç»“æœï¼Œå†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ·»åŠ å…¶ä»–å‚æ•°
```

---

## é™„å½•ï¼šå„ä»“åº“éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### MRT-OAST
- [ ] `main_batch.py`: æ·»åŠ  precision å’Œ weight_decay å‚æ•°
- [ ] `train.sh`: æ›´æ–°å¸®åŠ©ä¿¡æ¯

### bug-localization-by-dnn-and-rvsm
- [ ] `train_wrapper.py`: æ·»åŠ  seed å’Œ learning_rate_init å‚æ•°

### pytorch_resnet_cifar10
- [ ] `trainer.py`: æ·»åŠ  seed å’Œ bf16 å‚æ•°
- [ ] `train.sh`: æ›´æ–°å¸®åŠ©ä¿¡æ¯

### VulBERTa
- [ ] `train_vulberta.py`: æ·»åŠ  dropout, weight_decay, bf16 å‚æ•°
- [ ] æ¨¡å‹æ–‡ä»¶: ä¿®æ”¹æ¨¡å‹æ¶æ„æ·»åŠ dropoutå±‚

### Person_reID_baseline_pytorch
- [ ] `train.py`: æ·»åŠ  seed å‚æ•°

### examples
- [ ] `mnist/main.py`: æ·»åŠ  precision, dropout, weight_decay
- [ ] `mnist_rnn/main.py`: æ·»åŠ  precision, dropout, weight_decay
- [ ] `mnist_forward_forward/main.py`: æ·»åŠ  precision, dropout, weight_decay
- [ ] `siamese_network/main.py`: æ·»åŠ  precision, dropout, weight_decay

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-05
**ä½œè€…**: Claude Code Analysis
