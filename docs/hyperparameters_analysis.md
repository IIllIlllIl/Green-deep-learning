# æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒè¶…å‚æ•°åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¥æœŸï¼š** 2025-11-03
**é¡¹ç›®ï¼š** 6ä¸ªä»“åº“11ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¶…å‚æ•°å˜å¼‚æµ‹è¯•åˆ†æ
**ç›®çš„ï¼š** ä¸ºå˜å¼‚æµ‹è¯•æä¾›è¶…å‚æ•°ä¿®æ”¹æ–¹æ¡ˆï¼Œä»¥ç ”ç©¶è¶…å‚æ•°å¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„å½±å“

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [æ¨¡å‹åˆ—è¡¨](#æ¨¡å‹åˆ—è¡¨)
3. [å„æ¨¡å‹è¶…å‚æ•°è¯¦ç»†åˆ†æ](#å„æ¨¡å‹è¶…å‚æ•°è¯¦ç»†åˆ†æ)
4. [å¤šæ¨¡å‹å…±æœ‰è¶…å‚æ•°](#å¤šæ¨¡å‹å…±æœ‰è¶…å‚æ•°)
5. [ä»£ç çº§å¯å˜è¶…å‚æ•°](#ä»£ç çº§å¯å˜è¶…å‚æ•°)
6. [å˜å¼‚æµ‹è¯•å®æ–½å»ºè®®](#å˜å¼‚æµ‹è¯•å®æ–½å»ºè®®)
7. [é™„å½•ï¼šè¶…å‚æ•°é€ŸæŸ¥è¡¨](#é™„å½•è¶…å‚æ•°é€ŸæŸ¥è¡¨)

---

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®åŒ…å«6ä¸ªä»“åº“ï¼Œå…±è®¡11ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ¶µç›–ä»£ç å…‹éš†æ£€æµ‹ã€æ¼æ´æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€è¡Œäººé‡è¯†åˆ«ã€ç¼ºé™·å®šä½ç­‰å¤šä¸ªé¢†åŸŸã€‚

### ä»“åº“ä¸æ¨¡å‹å¯¹åº”å…³ç³»

| ä»“åº“å | æ¨¡å‹æ•°é‡ | æ¨¡å‹åç§° | é¢†åŸŸ |
|--------|---------|---------|------|
| MRT-OAST | 1 | MRT-OAST | ä»£ç å…‹éš†æ£€æµ‹ |
| VulBERTa | 1 | VulBERTa (MLP/CNN) | æ¼æ´æ£€æµ‹ |
| pytorch_resnet_cifar10 | 1 | ResNet | å›¾åƒåˆ†ç±» |
| bug-localization-by-dnn-and-rvsm | 1 | DNN | ç¼ºé™·å®šä½ |
| Person_reID_baseline_pytorch | 3 | DenseNet121, HRNet18, PCB | è¡Œäººé‡è¯†åˆ« |
| examples | 4 | MNIST(CNN), MNIST RNN, MNIST Forward-Forward, Siamese Network | åŸºç¡€ç¤ºä¾‹ |

### è®­ç»ƒå‘½ä»¤æ ¼å¼

```bash
# å•æ¨¡å‹ä»“åº“
./train.sh 2>&1 | tee training.log

# å¤šæ¨¡å‹ä»“åº“
./train.sh -n model_name 2>&1 | tee training.log
```

---

## æ¨¡å‹åˆ—è¡¨

å…±è®¡11ä¸ªæ¨¡å‹ï¼š

1. **MRT-OAST** - åŸºäºTransformerçš„ä»£ç å…‹éš†æ£€æµ‹æ¨¡å‹
2. **VulBERTa** - åŸºäºBERTçš„æ¼æ´æ£€æµ‹æ¨¡å‹
3. **ResNet-CIFAR10** - ResNetåœ¨CIFAR-10ä¸Šçš„å®ç°
4. **DNN (Bug Localization)** - ç”¨äºç¼ºé™·å®šä½çš„æ·±åº¦ç¥ç»ç½‘ç»œ
5. **DenseNet121 (Person ReID)** - è¡Œäººé‡è¯†åˆ«DenseNetå˜ä½“
6. **HRNet18 (Person ReID)** - è¡Œäººé‡è¯†åˆ«HRNetå˜ä½“
7. **PCB (Person ReID)** - åŸºäºå±€éƒ¨ç‰¹å¾çš„è¡Œäººé‡è¯†åˆ«
8. **MNIST CNN** - MNISTæ•°æ®é›†çš„å·ç§¯ç¥ç»ç½‘ç»œ
9. **MNIST RNN** - MNISTæ•°æ®é›†çš„å¾ªç¯ç¥ç»ç½‘ç»œ
10. **MNIST Forward-Forward** - Forward-Forwardç®—æ³•å®ç°
11. **Siamese Network** - å­ªç”Ÿç½‘ç»œç¤ºä¾‹

---

## å„æ¨¡å‹è¶…å‚æ•°è¯¦ç»†åˆ†æ

### 1. MRT-OAST (ä»£ç å…‹éš†æ£€æµ‹)

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh [OPTIONS]`

**å¯é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹çš„è¶…å‚æ•°ï¼š**

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| è®­ç»ƒè½®æ•° | `--epochs` | 10 | int | è®­ç»ƒepochæ•° |
| æ‰¹æ¬¡å¤§å° | `--batch-size` | 64 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| å­¦ä¹ ç‡ | `--lr` | 0.0001 | float | åˆå§‹å­¦ä¹ ç‡ |
| Dropoutç‡ | `--dropout` | 0.2 | float | Dropoutæ¦‚ç‡ |
| éšæœºç§å­ | `--seed` | 1334 | int | éšæœºæ•°ç§å­ |
| éªŒè¯æ­¥æ•° | `--valid-step` | 1750 | int | éªŒè¯é¢‘ç‡ï¼ˆ0è¡¨ç¤ºæ¯epochéªŒè¯ï¼‰|
| æœ€å¤§åºåˆ—é•¿åº¦ | `--max-len` | 256 | int | è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦ |
| Transformerå±‚æ•° | `--layers` | 2 | int | Transformerç¼–ç å™¨å±‚æ•° |
| æ¨¡å‹ç»´åº¦ | `--d-model` | 128 | int | æ¨¡å‹éšè—å±‚ç»´åº¦ |
| å‰é¦ˆç½‘ç»œç»´åº¦ | `--d-ff` | 512 | int | FFNä¸­é—´å±‚ç»´åº¦ |
| æ³¨æ„åŠ›å¤´æ•° | `--heads` | 8 | int | å¤šå¤´æ³¨æ„åŠ›å¤´æ•° |
| è¾“å‡ºç»´åº¦ | `--output-dim` | 512 | int | æœ€ç»ˆè¾“å‡ºç»´åº¦ |
| æµ‹è¯•é˜ˆå€¼ | `--threshold` | 0.9 | float | æµ‹è¯•æ—¶ç›¸ä¼¼åº¦é˜ˆå€¼ |
| éªŒè¯é˜ˆå€¼ | `--valid-threshold` | 0.8 | float | éªŒè¯æ—¶ç›¸ä¼¼åº¦é˜ˆå€¼ |

**ä»£ç ä¸­å®šä¹‰ä½†æœªæš´éœ²çš„è¶…å‚æ•°ï¼š**
- `gamma` (é»˜è®¤: 0.5) - å­¦ä¹ ç‡è¡°å‡ç³»æ•°

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°
./train.sh

# è‡ªå®šä¹‰å‚æ•°
./train.sh --epochs 20 --batch-size 32 --lr 0.0005 --layers 4
```

---

### 2. VulBERTa (æ¼æ´æ£€æµ‹)

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh [OPTIONS]`

**å¯é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹çš„è¶…å‚æ•°ï¼š**

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | MLPé»˜è®¤å€¼ | CNNé»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|----------|----------|------|------|
| æ‰¹æ¬¡å¤§å° | `--batch_size` | 4 | 128 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| è®­ç»ƒè½®æ•° | `--epochs` | 10 | 20 | int | è®­ç»ƒepochæ•° |
| å­¦ä¹ ç‡ | `--learning_rate` | 3e-05 | 0.0005 | float | åˆå§‹å­¦ä¹ ç‡ |
| éšæœºç§å­ | `--seed` | 42 | 1234 | int | éšæœºæ•°ç§å­ |
| æ··åˆç²¾åº¦ | `--fp16` | False | False | bool | ä½¿ç”¨FP16è®­ç»ƒ |

**æ³¨æ„äº‹é¡¹ï¼š**
- VulBERTaæ”¯æŒä¸¤ç§æ¨¡å‹æ¶æ„ï¼šMLPå’ŒCNNï¼Œå®ƒä»¬ä½¿ç”¨ä¸åŒçš„é»˜è®¤è¶…å‚æ•°
- å¿…é¡»æŒ‡å®šæ¨¡å‹åç§°ï¼ˆ`-n mlp` æˆ– `-n cnn`ï¼‰å’Œæ•°æ®é›†ï¼ˆ`-d dataset_name`ï¼‰

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# è®­ç»ƒMLPæ¨¡å‹
./train.sh -n mlp -d devign --batch_size 2 --epochs 5

# è®­ç»ƒCNNæ¨¡å‹
./train.sh -n cnn -d devign --batch_size 64 --epochs 10
```

---

### 3. pytorch_resnet_cifar10 (å›¾åƒåˆ†ç±»)

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh [OPTIONS]`

**å¯é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹çš„è¶…å‚æ•°ï¼š**

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| æ¨¡å‹æ¶æ„ | `-n, --name` | resnet20 | str | ResNetå˜ä½“ï¼ˆ20/32/44/56/110/1202ï¼‰|
| è®­ç»ƒè½®æ•° | `-e, --epochs` | 200 | int | è®­ç»ƒepochæ•° |
| æ‰¹æ¬¡å¤§å° | `-b, --batch-size` | 128 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| å­¦ä¹ ç‡ | `--lr` | 0.1 | float | åˆå§‹å­¦ä¹ ç‡ |
| SGDåŠ¨é‡ | `--momentum` | 0.9 | float | SGDä¼˜åŒ–å™¨åŠ¨é‡ |
| æƒé‡è¡°å‡ | `--wd` | 0.0001 | float | L2æ­£åˆ™åŒ–ç³»æ•° |
| æ•°æ®åŠ è½½çº¿ç¨‹ | `-j, --workers` | 4 | int | æ•°æ®åŠ è½½çš„workeræ•° |
| æ‰“å°é¢‘ç‡ | `--print-freq` | 50 | int | æ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ‰¹æ¬¡ï¼‰|
| ä¿å­˜é¢‘ç‡ | `--save-every` | 10 | int | æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆepochsï¼‰|
| åŠç²¾åº¦è®­ç»ƒ | `--half` | False | bool | ä½¿ç”¨FP16è®­ç»ƒ |

**å¯é€‰çš„ResNetæ¶æ„ï¼š**
- resnet20 (20å±‚)
- resnet32 (32å±‚)
- resnet44 (44å±‚)
- resnet56 (56å±‚)
- resnet110 (110å±‚)
- resnet1202 (1202å±‚ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# è®­ç»ƒResNet20
./train.sh -n resnet20

# è®­ç»ƒResNet56ï¼Œè‡ªå®šä¹‰å‚æ•°
./train.sh -n resnet56 -e 100 -b 64 --lr 0.05

# ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒResNet1202
./train.sh -n resnet1202 -b 32 --half
```

---

### 4. bug-localization-by-dnn-and-rvsm (ç¼ºé™·å®šä½)

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh -n dnn [OPTIONS]`

**å¯é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹çš„è¶…å‚æ•°ï¼ˆä»…DNNæ¨¡å‹ï¼‰ï¼š**

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| KæŠ˜äº¤å‰éªŒè¯ | `--kfold` | 10 | int | äº¤å‰éªŒè¯æŠ˜æ•° |
| éšè—å±‚å¤§å° | `--hidden_sizes` | 300 | int/list | éšè—å±‚ç¥ç»å…ƒæ•°ï¼ˆå¯å¤šä¸ªï¼‰|
| L2æ­£åˆ™åŒ– | `--alpha` | 1e-5 | float | L2æƒ©ç½šå‚æ•° |
| æœ€å¤§è¿­ä»£æ¬¡æ•° | `--max_iter` | 10000 | int | æœ€å¤§è®­ç»ƒè¿­ä»£æ•° |
| æ—©åœpatience | `--n_iter_no_change` | 30 | int | æ— æ”¹è¿›æ—¶çš„å®¹å¿æ¬¡æ•° |
| ä¼˜åŒ–å™¨ | `--solver` | sgd | str | ä¼˜åŒ–å™¨ç±»å‹ï¼ˆsgd/adam/lbfgsï¼‰|
| å¹¶è¡Œä½œä¸šæ•° | `--n_jobs` | -2 | int | å¹¶è¡Œè®­ç»ƒçš„ä½œä¸šæ•° |

**æ³¨æ„äº‹é¡¹ï¼š**
- æ­¤æ¨¡å‹ä½¿ç”¨`max_iter`è€Œé`epochs`ä½œä¸ºè®­ç»ƒé•¿åº¦æ§åˆ¶
- `hidden_sizes`å¯ä»¥æŒ‡å®šå¤šä¸ªå€¼æ¥åˆ›å»ºå¤šå±‚ç½‘ç»œï¼Œä¾‹å¦‚ï¼š`--hidden_sizes 300 200`

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# é»˜è®¤é…ç½®è®­ç»ƒDNN
./train.sh -n dnn

# è‡ªå®šä¹‰é…ç½®
./train.sh -n dnn --hidden_sizes 200 --kfold 5 --solver adam
```

---

### 5. Person_reID_baseline_pytorch (è¡Œäººé‡è¯†åˆ«)

æ­¤ä»“åº“åŒ…å«3ä¸ªæ¨¡å‹ï¼š**DenseNet121**, **HRNet18**, **PCB**

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh -n model_name [OPTIONS]`

#### é€šç”¨è¶…å‚æ•°ï¼ˆ3ä¸ªæ¨¡å‹å…±äº«ï¼‰

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼èŒƒå›´ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|-----------|------|------|
| æ‰¹æ¬¡å¤§å° | `--batchsize` | 24-32 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| å­¦ä¹ ç‡ | `--lr` | 0.02-0.05 | float | åˆå§‹å­¦ä¹ ç‡ |
| è®­ç»ƒè½®æ•° | `--total_epoch` | 60 | int | æ€»è®­ç»ƒepochæ•° |
| é¢„çƒ­è½®æ•° | `--warm_epoch` | 0 | int | å­¦ä¹ ç‡é¢„çƒ­çš„epochæ•° |
| ResNetæ­¥é•¿ | `--stride` | 2 | int | ResNetæœ€åå·ç§¯å±‚æ­¥é•¿ |
| éšæœºæ“¦é™¤æ¦‚ç‡ | `--erasing_p` | 0 | float | Random Erasingæ¦‚ç‡[0,1] |
| Dropoutç‡ | `--droprate` | 0.5 | float | Dropoutæ¦‚ç‡ |
| çº¿æ€§ç‰¹å¾ç»´åº¦ | `--linear_num` | 512 | int | å…¨è¿æ¥å±‚ç‰¹å¾ç»´åº¦ |
| æƒé‡è¡°å‡ | `--weight_decay` | 5e-4 | float | L2æ­£åˆ™åŒ–ç³»æ•° |

#### ç²¾åº¦é€‰é¡¹

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|-----------|--------|------|
| FP16 | `--fp16` | False | ä½¿ç”¨float16ç²¾åº¦ |
| BF16 | `--bf16` | False | ä½¿ç”¨bfloat16ç²¾åº¦ |

#### æŸå¤±å‡½æ•°é€‰é¡¹

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|-----------|--------|------|
| Circle Loss | `--circle` | False | ä½¿ç”¨CircleæŸå¤± |
| Triplet Loss | `--triplet` | False | ä½¿ç”¨TripletæŸå¤± |
| Contrastive Loss | `--contrast` | False | ä½¿ç”¨å¯¹æ¯”æŸå¤± |
| ArcFace Loss | `--arcface` | False | ä½¿ç”¨ArcFaceæŸå¤± |
| CosFace Loss | `--cosface` | False | ä½¿ç”¨CosFaceæŸå¤± |

#### å­¦ä¹ ç‡è°ƒåº¦

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|-----------|--------|------|
| ä½™å¼¦è°ƒåº¦ | `--cosine` | False | ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡ |

#### å„æ¨¡å‹ç‰¹å®šé»˜è®¤å€¼

| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | å­¦ä¹ ç‡ | ç‰¹æ®Šé…ç½® |
|------|---------|--------|---------|
| **DenseNet121** | 24 | 0.05 | `--use_dense` |
| **HRNet18** | 24 | 0.05 | `--use_hr` |
| **PCB** | 32 | 0.02 | `--PCB` |

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# è®­ç»ƒDenseNet121ï¼ˆé»˜è®¤é…ç½®ï¼‰
./train.sh -n densenet121

# è®­ç»ƒHRNet18ï¼Œä½¿ç”¨Circle Loss
./train.sh -n hrnet18 --circle --warm_epoch 5

# è®­ç»ƒPCBï¼Œè‡ªå®šä¹‰å‚æ•°
./train.sh -n pcb --batchsize 16 --lr 0.01 --total_epoch 40
```

---

### 6. examples (PyTorchç¤ºä¾‹æ¨¡å‹)

æ­¤ä»“åº“åŒ…å«4ä¸ªç¤ºä¾‹æ¨¡å‹ï¼š**MNIST CNN**, **MNIST RNN**, **MNIST Forward-Forward**, **Siamese Network**

**è®­ç»ƒå‘½ä»¤ï¼š** `./train.sh -n model_name [OPTIONS]`

#### 6.1 MNIST CNN

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| æ‰¹æ¬¡å¤§å° | `-b, --batch-size` | 32 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| æµ‹è¯•æ‰¹æ¬¡å¤§å° | `--test-batch-size` | 1000 | int | æµ‹è¯•æ‰¹æ¬¡å¤§å° |
| è®­ç»ƒè½®æ•° | `-e, --epochs` | 14 | int | è®­ç»ƒepochæ•° |
| å­¦ä¹ ç‡ | `-l, --lr` | 1.0 | float | åˆå§‹å­¦ä¹ ç‡ |
| Gamma | `--gamma` | 0.7 | float | å­¦ä¹ ç‡è¡°å‡ç³»æ•° |
| éšæœºç§å­ | `--seed` | 1 | int | éšæœºæ•°ç§å­ |
| æ—¥å¿—é—´éš” | `--log-interval` | 10 | int | æ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ‰¹æ¬¡ï¼‰|

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
./train.sh -n mnist -e 10 -b 64 -l 0.5
```

#### 6.2 MNIST RNN

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| æ‰¹æ¬¡å¤§å° | `-b, --batch-size` | 32 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| æµ‹è¯•æ‰¹æ¬¡å¤§å° | `--test-batch-size` | 1000 | int | æµ‹è¯•æ‰¹æ¬¡å¤§å° |
| è®­ç»ƒè½®æ•° | `-e, --epochs` | 14 | int | è®­ç»ƒepochæ•° |
| å­¦ä¹ ç‡ | `-l, --lr` | 0.1 | float | åˆå§‹å­¦ä¹ ç‡ |
| Gamma | `--gamma` | 0.7 | float | å­¦ä¹ ç‡è¡°å‡ç³»æ•° |
| éšæœºç§å­ | `--seed` | 1 | int | éšæœºæ•°ç§å­ |
| æ—¥å¿—é—´éš” | `--log-interval` | 10 | int | æ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ‰¹æ¬¡ï¼‰|

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
./train.sh -n mnist_rnn -e 10 -b 64 -l 0.05
```

#### 6.3 MNIST Forward-Forward

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| è®­ç»ƒè½®æ•° | `-e, --epochs` | 1000 | int | æ¯å±‚è®­ç»ƒçš„epochæ•° |
| å­¦ä¹ ç‡ | `-l, --lr` | 0.03 | float | å­¦ä¹ ç‡ |
| éšæœºç§å­ | `--seed` | 1 | int | éšæœºæ•°ç§å­ |
| è®­ç»ƒé›†å¤§å° | `--train-size` | 50000 | int | è®­ç»ƒæ ·æœ¬æ•° |
| æµ‹è¯•é›†å¤§å° | `--test-size` | 10000 | int | æµ‹è¯•æ ·æœ¬æ•° |
| é˜ˆå€¼ | `--threshold` | 2 | float | Forward-Forwardè®­ç»ƒé˜ˆå€¼ |
| æ—¥å¿—é—´éš” | `--log-interval` | 10 | int | æ—¥å¿—æ‰“å°é—´éš” |

**æ³¨æ„ï¼š** æ­¤æ¨¡å‹ä½¿ç”¨`train_size`å’Œ`test_size`è€Œéä¼ ç»Ÿçš„`batch_size`

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
./train.sh -n mnist_ff -e 500 -l 0.01 --threshold 1.5
```

#### 6.4 Siamese Network

| å‚æ•°å | å‘½ä»¤è¡Œé€‰é¡¹ | é»˜è®¤å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|-----------|--------|------|------|
| æ‰¹æ¬¡å¤§å° | `-b, --batch-size` | 32 | int | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| æµ‹è¯•æ‰¹æ¬¡å¤§å° | `--test-batch-size` | 1000 | int | æµ‹è¯•æ‰¹æ¬¡å¤§å° |
| è®­ç»ƒè½®æ•° | `-e, --epochs` | 14 | int | è®­ç»ƒepochæ•° |
| å­¦ä¹ ç‡ | `-l, --lr` | 1.0 | float | åˆå§‹å­¦ä¹ ç‡ |
| Gamma | `--gamma` | 0.7 | float | å­¦ä¹ ç‡è¡°å‡ç³»æ•° |
| éšæœºç§å­ | `--seed` | 1 | int | éšæœºæ•°ç§å­ |
| æ—¥å¿—é—´éš” | `--log-interval` | 10 | int | æ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ‰¹æ¬¡ï¼‰|

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
./train.sh -n siamese -e 10 -b 64 -l 0.5
```

---

## å¤šæ¨¡å‹å…±æœ‰è¶…å‚æ•°

æ­¤èŠ‚åˆ—å‡ºå¯ä»¥åœ¨å¤šä¸ªæ¨¡å‹ä¸­åŒæ—¶ä¿®æ”¹çš„è¶…å‚æ•°ï¼Œé€‚åˆè¿›è¡Œæ¨ªå‘å¯¹æ¯”çš„å˜å¼‚æµ‹è¯•ã€‚

### 1. æ ¸å¿ƒè®­ç»ƒè¶…å‚æ•°

#### 1.1 epochs (è®­ç»ƒè½®æ•°)

**è¦†ç›–èŒƒå›´ï¼š** 10/11ä¸ªæ¨¡å‹ï¼ˆé™¤bug-localizationï¼‰

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `--epochs` | 10 |
| VulBERTa | `--epochs` | 10-20 |
| pytorch_resnet_cifar10 | `--epochs` | 200 |
| Person_reID (3ä¸ª) | `--total_epoch` | 60 |
| examples (4ä¸ª) | `--epochs` | 14-1000 |

**å˜å¼‚å»ºè®®ï¼š**
- Ã—0.5: å¿«é€Ÿæµ‹è¯•ï¼ˆ5, 10, 100ï¼‰
- Ã—2: é•¿æ—¶é—´è®­ç»ƒï¼ˆ20, 40, 400ï¼‰
- Â±20%: å¾®è°ƒï¼ˆ8, 12, 48, 72ï¼‰

#### 1.2 batch_size (æ‰¹æ¬¡å¤§å°)

**è¦†ç›–èŒƒå›´ï¼š** 10/11ä¸ªæ¨¡å‹ï¼ˆé™¤bug-localizationï¼‰

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `--batch-size` | 64 |
| VulBERTa | `--batch_size` | 4-128 |
| pytorch_resnet_cifar10 | `--batch-size` | 128 |
| Person_reID (3ä¸ª) | `--batchsize` | 24-32 |
| examples (4ä¸ª) | `--batch-size` | 32-64 |

**å˜å¼‚å»ºè®®ï¼š**
- æ ‡å‡†å€¼ï¼š16, 32, 64, 128, 256
- æ³¨æ„ï¼šéœ€æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´

#### 1.3 learning_rate (å­¦ä¹ ç‡)

**è¦†ç›–èŒƒå›´ï¼š** 10/11ä¸ªæ¨¡å‹ï¼ˆé™¤bug-localizationï¼‰

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `--lr` | 0.0001 |
| VulBERTa | `--learning_rate` | 0.00003-0.0005 |
| pytorch_resnet_cifar10 | `--lr` | 0.1 |
| Person_reID (3ä¸ª) | `--lr` | 0.02-0.05 |
| examples (4ä¸ª) | `--lr` | 0.03-1.0 |

**å˜å¼‚å»ºè®®ï¼š**
- Ã—10: 0.001, 0.001, 1.0
- Ã—0.1: 0.00001, 0.00003, 0.01
- Ã—2: 0.0002, 0.0001, 0.2
- Ã—0.5: 0.00005, 0.00015, 0.05

#### 1.4 seed (éšæœºç§å­)

**è¦†ç›–èŒƒå›´ï¼š** 6/11ä¸ªæ¨¡å‹

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `--seed` | 1334 |
| VulBERTa | `--seed` | 42-1234 |
| examples (4ä¸ª) | `--seed` | 1 |

**å˜å¼‚å»ºè®®ï¼š**
- å¸¸ç”¨ç§å­ï¼š1, 42, 123, 1234, 2024, 9999

### 2. æ­£åˆ™åŒ–è¶…å‚æ•°

#### 2.1 dropout (Dropoutç‡)

**è¦†ç›–èŒƒå›´ï¼š** 4/11ä¸ªæ¨¡å‹

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `--dropout` | 0.2 |
| Person_reID (3ä¸ª) | `--droprate` | 0.5 |

**å˜å¼‚å»ºè®®ï¼š**
- 0.0 (æ— Dropout)
- 0.1, 0.2, 0.3, 0.5, 0.7

#### 2.2 weight_decay (æƒé‡è¡°å‡/L2æ­£åˆ™åŒ–)

**è¦†ç›–èŒƒå›´ï¼š** 4/11ä¸ªæ¨¡å‹

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| pytorch_resnet_cifar10 | `--weight-decay` | 0.0001 |
| bug-localization | `--alpha` | 0.00001 |
| Person_reID (3ä¸ª) | `--weight_decay` | 0.0005 |

**å˜å¼‚å»ºè®®ï¼š**
- 0.0001, 0.0005, 0.001, 0.00001, 0.000001

### 3. å­¦ä¹ ç‡è°ƒåº¦è¶…å‚æ•°

#### 3.1 gamma (å­¦ä¹ ç‡è¡°å‡ç³»æ•°)

**è¦†ç›–èŒƒå›´ï¼š** 4/11ä¸ªæ¨¡å‹

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| examples (MNIST, RNN, Siamese) | `--gamma` | 0.7 |

**å˜å¼‚å»ºè®®ï¼š**
- 0.5, 0.7, 0.9, 0.95

### 4. æ··åˆç²¾åº¦è®­ç»ƒ

#### 4.1 fp16/half/bf16

**è¦†ç›–èŒƒå›´ï¼š** 4/11ä¸ªæ¨¡å‹

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| VulBERTa | `--fp16` | False |
| pytorch_resnet_cifar10 | `--half` | False |
| Person_reID (3ä¸ª) | `--fp16, --bf16` | False |

**å˜å¼‚å»ºè®®ï¼š**
- å¼€å¯/å…³é—­æ··åˆç²¾åº¦ï¼Œè§‚å¯Ÿé€Ÿåº¦å’Œç²¾åº¦å˜åŒ–

---

## ä»£ç çº§å¯å˜è¶…å‚æ•°

å¦‚æœå…è®¸ä¿®æ”¹è®­ç»ƒä»£ç ï¼Œä»¥ä¸‹è¶…å‚æ•°å¯ä»¥åœ¨æ‰€æœ‰æˆ–å¤§éƒ¨åˆ†æ¨¡å‹ä¸­ç»Ÿä¸€è¿›è¡Œå˜å¼‚æµ‹è¯•ã€‚

### 1. ä¼˜åŒ–å™¨ç›¸å…³ï¼ˆ11/11æ¨¡å‹ï¼‰

#### 1.1 optimizer_type (ä¼˜åŒ–å™¨ç±»å‹)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**

| æ¨¡å‹ | å½“å‰ä¼˜åŒ–å™¨ | å¯æ›¿æ¢ä¸º |
|------|-----------|---------|
| MRT-OAST | Adam | SGD, AdamW, RMSprop |
| VulBERTa | AdamW | Adam, SGD |
| pytorch_resnet_cifar10 | SGD | Adam, AdamW |
| bug-localization | SGD/Adam/LBFGS | äº’ç›¸æ›¿æ¢ |
| Person_reID (3ä¸ª) | SGD | Adam, AdamW |
| examples (4ä¸ª) | Adadelta/Adam | SGD, AdamW |

**å®æ–½æ–¹æ³•ï¼š**
```python
# åŸä»£ç ç¤ºä¾‹
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

# ä¿®æ”¹ä¸º
if args.optimizer == 'sgd':
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)
elif args.optimizer == 'adam':
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
elif args.optimizer == 'adamw':
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
```

#### 1.2 momentum (SGDåŠ¨é‡)

**å½“å‰æš´éœ²æƒ…å†µï¼š**
- pytorch_resnet_cifar10: å·²æš´éœ²ï¼ˆ`--momentum`ï¼Œé»˜è®¤0.9ï¼‰
- å…¶ä»–æ¨¡å‹ï¼šéœ€è¦åœ¨ä»£ç ä¸­ä¿®æ”¹

**å˜å¼‚å»ºè®®ï¼š** 0.0, 0.5, 0.9, 0.95, 0.99

#### 1.3 beta1, beta2 (Adamä¼˜åŒ–å™¨å‚æ•°)

**é€‚ç”¨æ¨¡å‹ï¼š** æ‰€æœ‰ä½¿ç”¨Adam/AdamWçš„æ¨¡å‹ï¼ˆçº¦6ä¸ªï¼‰

**é»˜è®¤å€¼ï¼š** (0.9, 0.999)

**å˜å¼‚å»ºè®®ï¼š**
- beta1: 0.5, 0.9, 0.95
- beta2: 0.99, 0.999, 0.9999

### 2. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼ˆ11/11æ¨¡å‹ï¼‰

#### 2.1 lr_scheduler_type (è°ƒåº¦å™¨ç±»å‹)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**

| æ¨¡å‹ | å½“å‰è°ƒåº¦å™¨ |
|------|-----------|
| MRT-OAST | å›ºå®šå­¦ä¹ ç‡ |
| VulBERTa | çº¿æ€§è°ƒåº¦ |
| pytorch_resnet_cifar10 | MultiStepLR |
| Person_reID | StepLRï¼ˆå¯é€‰Cosineï¼‰|
| examples | StepLR |

**å¯æ›¿æ¢é€‰é¡¹ï¼š**
- StepLR: æ¯éš”å›ºå®šepoché™ä½å­¦ä¹ ç‡
- MultiStepLR: åœ¨æŒ‡å®šepoché™ä½å­¦ä¹ ç‡
- ExponentialLR: æŒ‡æ•°è¡°å‡
- CosineAnnealingLR: ä½™å¼¦é€€ç«
- ReduceLROnPlateau: åŸºäºéªŒè¯æŒ‡æ ‡è‡ªé€‚åº”è°ƒæ•´

**å®æ–½æ–¹æ³•ï¼š**
```python
if args.scheduler == 'step':
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
elif args.scheduler == 'cosine':
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
elif args.scheduler == 'multistep':
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)
```

#### 2.2 lr_scheduler_params (è°ƒåº¦å™¨å‚æ•°)

**StepLRå‚æ•°ï¼š**
- `step_size`: 10, 20, 30, 50
- `gamma`: 0.1, 0.2, 0.5

**MultiStepLRå‚æ•°ï¼š**
- `milestones`: [60, 120], [80, 150], [100, 200]
- `gamma`: 0.1, 0.2

**CosineAnnealingLRå‚æ•°ï¼š**
- `T_max`: ç­‰äºæ€»epochsæˆ–æ€»epochsçš„ä¸€åŠ

### 3. æ¢¯åº¦å¤„ç†ï¼ˆ11/11æ¨¡å‹ï¼‰

#### 3.1 gradient_clipping (æ¢¯åº¦è£å‰ª)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š** å¤§éƒ¨åˆ†æ¨¡å‹æœªä½¿ç”¨

**å®æ–½æ–¹æ³•ï¼š**
```python
# åœ¨optimizer.step()ä¹‹å‰æ·»åŠ 
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.clip_grad)
```

**å˜å¼‚å»ºè®®ï¼š**
- max_norm: 0.5, 1.0, 5.0, 10.0
- æˆ–ä¸ä½¿ç”¨æ¢¯åº¦è£å‰ª

### 4. è®­ç»ƒç­–ç•¥ï¼ˆ11/11æ¨¡å‹ï¼‰

#### 4.1 early_stopping_patience (æ—©åœ)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**
- bug-localization: å·²å®ç°ï¼ˆ`n_iter_no_change=30`ï¼‰
- å…¶ä»–æ¨¡å‹ï¼šå¤§éƒ¨åˆ†æœªå®ç°

**å®æ–½æ–¹æ³•ï¼š**
```python
best_loss = float('inf')
patience_counter = 0

for epoch in range(args.epochs):
    train_loss = train(...)
    val_loss = validate(...)

    if val_loss < best_loss:
        best_loss = val_loss
        patience_counter = 0
        save_checkpoint(...)
    else:
        patience_counter += 1
        if patience_counter >= args.patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

**å˜å¼‚å»ºè®®ï¼š** patience=5, 10, 20, 30

#### 4.2 warm_up_epochs (å­¦ä¹ ç‡é¢„çƒ­)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**
- Person_reID: å·²æš´éœ²ï¼ˆ`--warm_epoch`ï¼Œé»˜è®¤0ï¼‰
- å…¶ä»–æ¨¡å‹ï¼šéœ€è¦åœ¨ä»£ç ä¸­æ·»åŠ 

**å®æ–½æ–¹æ³•ï¼š**
```python
if epoch < args.warm_epochs:
    warmup_lr = args.lr * (epoch + 1) / args.warm_epochs
    for param_group in optimizer.param_groups:
        param_group['lr'] = warmup_lr
```

**å˜å¼‚å»ºè®®ï¼š** 0ï¼ˆæ— é¢„çƒ­ï¼‰ï¼Œ5, 10, 20 epochs

### 5. æ•°æ®å¢å¼ºï¼ˆé€‚ç”¨äºå›¾åƒæ¨¡å‹ï¼š8/11ï¼‰

#### 5.1 æ•°æ®å¢å¼ºç­–ç•¥

**é€‚ç”¨æ¨¡å‹ï¼š**
- pytorch_resnet_cifar10
- Person_reID (3ä¸ª)
- examples (MNISTç›¸å…³4ä¸ª)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**
- Person_reID: æ”¯æŒRandom Erasingï¼ˆ`--erasing_p`ï¼‰ï¼ŒColor Jitterï¼ˆ`--color_jitter`ï¼‰
- pytorch_resnet_cifar10: åŸºç¡€å¢å¼ºï¼ˆRandomCrop, RandomFlipï¼‰

**å¯æ·»åŠ çš„å¢å¼ºï¼š**
- RandomRotation
- RandomAffine
- ColorJitter
- RandomGrayscale
- GaussianBlur
- Cutout/Random Erasing
- MixUp
- CutMix

### 6. æ¨¡å‹æ¶æ„å‚æ•°

#### 6.1 hidden_size / hidden_units (éšè—å±‚å¤§å°)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `d_model` | 128 |
| bug-localization | `hidden_sizes` | 300 |
| Person_reID | `linear_num` | 512 |
| examples-FF | ç¡¬ç¼–ç  | [784, 500, 500] |

**å˜å¼‚å»ºè®®ï¼š**
- å‡åŠï¼š64, 150, 256, [784, 250, 250]
- åŠ å€ï¼š256, 600, 1024, [784, 1000, 1000]

#### 6.2 num_layers (ç½‘ç»œå±‚æ•°)

**å½“å‰ä½¿ç”¨æƒ…å†µï¼š**

| æ¨¡å‹ | å‚æ•°å | é»˜è®¤å€¼ |
|------|--------|--------|
| MRT-OAST | `transformer_nlayers` | 2 |
| pytorch_resnet_cifar10 | é€šè¿‡arché€‰æ‹© | 20-1202 |
| examples-FF | ç¡¬ç¼–ç  | 2 |

**å˜å¼‚å»ºè®®ï¼š**
- Transformer: 1, 2, 4, 6å±‚
- ResNet: 20, 32, 56, 110å±‚
- MLP: 2, 3, 4å±‚

---

## å˜å¼‚æµ‹è¯•å®æ–½å»ºè®®

### 1. ä¸‰å±‚å˜å¼‚æµ‹è¯•æ–¹æ¡ˆ

#### ç¬¬ä¸€å±‚ï¼šå‘½ä»¤è¡Œå‚æ•°å˜å¼‚ï¼ˆæ— éœ€æ”¹ä»£ç ï¼‰

**æ¨èä¼˜å…ˆçº§ï¼šğŸ”¥ æœ€é«˜**

ä¸“æ³¨äºå¯ä»¥é€šè¿‡ä¿®æ”¹train.shè°ƒç”¨å‚æ•°å®ç°çš„å˜å¼‚ï¼š

| è¶…å‚æ•° | è¦†ç›–æ¨¡å‹ | å˜å¼‚æ–¹æ¡ˆ | é¢„æœŸå½±å“ |
|--------|---------|---------|---------|
| **epochs** | 10/11 | Ã—0.5, Ã—2 | è®­ç»ƒæ—¶é—´ã€æ”¶æ•›æ€§ |
| **batch_size** | 10/11 | 16, 32, 64, 128 | å†…å­˜å ç”¨ã€æ”¶æ•›é€Ÿåº¦ |
| **learning_rate** | 10/11 | Ã—0.1, Ã—0.5, Ã—2, Ã—10 | æ”¶æ•›é€Ÿåº¦ã€æœ€ç»ˆç²¾åº¦ |
| **seed** | 6/11 | 1, 42, 123, 1234 | ç»“æœå¯é‡å¤æ€§ |

**å®æ–½æ­¥éª¤ï¼š**

1. åˆ›å»ºå˜å¼‚å‚æ•°é…ç½®æ–‡ä»¶ï¼š
```bash
# mutation_configs.txt
epochs: 5, 10, 20, 40
batch_size: 16, 32, 64, 128
lr_multiplier: 0.1, 0.5, 1.0, 2.0, 10.0
seed: 1, 42, 123, 1234
```

2. ç¼–å†™æ‰¹é‡æµ‹è¯•è„šæœ¬ï¼š
```bash
#!/bin/bash
# batch_mutation_test.sh

for epochs in 5 10 20; do
    for batch in 16 32 64; do
        for lr_mult in 0.1 0.5 2.0; do
            lr=$(echo "scale=6; $DEFAULT_LR * $lr_mult" | bc)
            echo "Testing: epochs=$epochs, batch=$batch, lr=$lr"
            ./train.sh --epochs $epochs --batch-size $batch --lr $lr \
                2>&1 | tee "logs/mutation_e${epochs}_b${batch}_lr${lr}.log"
        done
    done
done
```

#### ç¬¬äºŒå±‚ï¼šä»£ç çº§å¸¸è§å‚æ•°å˜å¼‚ï¼ˆç®€å•ä¿®æ”¹ï¼‰

**æ¨èä¼˜å…ˆçº§ï¼šğŸ”¥ é«˜**

ä¿®æ”¹ä»£ç æ·»åŠ å¸¸ç”¨è¶…å‚æ•°ï¼š

**éœ€è¦ä¿®æ”¹çš„å†…å®¹ï¼š**

1. **æ·»åŠ ä¼˜åŒ–å™¨é€‰æ‹©ï¼š**
```python
# åœ¨argparseä¸­æ·»åŠ 
parser.add_argument('--optimizer', type=str, default='adam',
                   choices=['sgd', 'adam', 'adamw', 'rmsprop'])

# åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ 
if args.optimizer == 'sgd':
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,
                                momentum=args.momentum, weight_decay=args.weight_decay)
elif args.optimizer == 'adam':
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,
                                 weight_decay=args.weight_decay)
elif args.optimizer == 'adamw':
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,
                                  weight_decay=args.weight_decay)
```

2. **æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨é€‰æ‹©ï¼š**
```python
# åœ¨argparseä¸­æ·»åŠ 
parser.add_argument('--scheduler', type=str, default='step',
                   choices=['none', 'step', 'cosine', 'multistep'])

# åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ 
if args.scheduler == 'step':
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
elif args.scheduler == 'cosine':
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
```

3. **æ·»åŠ æ¢¯åº¦è£å‰ªï¼š**
```python
# åœ¨argparseä¸­æ·»åŠ 
parser.add_argument('--clip_grad', type=float, default=0.0,
                   help='gradient clipping max norm (0 means no clipping)')

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼ˆoptimizer.step()ä¹‹å‰ï¼‰
if args.clip_grad > 0:
    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)
```

#### ç¬¬ä¸‰å±‚ï¼šæ·±åº¦æ¶æ„å‚æ•°å˜å¼‚ï¼ˆå¤æ‚ä¿®æ”¹ï¼‰

**æ¨èä¼˜å…ˆçº§ï¼šä¸­**

ä¿®æ”¹æ¨¡å‹æ¶æ„ç›¸å…³å‚æ•°ï¼š

- éšè—å±‚å¤§å°
- ç½‘ç»œå±‚æ•°
- å·ç§¯æ ¸å¤§å°
- æ³¨æ„åŠ›å¤´æ•°

**å®æ–½éš¾åº¦ï¼š** éœ€è¦å¯¹æ¨¡å‹æ¶æ„æœ‰æ·±å…¥ç†è§£

### 2. åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

#### Phase 1: å¿«é€Ÿæ¢ç´¢ï¼ˆ1-2å‘¨ï¼‰

**ç›®æ ‡ï¼š** éªŒè¯å˜å¼‚æµ‹è¯•æ¡†æ¶å¯è¡Œæ€§

**èŒƒå›´ï¼š** ä»…æµ‹è¯•3ä¸ªæœ€å¸¸ç”¨è¶…å‚æ•°
- epochs: Ã—0.5, Ã—2
- batch_size: Ã—0.5, Ã—2
- learning_rate: Ã—0.1, Ã—10

**é¢„æœŸå®éªŒæ•°é‡ï¼š**
- 11ä¸ªæ¨¡å‹ Ã— 3ä¸ªå‚æ•° Ã— 2ä¸ªå˜å¼‚å€¼ = 66æ¬¡å®éªŒ

**å®æ–½æ–¹å¼ï¼š** æ‰‹åŠ¨ä¿®æ”¹train.shå‚æ•°

#### Phase 2: ç³»ç»ŸåŒ–æµ‹è¯•ï¼ˆ2-4å‘¨ï¼‰

**ç›®æ ‡ï¼š** å…¨é¢æµ‹è¯•å‘½ä»¤è¡Œå¯ä¿®æ”¹å‚æ•°

**èŒƒå›´ï¼š**
- epochs: 3-5ä¸ªä¸åŒå€¼
- batch_size: 4-5ä¸ªä¸åŒå€¼
- learning_rate: 5ä¸ªä¸åŒå€¼
- seed: 4ä¸ªä¸åŒå€¼
- dropout: 3ä¸ªä¸åŒå€¼ï¼ˆé€‚ç”¨æ¨¡å‹ï¼‰

**é¢„æœŸå®éªŒæ•°é‡ï¼š** çº¦200-300æ¬¡

**å®æ–½æ–¹å¼ï¼š** ç¼–å†™è‡ªåŠ¨åŒ–æ‰¹é‡æµ‹è¯•è„šæœ¬

#### Phase 3: æ·±åº¦æ¢ç´¢ï¼ˆ4-8å‘¨ï¼‰

**ç›®æ ‡ï¼š** æµ‹è¯•ä»£ç çº§å¯å˜å‚æ•°

**èŒƒå›´ï¼š**
- ä¼˜åŒ–å™¨ç±»å‹ï¼š4ç§
- å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š4ç§
- æ¢¯åº¦è£å‰ªï¼š3ä¸ªå€¼
- æ—©åœç­–ç•¥ï¼š3ä¸ªå€¼
- æ•°æ®å¢å¼ºï¼šå¤šç§ç»„åˆ

**é¢„æœŸå®éªŒæ•°é‡ï¼š** çº¦500+æ¬¡

**å®æ–½æ–¹å¼ï¼š** ä¿®æ”¹ä»£ç ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å‚æ•°

### 3. å˜å¼‚æµ‹è¯•è‡ªåŠ¨åŒ–æ¡†æ¶

#### 3.1 é…ç½®æ–‡ä»¶é©±åŠ¨æ–¹æ¡ˆ

**åˆ›å»ºç»Ÿä¸€é…ç½®æ ¼å¼ï¼ˆYAMLï¼‰ï¼š**

```yaml
# config/mutation_test_001.yaml
model: MRT-OAST
hyperparameters:
  epochs: 20
  batch_size: 32
  lr: 0.0002
  dropout: 0.3
  seed: 42

# config/mutation_test_002.yaml
model: VulBERTa
hyperparameters:
  model_name: mlp
  epochs: 15
  batch_size: 8
  learning_rate: 0.00006
  seed: 123
```

**åˆ›å»ºé…ç½®æ–‡ä»¶ç”Ÿæˆå™¨ï¼š**

```python
# generate_configs.py
import yaml
import itertools

base_config = {
    'model': 'MRT-OAST',
    'hyperparameters': {
        'epochs': 10,
        'batch_size': 64,
        'lr': 0.0001,
        'seed': 1334
    }
}

mutations = {
    'epochs': [5, 10, 20],
    'batch_size': [32, 64, 128],
    'lr': [0.00001, 0.0001, 0.001]
}

# ç”Ÿæˆæ‰€æœ‰ç»„åˆ
configs = []
for epochs, batch, lr in itertools.product(
    mutations['epochs'],
    mutations['batch_size'],
    mutations['lr']
):
    config = base_config.copy()
    config['hyperparameters'] = {
        'epochs': epochs,
        'batch_size': batch,
        'lr': lr,
        'seed': base_config['hyperparameters']['seed']
    }
    configs.append(config)

# ä¿å­˜é…ç½®æ–‡ä»¶
for i, config in enumerate(configs):
    with open(f'config/mutation_{i:03d}.yaml', 'w') as f:
        yaml.dump(config, f)
```

#### 3.2 æ‰¹é‡æ‰§è¡Œæ¡†æ¶

```bash
#!/bin/bash
# run_mutation_tests.sh

CONFIG_DIR="config"
LOG_DIR="mutation_logs"
mkdir -p "$LOG_DIR"

for config_file in "$CONFIG_DIR"/mutation_*.yaml; do
    config_name=$(basename "$config_file" .yaml)
    log_file="$LOG_DIR/${config_name}.log"

    echo "Running test: $config_name"

    # ä»YAMLè¯»å–å‚æ•°å¹¶æ‰§è¡Œè®­ç»ƒ
    python run_from_config.py --config "$config_file" \
        2>&1 | tee "$log_file"

    # è®°å½•é€€å‡ºçŠ¶æ€
    if [ $? -eq 0 ]; then
        echo "SUCCESS: $config_name" >> "$LOG_DIR/summary.txt"
    else
        echo "FAILED: $config_name" >> "$LOG_DIR/summary.txt"
    fi
done
```

#### 3.3 ç»“æœæ”¶é›†ä¸åˆ†æ

```python
# analyze_results.py
import os
import re
import pandas as pd

def extract_metrics(log_file):
    """ä»æ—¥å¿—æ–‡ä»¶æå–æ€§èƒ½æŒ‡æ ‡"""
    with open(log_file, 'r') as f:
        content = f.read()

    metrics = {}

    # æå–å‡†ç¡®ç‡ï¼ˆæ ¹æ®ä¸åŒæ¨¡å‹è°ƒæ•´æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    acc_match = re.search(r'Accuracy[:\s]+(\d+\.?\d*)', content)
    if acc_match:
        metrics['accuracy'] = float(acc_match.group(1))

    # æå–æŸå¤±
    loss_match = re.search(r'(?:Final|Test)\s+[Ll]oss[:\s]+(\d+\.?\d*)', content)
    if loss_match:
        metrics['loss'] = float(loss_match.group(1))

    # æå–è®­ç»ƒæ—¶é—´
    time_match = re.search(r'Total [Dd]uration[:\s]+(\d+)h\s*(\d+)m', content)
    if time_match:
        hours = int(time_match.group(1))
        minutes = int(time_match.group(2))
        metrics['training_time_minutes'] = hours * 60 + minutes

    return metrics

# æ”¶é›†æ‰€æœ‰ç»“æœ
results = []
for log_file in os.listdir('mutation_logs'):
    if log_file.endswith('.log'):
        config_name = log_file.replace('.log', '')
        metrics = extract_metrics(f'mutation_logs/{log_file}')

        # ä»é…ç½®åè§£æå‚æ•°
        # ä¾‹å¦‚: mutation_e10_b64_lr0.001.log
        params = parse_config_name(config_name)

        results.append({
            'config': config_name,
            **params,
            **metrics
        })

# ä¿å­˜ä¸ºCSV
df = pd.DataFrame(results)
df.to_csv('mutation_test_results.csv', index=False)

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
print("=== Mutation Test Summary ===")
print(f"Total tests: {len(df)}")
print(f"\nBest accuracy: {df['accuracy'].max():.4f}")
print(f"Worst accuracy: {df['accuracy'].min():.4f}")
print(f"\nBest config:\n{df.loc[df['accuracy'].idxmax()]}")
```

### 4. æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ

#### 4.1 èµ„æºç®¡ç†

1. **GPUèµ„æºï¼š**
   - ä½¿ç”¨é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿé¿å…èµ„æºå†²çª
   - è€ƒè™‘ä½¿ç”¨`nvidia-smi`ç›‘æ§GPUä½¿ç”¨ç‡
   - ä¸ºä¸åŒå®éªŒåˆ†é…ä¸åŒGPUï¼š`CUDA_VISIBLE_DEVICES=0 ./train.sh ...`

2. **å­˜å‚¨ç©ºé—´ï¼š**
   - å®šæœŸæ¸…ç†ä¸­é—´æ¨¡å‹æ£€æŸ¥ç‚¹
   - ä»…ä¿å­˜æœ€ä½³æ¨¡å‹
   - å‹ç¼©æ—¥å¿—æ–‡ä»¶

3. **æ—¶é—´ç®¡ç†ï¼š**
   - ä¼˜å…ˆæµ‹è¯•å¿«é€Ÿæ¨¡å‹ï¼ˆexamplesç³»åˆ—ï¼‰
   - å¯¹é•¿æ—¶é—´è®­ç»ƒæ¨¡å‹ï¼ˆResNet200 epochsï¼‰å‡å°‘å˜å¼‚æ•°é‡
   - ä½¿ç”¨`--epochs 5`å¿«é€ŸéªŒè¯è„šæœ¬æ­£ç¡®æ€§

#### 4.2 å®éªŒè®¾è®¡åŸåˆ™

1. **æ§åˆ¶å˜é‡ï¼š**
   - æ¯æ¬¡åªæ”¹å˜ä¸€ä¸ªè¶…å‚æ•°
   - ä¿æŒå…¶ä»–å‚æ•°ä¸ºé»˜è®¤å€¼
   - ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ï¼ˆé™¤éæµ‹è¯•seedå½±å“ï¼‰

2. **é‡å¤å®éªŒï¼š**
   - å¯¹å…³é”®å‘ç°è¿›è¡Œ3-5æ¬¡é‡å¤å®éªŒ
   - ä½¿ç”¨ä¸åŒseedéªŒè¯ç»“æœç¨³å®šæ€§

3. **è®°å½•å®Œæ•´ä¿¡æ¯ï¼š**
   - ä¿å­˜å®Œæ•´çš„è®­ç»ƒæ—¥å¿—
   - è®°å½•ç¯å¢ƒä¿¡æ¯ï¼ˆGPUå‹å·ã€PyTorchç‰ˆæœ¬ç­‰ï¼‰
   - è®°å½•æ•°æ®é›†ç‰ˆæœ¬

#### 4.3 å¸¸è§é—®é¢˜å¤„ç†

1. **OOM (Out of Memory)ï¼š**
   - å‡å°batch_size
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ--fp16ï¼‰
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

2. **è®­ç»ƒä¸æ”¶æ•›ï¼š**
   - å‡å°learning_rate
   - å¢åŠ warm_up_epochs
   - æ£€æŸ¥æ•°æ®é¢„å¤„ç†

3. **è®­ç»ƒè¿‡æ…¢ï¼š**
   - å¢å¤§batch_sizeï¼ˆåœ¨æ˜¾å­˜å…è®¸èŒƒå›´å†…ï¼‰
   - å‡å°‘æ•°æ®åŠ è½½workers
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

---

## é™„å½•ï¼šè¶…å‚æ•°é€ŸæŸ¥è¡¨

### è¡¨A1: æ‰€æœ‰æ¨¡å‹è¶…å‚æ•°å¯¹æ¯”çŸ©é˜µ

| è¶…å‚æ•° | MRT-OAST | VulBERTa | ResNet | Bug-Loc | DenseNet | HRNet | PCB | MNIST | MNIST-RNN | MNIST-FF | Siamese |
|--------|---------|---------|--------|---------|---------|-------|-----|-------|-----------|----------|---------|
| **epochs** | âœ“ (10) | âœ“ (10-20) | âœ“ (200) | âœ— | âœ“ (60) | âœ“ (60) | âœ“ (60) | âœ“ (14) | âœ“ (14) | âœ“ (1000) | âœ“ (14) |
| **batch_size** | âœ“ (64) | âœ“ (4-128) | âœ“ (128) | âœ— | âœ“ (24) | âœ“ (24) | âœ“ (32) | âœ“ (32) | âœ“ (32) | âœ— | âœ“ (32) |
| **learning_rate** | âœ“ (0.0001) | âœ“ (3e-5~5e-4) | âœ“ (0.1) | âœ— | âœ“ (0.05) | âœ“ (0.05) | âœ“ (0.02) | âœ“ (1.0) | âœ“ (0.1) | âœ“ (0.03) | âœ“ (1.0) |
| **seed** | âœ“ (1334) | âœ“ (42-1234) | âœ— | âœ— | âœ— | âœ— | âœ— | âœ“ (1) | âœ“ (1) | âœ“ (1) | âœ“ (1) |
| **dropout** | âœ“ (0.2) | âœ— | âœ— | âœ— | âœ“ (0.5) | âœ“ (0.5) | âœ“ (0.5) | âœ— | âœ— | âœ— | âœ— |
| **weight_decay** | âœ— | âœ— | âœ“ (1e-4) | âœ“ (1e-5) | âœ“ (5e-4) | âœ“ (5e-4) | âœ“ (5e-4) | âœ— | âœ— | âœ— | âœ— |
| **momentum** | âœ— | âœ— | âœ“ (0.9) | âœ— | âœ— | âœ— | âœ— | âœ— | âœ— | âœ— | âœ— |
| **gamma** | ä»£ç  (0.5) | âœ— | âœ— | âœ— | âœ— | âœ— | âœ— | âœ“ (0.7) | âœ“ (0.7) | âœ— | âœ“ (0.7) |
| **mixed_precision** | âœ— | âœ“ | âœ“ | âœ— | âœ“ | âœ“ | âœ“ | âœ— | âœ— | âœ— | âœ— |
| **warm_epoch** | âœ— | âœ— | âœ— | âœ— | âœ“ (0) | âœ“ (0) | âœ“ (0) | âœ— | âœ— | âœ— | âœ— |

**å›¾ä¾‹ï¼š**
- âœ“ (å€¼): æ”¯æŒä¸”å¯é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹ï¼Œæ‹¬å·å†…ä¸ºé»˜è®¤å€¼
- ä»£ç  (å€¼): åœ¨ä»£ç ä¸­å®šä¹‰ä½†æœªæš´éœ²åˆ°å‘½ä»¤è¡Œ
- âœ—: ä¸æ”¯æŒæˆ–æœªå®ç°

### è¡¨A2: æ¨èå˜å¼‚ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | è¶…å‚æ•° | è¦†ç›–æ¨¡å‹æ•° | å˜å¼‚éš¾åº¦ | é¢„æœŸå½±å“ |
|-------|--------|-----------|---------|---------|
| ğŸ”¥ P0 | epochs | 10/11 | â­ ä½ | è®­ç»ƒæ—¶é—´ã€æ”¶æ•›æ€§ |
| ğŸ”¥ P0 | batch_size | 10/11 | â­ ä½ | æ”¶æ•›é€Ÿåº¦ã€å†…å­˜ |
| ğŸ”¥ P0 | learning_rate | 10/11 | â­ ä½ | æ”¶æ•›é€Ÿåº¦ã€ç²¾åº¦ |
| ğŸ”¥ P1 | seed | 6/11 | â­ ä½ | ç»“æœç¨³å®šæ€§ |
| ğŸ“Š P1 | optimizer | 11/11 | â­â­ ä¸­ | æ”¶æ•›æ€§èƒ½ |
| ğŸ“Š P1 | lr_scheduler | 11/11 | â­â­ ä¸­ | è®­ç»ƒç¨³å®šæ€§ |
| ğŸ“Š P2 | dropout | 4/11 | â­ ä½ | è¿‡æ‹Ÿåˆæ§åˆ¶ |
| ğŸ“Š P2 | weight_decay | 4/11 | â­ ä½ | æ³›åŒ–èƒ½åŠ› |
| ğŸ“Š P2 | gradient_clip | 11/11 | â­â­ ä¸­ | è®­ç»ƒç¨³å®šæ€§ |
| ğŸ”§ P3 | hidden_size | 11/11 | â­â­â­ é«˜ | æ¨¡å‹å®¹é‡ |
| ğŸ”§ P3 | num_layers | 11/11 | â­â­â­ é«˜ | æ¨¡å‹æ·±åº¦ |

### è¡¨A3: å„ä»“åº“train.shä½ç½®

| ä»“åº“ | train.shè·¯å¾„ | è®­ç»ƒè„šæœ¬è·¯å¾„ |
|------|------------|------------|
| MRT-OAST | `/home/green/energy_dl/success/MRT-OAST/train.sh` | `main_batch.py` |
| VulBERTa | `/home/green/energy_dl/success/VulBERTa/train.sh` | `train_vulberta.py` |
| pytorch_resnet_cifar10 | `/home/green/energy_dl/success/pytorch_resnet_cifar10/train.sh` | `trainer.py` |
| bug-localization | `/home/green/energy_dl/success/bug-localization-by-dnn-and-rvsm/train.sh` | `train_wrapper.py` |
| Person_reID | `/home/green/energy_dl/success/Person_reID_baseline_pytorch/train.sh` | `train.py` |
| examples | `/home/green/energy_dl/success/examples/train.sh` | å„å­ç›®å½•çš„`main.py` |

---

## ç‰ˆæœ¬å†å²

- **v1.0** (2025-11-03): åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæˆ11ä¸ªæ¨¡å‹çš„è¶…å‚æ•°åˆ†æ

---

**æ–‡æ¡£ç»´æŠ¤è€…ï¼š** Claude Code
**æœ€åæ›´æ–°ï¼š** 2025-11-03
