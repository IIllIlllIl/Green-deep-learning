================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_060_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_060_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -l 0.006611
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3829591
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -l 0.006611
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 10000
[0;34m[INFO][0m Learning Rate: 0.006611
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:43:12...

training layer:  0
Loss:  1.1267650127410889
Loss:  0.8878273367881775
Loss:  0.7734846472740173
Loss:  0.7166357040405273
Loss:  0.7044590711593628
Loss:  0.7022967338562012
Loss:  0.7000746130943298
Loss:  0.6981285214424133
Loss:  0.6967604160308838
Loss:  0.6958007216453552
Loss:  0.6950738430023193
Loss:  0.6944828629493713
Loss:  0.6939780116081238
Loss:  0.6935290098190308
Loss:  0.6931136250495911
Loss:  0.6927145719528198
Loss:  0.6923166513442993
Loss:  0.6919054985046387
Loss:  0.6914671063423157
Loss:  0.6909868717193604
Loss:  0.6904494762420654
Loss:  0.6898388266563416
Loss:  0.6891394257545471
Loss:  0.688336968421936
Loss:  0.6874182224273682
Loss:  0.6863703727722168
Loss:  0.6851810216903687
Loss:  0.6838390827178955
Loss:  0.682335615158081
Loss:  0.6806653141975403
Loss:  0.6788255572319031
Loss:  0.6768163442611694
Loss:  0.674640417098999
Loss:  0.6723021268844604
Loss:  0.6698077917098999
Loss:  0.6671652793884277
Loss:  0.6643849015235901
Loss:  0.6614795923233032
Loss:  0.6584651470184326
Loss:  0.6553587913513184
Loss:  0.6521781086921692
Loss:  0.6489415168762207
Loss:  0.6456672549247742
Loss:  0.6423726081848145
Loss:  0.6390712857246399
Loss:  0.6357748508453369
Loss:  0.6324917078018188
Loss:  0.6292279362678528
Loss:  0.6259868741035461
Loss:  0.6227706074714661
Loss:  0.6195798516273499
Loss:  0.6164156794548035
Loss:  0.6132783889770508
Loss:  0.6101685166358948
Loss:  0.6070859432220459
Loss:  0.6040304899215698
Loss:  0.6010016202926636
Loss:  0.5979986786842346
Loss:  0.5950212478637695
Loss:  0.5920693278312683
Loss:  0.5891426205635071
Loss:  0.5862413048744202
Loss:  0.5833654403686523
Loss:  0.5805151462554932
Loss:  0.5776901245117188
Loss:  0.5748901963233948
Loss:  0.5721153020858765
Loss:  0.5693655610084534
Loss:  0.5666411519050598
Loss:  0.5639421939849854
Loss:  0.56126868724823
Loss:  0.5586205720901489
Loss:  0.555997908115387
Loss:  0.5534009337425232
Loss:  0.5508294701576233
Loss:  0.5482833981513977
Loss:  0.5457625985145569
Loss:  0.5432670712471008
Loss:  0.5407971739768982
Loss:  0.5383531451225281
Loss:  0.5359354019165039
Loss:  0.5335440039634705
Loss:  0.5311790108680725
Loss:  0.5288402438163757
Loss:  0.5265277028083801
Loss:  0.5242413878440857
Loss:  0.5219812393188477
Loss:  0.5197470784187317
Loss:  0.517538845539093
Loss:  0.5153564214706421
Loss:  0.5131996273994446
Loss:  0.5110681653022766
Loss:  0.5089617967605591
Loss:  0.5068802833557129
Loss:  0.5048233866691589
Loss:  0.5027909874916077
Loss:  0.5007829666137695
Loss:  0.498799204826355
Loss:  0.49683913588523865
Loss:  0.4949023425579071
training layer:  1
Loss:  1.1266670227050781
Loss:  0.8860874772071838
Loss:  0.7866738438606262
Loss:  0.7358136773109436
Loss:  0.7245961427688599
Loss:  0.7234266400337219
Loss:  0.7216660976409912
Loss:  0.7191375494003296
Loss:  0.7160205841064453
Loss:  0.7122796773910522
Loss:  0.7075909972190857
Loss:  0.7019764184951782
Loss:  0.6954866051673889
Loss:  0.6883765459060669
Loss:  0.6810062527656555
Loss:  0.6737996339797974
Loss:  0.6670567989349365
Loss:  0.660904049873352
Loss:  0.6553340554237366
Loss:  0.6502939462661743
Loss:  0.6456969380378723
Loss:  0.6414609551429749
Loss:  0.6375178694725037
Loss:  0.6337973475456238
Loss:  0.630277693271637
Loss:  0.6269490122795105
Loss:  0.6238166093826294
Loss:  0.620884358882904
Loss:  0.618133544921875
Loss:  0.6155338883399963
Loss:  0.6130620241165161
Loss:  0.61070716381073
Loss:  0.608465313911438
Loss:  0.6063193082809448
Loss:  0.6042489409446716
Loss:  0.6022466421127319
Loss:  0.6003103256225586
Loss:  0.5984327793121338
Loss:  0.5966079235076904
Loss:  0.5948281288146973
Loss:  0.5930836200714111
Loss:  0.5913670063018799
Loss:  0.5896726250648499
Loss:  0.5879970788955688
Loss:  0.5863404870033264
Loss:  0.5847041606903076
Loss:  0.5830875039100647
Loss:  0.5814895033836365
Loss:  0.5799098610877991
Loss:  0.5783472061157227
Loss:  0.5767987966537476
Loss:  0.5752626061439514
Loss:  0.5737382769584656
Loss:  0.5722267627716064
Loss:  0.5707293748855591
Loss:  0.5692477226257324
Loss:  0.5677822828292847
Loss:  0.5663325786590576
Loss:  0.5648983120918274
Loss:  0.5634798407554626
Loss:  0.5620779395103455
Loss:  0.5606931447982788
Loss:  0.5593257546424866
Loss:  0.5579768419265747
Loss:  0.5566463470458984
Loss:  0.5553330183029175
Loss:  0.5540355443954468
Loss:  0.5527526140213013
Loss:  0.5514840483665466
Loss:  0.5502304434776306
Loss:  0.5489924550056458
Loss:  0.5477715730667114
Loss:  0.5465683937072754
Loss:  0.5453823208808899
Loss:  0.5442129969596863
Loss:  0.5430614948272705
Loss:  0.5419281125068665
Loss:  0.5408128499984741
Loss:  0.5397152900695801
Loss:  0.5386343598365784
Loss:  0.5375688076019287
Loss:  0.5365180373191833
Loss:  0.5354812741279602
Loss:  0.5344578623771667
Loss:  0.5334469676017761
Loss:  0.5324478149414062
Loss:  0.5314594507217407
Loss:  0.5304809212684631
Loss:  0.5295113921165466
Loss:  0.5285505652427673
Loss:  0.5275977849960327
Loss:  0.5266531705856323
Loss:  0.5257166624069214
Loss:  0.5247882008552551
Loss:  0.5238679647445679
Loss:  0.5229553580284119
Loss:  0.5220503807067871
Loss:  0.5211526155471802
Loss:  0.520261824131012
Loss:  0.5193775296211243
train error: 0.16519999504089355
test error: 0.1673000454902649

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:43:12
[0;34m[INFO][0m Training End: 2025-12-15 16:43:30
[0;34m[INFO][0m Total Duration: 0h 0m 18s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 83.2699954509735100%
[0;34m[INFO][0m Test Error: 0.1673000454902649
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.16519999504089355
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_060_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_060_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
