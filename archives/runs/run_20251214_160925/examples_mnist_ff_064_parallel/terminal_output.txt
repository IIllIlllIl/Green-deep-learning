================================================================================
STDOUT:
================================================================================
[Train Wrapper] Repository: repos/examples
[Train Wrapper] Training script: ./train.sh
[Train Wrapper] Log file: results/run_20251214_160925/examples_mnist_ff_064_parallel/training.log
[Train Wrapper] Energy directory: results/run_20251214_160925/examples_mnist_ff_064_parallel/energy
[Train Wrapper] Arguments: -n mnist_ff -b 117
[Train Wrapper] Changed to directory: /home/green/energy_dl/nightly/repos/examples
[Train Wrapper] Starting GPU monitoring...
[Train Wrapper] GPU monitoring PID: 3832097
[Train Wrapper] Starting training with integrated energy monitoring...
[Train Wrapper] Command: ./train.sh -n mnist_ff -b 117
========================================
[0;34m[INFO][0m Using Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python

========================================
  PyTorch Training Script
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Directory: /home/green/energy_dl/nightly/repos/examples/mnist_forward_forward
[0;34m[INFO][0m Python: /home/green/energy_dl/nightly/repos/examples/venv/bin/python
[0;34m[INFO][0m Epochs: 1000
[0;34m[INFO][0m Batch Size: 117
[0;34m[INFO][0m Learning Rate: 0.03
[0;34m[INFO][0m Seed: 1
========================================

[0;34m[INFO][0m Starting training at 2025-12-15 16:50:31...

training layer:  0
Loss:  1.126773476600647
Loss:  0.7512038350105286
Loss:  0.6983580589294434
Loss:  0.699694812297821
Loss:  0.6884225606918335
Loss:  0.6841703653335571
Loss:  0.6808537840843201
Loss:  0.6768864393234253
Loss:  0.6717636585235596
Loss:  0.6649240851402283
Loss:  0.6557892560958862
Loss:  0.6439605355262756
Loss:  0.6292557716369629
Loss:  0.6118344068527222
Loss:  0.592332661151886
Loss:  0.5716294646263123
Loss:  0.5504788160324097
Loss:  0.5294163227081299
Loss:  0.5087723135948181
Loss:  0.4887115955352783
Loss:  0.46936458349227905
Loss:  0.45080772042274475
Loss:  0.43308258056640625
Loss:  0.41620996594429016
Loss:  0.40017908811569214
Loss:  0.38497236371040344
Loss:  0.3705645501613617
Loss:  0.3569278120994568
Loss:  0.3440362513065338
Loss:  0.3318617343902588
Loss:  0.32036319375038147
Loss:  0.30949971079826355
Loss:  0.2992304861545563
Loss:  0.2895173132419586
Loss:  0.2803228199481964
Loss:  0.2716137766838074
Loss:  0.2633647620677948
Loss:  0.2555498778820038
Loss:  0.24814315140247345
Loss:  0.2411189079284668
Loss:  0.23445315659046173
Loss:  0.22812537848949432
Loss:  0.2221149057149887
Loss:  0.21640154719352722
Loss:  0.21096748113632202
Loss:  0.2057952582836151
Loss:  0.20086674392223358
Loss:  0.1961677521467209
Loss:  0.19168797135353088
Loss:  0.18741419911384583
Loss:  0.18333394825458527
Loss:  0.1794344037771225
Loss:  0.17570532858371735
Loss:  0.1721375584602356
Loss:  0.1687220185995102
Loss:  0.1654515415430069
Loss:  0.16231827437877655
Loss:  0.15931405127048492
Loss:  0.15643152594566345
Loss:  0.15366405248641968
Loss:  0.15100525319576263
Loss:  0.14844924211502075
Loss:  0.1459905207157135
Loss:  0.14362479746341705
Loss:  0.14134833216667175
Loss:  0.13915714621543884
Loss:  0.13704729080200195
Loss:  0.13501423597335815
Loss:  0.13305385410785675
Loss:  0.13116313517093658
Loss:  0.12933988869190216
Loss:  0.12758103013038635
Loss:  0.12588343024253845
Loss:  0.12424422055482864
Loss:  0.12266086041927338
Loss:  0.12113086134195328
Loss:  0.11965184658765793
Loss:  0.11822156608104706
Loss:  0.11683805286884308
Loss:  0.1154993399977684
Loss:  0.11420340090990067
Loss:  0.11294829100370407
Loss:  0.1117321327328682
Loss:  0.1105530634522438
Loss:  0.10940923541784286
Loss:  0.10829942673444748
Loss:  0.10722243785858154
Loss:  0.10617769509553909
Loss:  0.10516554117202759
Loss:  0.10418476909399033
Loss:  0.10323378443717957
Loss:  0.10231123119592667
Loss:  0.10141589492559433
Loss:  0.10054675489664078
Loss:  0.09970288723707199
Loss:  0.09888357669115067
Loss:  0.09808783233165741
Loss:  0.09731470793485641
Loss:  0.09656334668397903
Loss:  0.09583289176225662
training layer:  1
Loss:  1.1266754865646362
Loss:  0.7477067112922668
Loss:  0.7345827221870422
Loss:  0.7175106406211853
Loss:  0.7101874947547913
Loss:  0.7036095261573792
Loss:  0.6945067048072815
Loss:  0.6839307546615601
Loss:  0.6718817949295044
Loss:  0.6591261029243469
Loss:  0.6467026472091675
Loss:  0.6349775195121765
Loss:  0.6236164569854736
Loss:  0.6122053265571594
Loss:  0.6005092263221741
Loss:  0.5882752537727356
Loss:  0.5753792524337769
Loss:  0.5618125796318054
Loss:  0.5476715564727783
Loss:  0.5331259369850159
Loss:  0.518305242061615
Loss:  0.5033825039863586
Loss:  0.488590806722641
Loss:  0.4741300940513611
Loss:  0.46015098690986633
Loss:  0.4466765820980072
Loss:  0.43374794721603394
Loss:  0.42138177156448364
Loss:  0.4096231460571289
Loss:  0.3984854221343994
Loss:  0.38793253898620605
Loss:  0.3779306411743164
Loss:  0.3684350252151489
Loss:  0.35939204692840576
Loss:  0.35079532861709595
Loss:  0.34263038635253906
Loss:  0.3348758816719055
Loss:  0.32750317454338074
Loss:  0.32048967480659485
Loss:  0.3138117790222168
Loss:  0.3074397146701813
Loss:  0.30135372281074524
Loss:  0.29554304480552673
Loss:  0.2899905741214752
Loss:  0.2846775949001312
Loss:  0.27958711981773376
Loss:  0.27470776438713074
Loss:  0.27002841234207153
Loss:  0.26553741097450256
Loss:  0.26122570037841797
Loss:  0.2570825219154358
Loss:  0.2530958652496338
Loss:  0.2492576390504837
Loss:  0.2455570101737976
Loss:  0.2419879287481308
Loss:  0.23854853212833405
Loss:  0.23522911965847015
Loss:  0.23201923072338104
Loss:  0.22891511023044586
Loss:  0.2259146273136139
Loss:  0.22301027178764343
Loss:  0.22019536793231964
Loss:  0.21746398508548737
Loss:  0.21481138467788696
Loss:  0.21223479509353638
Loss:  0.20973046123981476
Loss:  0.20729486644268036
Loss:  0.20492519438266754
Loss:  0.20261840522289276
Loss:  0.20037095248699188
Loss:  0.1981792151927948
Loss:  0.19604110717773438
Loss:  0.19395649433135986
Loss:  0.19192355871200562
Loss:  0.18994024395942688
Loss:  0.18800432980060577
Loss:  0.18611374497413635
Loss:  0.18426768481731415
Loss:  0.18246522545814514
Loss:  0.18070481717586517
Loss:  0.17898468673229218
Loss:  0.17730379104614258
Loss:  0.17566068470478058
Loss:  0.17405401170253754
Loss:  0.1724824607372284
Loss:  0.17094480991363525
Loss:  0.16943995654582977
Loss:  0.16796711087226868
Loss:  0.16652536392211914
Loss:  0.16511385142803192
Loss:  0.16373157501220703
Loss:  0.16237753629684448
Loss:  0.16105088591575623
Loss:  0.1597507745027542
Loss:  0.1584765911102295
Loss:  0.1572275608778
Loss:  0.15600305795669556
Loss:  0.15480244159698486
Loss:  0.1536250114440918
Loss:  0.15247012674808502
train error: 0.8717948645353317
test error: 0.936999998986721

========================================
  TRAINING REPORT
========================================
[0;34m[INFO][0m Model: mnist_ff
[0;34m[INFO][0m Training Start: 2025-12-15 16:50:31
[0;34m[INFO][0m Training End: 2025-12-15 16:50:39
[0;34m[INFO][0m Total Duration: 0h 0m 8s
----------------------------------------
[0;32m[SUCCESS][0m Training completed successfully!
----------------------------------------
[0;34m[INFO][0m Performance Metrics:
[0;32m[SUCCESS][0m Test Accuracy: 6.300000101327900%
[0;34m[INFO][0m Test Error: 0.936999998986721
----------------------------------------
[1;33m[WARNING][0m Errors/Warnings found during training:
train error: 0.8717948645353317
========================================

========================================
[Train Wrapper] Training finished with exit code: 0
[Train Wrapper] Stopping GPU monitoring...
[Train Wrapper] Processing CPU energy data...
[Train Wrapper] CPU energy saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_064_parallel/energy/cpu_energy.txt
[Train Wrapper] GPU monitoring data saved to: /home/green/energy_dl/nightly/results/run_20251214_160925/examples_mnist_ff_064_parallel/energy
[Train Wrapper] Energy monitoring completed

================================================================================
STDERR:
================================================================================
(empty)
