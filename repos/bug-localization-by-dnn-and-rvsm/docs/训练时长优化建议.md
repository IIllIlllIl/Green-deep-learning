# 训练时长优化建议

## 当前训练配置分析

### 模型参数
- **DNN模型**: MLPRegressor
  - 隐藏层: 1层，300个神经元
  - 最大迭代: 10,000次
  - 早停策略: 30次迭代无改善
  - 优化器: SGD
  - 正则化: alpha=1e-5

### 数据规模
- 总样本数: 262,600个
- 训练集大小 (90%): ~236,340个样本
- 特征维度: 5
- 交叉验证折数: 10折

### 硬件资源
- CPU: 20核心 (Intel Xeon Silver 4210R @ 2.40GHz)
- 内存: 30GB
- GPU: NVIDIA RTX 3080 10GB (不支持scikit-learn)

## 训练时长估计

### 单折训练时间估计
基于MLPRegressor的计算复杂度：
- 前向传播: O(n × d × h) = O(236,340 × 5 × 300) ≈ 355M 操作/迭代
- 反向传播: 类似复杂度
- 预计每次迭代: ~0.1-0.3秒
- 预计平均迭代次数: 100-500次（由于早停）
- **单折训练时间**: 3-8分钟

### 总训练时间估计
由于使用并行训练（n_jobs=-2，即19个核心）：
- 理想情况: max(单折时间) = 3-8分钟
- 考虑调度开销: **10-15分钟**
- rVSM模型: < 1分钟
- **总计**: **10-20分钟**

## 结论

**✓ 当前配置下训练时间远低于2小时阈值，无需优化**

---

## 可选优化方案（如需加速）

### 方案1: 减少交叉验证折数 ⭐⭐⭐

**修改位置**: `src/main.py`

```python
# 当前配置
print(dnn_model_kfold(10))

# 优化配置
print(dnn_model_kfold(5))  # 5折交叉验证
```

**效果**:
- 训练时间: 减少约40-50%
- 模型性能: 评估精度略微降低（可接受）
- 推荐指数: ⭐⭐⭐

---

### 方案2: 减少最大迭代次数 ⭐⭐

**修改位置**: `src/dnn_model.py` 第101-108行

```python
# 当前配置
clf = MLPRegressor(
    solver="sgd",
    alpha=1e-5,
    hidden_layer_sizes=(300,),
    random_state=1,
    max_iter=10000,        # 修改此行
    n_iter_no_change=30,   # 修改此行
)

# 优化配置
clf = MLPRegressor(
    solver="sgd",
    alpha=1e-5,
    hidden_layer_sizes=(300,),
    random_state=1,
    max_iter=5000,         # 减少到5000
    n_iter_no_change=20,   # 减少到20
)
```

**效果**:
- 训练时间: 减少约20-30%
- 模型性能: 由于早停机制，影响较小
- 推荐指数: ⭐⭐

---

### 方案3: 优化隐藏层大小 ⭐⭐

**修改位置**: `src/dnn_model.py` 第101-108行

```python
# 当前配置
hidden_layer_sizes=(300,)

# 优化配置（推荐测试）
hidden_layer_sizes=(200,)  # 减少神经元数量
# 或
hidden_layer_sizes=(150,)  # 进一步减少
```

**效果**:
- 训练时间: 减少约30-50%（与神经元数成正比）
- 模型性能: 可能略有下降，需要实验验证
- 推荐指数: ⭐⭐

---

### 方案4: 减少负样本数量 ⭐ (不推荐)

**修改位置**: `src/util.py` 第93-124行

```python
# 当前配置
def top_k_wrong_files(right_files, br_raw_text, java_files, k=50):

# 优化配置
def top_k_wrong_files(right_files, br_raw_text, java_files, k=25):
```

**效果**:
- 训练时间: 减少约50%
- 模型性能: **显著下降**（负样本是重要的训练信号）
- 推荐指数: ⭐ (不推荐)

---

### 方案5: 使用更高效的优化器 ⭐⭐⭐⭐

**修改位置**: `src/dnn_model.py` 第101-108行

```python
# 当前配置
clf = MLPRegressor(
    solver="sgd",
    ...
)

# 优化配置
clf = MLPRegressor(
    solver="adam",        # 使用Adam优化器
    alpha=1e-5,
    hidden_layer_sizes=(300,),
    random_state=1,
    max_iter=5000,        # Adam通常收敛更快
    n_iter_no_change=20,
)
```

**效果**:
- 训练时间: 减少约30-40%（更快收敛）
- 模型性能: 通常相当或略好
- 推荐指数: ⭐⭐⭐⭐

---

### 方案6: 调整过采样倍率 ⭐⭐

**修改位置**: `src/dnn_model.py` 第9-24行

```python
# 当前配置
def oversample(samples):
    samples_ = []
    for i, sample in enumerate(samples):
        samples_.append(sample)
        if i % 51 == 0:
            for _ in range(9):  # 每个正样本复制9次
                samples_.append(sample)
    return samples_

# 优化配置
def oversample(samples):
    samples_ = []
    for i, sample in enumerate(samples):
        samples_.append(sample)
        if i % 51 == 0:
            for _ in range(4):  # 减少到4次
                samples_.append(sample)
    return samples_
```

**效果**:
- 训练时间: 减少约20-30%
- 模型性能: 可能略有影响（正负样本平衡变化）
- 推荐指数: ⭐⭐

---

### 方案7: 使用学习率调度 ⭐⭐⭐

**修改位置**: `src/dnn_model.py` 第101-108行

```python
# 优化配置
clf = MLPRegressor(
    solver="sgd",
    alpha=1e-5,
    hidden_layer_sizes=(300,),
    random_state=1,
    max_iter=5000,
    n_iter_no_change=20,
    learning_rate="adaptive",  # 添加自适应学习率
    learning_rate_init=0.01,   # 设置初始学习率
)
```

**效果**:
- 训练时间: 减少约20-30%（更快收敛）
- 模型性能: 通常相当或略好
- 推荐指数: ⭐⭐⭐

---

## 推荐优化组合

如果确实需要加速训练，建议采用以下组合：

### 组合1: 温和优化（推荐）
```python
# src/main.py
print(dnn_model_kfold(5))  # 5折交叉验证

# src/dnn_model.py
clf = MLPRegressor(
    solver="adam",              # 使用Adam优化器
    alpha=1e-5,
    hidden_layer_sizes=(300,),
    random_state=1,
    max_iter=5000,              # 减少迭代次数
    n_iter_no_change=20,
)
```

**预计训练时间**: 3-5分钟
**性能影响**: 最小（<2% 准确率下降）

### 组合2: 激进优化
```python
# src/main.py
print(dnn_model_kfold(5))

# src/dnn_model.py
clf = MLPRegressor(
    solver="adam",
    alpha=1e-5,
    hidden_layer_sizes=(200,),   # 减少神经元
    random_state=1,
    max_iter=3000,
    n_iter_no_change=15,
    learning_rate="adaptive",
)
```

**预计训练时间**: 2-3分钟
**性能影响**: 中等（3-5% 准确率可能下降）

---

## 性能监控

### 创建训练时间监控脚本

`scripts/monitor_training.py`:

```python
import time
import sys
sys.path.append('../src')

from dnn_model import dnn_model_kfold
from rvsm_model import rsvm_model

# 监控DNN训练
start = time.time()
dnn_results = dnn_model_kfold(10)
dnn_time = time.time() - start

print(f"\nDNN训练时间: {dnn_time/60:.2f} 分钟")
print(f"DNN Top-20准确率: {dnn_results[20]}")

# 监控rVSM
start = time.time()
rvsm_results = rsvm_model()
rvsm_time = time.time() - start

print(f"\nrVSM训练时间: {rvsm_time:.2f} 秒")
print(f"rVSM Top-20准确率: {rvsm_results[20]}")
```

---

## 总结

| 优化方案 | 时间节省 | 性能影响 | 推荐指数 | 实施难度 |
|---------|---------|---------|---------|---------|
| 减少交叉验证折数 | 40-50% | 低 | ⭐⭐⭐ | 简单 |
| 减少最大迭代次数 | 20-30% | 低 | ⭐⭐ | 简单 |
| 优化隐藏层大小 | 30-50% | 中 | ⭐⭐ | 简单 |
| 减少负样本数量 | 50% | 高 | ⭐ | 简单 |
| 使用Adam优化器 | 30-40% | 低 | ⭐⭐⭐⭐ | 简单 |
| 调整过采样倍率 | 20-30% | 中 | ⭐⭐ | 简单 |
| 自适应学习率 | 20-30% | 低 | ⭐⭐⭐ | 简单 |

**最终建议**:
- 当前配置已经很优化，训练时间在可接受范围内（10-20分钟）
- 如需加速，推荐使用"组合1: 温和优化"（减少到3-5分钟）
- 不建议使用激进优化，除非时间极其紧迫
