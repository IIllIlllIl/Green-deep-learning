# Person ReID Baseline 复现指南

## 系统环境信息

### 硬件配置
- GPU: NVIDIA GeForce RTX 3080 (10GB VRAM)
- CUDA Version: 12.2
- Driver Version: 535.183.01

### 软件环境
- Python: 3.10
- PyTorch: 2.5.1+cu121
- Torchvision: 0.20.1+cu121
- Conda环境名称: `reid_baseline`

## 数据集信息

### Market-1501数据集
- **下载路径**: `/home/green/energy_dl/test/Market/Market-1501-v15.09.15`
- **数据集大小**: 153 MB (压缩包)
- **数据统计**:
  - 训练集: 751个类别, 12,936张图片
  - 查询集: 750个类别, 3,368张图片
  - Gallery: 752个类别, 19,732张图片

### 数据准备
数据集已通过`prepare.py`脚本组织为PyTorch ImageFolder格式，位于：
```
/home/green/energy_dl/test/Market/Market-1501-v15.09.15/pytorch/
├── train_all/  # 全部训练数据
├── train/      # 训练集
├── val/        # 验证集
├── query/      # 查询集
├── gallery/    # Gallery集
└── multi-query/  # 多查询集
```

## 模型训练配置

### 基础模型配置（ResNet-50）

#### 1. 最小配置 (推荐用于快速验证)
```bash
# 使用较小的batch size和较少的epoch
python train.py --gpu_ids 0 --name ft_ResNet50_quick \
    --train_all --batchsize 16 --total_epoch 30 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约30-40分钟
**预计性能**: Rank@1 ≈ 85-87%, mAP ≈ 65-68%

#### 2. 标准配置 (复现baseline性能)
```bash
# 默认配置，复现README中的baseline结果
python train.py --gpu_ids 0 --name ft_ResNet50 \
    --train_all --batchsize 32 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约1-1.5小时 (60 epochs)
**预计性能**: Rank@1 ≈ 88.84%, mAP ≈ 71.59%

#### 3. 高性能配置 (所有tricks)
```bash
# 使用所有优化技巧
python train.py --gpu_ids 0 --name ft_ResNet50_all_tricks \
    --train_all --batchsize 8 --lr 0.02 \
    --warm_epoch 5 --stride 1 --erasing_p 0.5 --circle \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约2-3小时 (60 epochs)
**预计性能**: Rank@1 ≈ 92.13%, mAP ≈ 79.84%

#### 4. 低显存配置 (仅需2GB VRAM)
```bash
# 使用bf16降低显存占用
python train.py --gpu_ids 0 --name ft_ResNet50_bf16 \
    --train_all --batchsize 32 --bf16 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约1-1.5小时
**预计性能**: Rank@1 ≈ 88%, mAP ≈ 71%
**显存占用**: 约2-3GB

### 其他模型配置

#### DenseNet-121 (推荐)
```bash
python train.py --gpu_ids 0 --name ft_DenseNet \
    --use_dense --train_all --batchsize 32 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约1.5-2小时
**预计性能**: Rank@1 ≈ 90.17%, mAP ≈ 74.02%

#### PCB (Part-based Model)
```bash
python train.py --gpu_ids 0 --name PCB \
    --PCB --train_all --batchsize 64 --lr 0.02 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约2-2.5小时
**预计性能**: Rank@1 ≈ 92.64%, mAP ≈ 77.47%

#### Swin Transformer (最佳性能)
```bash
python train.py --gpu_ids 0 --name swin_best \
    --use_swin --train_all --erasing_p 0.5 --circle --warm_epoch 5 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约3-4小时
**预计性能**: Rank@1 ≈ 94.12%, mAP ≈ 84.39%
**注意**: 需要约11GB显存，可能在RTX 3080 (10GB)上显存不足

## 训练时长估计

基于RTX 3080 (10GB)的训练时间估计：

| 模型 | Batch Size | Epochs | 预计时长 | 性能 (Rank@1/mAP) |
|------|-----------|--------|----------|-------------------|
| ResNet-50 | 32 | 60 | 1-1.5h | 88.84% / 71.59% |
| ResNet-50 | 16 | 30 | 0.5-0.7h | ~85% / ~67% |
| ResNet-50 (all tricks) | 8 | 60 | 2-3h | 92.13% / 79.84% |
| ResNet-50 (bf16) | 32 | 60 | 1-1.5h | 88% / 71% |
| DenseNet-121 | 32 | 60 | 1.5-2h | 90.17% / 74.02% |
| PCB | 64 | 60 | 2-2.5h | 92.64% / 77.47% |

## 缩短训练时间的建议

### 1. 减少训练轮数 (推荐)
```bash
# 从60 epochs减少到30-40 epochs
--total_epoch 30
```
- 时间节省: 50%
- 性能影响: 轻微 (约1-2%下降)

### 2. 减小Batch Size
```bash
# 从32减小到16
--batchsize 16
```
- 时间节省: 约30-40%
- 性能影响: 轻微，但需要调整学习率
- 建议同时使用: `--lr 0.035` (0.05 * 16/32)

### 3. 使用混合精度训练
```bash
# 使用bf16或fp16
--bf16
# 或
--fp16
```
- 时间节省: 约10-20%
- 性能影响: 极小
- 额外优势: 减少显存占用约50%

### 4. 减少数据增强
```bash
# 不使用random erasing
# 移除 --erasing_p 参数
```
- 时间节省: 约5-10%
- 性能影响: 约1-2%下降

### 5. 仅在训练集训练（不用train_all）
```bash
# 移除 --train_all 参数
# 这样会使用train/val分割，数据量减少
```
- 时间节省: 约30%
- 性能影响: 约2-3%下降

### 6. 综合优化方案 (训练时长 < 30分钟)
```bash
python train.py --gpu_ids 0 --name ft_ResNet50_fast \
    --batchsize 16 --total_epoch 20 --bf16 \
    --data_dir ../Market/Market-1501-v15.09.15/pytorch
```
**预计训练时长**: 约20-30分钟
**预计性能**: Rank@1 ≈ 84-86%, mAP ≈ 64-67%

## 测试和评估

### 测试模型
```bash
# 提取特征
python test.py --gpu_ids 0 --name ft_ResNet50 \
    --test_dir ../Market/Market-1501-v15.09.15/pytorch \
    --batchsize 32 --which_epoch last

# 评估结果
python evaluate.py

# 或使用GPU加速评估
python evaluate_gpu.py
```

### 可视化结果
```bash
# 查看某个查询图片的检索结果
python demo.py --query_index 777
```

## 测试脚本

参考 `scripts/train_and_test.sh` 脚本进行完整的训练和测试流程。

## 预期结果对比

### Baseline结果 (来自README)
- ResNet-50: Rank@1=88.84%, mAP=71.59%
- ResNet-50 (all tricks): Rank@1=92.13%, mAP=79.84%
- DenseNet-121: Rank@1=90.17%, mAP=74.02%
- PCB: Rank@1=92.64%, mAP=77.47%

### 与原论文对比
原论文报告的baseline结果：
- Rank@1=88.24%, mAP=70.68%

我们的复现应该达到或超过这个结果。

## 常见问题

### Q1: 训练时显存不足
**解决方案**:
1. 减小batch size: `--batchsize 16` 或 `--batchsize 8`
2. 使用混合精度: `--bf16` 或 `--fp16`
3. 减小图像分辨率（修改model.py中的h, w参数）

### Q2: 训练速度慢
**解决方案**:
1. 检查数据加载器workers数量（train.py第173行）
2. 使用混合精度训练
3. 减少数据增强操作

### Q3: 性能未达到预期
**解决方案**:
1. 确保训练足够的epochs（至少40-60）
2. 检查数据集是否正确准备
3. 尝试调整学习率和warm-up策略
4. 使用所有优化技巧（stride=1, random erasing, circle loss等）

## 下一步

1. 运行快速验证训练（30分钟配置）
2. 如果结果正常，运行标准配置复现baseline性能
3. 根据需要尝试高性能配置或其他模型
