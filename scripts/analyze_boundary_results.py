#!/usr/bin/env python3
"""
Analyze boundary test results and validate mutation ranges
"""
import json
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Load the test configuration to get baselines
with open('settings/boundary_test_elite_4models.json', 'r') as f:
    test_configs = json.load(f)['experiments']

# Group configs by model to identify baselines
model_configs = defaultdict(list)
for idx, config in enumerate(test_configs):
    model_key = f"{config['repo']}_{config['model']}"
    model_configs[model_key].append((idx, config))

# Identify baseline configs (should be first for each model)
baselines = {}
for model_key, configs in model_configs.items():
    # First config for each model should be baseline
    baselines[model_key] = configs[0]

print("="*80)
print("è¾¹ç•Œæµ‹è¯•ç»“æœåˆ†æ - Boundary Test Results Analysis")
print("="*80)
print(f"\nåˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Load all result files from Nov 10 20:00 onwards (boundary test period)
results_dir = Path('results')
result_files = sorted(results_dir.glob('*.json'))

# Filter to boundary test timeframe (after Nov 10 20:00)
boundary_results = []
for rf in result_files:
    if rf.stem >= '20251110_200000':  # After Nov 10 20:00
        try:
            with open(rf, 'r') as f:
                data = json.load(f)
                boundary_results.append(data)
        except Exception as e:
            print(f"âš ï¸  Error reading {rf}: {e}")

print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(boundary_results)} ä¸ªè®­ç»ƒç»“æœ\n")

# Group results by model
model_results = defaultdict(list)
for result in boundary_results:
    model_key = f"{result['repository']}_{result['model']}"
    model_results[model_key].append(result)

# Analyze each model
all_models_summary = []

for model_key in sorted(model_results.keys()):
    results = model_results[model_key]
    print("="*80)
    print(f"ğŸ“Œ æ¨¡å‹: {model_key}")
    print("="*80)

    # Find baseline result
    baseline_idx, baseline_config = baselines.get(model_key, (None, None))
    if baseline_config is None:
        print("âš ï¸  æœªæ‰¾åˆ°baselineé…ç½®\n")
        continue

    baseline_hp = baseline_config['hyperparameters']

    # Find baseline result
    baseline_result = None
    for result in results:
        result_hp = result['hyperparameters']
        # Check if all hyperparameters match baseline
        if all(result_hp.get(k) == v for k, v in baseline_hp.items()):
            baseline_result = result
            break

    if baseline_result is None:
        print(f"âš ï¸  æœªæ‰¾åˆ°baselineç»“æœ")
        print(f"   æœŸæœ›è¶…å‚æ•°: {baseline_hp}\n")
        continue

    baseline_success = baseline_result['training_success']
    baseline_perf = baseline_result['performance_metrics']

    print(f"\nğŸ¯ Baselineç»“æœ:")
    print(f"   è¶…å‚æ•°: {baseline_hp}")
    print(f"   è®­ç»ƒçŠ¶æ€: {'âœ… æˆåŠŸ' if baseline_success else 'âŒ å¤±è´¥'}")
    if baseline_success:
        print(f"   æ€§èƒ½æŒ‡æ ‡: {baseline_perf}")
    else:
        print(f"   é”™è¯¯ä¿¡æ¯: {baseline_result.get('error_message', 'Unknown')}")

    if not baseline_success:
        print(f"\nâš ï¸  Baselineè®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”åˆ†æ\n")
        all_models_summary.append({
            'model': model_key,
            'baseline_success': False,
            'tests_completed': len(results),
            'tests_failed': sum(1 for r in results if not r['training_success']),
            'performance_degradations': 'N/A'
        })
        continue

    # Extract baseline metric
    baseline_metric = None
    metric_name = None
    if 'test_accuracy' in baseline_perf:
        baseline_metric = baseline_perf['test_accuracy']
        metric_name = 'test_accuracy'
    elif 'map' in baseline_perf:
        baseline_metric = baseline_perf['map']
        metric_name = 'map'
    elif 'accuracy' in baseline_perf:
        baseline_metric = baseline_perf['accuracy']
        metric_name = 'accuracy'

    if baseline_metric is None:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°æ€§èƒ½æŒ‡æ ‡\n")
        continue

    print(f"\n   ä¸»è¦æ€§èƒ½æŒ‡æ ‡: {metric_name} = {baseline_metric}")

    # Analyze boundary results
    print(f"\nğŸ“ˆ è¾¹ç•Œæµ‹è¯•ç»“æœ:")
    print(f"   {'é…ç½®æè¿°':<60} {'çŠ¶æ€':<8} {metric_name:<12} {'æ€§èƒ½ä¸‹é™':<10} {'åˆ¤å®š'}")
    print("   " + "-"*105)

    failed_tests = []
    degradation_tests = []

    for result in sorted(results, key=lambda r: (r['hyperparameters'].get('epochs', 0),
                                                   r['hyperparameters'].get('learning_rate', 0))):
        result_hp = result['hyperparameters']

        # Skip baseline
        if all(result_hp.get(k) == baseline_hp.get(k) for k in result_hp.keys()):
            continue

        # Find description from config
        desc = "Unknown"
        for idx, cfg in model_configs[model_key]:
            if all(result_hp.get(k) == cfg['hyperparameters'].get(k) for k in result_hp.keys()):
                desc = cfg.get('description', 'Unknown')
                break

        success = result['training_success']
        perf = result['performance_metrics']

        if not success:
            status_icon = "âŒ å¤±è´¥"
            metric_val = "N/A"
            degradation = "N/A"
            verdict = "â›” è®­ç»ƒå¤±è´¥"
            failed_tests.append((desc, result))
        else:
            status_icon = "âœ… æˆåŠŸ"
            result_metric = perf.get(metric_name, None)

            if result_metric is None:
                metric_val = "N/A"
                degradation = "N/A"
                verdict = "âš ï¸  æ— æŒ‡æ ‡"
            else:
                metric_val = f"{result_metric:.4f}" if isinstance(result_metric, float) else str(result_metric)

                # Calculate degradation
                if baseline_metric > 0:
                    deg_pct = (baseline_metric - result_metric) / baseline_metric * 100
                else:
                    deg_pct = 0

                degradation = f"{deg_pct:+.2f}%"

                # Verdict
                if deg_pct < 0:  # Performance improved
                    verdict = "âœ… æå‡"
                elif deg_pct < 5:
                    verdict = "âœ… ä¼˜ç§€"
                elif deg_pct < 10:
                    verdict = "âš ï¸  è­¦å‘Š"
                else:
                    verdict = "âŒ ä¸å¯æ¥å—"
                    degradation_tests.append((desc, result, deg_pct))

        # Truncate description for display
        short_desc = desc[:58] + ".." if len(desc) > 60 else desc
        print(f"   {short_desc:<60} {status_icon:<8} {metric_val:<12} {degradation:<10} {verdict}")

    # Summary for this model
    print(f"\nğŸ“‹ æ¨¡å‹æ€»ç»“:")
    print(f"   - æ€»æµ‹è¯•æ•°: {len(results)}")
    print(f"   - è®­ç»ƒå¤±è´¥: {len(failed_tests)}")
    print(f"   - æ€§èƒ½ä¸‹é™>10%: {len(degradation_tests)}")

    if failed_tests:
        print(f"\n   âŒ å¤±è´¥çš„æµ‹è¯•:")
        for desc, result in failed_tests:
            print(f"      - {desc}")
            print(f"        è¶…å‚æ•°: {result['hyperparameters']}")
            print(f"        é”™è¯¯: {result.get('error_message', 'Unknown')}")

    if degradation_tests:
        print(f"\n   âš ï¸  æ€§èƒ½ä¸‹é™>10%çš„æµ‹è¯•:")
        for desc, result, deg_pct in degradation_tests:
            print(f"      - {desc}")
            print(f"        è¶…å‚æ•°: {result['hyperparameters']}")
            print(f"        æ€§èƒ½ä¸‹é™: {deg_pct:.2f}%")

    all_models_summary.append({
        'model': model_key,
        'baseline_success': True,
        'baseline_metric': baseline_metric,
        'metric_name': metric_name,
        'tests_completed': len(results),
        'tests_failed': len(failed_tests),
        'tests_degraded': len(degradation_tests)
    })

    print()

# Overall summary
print("="*80)
print("ğŸ¯ æ€»ä½“åˆ†æç»“è®º")
print("="*80)

for summary in all_models_summary:
    print(f"\nğŸ“Œ {summary['model']}:")
    if not summary['baseline_success']:
        print(f"   âš ï¸  Baselineè®­ç»ƒå¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ¨¡å‹é…ç½®")
    else:
        print(f"   âœ… Baseline {summary['metric_name']}: {summary['baseline_metric']}")
        print(f"   ğŸ“Š æµ‹è¯•å®Œæˆ: {summary['tests_completed']}")
        print(f"   âŒ è®­ç»ƒå¤±è´¥: {summary['tests_failed']}")
        print(f"   âš ï¸  æ€§èƒ½ä¸‹é™>10%: {summary['tests_degraded']}")

        if summary['tests_failed'] == 0 and summary['tests_degraded'] == 0:
            print(f"   âœ… ç»“è®º: å½“å‰å˜å¼‚èŒƒå›´åˆç†")
        elif summary['tests_failed'] > 0:
            print(f"   âš ï¸  ç»“è®º: å­˜åœ¨è®­ç»ƒå¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å¤±è´¥åŸå› ")
        else:
            print(f"   âš ï¸  ç»“è®º: å­˜åœ¨æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®ç¼©å°å˜å¼‚èŒƒå›´")

print("\n" + "="*80)
print("ğŸ“ æ ‡å‡†åŒ–å˜å¼‚èŒƒå›´:")
print("="*80)
print("""
| è¶…å‚æ•° | å˜å¼‚èŒƒå›´è¡¨è¾¾å¼ | åˆ†å¸ƒç±»å‹ | è¯´æ˜ |
|--------|---------------|----------|------|
| Epochs | [defaultÃ—0.5, defaultÃ—2.0] | å¯¹æ•°å‡åŒ€ | åŠå€åˆ°ä¸¤å€ |
| Learning Rate | [defaultÃ—0.1, defaultÃ—10.0] | å¯¹æ•°å‡åŒ€ | åå€èŒƒå›´ |
| Weight Decay | [0.0, defaultÃ—100] | 30%é›¶å€¼+70%å¯¹æ•° | å…è®¸æ— æ­£åˆ™åŒ– |
| Dropout | [0.0, 0.7] | å‡åŒ€åˆ†å¸ƒ | ç»å¯¹å€¼èŒƒå›´ |
| Seed | [0, 9999] | å‡åŒ€æ•´æ•° | è¯„ä¼°ç¨³å®šæ€§ |
""")

print("\nâœ… åˆ†æå®Œæˆï¼")
