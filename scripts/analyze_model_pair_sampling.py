#!/usr/bin/env python3
"""
å¹¶å‘è®­ç»ƒè¶…å‚æ•°å½±å“å®éªŒè®¾è®¡ - æ¨¡å‹ç»„åˆé€‰å–æ–¹æ³•åˆ†æ
Concurrent Training Hyperparameter Study - Model Pair Sampling Analysis
"""
import json
import random
from pathlib import Path
from itertools import combinations
import math

# Load model data
with open('config/models_config.json') as f:
    config = json.load(f)

# Load concurrent feasibility analysis results
# Simplified model data based on previous analysis
model_data = {
    'examples_mnist': {'memory': 450, 'gpu_util': 12.0, 'duration': 155, 'category': 'low', 'success_count': 16},
    'examples_mnist_rnn': {'memory': 450, 'gpu_util': 50, 'duration': 180, 'category': 'low', 'success_count': 0},
    'examples_mnist_ff': {'memory': 450, 'gpu_util': 50, 'duration': 180, 'category': 'low', 'success_count': 0},
    'pytorch_resnet_cifar10_resnet20': {'memory': 720, 'gpu_util': 50, 'duration': 300, 'category': 'low', 'success_count': 0},
    'pytorch_resnet_cifar10_resnet32': {'memory': 900, 'gpu_util': 50, 'duration': 350, 'category': 'low', 'success_count': 0},
    'pytorch_resnet_cifar10_resnet44': {'memory': 1080, 'gpu_util': 50, 'duration': 400, 'category': 'low', 'success_count': 0},
    'pytorch_resnet_cifar10_resnet56': {'memory': 1350, 'gpu_util': 50, 'duration': 500, 'category': 'low', 'success_count': 0},
    'Person_reID_baseline_pytorch_densenet121': {'memory': 3300, 'gpu_util': 71.9, 'duration': 2329, 'category': 'high', 'success_count': 7},
    'Person_reID_baseline_pytorch_hrnet18': {'memory': 2250, 'gpu_util': 50, 'duration': 1500, 'category': 'medium', 'success_count': 0},
    'Person_reID_baseline_pytorch_pcb': {'memory': 1800, 'gpu_util': 50, 'duration': 1200, 'category': 'medium', 'success_count': 0},
    'MRT-OAST_default': {'memory': 1950, 'gpu_util': 92.9, 'duration': 1386, 'category': 'medium', 'success_count': 9},
    'VulBERTa_mlp': {'memory': 1350, 'gpu_util': 50, 'duration': 600, 'category': 'low', 'success_count': 0},
    'VulBERTa_cnn': {'memory': 1350, 'gpu_util': 50, 'duration': 600, 'category': 'low', 'success_count': 0},
    'bug-localization-by-dnn-and-rvsm_default': {'memory': 1800, 'gpu_util': 50, 'duration': 800, 'category': 'medium', 'success_count': 0},
    'examples_siamese': {'memory': 1350, 'gpu_util': 50, 'duration': 400, 'category': 'low', 'success_count': 0},
    'examples_word_lm': {'memory': 1350, 'gpu_util': 50, 'duration': 500, 'category': 'low', 'success_count': 0},
}

print("="*100)
print("å¹¶å‘è®­ç»ƒè¶…å‚æ•°å½±å“ç ”ç©¶ - æ¨¡å‹ç»„åˆé€‰å–æ–¹æ³•åˆ†æ")
print("Concurrent Training Hyperparameter Study - Sampling Method Analysis")
print("="*100)

print("\nğŸ“‹ ç ”ç©¶ç›®æ ‡:")
print("""
1. ç ”ç©¶å¹¶å‘è®­ç»ƒæ—¶ï¼Œè¶…å‚æ•°å˜å¼‚å¯¹èƒ½è€—å’Œæ€§èƒ½çš„å½±å“
2. å¯¹æ¯”ï¼šå•ç‹¬è®­ç»ƒ vs å¹¶å‘è®­ç»ƒä¸‹çš„è¶…å‚æ•°æ•æ„Ÿæ€§
3. å‘ç°ï¼šå¹¶å‘æ˜¯å¦æ”¹å˜è¶…å‚æ•°-èƒ½è€—-æ€§èƒ½çš„å…³ç³»
""")

print("\nğŸ”¬ å®éªŒè®¾è®¡:")
print("""
å¯¹äºæ¯ä¸ªæ¨¡å‹ç»„åˆ (ModelA, ModelB):
  - æ§åˆ¶ç»„: ModelAå›ºå®šé»˜è®¤è¶…å‚æ•° + ModelBå›ºå®šé»˜è®¤è¶…å‚æ•° (å¹¶å‘)
  - å®éªŒç»„: ModelAå›ºå®šé»˜è®¤è¶…å‚æ•° + ModelBå˜å¼‚è¶…å‚æ•° (å¹¶å‘)
  - å¯¹ç…§ç»„: ModelBå˜å¼‚è¶…å‚æ•° (å•ç‹¬è®­ç»ƒ)

æµ‹é‡æŒ‡æ ‡:
  - èƒ½è€—: GPUèƒ½è€—ã€CPUèƒ½è€—ã€æ€»èƒ½è€—
  - æ€§èƒ½: å‡†ç¡®ç‡ã€mAPç­‰
  - æ—¶é—´: è®­ç»ƒæ—¶é•¿

åˆ†æç»´åº¦:
  - å¹¶å‘å¯¹è¶…å‚æ•°æ•æ„Ÿæ€§çš„å½±å“
  - ä¸åŒæ¨¡å‹ç»„åˆä¸‹çš„èƒ½è€—å¹²æ‰°
  - æœ€ä¼˜è¶…å‚æ•°åœ¨å¹¶å‘åœºæ™¯ä¸‹æ˜¯å¦æ”¹å˜
""")

# Generate all feasible pairs
all_pairs = []
models = list(model_data.keys())

for i, model1 in enumerate(models):
    for model2 in models[i+1:]:
        data1 = model_data[model1]
        data2 = model_data[model2]

        total_memory = data1['memory'] + data2['memory']
        total_gpu_util = data1['gpu_util'] + data2['gpu_util']

        # Feasibility check
        if total_memory <= 9000:  # 9GB safe limit
            is_complementary = (data1['gpu_util'] > 70 and data2['gpu_util'] < 30) or \
                             (data1['gpu_util'] < 30 and data2['gpu_util'] > 70)

            is_safe = total_gpu_util < 150

            # Calculate diversity score
            memory_diff = abs(data1['memory'] - data2['memory'])
            gpu_diff = abs(data1['gpu_util'] - data2['gpu_util'])

            all_pairs.append({
                'model1': model1,
                'model2': model2,
                'total_memory': total_memory,
                'total_gpu_util': total_gpu_util,
                'is_complementary': is_complementary,
                'is_safe': is_safe,
                'category_combo': f"{data1['category']}+{data2['category']}",
                'memory_diff': memory_diff,
                'gpu_diff': gpu_diff,
                'has_data': data1['success_count'] > 0 or data2['success_count'] > 0,
                'both_have_data': data1['success_count'] > 0 and data2['success_count'] > 0,
            })

print(f"\nğŸ“Š å¯è¡Œçš„æ¨¡å‹ç»„åˆæ€»æ•°: {len(all_pairs)}")

# Analyze category combinations
category_counts = {}
for pair in all_pairs:
    cat = pair['category_combo']
    category_counts[cat] = category_counts.get(cat, 0) + 1

print(f"\næŒ‰ç±»åˆ«ç»„åˆåˆ†å¸ƒ:")
for cat, count in sorted(category_counts.items()):
    print(f"  {cat:<20} {count:>3}ä¸ªç»„åˆ")

print("\n" + "="*100)
print("æ–¹æ³•1: éšæœºæŠ½æ · (Random Sampling)")
print("="*100)
print("""
åŸç†: ä»æ‰€æœ‰å¯è¡Œç»„åˆä¸­å®Œå…¨éšæœºé€‰å–12ä¸ª
ä¼˜ç‚¹:
  âœ… æ— åå·®ï¼Œç»Ÿè®¡æ„ä¹‰æ˜ç¡®
  âœ… ç®€å•æ˜“å®ç°
  âœ… å¯é‡å¤ï¼ˆè®¾ç½®éšæœºç§å­ï¼‰
ç¼ºç‚¹:
  âŒ å¯èƒ½é—æ¼é‡è¦ç»„åˆ
  âŒ å¯èƒ½é›†ä¸­åœ¨æŸäº›ç±»å‹
  âŒ ä¸ä¿è¯è¦†ç›–å¤šæ ·æ€§
é€‚ç”¨åœºæ™¯: åˆæ­¥æ¢ç´¢ï¼Œæ ·æœ¬é‡å¤§æ—¶
""")

random.seed(42)
random_sample = random.sample(all_pairs, 12)
print("\néšæœºæŠ½æ ·ç»“æœ (seed=42):")
for i, pair in enumerate(random_sample, 1):
    print(f"  {i:2}. {pair['model1']:<45} + {pair['model2']:<45} "
          f"[{pair['category_combo']:15}] {pair['total_memory']:4}MB")

print("\n" + "="*100)
print("æ–¹æ³•2: åˆ†å±‚æŠ½æ · (Stratified Sampling)")
print("="*100)
print("""
åŸç†: æŒ‰æ¨¡å‹ç»„åˆç±»å‹åˆ†å±‚ï¼Œä»æ¯å±‚æŒ‰æ¯”ä¾‹æŠ½å–
ä¼˜ç‚¹:
  âœ… ä¿è¯å„ç±»å‹ç»„åˆéƒ½æœ‰ä»£è¡¨
  âœ… è¦†ç›–åº¦é«˜
  âœ… ç»Ÿè®¡å¯é æ€§å¥½
ç¼ºç‚¹:
  âŒ éœ€è¦äº‹å…ˆå®šä¹‰åˆ†å±‚æ ‡å‡†
  âŒ å¯èƒ½é”™è¿‡æç«¯æ¡ˆä¾‹
é€‚ç”¨åœºæ™¯: éœ€è¦å…¨é¢è¦†ç›–ä¸åŒç±»å‹æ—¶
""")

# Stratify by category combination
stratified_sample = []
target_per_category = {
    'low+low': 3,
    'low+medium': 3,
    'low+high': 2,
    'medium+medium': 2,
    'medium+high': 2,
}

for cat, target in target_per_category.items():
    cat_pairs = [p for p in all_pairs if p['category_combo'] == cat]
    if cat_pairs:
        sample_size = min(target, len(cat_pairs))
        random.seed(42)
        stratified_sample.extend(random.sample(cat_pairs, sample_size))

print(f"\nåˆ†å±‚æŠ½æ ·ç»“æœ (ç›®æ ‡12ä¸ªï¼Œå®é™…{len(stratified_sample)}ä¸ª):")
for i, pair in enumerate(stratified_sample, 1):
    print(f"  {i:2}. {pair['model1']:<45} + {pair['model2']:<45} "
          f"[{pair['category_combo']:15}] {pair['total_memory']:4}MB")

print("\n" + "="*100)
print("æ–¹æ³•3: ä»£è¡¨æ€§æŠ½æ · (Representative Sampling)")
print("="*100)
print("""
åŸç†: é€‰æ‹©èƒ½ä»£è¡¨ä¸åŒç»´åº¦çš„å…¸å‹ç»„åˆ
ç»´åº¦:
  - æ˜¾å­˜: ä½+ä½, ä½+ä¸­, ä½+é«˜, ä¸­+ä¸­, ä¸­+é«˜
  - GPUåˆ©ç”¨ç‡: äº’è¡¥å‹, å‡è¡¡å‹, ç«äº‰å‹
  - è®­ç»ƒæ—¶é•¿: å¿«+å¿«, å¿«+æ…¢, æ…¢+æ…¢
ä¼˜ç‚¹:
  âœ… è¦†ç›–å…³é”®åœºæ™¯
  âœ… ç»“æœå¯è§£é‡Šæ€§å¼º
  âœ… é’ˆå¯¹æ€§å¼º
ç¼ºç‚¹:
  âŒ ä¸»è§‚æ€§è¾ƒå¼º
  âŒ å¯èƒ½é—æ¼æœªçŸ¥æ¨¡å¼
é€‚ç”¨åœºæ™¯: éœ€è¦æ·±å…¥ç†è§£ç‰¹å®šåœºæ™¯æ—¶
""")

# Select representative samples
representative_sample = []

# 1. Best complementary pairs (high + low GPU)
complementary = [p for p in all_pairs if p['is_complementary']]
if complementary:
    representative_sample.append(max(complementary, key=lambda x: x['gpu_diff']))  # Most complementary

# 2. Safe balanced pairs (both medium)
balanced = [p for p in all_pairs if 40 < p['total_gpu_util'] < 120 and p['is_safe']]
if balanced:
    representative_sample.append(balanced[0])

# 3. High competition pair (both high GPU, but safe memory)
competitive = [p for p in all_pairs if p['total_gpu_util'] > 120 and p['is_safe']]
if competitive:
    representative_sample.append(competitive[0])

# 4. Extreme memory difference
memory_diverse = sorted(all_pairs, key=lambda x: x['memory_diff'], reverse=True)
representative_sample.append(memory_diverse[0])

# 5. Similar memory
memory_similar = sorted(all_pairs, key=lambda x: x['memory_diff'])
representative_sample.append(memory_similar[0])

# 6-12. Fill with diverse category combinations
remaining_categories = set(['low+low', 'low+medium', 'low+high', 'medium+medium', 'medium+high'])
for cat in remaining_categories:
    cat_pairs = [p for p in all_pairs if p['category_combo'] == cat and p not in representative_sample]
    if cat_pairs and len(representative_sample) < 12:
        # Prefer pairs with existing data
        with_data = [p for p in cat_pairs if p['has_data']]
        if with_data:
            representative_sample.append(with_data[0])
        else:
            representative_sample.append(cat_pairs[0])

# Fill remaining spots
while len(representative_sample) < 12:
    remaining = [p for p in all_pairs if p not in representative_sample]
    if not remaining:
        break
    # Prefer pairs with data
    with_data = [p for p in remaining if p['has_data']]
    if with_data:
        representative_sample.append(with_data[0])
    else:
        representative_sample.append(remaining[0])

print(f"\nä»£è¡¨æ€§æŠ½æ ·ç»“æœ ({len(representative_sample)}ä¸ª):")
for i, pair in enumerate(representative_sample, 1):
    complementary_mark = "âœ…äº’è¡¥" if pair['is_complementary'] else ""
    safe_mark = "âœ…å®‰å…¨" if pair['is_safe'] else "âš ï¸ç«äº‰"
    data_mark = "ğŸ“Šæœ‰æ•°æ®" if pair['has_data'] else "â“æ— æ•°æ®"
    print(f"  {i:2}. {pair['model1']:<45} + {pair['model2']:<45}")
    print(f"      [{pair['category_combo']:15}] {pair['total_memory']:4}MB, GPU:{pair['total_gpu_util']:.1f}% "
          f"{complementary_mark} {safe_mark} {data_mark}")

print("\n" + "="*100)
print("æ–¹æ³•4: æ­£äº¤è®¾è®¡ (Orthogonal Design)")
print("="*100)
print("""
åŸç†: ç³»ç»Ÿæ€§åœ°è¦†ç›–å¤šä¸ªå› ç´ çš„ä¸åŒæ°´å¹³ç»„åˆ
å› ç´ :
  - å› ç´ A (æ˜¾å­˜): ä½ (L1), ä¸­ (L2), é«˜ (L3)
  - å› ç´ B (GPUåˆ©ç”¨ç‡): ä½ (<30%), ä¸­ (30-70%), é«˜ (>70%)
  - å› ç´ C (è®­ç»ƒæ—¶é•¿): å¿« (<500s), ä¸­ (500-1500s), æ…¢ (>1500s)
ä¼˜ç‚¹:
  âœ… æœ€å°å®éªŒæ¬¡æ•°è·å¾—æœ€å¤§ä¿¡æ¯
  âœ… å¯ä»¥åˆ†æå› ç´ äº¤äº’ä½œç”¨
  âœ… ç»Ÿè®¡æ•ˆç‡é«˜
ç¼ºç‚¹:
  âŒ è®¾è®¡å¤æ‚
  âŒ å¯èƒ½é€‰åˆ°ä¸ç°å®çš„ç»„åˆ
é€‚ç”¨åœºæ™¯: éœ€è¦åˆ†æå¤šå› ç´ å½±å“æ—¶
""")

# Simplified orthogonal design: Cover key combinations
orthogonal_sample = []

# Factor combinations (simplified L9 orthogonal array)
factor_combinations = [
    ('low', 'low', 'fast'),      # 1
    ('low', 'medium', 'medium'), # 2
    ('low', 'high', 'slow'),     # 3
    ('medium', 'low', 'medium'), # 4
    ('medium', 'medium', 'slow'),# 5
    ('medium', 'high', 'fast'),  # 6
    ('high', 'low', 'slow'),     # 7
    ('high', 'medium', 'fast'),  # 8
    ('high', 'high', 'medium'),  # 9
]

def classify_duration(duration):
    if duration < 500:
        return 'fast'
    elif duration < 1500:
        return 'medium'
    else:
        return 'slow'

def classify_gpu(gpu_util):
    if gpu_util < 30:
        return 'low'
    elif gpu_util < 70:
        return 'medium'
    else:
        return 'high'

# Match pairs to factor combinations
for mem_cat, gpu_cat, dur_cat in factor_combinations:
    candidates = []
    for pair in all_pairs:
        # Get combined characteristics
        m1_data = model_data[pair['model1']]
        m2_data = model_data[pair['model2']]

        # Average duration category
        avg_dur = (m1_data['duration'] + m2_data['duration']) / 2
        dur_class = classify_duration(avg_dur)

        # Check if matches (roughly)
        if pair['category_combo'].startswith(mem_cat) or pair['category_combo'].endswith(mem_cat):
            # Check GPU util classification
            avg_gpu = pair['total_gpu_util'] / 2
            gpu_class = classify_gpu(avg_gpu)

            if dur_class == dur_cat:
                candidates.append(pair)

    if candidates and len(orthogonal_sample) < 12:
        # Prefer pairs with data
        with_data = [p for p in candidates if p['has_data']]
        if with_data:
            orthogonal_sample.append(with_data[0])
        elif candidates:
            orthogonal_sample.append(candidates[0])

# Fill to 12 if needed
while len(orthogonal_sample) < 12:
    remaining = [p for p in all_pairs if p not in orthogonal_sample]
    if not remaining:
        break
    orthogonal_sample.append(remaining[0])

print(f"\næ­£äº¤è®¾è®¡æŠ½æ ·ç»“æœ ({len(orthogonal_sample)}ä¸ª):")
for i, pair in enumerate(orthogonal_sample, 1):
    print(f"  {i:2}. {pair['model1']:<45} + {pair['model2']:<45} "
          f"[{pair['category_combo']:15}] {pair['total_memory']:4}MB")

print("\n" + "="*100)
print("æ–¹æ³•5: å®ç”¨æ€§æŠ½æ · (Practical Sampling)")
print("="*100)
print("""
åŸç†: é€‰æ‹©å®é™…åº”ç”¨ä¸­æœ€æœ‰ä»·å€¼çš„ç»„åˆ
ä¼˜å…ˆçº§:
  1. å·²æœ‰è®­ç»ƒæ•°æ®çš„æ¨¡å‹ (ä¾¿äºå¯¹æ¯”åˆ†æ)
  2. è®­ç»ƒæ—¶é•¿é€‚ä¸­çš„ç»„åˆ (ä¸è¦å¤ªå¿«æˆ–å¤ªæ…¢)
  3. å®‰å…¨çš„å¹¶å‘ç»„åˆ (é¿å…OOMæˆ–ä¸¥é‡ç«äº‰)
  4. ä¸åŒåº”ç”¨é¢†åŸŸçš„ç»„åˆ (æé«˜æ³›åŒ–æ€§)
ä¼˜ç‚¹:
  âœ… ç»“æœå®ç”¨æ€§å¼º
  âœ… ä¾¿äºåç»­åˆ†æ
  âœ… å‡å°‘å¤±è´¥é£é™©
ç¼ºç‚¹:
  âŒ å¯èƒ½åå‘å·²çŸ¥æ¨¡å‹
  âŒ æ¢ç´¢æ€§ä¸è¶³
é€‚ç”¨åœºæ™¯: èµ„æºæœ‰é™ï¼Œè¿½æ±‚ç¨³å¦¥ç»“æœæ—¶
""")

practical_sample = []

# Priority 1: Both models have training data
both_data = [p for p in all_pairs if p['both_have_data']]
practical_sample.extend(both_data[:4])  # Top 4

# Priority 2: At least one model has data + safe
one_data_safe = [p for p in all_pairs if p['has_data'] and p['is_safe'] and p not in practical_sample]
practical_sample.extend(one_data_safe[:4])  # Next 4

# Priority 3: Complementary pairs
complementary_remaining = [p for p in all_pairs if p['is_complementary'] and p not in practical_sample]
practical_sample.extend(complementary_remaining[:2])  # Next 2

# Priority 4: Fill with diverse safe pairs
diverse_safe = [p for p in all_pairs if p['is_safe'] and p not in practical_sample]
practical_sample.extend(diverse_safe[:2])  # Final 2

print(f"\nå®ç”¨æ€§æŠ½æ ·ç»“æœ ({len(practical_sample)}ä¸ª):")
for i, pair in enumerate(practical_sample, 1):
    data_status = "âœ…âœ…åŒæ–¹æœ‰æ•°æ®" if pair['both_have_data'] else ("âœ…å•æ–¹æœ‰æ•°æ®" if pair['has_data'] else "â“æ— æ•°æ®")
    safe_status = "âœ…å®‰å…¨" if pair['is_safe'] else "âš ï¸ç«äº‰"
    comp_status = "â­äº’è¡¥" if pair['is_complementary'] else ""
    print(f"  {i:2}. {pair['model1']:<45} + {pair['model2']:<45}")
    print(f"      [{pair['category_combo']:15}] {data_status} {safe_status} {comp_status}")

print("\n" + "="*100)
print("ğŸ’¡ æ¨èæ–¹æ¡ˆå¯¹æ¯”")
print("="*100)

methods = [
    ("éšæœºæŠ½æ ·", "æ— åå·®ï¼Œç®€å•", "å¯èƒ½é—æ¼å…³é”®ç»„åˆ", "â­â­â­"),
    ("åˆ†å±‚æŠ½æ ·", "è¦†ç›–å…¨é¢ï¼Œä»£è¡¨æ€§å¼º", "éœ€è¦åˆ†å±‚æ ‡å‡†", "â­â­â­â­"),
    ("ä»£è¡¨æ€§æŠ½æ ·", "é’ˆå¯¹æ€§å¼ºï¼Œå¯è§£é‡Š", "ä¸»è§‚æ€§å¼º", "â­â­â­â­"),
    ("æ­£äº¤è®¾è®¡", "ç»Ÿè®¡æ•ˆç‡é«˜", "è®¾è®¡å¤æ‚", "â­â­â­"),
    ("å®ç”¨æ€§æŠ½æ ·", "ç¨³å¦¥ï¼Œä¾¿äºåˆ†æ", "æ¢ç´¢æ€§ä¸è¶³", "â­â­â­â­â­")
]

print(f"\n{'æ–¹æ³•':<15} {'ä¼˜ç‚¹':<25} {'ç¼ºç‚¹':<25} {'æ¨èåº¦':<10}")
print("-" * 80)
for method, pros, cons, rating in methods:
    print(f"{method:<15} {pros:<25} {cons:<25} {rating:<10}")

print("\n" + "="*100)
print("ğŸ¯ æœ€ç»ˆæ¨è")
print("="*100)
print("""
æ¨èä½¿ç”¨: å®ç”¨æ€§æŠ½æ · + åˆ†å±‚æŠ½æ ·æ··åˆç­–ç•¥

ç†ç”±:
1. å®ç”¨æ€§æŠ½æ ·ä¿è¯å®éªŒæˆåŠŸç‡å’Œåˆ†æå¯è¡Œæ€§
2. åˆ†å±‚æŠ½æ ·ä¿è¯è¦†ç›–ä¸åŒç±»å‹ç»„åˆ
3. ä¸¤è€…ç»“åˆå…¼é¡¾ç¨³å¦¥æ€§å’Œå…¨é¢æ€§

å…·ä½“æ–¹æ¡ˆ:
- 6ä¸ªç»„åˆ: å®ç”¨æ€§æŠ½æ · (ä¼˜å…ˆæœ‰æ•°æ®ã€å®‰å…¨ã€äº’è¡¥çš„ç»„åˆ)
- 6ä¸ªç»„åˆ: åˆ†å±‚æŠ½æ · (è¦†ç›–ä¸åŒç±»å‹ç»„åˆ)
- æ€»è®¡12ä¸ªç»„åˆ

é¢„æœŸæ•ˆæœ:
âœ… æœ‰è¶³å¤Ÿçš„å·²çŸ¥æ¨¡å‹æ•°æ®ä½œä¸ºbaseline
âœ… è¦†ç›–é«˜/ä¸­/ä½æ˜¾å­˜çš„ä¸åŒç»„åˆ
âœ… åŒ…å«äº’è¡¥å‹å’Œç«äº‰å‹ç»„åˆ
âœ… é™ä½å®éªŒå¤±è´¥é£é™©
âœ… ç»“æœå…·æœ‰æ³›åŒ–æ€§
""")

# Generate final recommendation
print("\n" + "="*100)
print("ğŸ“‹ æœ€ç»ˆæ¨èçš„12ä¸ªæ¨¡å‹ç»„åˆ")
print("="*100)

final_recommendation = []

# From practical sampling: Top 6
final_recommendation.extend(practical_sample[:6])

# From stratified sampling: 6 different ones
stratified_remaining = [p for p in stratified_sample if p not in final_recommendation]
final_recommendation.extend(stratified_remaining[:6])

print(f"\nå…±{len(final_recommendation)}ä¸ªç»„åˆ:\n")
for i, pair in enumerate(final_recommendation, 1):
    data_status = "âœ…âœ…" if pair['both_have_data'] else ("âœ…" if pair['has_data'] else "â“")
    safe_status = "âœ…" if pair['is_safe'] else "âš ï¸"
    comp_status = "â­" if pair['is_complementary'] else "  "

    print(f"{i:2}. {pair['model1']:<50}")
    print(f"    + {pair['model2']:<50}")
    print(f"    ç±»å‹:[{pair['category_combo']:15}] æ˜¾å­˜:{pair['total_memory']:4}MB GPU:{pair['total_gpu_util']:5.1f}% "
          f"{data_status}{safe_status}{comp_status}")
    print()

print("å›¾ä¾‹:")
print("  âœ…âœ… = åŒæ–¹éƒ½æœ‰è®­ç»ƒæ•°æ®")
print("  âœ…   = è‡³å°‘ä¸€æ–¹æœ‰è®­ç»ƒæ•°æ®")
print("  â“   = åŒæ–¹éƒ½æ— è®­ç»ƒæ•°æ®")
print("  âœ…   = å®‰å…¨å¹¶å‘ (GPU<150%)")
print("  âš ï¸   = æœ‰ç«äº‰ (GPU>150%)")
print("  â­   = GPUåˆ©ç”¨ç‡äº’è¡¥")

print("\n" + "="*100)
print("åˆ†æå®Œæˆï¼ä½¿ç”¨ -o å‚æ•°å¯ä»¥å¯¼å‡ºJSONé…ç½®")
print("="*100)

# Export to JSON if needed
export_data = {
    'methods': {
        'random': [{'model1': p['model1'], 'model2': p['model2']} for p in random_sample],
        'stratified': [{'model1': p['model1'], 'model2': p['model2']} for p in stratified_sample],
        'representative': [{'model1': p['model1'], 'model2': p['model2']} for p in representative_sample],
        'orthogonal': [{'model1': p['model1'], 'model2': p['model2']} for p in orthogonal_sample],
        'practical': [{'model1': p['model1'], 'model2': p['model2']} for p in practical_sample],
    },
    'recommended': [{'model1': p['model1'], 'model2': p['model2'], 'details': p} for p in final_recommendation]
}

with open('model_pair_sampling_analysis.json', 'w') as f:
    json.dump(export_data, f, indent=2)

print(f"\nç»“æœå·²å¯¼å‡ºåˆ°: model_pair_sampling_analysis.json")
