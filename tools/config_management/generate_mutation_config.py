#!/usr/bin/env python3
"""
å˜å¼‚å®éªŒé…ç½®ç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
ä¸º11ä¸ªæ¨¡å‹ç”Ÿæˆå˜å¼‚å®éªŒé…ç½®ï¼Œæ¯ä¸ªè¶…å‚æ•°å˜å¼‚3æ¬¡ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰
åªæ”¹å˜ä¸€ä¸ªè¶…å‚æ•°ï¼Œå…¶ä»–ä¿æŒé»˜è®¤å€¼ï¼ˆå•å› ç´ å®éªŒè®¾è®¡ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 generate_mutation_config.py [--output CONFIG.json]
"""

import json
import argparse
from pathlib import Path

# é»˜è®¤å€¼é…ç½®ï¼ˆæ¥è‡ªåŸºçº¿æµ‹è¯•ï¼‰
DEFAULT_CONFIGS = {
    "MRT-OAST": {
        "repo": "MRT-OAST",
        "model": "default",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 0.0001,
            "dropout": 0.2,
            "weight_decay": 0.0,
            "seed": 1334
        },
        "mutable": ["epochs", "learning_rate", "dropout", "weight_decay"]
    },
    "bug-localization": {
        "repo": "bug-localization-by-dnn-and-rvsm",
        "model": "default",
        "hyperparameters": {
            "max_iter": 10000,
            "alpha": 1e-05,
            "kfold": 10,
            "seed": 42
        },
        "mutable": ["max_iter", "alpha"]  # ç‰¹æ®Šï¼šmax_iter=epochs, alpha=weight_decay(L2 penalty); ä¸æ”¯æŒlearning_rate
    },
    "resnet20": {
        "repo": "pytorch_resnet_cifar10",
        "model": "resnet20",
        "hyperparameters": {
            "epochs": 200,
            "learning_rate": 0.1,
            "weight_decay": 0.0001,
            "seed": 1334
        },
        "mutable": ["epochs", "learning_rate", "weight_decay"]
    },
    "VulBERTa_mlp": {
        "repo": "VulBERTa",
        "model": "mlp",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 3e-05,
            "weight_decay": 0.0,
            "seed": 42
        },
        "mutable": ["epochs", "learning_rate", "weight_decay"]
    },
    "densenet121": {
        "repo": "Person_reID_baseline_pytorch",
        "model": "densenet121",
        "hyperparameters": {
            "epochs": 60,
            "learning_rate": 0.05,
            "dropout": 0.5,
            "seed": 1334
        },
        "mutable": ["epochs", "learning_rate", "dropout"]
    },
    "hrnet18": {
        "repo": "Person_reID_baseline_pytorch",
        "model": "hrnet18",
        "hyperparameters": {
            "epochs": 60,
            "learning_rate": 0.05,
            "dropout": 0.5,
            "seed": 1334
        },
        "mutable": ["epochs", "learning_rate", "dropout"]
    },
    "pcb": {
        "repo": "Person_reID_baseline_pytorch",
        "model": "pcb",
        "hyperparameters": {
            "epochs": 60,
            "learning_rate": 0.05,
            "dropout": 0.5,
            "seed": 1334
        },
        "mutable": ["epochs", "learning_rate", "dropout"]
    },
    "mnist": {
        "repo": "examples",
        "model": "mnist",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 0.01,
            "batch_size": 32,
            "seed": 1
        },
        "mutable": ["epochs", "learning_rate"]
    },
    "mnist_rnn": {
        "repo": "examples",
        "model": "mnist_rnn",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 0.01,
            "batch_size": 32,
            "seed": 1
        },
        "mutable": ["epochs", "learning_rate"]
    },
    "mnist_ff": {
        "repo": "examples",
        "model": "mnist_ff",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 0.01,
            "batch_size": 32,
            "seed": 1
        },
        "mutable": ["epochs", "learning_rate"]
    },
    "siamese": {
        "repo": "examples",
        "model": "siamese",
        "hyperparameters": {
            "epochs": 10,
            "learning_rate": 0.01,
            "batch_size": 32,
            "seed": 1
        },
        "mutable": ["epochs", "learning_rate"]
    }
}

# å¹¶è¡Œè®­ç»ƒèƒŒæ™¯ä»»åŠ¡é…ç½®
PARALLEL_BACKGROUNDS = {
    "MRT-OAST": {"repo": "examples", "model": "mnist_rnn"},
    "bug-localization": {"repo": "Person_reID_baseline_pytorch", "model": "pcb"},
    "resnet20": {"repo": "examples", "model": "mnist_ff"},
    "VulBERTa_mlp": {"repo": "examples", "model": "mnist"},
    "densenet121": {"repo": "VulBERTa", "model": "mlp"},
    "hrnet18": {"repo": "examples", "model": "mnist_rnn"},
    "pcb": {"repo": "examples", "model": "mnist_rnn"},
    "mnist": {"repo": "VulBERTa", "model": "mlp"},
    "mnist_rnn": {"repo": "Person_reID_baseline_pytorch", "model": "pcb"},
    "mnist_ff": {"repo": "Person_reID_baseline_pytorch", "model": "densenet121"},
    "siamese": {"repo": "Person_reID_baseline_pytorch", "model": "pcb"}
}

def generate_mutation_values(param_name, default_value):
    """
    ä¸ºè¶…å‚æ•°ç”Ÿæˆ3ä¸ªå˜å¼‚å€¼ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰

    è¿”å›: [(value, multiplier_description), ...]
    """
    if param_name in ["epochs", "max_iter"]:
        # Epochs: [0.5Ã—, 1.0Ã—, 1.5Ã—]
        low = max(1, int(default_value * 0.5))
        mid = default_value
        high = int(default_value * 1.5)
        return [
            (low, "0.5Ã—"),
            (mid, "1.0Ã—"),
            (high, "1.5Ã—")
        ]

    elif param_name == "learning_rate" or param_name == "alpha":
        # Learning rate: [0.5Ã—, 1.0Ã—, 2.0Ã—] (log-uniform)
        low = default_value * 0.5
        mid = default_value
        high = default_value * 2.0
        return [
            (low, "0.5Ã—"),
            (mid, "1.0Ã—"),
            (high, "2.0Ã—")
        ]

    elif param_name == "dropout":
        # Dropout: [max(0, default-0.2), default, min(0.4, default+0.2)]
        low = max(0.0, default_value - 0.2)
        mid = default_value
        high = min(0.7, default_value + 0.2)  # 0.7 æ˜¯å¸¸è§ä¸Šé™
        return [
            (low, f"{low:.1f}"),
            (mid, f"{mid:.1f}"),
            (high, f"{high:.1f}")
        ]

    elif param_name == "weight_decay":
        # Weight decay: [0.1Ã—, 1.0Ã—, 10Ã—] (log-uniform)
        if default_value == 0.0:
            # å¦‚æœé»˜è®¤å€¼æ˜¯0ï¼Œåˆ™æµ‹è¯• [0, 0.0001, 0.001]
            return [
                (0.0, "0"),
                (0.0001, "1e-4"),
                (0.001, "1e-3")
            ]
        else:
            low = default_value * 0.1
            mid = default_value
            high = min(0.1, default_value * 10)  # ä¸Šé™0.1
            return [
                (low, "0.1Ã—"),
                (mid, "1.0Ã—"),
                (high, "10Ã—")
            ]

    else:
        # å…¶ä»–å‚æ•°ä¿æŒä¸å˜
        return [(default_value, "default")]

def generate_experiments():
    """ç”Ÿæˆæ‰€æœ‰å˜å¼‚å®éªŒ"""
    experiments = []
    exp_counter = 1

    # éå†11ä¸ªæ¨¡å‹
    for model_name, config in DEFAULT_CONFIGS.items():
        repo = config["repo"]
        model = config["model"]
        default_params = config["hyperparameters"].copy()
        mutable_params = config["mutable"]

        # éå†æ¯ä¸ªå¯å˜å¼‚è¶…å‚æ•°
        for param_name in mutable_params:
            default_value = default_params[param_name]
            mutation_values = generate_mutation_values(param_name, default_value)

            # ä¸ºï¿½ï¿½ä¸ªå˜å¼‚å€¼ç”Ÿæˆä¸€ä¸ªå®éªŒï¼ˆé¡ºåºè®­ç»ƒï¼‰
            for value, description in mutation_values:
                # åˆ›å»ºå˜å¼‚åçš„è¶…å‚æ•°
                mutated_params = default_params.copy()
                mutated_params[param_name] = value

                # é¡ºåºè®­ç»ƒå®éªŒ
                seq_exp = {
                    "mode": "default",
                    "repo": repo,
                    "model": model,
                    "hyperparameters": mutated_params,
                    "note": f"Sequential {exp_counter}: {model_name} - mutate {param_name}={description}"
                }
                experiments.append(seq_exp)
                exp_counter += 1

        # å¹¶è¡Œè®­ç»ƒå®éªŒï¼ˆæ¯ä¸ªå¯å˜å¼‚è¶…å‚æ•°ï¼‰
        for param_name in mutable_params:
            default_value = default_params[param_name]
            mutation_values = generate_mutation_values(param_name, default_value)

            # ä¸ºæ¯ä¸ªå˜å¼‚å€¼ç”Ÿæˆä¸€ä¸ªå¹¶è¡Œå®éªŒ
            for value, description in mutation_values:
                # åˆ›å»ºå˜å¼‚åçš„è¶…å‚æ•°
                mutated_params = default_params.copy()
                mutated_params[param_name] = value

                # è·å–èƒŒæ™¯ä»»åŠ¡é…ç½®
                bg_config = DEFAULT_CONFIGS.get(
                    PARALLEL_BACKGROUNDS[model_name]["model"],
                    DEFAULT_CONFIGS["mnist"]  # fallback
                )

                # èƒŒæ™¯ä»»åŠ¡ä½¿ç”¨é»˜è®¤è¶…å‚æ•°
                if PARALLEL_BACKGROUNDS[model_name]["model"] == "pcb":
                    bg_params = DEFAULT_CONFIGS["pcb"]["hyperparameters"].copy()
                elif PARALLEL_BACKGROUNDS[model_name]["model"] == "densenet121":
                    bg_params = DEFAULT_CONFIGS["densenet121"]["hyperparameters"].copy()
                elif PARALLEL_BACKGROUNDS[model_name]["model"] == "mnist_rnn":
                    bg_params = DEFAULT_CONFIGS["mnist_rnn"]["hyperparameters"].copy()
                elif PARALLEL_BACKGROUNDS[model_name]["model"] == "mnist_ff":
                    bg_params = DEFAULT_CONFIGS["mnist_ff"]["hyperparameters"].copy()
                elif PARALLEL_BACKGROUNDS[model_name]["model"] == "mnist":
                    bg_params = DEFAULT_CONFIGS["mnist"]["hyperparameters"].copy()
                elif PARALLEL_BACKGROUNDS[model_name]["model"] == "mlp":
                    bg_params = DEFAULT_CONFIGS["VulBERTa_mlp"]["hyperparameters"].copy()
                else:
                    bg_params = {"epochs": 10, "learning_rate": 0.01, "seed": 1}

                # å¹¶è¡Œè®­ç»ƒå®éªŒ
                par_exp = {
                    "mode": "parallel",
                    "foreground": {
                        "repo": repo,
                        "model": model,
                        "mode": "default",
                        "hyperparameters": mutated_params
                    },
                    "background": {
                        "repo": PARALLEL_BACKGROUNDS[model_name]["repo"],
                        "model": PARALLEL_BACKGROUNDS[model_name]["model"],
                        "hyperparameters": bg_params
                    },
                    "note": f"Parallel {exp_counter}: {model_name} - mutate {param_name}={description}"
                }
                experiments.append(par_exp)
                exp_counter += 1

    return experiments

def count_experiments_by_model():
    """ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹çš„å®éªŒæ•°é‡"""
    counts = {}
    total_seq = 0
    total_par = 0

    for model_name, config in DEFAULT_CONFIGS.items():
        mutable_count = len(config["mutable"])
        exp_per_model = mutable_count * 3  # æ¯ä¸ªå‚æ•°3ä¸ªå˜å¼‚å€¼
        counts[model_name] = {
            "mutable_params": config["mutable"],
            "experiments_per_mode": exp_per_model,
            "total_experiments": exp_per_model * 2  # sequential + parallel
        }
        total_seq += exp_per_model
        total_par += exp_per_model

    return counts, total_seq, total_par

def estimate_time(baseline_times):
    """
    ä¼°ç®—æ€»è¿è¡Œæ—¶é—´

    baseline_times: dict with model_name -> {"sequential": minutes, "parallel": minutes}
    """
    counts, total_seq, total_par = count_experiments_by_model()

    total_time_seq = 0
    total_time_par = 0

    for model_name, config in DEFAULT_CONFIGS.items():
        exp_count = counts[model_name]["experiments_per_mode"]

        # è·å–è¯¥æ¨¡å‹çš„åŸºçº¿æ—¶é—´
        base_time_seq = baseline_times.get(model_name, {}).get("sequential", 30)  # é»˜è®¤30åˆ†é’Ÿ
        base_time_par = baseline_times.get(model_name, {}).get("parallel", 35)

        # Epochså˜å¼‚ä¼šå½±å“æ—¶é—´ï¼š0.5Ã—çº¦å‡åŠï¼Œ1.5Ã—çº¦å¢åŠ 50%
        # å…¶ä»–è¶…å‚æ•°å˜å¼‚æ—¶é—´åŸºæœ¬ä¸å˜
        # å¹³å‡æ—¶é—´ç³»æ•°ï¼š(0.5 + 1.0 + 1.5) / 3 = 1.0 å¯¹äºepochs
        # å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œå‡è®¾epochså˜å¼‚å 1/3çš„å®éªŒï¼ˆå¦‚æœæœ‰epochsï¼‰

        # ç®€åŒ–ä¼°ç®—ï¼šä½¿ç”¨åŸºçº¿æ—¶é—´çš„å¹³å‡å€¼
        total_time_seq += exp_count * base_time_seq
        total_time_par += exp_count * base_time_par

    total_time = total_time_seq + total_time_par

    return {
        "sequential_minutes": total_time_seq,
        "parallel_minutes": total_time_par,
        "total_minutes": total_time,
        "total_hours": total_time / 60,
        "total_days": total_time / 60 / 24
    }

def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå˜å¼‚å®éªŒé…ç½®æ–‡ä»¶")
    parser.add_argument("--output", "-o",
                       default="settings/mutation_all_models_3x.json",
                       help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--stats-only", action="store_true",
                       help="åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ç”Ÿæˆæ–‡ä»¶")

    args = parser.parse_args()

    # ç»Ÿè®¡å®éªŒæ•°é‡
    counts, total_seq, total_par = count_experiments_by_model()

    print("=" * 80)
    print("å˜å¼‚å®éªŒé…ç½®ç”Ÿæˆå™¨")
    print("=" * 80)
    print()
    print("ğŸ“Š å®éªŒæ•°é‡ç»Ÿè®¡")
    print("-" * 80)
    print(f"{'æ¨¡å‹':<20} {'å¯å˜å¼‚å‚æ•°':<30} {'é¡ºåº':<8} {'å¹¶è¡Œ':<8} {'å°è®¡':<8}")
    print("-" * 80)

    for model_name, count in counts.items():
        params_str = ", ".join(count["mutable_params"])
        exp_per = count["experiments_per_mode"]
        total = count["total_experiments"]
        print(f"{model_name:<20} {params_str:<30} {exp_per:<8} {exp_per:<8} {total:<8}")

    print("-" * 80)
    print(f"{'æ€»è®¡':<20} {'':<30} {total_seq:<8} {total_par:<8} {total_seq + total_par:<8}")
    print()

    # åŸºäºåŸºçº¿æµ‹è¯•çš„å®é™…æ—¶é—´ä¼°ç®—
    baseline_times = {
        "MRT-OAST": {"sequential": 21, "parallel": 22},
        "bug-localization": {"sequential": 15, "parallel": 20},
        "resnet20": {"sequential": 19, "parallel": 19},
        "VulBERTa_mlp": {"sequential": 52, "parallel": 63},
        "densenet121": {"sequential": 54, "parallel": 59},
        "hrnet18": {"sequential": 71, "parallel": 83},
        "pcb": {"sequential": 72, "parallel": 72},
        "mnist": {"sequential": 2, "parallel": 2},
        "mnist_rnn": {"sequential": 4, "parallel": 4},
        "mnist_ff": {"sequential": 0.13, "parallel": 0.12},
        "siamese": {"sequential": 5, "parallel": 8}
    }

    time_est = estimate_time(baseline_times)

    print("â±ï¸  è¿è¡Œæ—¶é—´ä¼°ç®—")
    print("-" * 80)
    print(f"é¡ºåºè®­ç»ƒæ€»æ—¶é•¿:       {time_est['sequential_minutes']:.1f} åˆ†é’Ÿ ({time_est['sequential_minutes']/60:.1f} å°æ—¶)")
    print(f"å¹¶è¡Œè®­ç»ƒæ€»æ—¶é•¿:       {time_est['parallel_minutes']:.1f} åˆ†é’Ÿ ({time_est['parallel_minutes']/60:.1f} å°æ—¶)")
    print(f"æ€»è®¡:               {time_est['total_minutes']:.1f} åˆ†é’Ÿ ({time_est['total_hours']:.1f} å°æ—¶)")
    print(f"é¢„è®¡å¤©æ•°:            {time_est['total_days']:.1f} å¤©")
    print()
    print("âš ï¸  æ³¨æ„:")
    print("  - Epochså˜å¼‚ä¼šå½±å“è¿è¡Œæ—¶é—´ï¼ˆ0.5Ã—çº¦å¿«ä¸€åŠï¼Œ1.5Ã—çº¦æ…¢50%ï¼‰")
    print("  - ä»¥ä¸Šä¼°ç®—åŸºäºé»˜è®¤epochsçš„å¹³å‡æ—¶é—´")
    print("  - å®é™…æ—¶é—´å¯èƒ½åœ¨ Â±30% èŒƒå›´å†…æ³¢åŠ¨")
    print()

    if args.stats_only:
        print("â„¹ï¸  ä»…æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œæœªç”Ÿæˆé…ç½®æ–‡ä»¶")
        print("    ä½¿ç”¨ --output å‚æ•°ç”Ÿæˆé…ç½®æ–‡ä»¶")
        return

    # ç”Ÿæˆå®éªŒé…ç½®
    print("ğŸ”§ ç”Ÿæˆå®éªŒé…ç½®...")
    experiments = generate_experiments()

    # åˆ›å»ºå®Œæ•´é…ç½®
    config = {
        "experiment_name": "mutation_all_models_3x",
        "description": "å®Œæ•´å˜å¼‚æµ‹è¯•ï¼š11ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªè¶…å‚æ•°å˜å¼‚3æ¬¡ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰ï¼Œé¡ºåº+å¹¶è¡Œè®­ç»ƒ",
        "governor": "performance",
        "runs_per_config": 1,
        "max_retries": 2,
        "mode": "mixed",
        "experiments": experiments
    }

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"âœ… é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
    print(f"   æ€»å®éªŒæ•°: {len(experiments)}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
    print()
    print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   export HF_HUB_OFFLINE=1")
    print(f"   sudo -E python3 mutation.py -ec {output_path}")
    print()
    print("=" * 80)

if __name__ == "__main__":
    main()
