#!/usr/bin/env python3
"""
åˆå¹¶summary_old.csvå’Œsummary_new.csvä¸ºraw_data.csvï¼ˆ80åˆ—æ ¼å¼ï¼‰

åŠŸèƒ½:
1. è¯»å–summary_old.csvï¼ˆ93åˆ—ï¼‰å’Œsummary_new.csvï¼ˆ80åˆ—ï¼‰
2. ä»summary_old.csvä¸­æå–80åˆ—ï¼ˆç§»é™¤13ä¸ªbg_hyperparamå’Œbg_energyåˆ—ï¼‰
3. åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶ä¸ºraw_data.csv
4. éªŒè¯æ•°æ®å®Œæ•´æ€§
"""

import csv
import sys
from pathlib import Path

# 80åˆ—æ ‡å‡†æ ¼å¼ï¼ˆæ¥è‡ªsummary_new.csvï¼‰
STANDARD_80_COLUMNS = [
    'experiment_id', 'timestamp', 'repository', 'model', 'training_success',
    'duration_seconds', 'retries', 'hyperparam_alpha', 'hyperparam_batch_size',
    'hyperparam_dropout', 'hyperparam_epochs', 'hyperparam_kfold',
    'hyperparam_learning_rate', 'hyperparam_max_iter', 'hyperparam_seed',
    'hyperparam_weight_decay', 'perf_accuracy', 'perf_best_val_accuracy',
    'perf_map', 'perf_precision', 'perf_rank1', 'perf_rank5', 'perf_recall',
    'perf_test_accuracy', 'perf_test_loss', 'energy_cpu_pkg_joules',
    'energy_cpu_ram_joules', 'energy_cpu_total_joules', 'energy_gpu_avg_watts',
    'energy_gpu_max_watts', 'energy_gpu_min_watts', 'energy_gpu_total_joules',
    'energy_gpu_temp_avg_celsius', 'energy_gpu_temp_max_celsius',
    'energy_gpu_util_avg_percent', 'energy_gpu_util_max_percent',
    'experiment_source', 'num_mutated_params', 'mutated_param', 'mode',
    'error_message', 'fg_repository', 'fg_model', 'fg_duration_seconds',
    'fg_training_success', 'fg_retries', 'fg_error_message',
    'fg_hyperparam_alpha', 'fg_hyperparam_batch_size', 'fg_hyperparam_dropout',
    'fg_hyperparam_epochs', 'fg_hyperparam_kfold', 'fg_hyperparam_learning_rate',
    'fg_hyperparam_max_iter', 'fg_hyperparam_seed', 'fg_hyperparam_weight_decay',
    'fg_perf_accuracy', 'fg_perf_best_val_accuracy', 'fg_perf_map',
    'fg_perf_precision', 'fg_perf_rank1', 'fg_perf_rank5', 'fg_perf_recall',
    'fg_perf_test_accuracy', 'fg_perf_test_loss', 'fg_energy_cpu_pkg_joules',
    'fg_energy_cpu_ram_joules', 'fg_energy_cpu_total_joules',
    'fg_energy_gpu_avg_watts', 'fg_energy_gpu_max_watts', 'fg_energy_gpu_min_watts',
    'fg_energy_gpu_total_joules', 'fg_energy_gpu_temp_avg_celsius',
    'fg_energy_gpu_temp_max_celsius', 'fg_energy_gpu_util_avg_percent',
    'fg_energy_gpu_util_max_percent', 'bg_repository', 'bg_model', 'bg_note',
    'bg_log_directory'
]

# éœ€è¦ä»93åˆ—ä¸­ç§»é™¤çš„13åˆ—
COLUMNS_TO_REMOVE = [
    'bg_hyperparam_batch_size', 'bg_hyperparam_dropout', 'bg_hyperparam_epochs',
    'bg_hyperparam_learning_rate', 'bg_hyperparam_seed', 'bg_hyperparam_weight_decay',
    'bg_energy_cpu_pkg_joules', 'bg_energy_cpu_ram_joules', 'bg_energy_cpu_total_joules',
    'bg_energy_gpu_avg_watts', 'bg_energy_gpu_max_watts', 'bg_energy_gpu_min_watts',
    'bg_energy_gpu_total_joules'
]

def read_csv_file(filepath, expected_cols):
    """è¯»å–CSVæ–‡ä»¶å¹¶éªŒè¯åˆ—æ•°"""
    with open(filepath, 'r') as f:
        reader = csv.DictReader(f)
        header = reader.fieldnames

        if len(header) != expected_cols:
            print(f"âš ï¸  è­¦å‘Š: {filepath} æœ‰ {len(header)} åˆ—ï¼Œé¢„æœŸ {expected_cols} åˆ—")

        rows = list(reader)
        return header, rows

def convert_93_to_80_columns(row_93col, header_93col):
    """å°†93åˆ—æ ¼å¼çš„è¡Œè½¬æ¢ä¸º80åˆ—æ ¼å¼"""
    # åˆ›å»º80åˆ—çš„è¡Œ
    row_80col = {}

    for col in STANDARD_80_COLUMNS:
        if col in header_93col:
            row_80col[col] = row_93col.get(col, '')
        else:
            row_80col[col] = ''

    return row_80col

def merge_csv_files(old_file, new_file, output_file):
    """åˆå¹¶ä¸¤ä¸ªCSVæ–‡ä»¶"""
    print(f"ğŸ“– è¯»å– {old_file}...")
    old_header, old_rows = read_csv_file(old_file, 93)
    print(f"   âœ“ {len(old_rows)} è¡Œæ•°æ®")

    print(f"ğŸ“– è¯»å– {new_file}...")
    new_header, new_rows = read_csv_file(new_file, 80)
    print(f"   âœ“ {len(new_rows)} è¡Œæ•°æ®")

    # éªŒè¯new_fileçš„åˆ—é¡ºåºæ˜¯å¦ä¸æ ‡å‡†ä¸€è‡´
    if new_header != STANDARD_80_COLUMNS:
        print("âš ï¸  è­¦å‘Š: summary_new.csvçš„åˆ—é¡ºåºä¸é¢„æœŸä¸ä¸€è‡´")
        print(f"   é¢„æœŸ: {len(STANDARD_80_COLUMNS)} åˆ—")
        print(f"   å®é™…: {len(new_header)} åˆ—")

    # è½¬æ¢old_rowsä¸º80åˆ—æ ¼å¼
    print(f"ğŸ”„ è½¬æ¢ {old_file} ä»93åˆ—åˆ°80åˆ—æ ¼å¼...")
    old_rows_80col = []
    for row in old_rows:
        row_80col = convert_93_to_80_columns(row, old_header)
        old_rows_80col.append(row_80col)
    print(f"   âœ“ è½¬æ¢å®Œæˆ")

    # åˆå¹¶æ•°æ®
    print(f"ğŸ”— åˆå¹¶æ•°æ®...")
    all_rows = old_rows_80col + new_rows
    print(f"   âœ“ æ€»è®¡ {len(all_rows)} è¡Œæ•°æ®")

    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    print(f"ğŸ’¾ å†™å…¥ {output_file}...")
    with open(output_file, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=STANDARD_80_COLUMNS)
        writer.writeheader()
        writer.writerows(all_rows)
    print(f"   âœ“ å†™å…¥å®Œæˆ")

    return len(old_rows_80col), len(new_rows), len(all_rows)

def validate_merged_file(output_file, expected_old_rows, expected_new_rows):
    """éªŒè¯åˆå¹¶åçš„æ–‡ä»¶"""
    print(f"\nğŸ” éªŒè¯ {output_file}...")

    with open(output_file, 'r') as f:
        reader = csv.DictReader(f)
        header = reader.fieldnames
        rows = list(reader)

    # æ£€æŸ¥åˆ—æ•°
    if len(header) != 80:
        print(f"   âŒ åˆ—æ•°é”™è¯¯: {len(header)}ï¼Œé¢„æœŸ 80")
        return False
    else:
        print(f"   âœ“ åˆ—æ•°æ­£ç¡®: 80åˆ—")

    # æ£€æŸ¥åˆ—å
    if header != STANDARD_80_COLUMNS:
        print(f"   âš ï¸  åˆ—é¡ºåºå¯èƒ½ä¸ä¸€è‡´")
    else:
        print(f"   âœ“ åˆ—é¡ºåºæ­£ç¡®")

    # æ£€æŸ¥è¡Œæ•°
    expected_total = expected_old_rows + expected_new_rows
    if len(rows) != expected_total:
        print(f"   âŒ è¡Œæ•°é”™è¯¯: {len(rows)}ï¼Œé¢„æœŸ {expected_total}")
        return False
    else:
        print(f"   âœ“ è¡Œæ•°æ­£ç¡®: {len(rows)} ({expected_old_rows} è€å®éªŒ + {expected_new_rows} æ–°å®éªŒ)")

    # æ£€æŸ¥experiment_idå”¯ä¸€æ€§
    exp_ids = [row['experiment_id'] for row in rows]
    unique_ids = set(exp_ids)
    if len(exp_ids) != len(unique_ids):
        duplicates = len(exp_ids) - len(unique_ids)
        print(f"   âš ï¸  å‘ç° {duplicates} ä¸ªé‡å¤çš„experiment_id")
    else:
        print(f"   âœ“ æ‰€æœ‰experiment_idå”¯ä¸€")

    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    required_fields = ['experiment_id', 'timestamp', 'repository', 'model', 'training_success']
    missing_count = 0
    for field in required_fields:
        empty_count = sum(1 for row in rows if not row.get(field, '').strip())
        if empty_count > 0:
            print(f"   âš ï¸  {field}: {empty_count} è¡Œä¸ºç©º")
            missing_count += empty_count

    if missing_count == 0:
        print(f"   âœ“ æ‰€æœ‰å¿…å¡«å­—æ®µå®Œæ•´")

    # ç»Ÿè®¡è®­ç»ƒæˆåŠŸç‡
    success_count = sum(1 for row in rows if row.get('training_success', '').lower() == 'true')
    print(f"   â„¹ï¸  è®­ç»ƒæˆåŠŸç‡: {success_count}/{len(rows)} ({success_count/len(rows)*100:.1f}%)")

    # ç»Ÿè®¡æ¨¡å¼åˆ†å¸ƒ
    modes = {}
    for row in rows:
        mode = row.get('mode', 'unknown')
        modes[mode] = modes.get(mode, 0) + 1
    print(f"   â„¹ï¸  æ¨¡å¼åˆ†å¸ƒ:")
    for mode, count in sorted(modes.items()):
        print(f"      - {mode}: {count}")

    print(f"\nâœ… éªŒè¯å®Œæˆ: {output_file} æ•°æ®å®Œæ•´ä¸”å®‰å…¨")
    return True

def main():
    # æ–‡ä»¶è·¯å¾„
    results_dir = Path('/home/green/energy_dl/nightly/results')
    old_file = results_dir / 'summary_old.csv'
    new_file = results_dir / 'summary_new.csv'
    output_file = results_dir / 'raw_data.csv'

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not old_file.exists():
        print(f"âŒ é”™è¯¯: {old_file} ä¸å­˜åœ¨")
        sys.exit(1)

    if not new_file.exists():
        print(f"âŒ é”™è¯¯: {new_file} ä¸å­˜åœ¨")
        sys.exit(1)

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if output_file.exists():
        print(f"âš ï¸  è­¦å‘Š: {output_file} å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")
        backup_file = output_file.with_suffix('.csv.backup_before_merge')
        print(f"   åˆ›å»ºå¤‡ä»½: {backup_file}")
        import shutil
        shutil.copy2(output_file, backup_file)

    print("=" * 60)
    print("åˆå¹¶ summary_old.csv å’Œ summary_new.csv ä¸º raw_data.csv")
    print("=" * 60)

    # æ‰§è¡Œåˆå¹¶
    old_count, new_count, total_count = merge_csv_files(old_file, new_file, output_file)

    # éªŒè¯åˆå¹¶ç»“æœ
    success = validate_merged_file(output_file, old_count, new_count)

    if success:
        print(f"\nğŸ‰ æˆåŠŸåˆ›å»º {output_file}")
        print(f"   - è€å®éªŒ: {old_count} è¡Œ")
        print(f"   - æ–°å®éªŒ: {new_count} è¡Œ")
        print(f"   - æ€»è®¡: {total_count} è¡Œ")
        print(f"   - æ ¼å¼: 80åˆ—æ ‡å‡†æ ¼å¼")
    else:
        print(f"\nâŒ éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶")
        sys.exit(1)

if __name__ == '__main__':
    main()
