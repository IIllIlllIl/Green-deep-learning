#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥æ•°æ®å±æ€§æ˜ å°„å®Œæ•´æ€§

åŠŸèƒ½:
- æ¯”è¾ƒ experiment.json å’Œ raw_data.csv çš„å±æ€§æ˜ å°„
- æ£€æŸ¥æ˜¯å¦æœ‰å±æ€§ç¼ºå¤±
- ç”Ÿæˆè¯¦ç»†çš„å±æ€§å¯¹ç…§è¡¨
"""

import csv
import json
from pathlib import Path
from collections import defaultdict

def flatten_json_keys(data, prefix=''):
    """é€’å½’å±•å¼€JSONé”®ä¸ºæ‰å¹³ç»“æ„"""
    keys = set()

    for key, value in data.items():
        full_key = f"{prefix}{key}" if prefix else key

        if isinstance(value, dict):
            # é€’å½’å±•å¼€åµŒå¥—å­—å…¸
            nested_keys = flatten_json_keys(value, f"{full_key}_")
            keys.update(nested_keys)
        else:
            keys.add(full_key)

    return keys

def main():
    base_dir = Path(__file__).parent.parent
    raw_data_csv = base_dir / "results" / "raw_data.csv"

    # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç›®å½•
    results_dir = base_dir / "results"
    run_dirs = sorted([d for d in results_dir.iterdir() if d.is_dir() and d.name.startswith('run_')],
                     key=lambda x: x.stat().st_mtime, reverse=True)

    if not run_dirs:
        print("âŒ æœªæ‰¾åˆ°è¿è¡Œç›®å½•")
        return

    latest_run_dir = run_dirs[0]

    print("=" * 80)
    print("ğŸ” æ£€æŸ¥æ•°æ®å±æ€§æ˜ å°„å®Œæ•´æ€§")
    print("=" * 80)
    print(f"\næœ€æ–°è¿è¡Œç›®å½•: {latest_run_dir.name}")

    # 1. è¯»å– experiment.json çš„å±æ€§
    print("\n[1/3] åˆ†æ experiment.json çš„å±æ€§ç»“æ„...")

    # æ‰¾ä¸€ä¸ªéå¹¶è¡Œå®éªŒçš„JSONæ–‡ä»¶
    sample_json = None
    for exp_dir in latest_run_dir.iterdir():
        if exp_dir.is_dir():
            exp_json = exp_dir / "experiment.json"
            if exp_json.exists():
                sample_json = exp_json
                break

    if not sample_json:
        print("âŒ æœªæ‰¾åˆ° experiment.json æ–‡ä»¶")
        return

    with open(sample_json, 'r', encoding='utf-8') as f:
        sample_data = json.load(f)

    # å±•å¼€JSONçš„æ‰€æœ‰é”®
    json_keys = flatten_json_keys(sample_data)
    print(f"   experiment.json ä¸­çš„å±æ€§æ•°: {len(json_keys)}")

    # 2. è¯»å– raw_data.csv çš„åˆ—
    print("\n[2/3] åˆ†æ raw_data.csv çš„åˆ—ç»“æ„...")

    with open(raw_data_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        csv_columns = set(reader.fieldnames) if reader.fieldnames else set()

    # ç§»é™¤å‰ç¼€ä¸º fg_ å’Œ bg_ çš„åˆ—ï¼ˆè¿™äº›æ˜¯å¹¶è¡Œæ¨¡å¼ä¸“ç”¨çš„ï¼‰
    non_parallel_csv_columns = {col for col in csv_columns
                                if not col.startswith('fg_') and not col.startswith('bg_')}

    print(f"   raw_data.csv ä¸­çš„åˆ—æ•°: {len(csv_columns)}")
    print(f"   éå¹¶è¡Œæ¨¡å¼ç›¸å…³åˆ—æ•°: {len(non_parallel_csv_columns)}")

    # 3. æ¯”è¾ƒå±æ€§æ˜ å°„
    print("\n[3/3] æ¯”è¾ƒå±æ€§æ˜ å°„...")

    # åˆ›å»ºæ˜ å°„å…³ç³»
    # experiment.json -> raw_data.csv çš„æ˜ å°„è§„åˆ™
    json_to_csv_mapping = {
        # åŸºæœ¬ä¿¡æ¯
        'experiment_id': 'experiment_id',
        'timestamp': 'timestamp',
        'repository': 'repository',
        'model': 'model',
        'training_success': 'training_success',
        'duration_seconds': 'duration_seconds',
        'retries': 'retries',
        'error_message': 'error_message',

        # è¶…å‚æ•° (hyperparameters_*)
        'hyperparameters_alpha': 'hyperparam_alpha',
        'hyperparameters_batch_size': 'hyperparam_batch_size',
        'hyperparameters_dropout': 'hyperparam_dropout',
        'hyperparameters_epochs': 'hyperparam_epochs',
        'hyperparameters_kfold': 'hyperparam_kfold',
        'hyperparameters_learning_rate': 'hyperparam_learning_rate',
        'hyperparameters_max_iter': 'hyperparam_max_iter',
        'hyperparameters_seed': 'hyperparam_seed',
        'hyperparameters_weight_decay': 'hyperparam_weight_decay',

        # èƒ½è€—æŒ‡æ ‡ (energy_metrics_*)
        'energy_metrics_cpu_energy_pkg_joules': 'energy_cpu_pkg_joules',
        'energy_metrics_cpu_energy_ram_joules': 'energy_cpu_ram_joules',
        'energy_metrics_cpu_energy_total_joules': 'energy_cpu_total_joules',
        'energy_metrics_gpu_power_avg_watts': 'energy_gpu_avg_watts',
        'energy_metrics_gpu_power_max_watts': 'energy_gpu_max_watts',
        'energy_metrics_gpu_power_min_watts': 'energy_gpu_min_watts',
        'energy_metrics_gpu_energy_total_joules': 'energy_gpu_total_joules',
        'energy_metrics_gpu_temp_avg_celsius': 'energy_gpu_temp_avg_celsius',
        'energy_metrics_gpu_temp_max_celsius': 'energy_gpu_temp_max_celsius',
        'energy_metrics_gpu_util_avg_percent': 'energy_gpu_util_avg_percent',
        'energy_metrics_gpu_util_max_percent': 'energy_gpu_util_max_percent',

        # æ€§èƒ½æŒ‡æ ‡ (performance_metrics_*)
        'performance_metrics_accuracy': 'perf_accuracy',
        'performance_metrics_best_val_accuracy': 'perf_best_val_accuracy',
        'performance_metrics_map': 'perf_map',
        'performance_metrics_precision': 'perf_precision',
        'performance_metrics_rank1': 'perf_rank1',
        'performance_metrics_rank5': 'perf_rank5',
        'performance_metrics_recall': 'perf_recall',
        'performance_metrics_test_accuracy': 'perf_test_accuracy',
        'performance_metrics_test_loss': 'perf_test_loss',
        'performance_metrics_eval_loss': 'perf_eval_loss',
        'performance_metrics_final_training_loss': 'perf_final_training_loss',
        'performance_metrics_eval_samples_per_second': 'perf_eval_samples_per_second',
        'performance_metrics_top1_accuracy': 'perf_top1_accuracy',
        'performance_metrics_top5_accuracy': 'perf_top5_accuracy',
        'performance_metrics_top10_accuracy': 'perf_top10_accuracy',
        'performance_metrics_top20_accuracy': 'perf_top20_accuracy',
        'performance_metrics_f1': 'perf_f1',
    }

    print("\n" + "=" * 80)
    print("ğŸ“Š å±æ€§æ˜ å°„æ£€æŸ¥")
    print("=" * 80)

    # æ£€æŸ¥æ¯ä¸ªJSONå±æ€§æ˜¯å¦æœ‰å¯¹åº”çš„CSVåˆ—
    missing_in_csv = []
    mapped_correctly = []

    for json_key in sorted(json_keys):
        if json_key in json_to_csv_mapping:
            csv_col = json_to_csv_mapping[json_key]
            if csv_col in csv_columns:
                mapped_correctly.append((json_key, csv_col))
            else:
                missing_in_csv.append((json_key, csv_col, 'æ˜ å°„åˆ—ä¸å­˜åœ¨'))
        else:
            missing_in_csv.append((json_key, None, 'æ— æ˜ å°„è§„åˆ™'))

    print(f"\nâœ… æ­£ç¡®æ˜ å°„çš„å±æ€§: {len(mapped_correctly)}/{len(json_keys)}")
    print(f"âŒ ç¼ºå¤±æˆ–æœªæ˜ å°„çš„å±æ€§: {len(missing_in_csv)}/{len(json_keys)}")

    if missing_in_csv:
        print("\n" + "=" * 80)
        print("âš ï¸  ä»¥ä¸‹ experiment.json å±æ€§ç¼ºå¤±æˆ–æœªæ­£ç¡®æ˜ å°„:")
        print("=" * 80)

        for json_key, csv_col, reason in missing_in_csv:
            if csv_col:
                print(f"  {json_key:<50} -> {csv_col:<40} [{reason}]")
            else:
                print(f"  {json_key:<50} -> {reason}")

    # æ£€æŸ¥CSVä¸­æœ‰å“ªäº›åˆ—ä¸æ¥è‡ªexperiment.json
    print("\n" + "=" * 80)
    print("ğŸ“‹ raw_data.csv ä¸­çš„é¢å¤–åˆ— (ä¸ç›´æ¥æ¥è‡ª experiment.json)")
    print("=" * 80)

    extra_columns = []
    reverse_mapping = {v: k for k, v in json_to_csv_mapping.items()}

    for col in sorted(non_parallel_csv_columns):
        if col not in reverse_mapping.values():
            extra_columns.append(col)

    if extra_columns:
        print(f"\næ‰¾åˆ° {len(extra_columns)} ä¸ªé¢å¤–åˆ—:")
        for col in extra_columns:
            print(f"  - {col}")
    else:
        print("\næ²¡æœ‰é¢å¤–åˆ—")

    # æ˜¾ç¤ºè¯¦ç»†çš„æ˜ å°„è¡¨
    print("\n" + "=" * 80)
    print("ğŸ“– å®Œæ•´å±æ€§æ˜ å°„è¡¨")
    print("=" * 80)

    print(f"\n{'experiment.json':<60} {'raw_data.csv':<40} {'çŠ¶æ€':<10}")
    print("-" * 115)

    for json_key in sorted(json_keys):
        if json_key in json_to_csv_mapping:
            csv_col = json_to_csv_mapping[json_key]
            status = "âœ…" if csv_col in csv_columns else "âŒ"
            print(f"{json_key:<60} {csv_col:<40} {status:<10}")
        else:
            print(f"{json_key:<60} {'[æ— æ˜ å°„]':<40} {'âš ï¸':<10}")

    print("\nâœ… æ£€æŸ¥å®Œæˆ!")

if __name__ == "__main__":
    main()
