#!/usr/bin/env python3
"""
Independent Data Quality Assessment for raw_data.csv
å®Œå…¨ç‹¬ç«‹çš„æ•°æ®è´¨é‡è¯„ä¼°è„šæœ¬
"""

import pandas as pd
import numpy as np
from collections import defaultdict
import json

# è¯»å–æ•°æ®
csv_path = '/home/green/energy_dl/nightly/data/raw_data.csv'
df = pd.read_csv(csv_path)

print("=" * 80)
print("ç‹¬ç«‹æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š")
print("=" * 80)
print(f"\næ•°æ®æ–‡ä»¶: {csv_path}")
print(f"åˆ†ææ—¶é—´: 2026-01-14\n")

# ============================================================
# 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
# ============================================================
print("\n" + "=" * 80)
print("1. åŸºæœ¬æ•°æ®ç»Ÿè®¡")
print("=" * 80)

total_records = len(df)
total_columns = len(df.columns)

print(f"\næ€»è®°å½•æ•°ï¼ˆå«headerï¼‰: {total_records + 1}")
print(f"å®é™…æ•°æ®è®°å½•æ•°: {total_records}")
print(f"æ€»åˆ—æ•°: {total_columns}")

# æ˜¾ç¤ºåˆ—å
print(f"\nåˆ—ååˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰:")
for i, col in enumerate(df.columns[:20], 1):
    print(f"  {i:2d}. {col}")
if total_columns > 20:
    print(f"  ... (è¿˜æœ‰ {total_columns - 20} åˆ—)")

# ============================================================
# 2. å…³é”®å­—æ®µç¼ºå¤±æƒ…å†µåˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("2. å…³é”®å­—æ®µç¼ºå¤±æƒ…å†µåˆ†æ")
print("=" * 80)

# å®šä¹‰å…³é”®å­—æ®µ
key_fields = {
    'æ¨¡å‹æ ‡è¯†': ['repository', 'model'],
    'è®­ç»ƒçŠ¶æ€': ['training_success'],
    'èƒ½è€—æ•°æ®': ['energy_cpu_total_joules', 'energy_gpu_total_joules'],
    'æ€§èƒ½æŒ‡æ ‡': [col for col in df.columns if col.startswith('perf_')]
}

# åˆ†ææ¯ä¸ªç±»åˆ«çš„ç¼ºå¤±æƒ…å†µ
print("\nå…³é”®å­—æ®µç¼ºå¤±ç»Ÿè®¡:")
print(f"\n{'å­—æ®µç±»åˆ«':<15} {'å­—æ®µå':<35} {'éç©ºæ•°':<10} {'ç¼ºå¤±æ•°':<10} {'ç¼ºå¤±ç‡':<10}")
print("-" * 80)

for category, fields in key_fields.items():
    for field in fields:
        if field in df.columns:
            non_null = df[field].notna().sum()
            null_count = df[field].isna().sum()
            null_rate = (null_count / total_records) * 100
            print(f"{category:<15} {field:<35} {non_null:<10} {null_count:<10} {null_rate:>6.2f}%")
        else:
            print(f"{category:<15} {field:<35} {'å­—æ®µä¸å­˜åœ¨'}")

# ============================================================
# 3. æ¨¡å‹åˆ†å¸ƒåˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("3. æ¨¡å‹åˆ†å¸ƒåˆ†æ")
print("=" * 80)

# åˆ›å»ºå¤åˆæ¨¡å‹æ ‡è¯†
df['model_id'] = df['repository'].fillna('/') + '/' + df['model'].fillna('')

# ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹çš„è®°å½•æ•°
model_counts = df['model_id'].value_counts()

print(f"\næ€»æ¨¡å‹æ•°: {len(model_counts)}")
print(f"\næ¨¡å‹è®°å½•æ•°åˆ†å¸ƒï¼ˆTop 15ï¼‰:")
print(f"{'æ¨¡å‹':<50} {'è®°å½•æ•°':<10} {'å æ¯”':<10}")
print("-" * 70)
for model, count in model_counts.head(15).items():
    percentage = (count / total_records) * 100
    print(f"{model:<50} {count:<10} {percentage:>6.2f}%")

# ============================================================
# 4. è®­ç»ƒæˆåŠŸç‡åˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("4. è®­ç»ƒæˆåŠŸç‡åˆ†æ")
print("=" * 80)

# å¤„ç† training_success å­—æ®µ
# åªæŸ¥çœ‹éç©ºè®°å½•
valid_training_records = df[df['training_success'].notna()]
fg_valid_training = df[df['fg_training_success'].notna()]

print(f"\nå‰å°è®­ç»ƒè®°å½•:")
print(f"  æ€»è®°å½•æ•°: {len(valid_training_records)}")
if len(valid_training_records) > 0:
    success_count = (valid_training_records['training_success'] == True).sum() + \
                   (valid_training_records['training_success'] == 'True').sum()
    print(f"  è®­ç»ƒæˆåŠŸ: {success_count} ({success_count/len(valid_training_records)*100:.2f}%)")
    print(f"  è®­ç»ƒå¤±è´¥: {len(valid_training_records) - success_count} ({(len(valid_training_records) - success_count)/len(valid_training_records)*100:.2f}%)")

print(f"\nåå°è®­ç»ƒè®°å½• (fg_*):")
print(f"  æ€»è®°å½•æ•°: {len(fg_valid_training)}")
if len(fg_valid_training) > 0:
    fg_success = (fg_valid_training['fg_training_success'] == True).sum() + \
                 (fg_valid_training['fg_training_success'] == 'True').sum()
    print(f"  è®­ç»ƒæˆåŠŸ: {fg_success} ({fg_success/len(fg_valid_training)*100:.2f}%)")

# ============================================================
# 5. èƒ½è€—æ•°æ®å®Œæ•´æ€§åˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("5. èƒ½è€—æ•°æ®å®Œæ•´æ€§åˆ†æ")
print("=" * 80)

# å®šä¹‰æœ‰èƒ½è€—æ•°æ®çš„æ¡ä»¶
def has_energy_data(row):
    """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„èƒ½è€—æ•°æ®"""
    cpu_valid = pd.notna(row['energy_cpu_total_joules']) and \
                row['energy_cpu_total_joules'] not in ['', 'N/A', 'NA']
    gpu_valid = pd.notna(row['energy_gpu_total_joules']) and \
                row['energy_gpu_total_joules'] not in ['', 'N/A', 'NA']
    return cpu_valid or gpu_valid

# åº”ç”¨åˆ°å‰å°å’Œåå°æ•°æ®
df['has_energy'] = df.apply(has_energy_data, axis=1)

# å‰å°èƒ½è€—æ•°æ®
foreground_records = df[df['repository'].notna()]
fg_with_energy = foreground_records[foreground_records['has_energy']].shape[0]

print(f"\nå‰å°è®­ç»ƒèƒ½è€—æ•°æ®:")
print(f"  æ€»è®°å½•æ•°: {len(foreground_records)}")
print(f"  æœ‰èƒ½è€—æ•°æ®: {fg_with_energy} ({fg_with_energy/len(foreground_records)*100:.2f}%)")
print(f"  ç¼ºå¤±èƒ½è€—: {len(foreground_records) - fg_with_energy} ({(len(foreground_records) - fg_with_energy)/len(foreground_records)*100:.2f}%)")

# æŒ‰æ¨¡å‹åˆ†æèƒ½è€—æ•°æ®ç¼ºå¤±
print(f"\næŒ‰æ¨¡å‹ç»Ÿè®¡èƒ½è€—æ•°æ®å®Œæ•´æ€§ï¼ˆTop 10ç¼ºå¤±æœ€å¤šï¼‰:")
model_energy_stats = foreground_records.groupby('model_id').agg({
    'has_energy': ['count', 'sum']
}).round(2)
model_energy_stats.columns = ['æ€»æ•°', 'æœ‰èƒ½è€—']
model_energy_stats['ç¼ºå¤±'] = model_energy_stats['æ€»æ•°'] - model_energy_stats['æœ‰èƒ½è€—']
model_energy_stats['ç¼ºå¤±ç‡%'] = (model_energy_stats['ç¼ºå¤±'] / model_energy_stats['æ€»æ•°'] * 100).round(2)
model_energy_stats = model_energy_stats.sort_values('ç¼ºå¤±', ascending=False)

print(f"\n{'æ¨¡å‹':<50} {'æ€»æ•°':<8} {'æœ‰èƒ½è€—':<8} {'ç¼ºå¤±':<8} {'ç¼ºå¤±ç‡':<10}")
print("-" * 84)
for model, row in model_energy_stats.head(10).iterrows():
    print(f"{model:<50} {int(row['æ€»æ•°']):<8} {int(row['æœ‰èƒ½è€—']):<8} {int(row['ç¼ºå¤±']):<8} {row['ç¼ºå¤±ç‡%']:>6.2f}%")

# ============================================================
# 6. æ€§èƒ½æŒ‡æ ‡å®Œæ•´æ€§åˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("6. æ€§èƒ½æŒ‡æ ‡å®Œæ•´æ€§åˆ†æ")
print("=" * 80)

# è·å–æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡åˆ—
perf_cols = [col for col in df.columns if col.startswith('perf_')]
print(f"\næ€§èƒ½æŒ‡æ ‡å­—æ®µæ€»æ•°: {len(perf_cols)}")
print(f"æ€§èƒ½æŒ‡æ ‡å­—æ®µåˆ—è¡¨: {', '.join(perf_cols[:10])}" + (f" ... (è¿˜æœ‰{len(perf_cols)-10}ä¸ª)" if len(perf_cols) > 10 else ""))

# å®šä¹‰æœ‰æ€§èƒ½æŒ‡æ ‡çš„æ¡ä»¶
def has_performance_metrics(row):
    """æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆçš„æ€§èƒ½æŒ‡æ ‡"""
    for col in perf_cols:
        if pd.notna(row[col]) and row[col] not in ['', 'N/A', 'NA']:
            return True
    return False

df['has_perf'] = df.apply(has_performance_metrics, axis=1)

# ç»Ÿè®¡æ€§èƒ½æŒ‡æ ‡å®Œæ•´æ€§
fg_with_perf = foreground_records[foreground_records.apply(has_performance_metrics, axis=1)].shape[0]

print(f"\nå‰å°è®­ç»ƒæ€§èƒ½æŒ‡æ ‡:")
print(f"  æ€»è®°å½•æ•°: {len(foreground_records)}")
print(f"  æœ‰æ€§èƒ½æŒ‡æ ‡: {fg_with_perf} ({fg_with_perf/len(foreground_records)*100:.2f}%)")
print(f"  ç¼ºå¤±æ€§èƒ½æŒ‡æ ‡: {len(foreground_records) - fg_with_perf} ({(len(foreground_records) - fg_with_perf)/len(foreground_records)*100:.2f}%)")

# æŒ‰æ¨¡å‹åˆ†ææ€§èƒ½æŒ‡æ ‡ç¼ºå¤±
print(f"\næŒ‰æ¨¡å‹ç»Ÿè®¡æ€§èƒ½æŒ‡æ ‡å®Œæ•´æ€§ï¼ˆTop 10ç¼ºå¤±æœ€å¤šï¼‰:")
foreground_records['has_perf'] = foreground_records.apply(has_performance_metrics, axis=1)
model_perf_stats = foreground_records.groupby('model_id').agg({
    'has_perf': ['count', 'sum']
}).round(2)
model_perf_stats.columns = ['æ€»æ•°', 'æœ‰æ€§èƒ½æŒ‡æ ‡']
model_perf_stats['ç¼ºå¤±'] = model_perf_stats['æ€»æ•°'] - model_perf_stats['æœ‰æ€§èƒ½æŒ‡æ ‡']
model_perf_stats['ç¼ºå¤±ç‡%'] = (model_perf_stats['ç¼ºå¤±'] / model_perf_stats['æ€»æ•°'] * 100).round(2)
model_perf_stats = model_perf_stats.sort_values('ç¼ºå¤±', ascending=False)

print(f"\n{'æ¨¡å‹':<50} {'æ€»æ•°':<8} {'æœ‰æŒ‡æ ‡':<8} {'ç¼ºå¤±':<8} {'ç¼ºå¤±ç‡':<10}")
print("-" * 84)
for model, row in model_perf_stats.head(10).iterrows():
    print(f"{model:<50} {int(row['æ€»æ•°']):<8} {int(row['æœ‰æ€§èƒ½æŒ‡æ ‡']):<8} {int(row['ç¼ºå¤±']):<8} {row['ç¼ºå¤±ç‡%']:>6.2f}%")

# ============================================================
# 7. æ•°æ®å¯ç”¨æ€§ç»¼åˆåˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("7. æ•°æ®å¯ç”¨æ€§ç»¼åˆåˆ†æ")
print("=" * 80)

# å®šä¹‰"å¯ç”¨è®°å½•"æ ‡å‡†
def is_usable_record(row):
    """
    å¯ç”¨è®°å½•æ ‡å‡†ï¼š
    1. training_success = True
    2. æœ‰èƒ½è€—æ•°æ®ï¼ˆCPUæˆ–GPUè‡³å°‘ä¸€ä¸ªï¼‰
    3. æœ‰æ€§èƒ½æŒ‡æ ‡ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰
    """
    # æ£€æŸ¥è®­ç»ƒæˆåŠŸ
    training_success = row['training_success'] in [True, 'True']

    # æ£€æŸ¥èƒ½è€—æ•°æ®
    has_energy = has_energy_data(row)

    # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
    has_perf = has_performance_metrics(row)

    return training_success and has_energy and has_perf

# åº”ç”¨å¯ç”¨æ€§åˆ¤æ–­
foreground_records['is_usable'] = foreground_records.apply(is_usable_record, axis=1)

usable_count = foreground_records['is_usable'].sum()
unusable_count = len(foreground_records) - usable_count

print(f"\næ•°æ®å¯ç”¨æ€§æ€»è§ˆ:")
print(f"  æ€»å‰å°è®°å½•æ•°: {len(foreground_records)}")
print(f"  âœ… å®Œå…¨å¯ç”¨: {usable_count} ({usable_count/len(foreground_records)*100:.2f}%)")
print(f"  âŒ ä¸å¯ç”¨: {unusable_count} ({unusable_count/len(foreground_records)*100:.2f}%)")

# åˆ†æä¸å¯ç”¨åŸå› 
print(f"\nä¸å¯ç”¨åŸå› è¯¦ç»†åˆ†æ:")

# å‡†å¤‡åˆ†ç±»ç»Ÿè®¡
training_failed = foreground_records[~foreground_records['training_success'].isin([True, 'True'])].shape[0]
no_energy = foreground_records[
    foreground_records['training_success'].isin([True, 'True']) &
    ~foreground_records.apply(has_energy_data, axis=1)
].shape[0]
no_perf = foreground_records[
    foreground_records['training_success'].isin([True, 'True']) &
    ~foreground_records.apply(has_performance_metrics, axis=1)
].shape[0]

# ç»„åˆé—®é¢˜ç»Ÿè®¡
training_success_records = foreground_records[foreground_records['training_success'].isin([True, 'True'])]
has_energy_col = training_success_records.apply(has_energy_data, axis=1)
has_perf_col = training_success_records.apply(has_performance_metrics, axis=1)

no_energy_only = ((~has_energy_col) & has_perf_col).sum()
no_perf_only = (has_energy_col & (~has_perf_col)).sum()
no_both = ((~has_energy_col) & (~has_perf_col)).sum()

print(f"  è®­ç»ƒå¤±è´¥: {training_failed} ({training_failed/len(foreground_records)*100:.2f}%)")
print(f"  ç¼ºå¤±èƒ½è€—æ•°æ®ï¼ˆè®­ç»ƒæˆåŠŸï¼‰: {no_energy} ({no_energy/len(foreground_records)*100:.2f}%)")
print(f"  ç¼ºå¤±æ€§èƒ½æŒ‡æ ‡ï¼ˆè®­ç»ƒæˆåŠŸï¼‰: {no_perf} ({no_perf/len(foreground_records)*100:.2f}%)")
print(f"\n  ç»„åˆé—®é¢˜åˆ†æï¼ˆè®­ç»ƒæˆåŠŸçš„è®°å½•ï¼‰:")
print(f"    ä»…ç¼ºèƒ½è€—: {no_energy_only} ({no_energy_only/len(training_success_records)*100:.2f}%)")
print(f"    ä»…ç¼ºæ€§èƒ½æŒ‡æ ‡: {no_perf_only} ({no_perf_only/len(training_success_records)*100:.2f}%)")
print(f"    èƒ½è€—å’Œæ€§èƒ½æŒ‡æ ‡éƒ½ç¼º: {no_both} ({no_both/len(training_success_records)*100:.2f}%)")

# ============================================================
# 8. æŒ‰æ¨¡å‹å¯ç”¨æ€§åˆ†æ
# ============================================================
print("\n" + "=" * 80)
print("8. æŒ‰æ¨¡å‹å¯ç”¨æ€§åˆ†æ")
print("=" * 80)

model_usability = foreground_records.groupby('model_id').agg({
    'is_usable': ['count', 'sum']
}).round(2)
model_usability.columns = ['æ€»æ•°', 'å¯ç”¨æ•°']
model_usability['ä¸å¯ç”¨'] = model_usability['æ€»æ•°'] - model_usability['å¯ç”¨æ•°']
model_usability['å¯ç”¨ç‡%'] = (model_usability['å¯ç”¨æ•°'] / model_usability['æ€»æ•°'] * 100).round(2)
model_usability = model_usability.sort_values('æ€»æ•°', ascending=False)

print(f"\næ¨¡å‹å¯ç”¨æ€§ç»Ÿè®¡ï¼ˆæŒ‰è®°å½•æ•°æ’åºï¼‰:")
print(f"\n{'æ¨¡å‹':<50} {'æ€»æ•°':<8} {'å¯ç”¨':<8} {'ä¸å¯ç”¨':<8} {'å¯ç”¨ç‡':<10}")
print("-" * 84)
for model, row in model_usability.iterrows():
    print(f"{model:<50} {int(row['æ€»æ•°']):<8} {int(row['å¯ç”¨æ•°']):<8} {int(row['ä¸å¯ç”¨']):<8} {row['å¯ç”¨ç‡%']:>6.2f}%")

# é«˜è´¨é‡æ¨¡å‹ï¼ˆ100%å¯ç”¨ç‡ï¼‰
high_quality_models = model_usability[model_usability['å¯ç”¨ç‡%'] == 100.0]
print(f"\nâœ… é«˜è´¨é‡æ¨¡å‹ï¼ˆ100%å¯ç”¨ç‡ï¼‰: {len(high_quality_models)}ä¸ª")
if len(high_quality_models) > 0:
    print(f"   æ€»è®°å½•æ•°: {int(high_quality_models['æ€»æ•°'].sum())}")
    for model, row in high_quality_models.iterrows():
        print(f"   - {model}: {int(row['æ€»æ•°'])}æ¡")

# ============================================================
# 9. å¼‚å¸¸æ•°æ®è¯†åˆ«
# ============================================================
print("\n" + "=" * 80)
print("9. å¼‚å¸¸æ•°æ®è¯†åˆ«")
print("=" * 80)

# è¯†åˆ«å¼‚å¸¸æ¨¡å¼
print(f"\nå¼‚å¸¸æ¨¡å¼æ£€æµ‹:")

# 1. ç©ºæ¨¡å‹å
empty_model = df[(df['repository'].isna()) | (df['model'].isna()) |
                 (df['repository'] == '/') | (df['model'] == '')].shape[0]
print(f"  1. ç©ºæ¨¡å‹åæˆ–'/'æ¨¡å‹: {empty_model}æ¡")

# 2. è®­ç»ƒæˆåŠŸä½†æ— èƒ½è€—æ•°æ®
success_no_energy = foreground_records[
    foreground_records['training_success'].isin([True, 'True']) &
    ~foreground_records.apply(has_energy_data, axis=1)
].shape[0]
print(f"  2. è®­ç»ƒæˆåŠŸä½†æ— èƒ½è€—æ•°æ®: {success_no_energy}æ¡")

# 3. è®­ç»ƒæˆåŠŸä½†æ— æ€§èƒ½æŒ‡æ ‡
success_no_perf = foreground_records[
    foreground_records['training_success'].isin([True, 'True']) &
    ~foreground_records.apply(has_performance_metrics, axis=1)
].shape[0]
print(f"  3. è®­ç»ƒæˆåŠŸä½†æ— æ€§èƒ½æŒ‡æ ‡: {success_no_perf}æ¡")

# 4. æœ‰èƒ½è€—ä½†è®­ç»ƒå¤±è´¥
failed_with_energy = foreground_records[
    ~foreground_records['training_success'].isin([True, 'True']) &
    foreground_records.apply(has_energy_data, axis=1)
].shape[0]
print(f"  4. è®­ç»ƒå¤±è´¥ä½†æœ‰èƒ½è€—æ•°æ®: {failed_with_energy}æ¡")

# 5. å¼‚å¸¸æŒç»­æ—¶é—´
duration_stats = foreground_records['duration_seconds'].describe()
print(f"\n  5. è®­ç»ƒæŒç»­æ—¶é—´ç»Ÿè®¡:")
print(f"     å¹³å‡: {duration_stats['mean']:.2f}ç§’ ({duration_stats['mean']/60:.2f}åˆ†é’Ÿ)")
print(f"     ä¸­ä½æ•°: {duration_stats['50%']:.2f}ç§’")
print(f"     æœ€å°å€¼: {duration_stats['min']:.2f}ç§’")
print(f"     æœ€å¤§å€¼: {duration_stats['max']:.2f}ç§’")

# å¼‚å¸¸çŸ­æˆ–é•¿çš„è®­ç»ƒ
very_short = foreground_records[foreground_records['duration_seconds'] < 60].shape[0]
very_long = foreground_records[foreground_records['duration_seconds'] > 10000].shape[0]
print(f"     å¼‚å¸¸çŸ­(<1åˆ†é’Ÿ): {very_short}æ¡")
print(f"     å¼‚å¸¸é•¿(>2.78å°æ—¶): {very_long}æ¡")

# ============================================================
# 10. æ•°æ®è´¨é‡é—®é¢˜æ€»ç»“
# ============================================================
print("\n" + "=" * 80)
print("10. æ•°æ®è´¨é‡é—®é¢˜æ€»ç»“")
print("=" * 80)

issues = []

# P0 - ä¸¥é‡é—®é¢˜
if no_perf_only > 0:
    issues.append({
        'priority': 'P0',
        'issue': 'æ€§èƒ½æŒ‡æ ‡å¤§é‡ç¼ºå¤±',
        'count': no_perf_only,
        'percentage': f"{no_perf_only/len(foreground_records)*100:.2f}%",
        'impact': 'ä¸¥é‡å½±å“æ•°æ®å¯ç”¨æ€§',
        'fixability': 'å›°éš¾ - éœ€è¦é‡æ–°è¿è¡Œå®éªŒæˆ–ä»æ—¥å¿—æ¢å¤'
    })

if training_failed > len(foreground_records) * 0.1:
    issues.append({
        'priority': 'P0',
        'issue': 'è®­ç»ƒå¤±è´¥ç‡è¿‡é«˜',
        'count': training_failed,
        'percentage': f"{training_failed/len(foreground_records)*100:.2f}%",
        'impact': 'å¤§é‡å®éªŒæ— æ•ˆ',
        'fixability': 'å›°éš¾ - éœ€è¦è°ƒè¯•å¹¶é‡æ–°è¿è¡Œ'
    })

# P1 - é‡è¦é—®é¢˜
if no_energy_only > 0:
    issues.append({
        'priority': 'P1',
        'issue': 'èƒ½è€—æ•°æ®ç¼ºå¤±',
        'count': no_energy_only,
        'percentage': f"{no_energy_only/len(foreground_records)*100:.2f}%",
        'impact': 'å½±å“èƒ½è€—åˆ†æ',
        'fixability': 'ä¸­ç­‰ - å¯èƒ½å¯ä»¥ä»recoverableæ•°æ®æ¢å¤'
    })

# P2 - æ¬¡è¦é—®é¢˜
if empty_model > 0:
    issues.append({
        'priority': 'P2',
        'issue': 'å¼‚å¸¸æ¨¡å‹æ ‡è¯†',
        'count': empty_model,
        'percentage': f"{empty_model/len(df)*100:.2f}%",
        'impact': 'æ•°æ®è´¨é‡ä½',
        'fixability': 'å®¹æ˜“ - æ¸…ç†æˆ–æ ‡è®°'
    })

print(f"\nå‘ç° {len(issues)} ä¸ªä¸»è¦æ•°æ®è´¨é‡é—®é¢˜:\n")
for i, issue in enumerate(issues, 1):
    print(f"{i}. [{issue['priority']}] {issue['issue']}")
    print(f"   å½±å“è®°å½•: {issue['count']}æ¡ ({issue['percentage']})")
    print(f"   å½±å“ç¨‹åº¦: {issue['impact']}")
    print(f"   ä¿®å¤å¯è¡Œæ€§: {issue['fixability']}")
    print()

# ============================================================
# 11. ä¿®å¤å»ºè®®
# ============================================================
print("\n" + "=" * 80)
print("11. ä¿®å¤å»ºè®®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰")
print("=" * 80)

recommendations = [
    {
        'priority': 'P0',
        'action': 'ä¿®å¤æ€§èƒ½æŒ‡æ ‡ç¼ºå¤±é—®é¢˜',
        'steps': [
            '1. æ£€æŸ¥å®éªŒæ—¥å¿—ï¼Œç¡®è®¤æ˜¯å¦æœ‰æ€§èƒ½æŒ‡æ ‡è¾“å‡º',
            '2. è¯†åˆ«å“ªäº›æ¨¡å‹ç³»ç»Ÿæ€§ç¼ºå¤±æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚VulBERTa/mlp, bug-localizationï¼‰',
            '3. åˆ†æä»£ç ï¼Œä¿®å¤æ€§èƒ½æŒ‡æ ‡æ”¶é›†é€»è¾‘',
            '4. è€ƒè™‘é‡æ–°è¿è¡Œå—å½±å“çš„å®éªŒï¼ˆå¦‚æœä¿®å¤å¯è¡Œï¼‰',
            '5. æˆ–è€…åœ¨åˆ†æä¸­æ’é™¤è¿™äº›æ— æ€§èƒ½æŒ‡æ ‡çš„è®°å½•'
        ],
        'expected_impact': f'å¯æ¢å¤ {no_perf_only} æ¡è®°å½•ï¼ˆå¦‚æœå¯ä»¥ä»æ—¥å¿—æå–ï¼‰'
    },
    {
        'priority': 'P1',
        'action': 'æ¢å¤èƒ½è€—æ•°æ®',
        'steps': [
            '1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ recoverable_energy_data.json',
            '2. ä½¿ç”¨ç°æœ‰çš„ repair_missing_energy_data.py è„šæœ¬',
            '3. éªŒè¯ä¿®å¤åçš„æ•°æ®å®Œæ•´æ€§',
            '4. å¤‡ä»½ä¿®å¤å‰åçš„æ•°æ®è¿›è¡Œå¯¹æ¯”'
        ],
        'expected_impact': f'å¯èƒ½æ¢å¤éƒ¨åˆ† {no_energy_only} æ¡ç¼ºå¤±èƒ½è€—çš„è®°å½•'
    },
    {
        'priority': 'P1',
        'action': 'åˆ†æè®­ç»ƒå¤±è´¥åŸå› ',
        'steps': [
            '1. æ”¶é›†æ‰€æœ‰è®­ç»ƒå¤±è´¥è®°å½•çš„ error_message',
            '2. æŒ‰é”™è¯¯ç±»å‹åˆ†ç±»ç»Ÿè®¡',
            '3. ä¿®å¤å¯ä¿®å¤çš„é”™è¯¯ï¼ˆå¦‚é…ç½®é—®é¢˜ã€ä¾èµ–é—®é¢˜ï¼‰',
            '4. å¯¹äºæ— æ³•ä¿®å¤çš„ï¼Œåœ¨åˆ†æä¸­æ’é™¤'
        ],
        'expected_impact': f'ç†è§£ {training_failed} æ¡å¤±è´¥è®°å½•çš„åŸå› '
    },
    {
        'priority': 'P2',
        'action': 'æ¸…ç†å¼‚å¸¸æ•°æ®',
        'steps': [
            '1. è¯†åˆ«å¹¶æ ‡è®°æ‰€æœ‰ "/" æˆ–ç©ºæ¨¡å‹åçš„è®°å½•',
            '2. æ£€æŸ¥è¿™äº›è®°å½•æ˜¯å¦æœ‰ä»»ä½•ä»·å€¼',
            '3. è€ƒè™‘åˆ›å»ºä¸€ä¸ªæ¸…ç†åçš„æ•°æ®é›†ç”¨äºåˆ†æ',
            '4. ä¿ç•™åŸå§‹æ•°æ®ä½œä¸ºå¤‡ä»½'
        ],
        'expected_impact': f'æ¸…ç† {empty_model} æ¡å¼‚å¸¸è®°å½•ï¼Œæå‡æ•°æ®è´¨é‡'
    }
]

for i, rec in enumerate(recommendations, 1):
    print(f"\n{i}. [{rec['priority']}] {rec['action']}")
    print(f"\n   å…·ä½“æ­¥éª¤:")
    for step in rec['steps']:
        print(f"   {step}")
    print(f"\n   é¢„æœŸæ•ˆæœ: {rec['expected_impact']}")
    print()

# ============================================================
# 12. æ•°æ®ä½¿ç”¨å»ºè®®
# ============================================================
print("\n" + "=" * 80)
print("12. æ•°æ®ä½¿ç”¨å»ºè®®")
print("=" * 80)

print(f"\næ ¹æ®æ•°æ®è´¨é‡è¯„ä¼°ï¼Œæ¨èä»¥ä¸‹æ•°æ®ä½¿ç”¨ç­–ç•¥:\n")

# ç­–ç•¥1: é«˜è´¨é‡æ•°æ®é›†
if len(high_quality_models) > 0:
    hq_record_count = int(high_quality_models['æ€»æ•°'].sum())
    print(f"ğŸ“Š ç­–ç•¥1: é«˜è´¨é‡æ•°æ®é›†ï¼ˆæ¨èç”¨äºç²¾ç¡®åˆ†æï¼‰")
    print(f"   èŒƒå›´: ä»…ä½¿ç”¨100%å¯ç”¨ç‡çš„æ¨¡å‹")
    print(f"   æ¨¡å‹æ•°: {len(high_quality_models)}ä¸ª")
    print(f"   è®°å½•æ•°: {hq_record_count}æ¡")
    print(f"   ä¼˜ç‚¹: æ•°æ®å®Œæ•´ï¼Œç»“æœå¯é ")
    print(f"   ç¼ºç‚¹: æ ·æœ¬é‡è¾ƒå°ï¼Œæ¨¡å‹è¦†ç›–æœ‰é™")
    print()

# ç­–ç•¥2: å¹³è¡¡æ•°æ®é›†
balanced_models = model_usability[model_usability['å¯ç”¨ç‡%'] >= 80.0]
if len(balanced_models) > 0:
    balanced_count = int(balanced_models['å¯ç”¨æ•°'].sum())
    print(f"ğŸ“Š ç­–ç•¥2: å¹³è¡¡æ•°æ®é›†ï¼ˆæ¨èç”¨äºä¸€èˆ¬åˆ†æï¼‰")
    print(f"   èŒƒå›´: ä½¿ç”¨å¯ç”¨ç‡â‰¥80%çš„æ¨¡å‹")
    print(f"   æ¨¡å‹æ•°: {len(balanced_models)}ä¸ª")
    print(f"   å¯ç”¨è®°å½•æ•°: {balanced_count}æ¡")
    print(f"   ä¼˜ç‚¹: æ ·æœ¬é‡è¾ƒå¤§ï¼Œè´¨é‡å¯æ¥å—")
    print(f"   ç¼ºç‚¹: å¯èƒ½æœ‰å°‘é‡ä¸å®Œæ•´æ•°æ®")
    print()

# ç­–ç•¥3: æœ€å¤§åŒ–æ•°æ®é›†
print(f"ğŸ“Š ç­–ç•¥3: æœ€å¤§åŒ–æ•°æ®é›†ï¼ˆç”¨äºæ¢ç´¢æ€§åˆ†æï¼‰")
print(f"   èŒƒå›´: ä½¿ç”¨æ‰€æœ‰å¯ç”¨è®°å½•")
print(f"   å¯ç”¨è®°å½•æ•°: {usable_count}æ¡")
print(f"   ä¼˜ç‚¹: æ ·æœ¬é‡æœ€å¤§ï¼Œè¦†ç›–é¢å¹¿")
print(f"   ç¼ºç‚¹: æ•°æ®è´¨é‡å‚å·®ä¸é½")
print()

# ç­–ç•¥4: ç‰¹å®šåˆ†ææ•°æ®é›†
print(f"ğŸ“Š ç­–ç•¥4: ç‰¹å®šåˆ†ææ•°æ®é›†")
print(f"   èƒ½è€—åˆ†æ: ä½¿ç”¨æœ‰èƒ½è€—æ•°æ®çš„è®°å½• ({fg_with_energy}æ¡)")
print(f"   æ€§èƒ½åˆ†æ: ä½¿ç”¨æœ‰æ€§èƒ½æŒ‡æ ‡çš„è®°å½• ({fg_with_perf}æ¡)")
print(f"   ç»¼åˆåˆ†æ: ä½¿ç”¨å®Œå…¨å¯ç”¨çš„è®°å½• ({usable_count}æ¡)")
print()

# ============================================================
# 13. è¾“å‡ºç»Ÿè®¡æ‘˜è¦åˆ°JSON
# ============================================================
summary = {
    'analysis_date': '2026-01-14',
    'total_records': total_records,
    'total_columns': total_columns,
    'foreground_records': len(foreground_records),
    'usable_records': int(usable_count),
    'usability_rate': f"{usable_count/len(foreground_records)*100:.2f}%",
    'training_success_rate': f"{(len(foreground_records) - training_failed)/len(foreground_records)*100:.2f}%",
    'energy_completeness': f"{fg_with_energy/len(foreground_records)*100:.2f}%",
    'performance_completeness': f"{fg_with_perf/len(foreground_records)*100:.2f}%",
    'high_quality_models': len(high_quality_models),
    'high_quality_records': int(high_quality_models['æ€»æ•°'].sum()) if len(high_quality_models) > 0 else 0,
    'issues': issues,
    'top_10_models': model_usability.head(10).to_dict('index')
}

output_json = '/home/green/energy_dl/nightly/data_quality_assessment_summary.json'
with open(output_json, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"\n" + "=" * 80)
print(f"ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {output_json}")
print("=" * 80)
