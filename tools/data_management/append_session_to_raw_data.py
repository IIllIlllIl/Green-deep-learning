#!/usr/bin/env python3
"""
é€šç”¨è„šæœ¬ï¼šä»ä»»æ„sessionç›®å½•æå–å®éªŒæ•°æ®å¹¶è¿½åŠ åˆ°raw_data.csv

ç”¨é€”ï¼š
1. è‡ªåŠ¨ä»æŒ‡å®šsessionç›®å½•æå–æ‰€æœ‰å®éªŒ
2. ä»experiment.jsonå’Œterminal_output.txtæå–å®Œæ•´æ•°æ®
3. å»é‡æ£€æŸ¥ï¼Œåªè¿½åŠ æ–°å®éªŒ
4. è‡ªåŠ¨å¤‡ä»½raw_data.csv
5. éªŒè¯æ•°æ®å®Œæ•´æ€§

ç”¨æ³•ï¼š
    python3 tools/data_management/append_session_to_raw_data.py results/run_YYYYMMDD_HHMMSS
    python3 tools/data_management/append_session_to_raw_data.py results/run_YYYYMMDD_HHMMSS --dry-run
    python3 tools/data_management/append_session_to_raw_data.py results/run_YYYYMMDD_HHMMSS --no-backup

ç‰ˆæœ¬ï¼š1.0
åˆ›å»ºæ—¥æœŸï¼š2025-12-13
"""

import json
import csv
import re
import sys
import argparse
from pathlib import Path
from datetime import datetime

# é»˜è®¤é…ç½®
DEFAULT_RAW_DATA_CSV = Path('data/raw_data.csv')
DEFAULT_MODELS_CONFIG = Path('mutation/models_config.json')


class SessionDataAppender:
    """Sessionæ•°æ®è¿½åŠ å™¨"""

    def __init__(self, session_dir, raw_data_csv=None, models_config_path=None,
                 dry_run=False, create_backup=True, verbose=True):
        """
        åˆå§‹åŒ–

        Args:
            session_dir: sessionç›®å½•è·¯å¾„
            raw_data_csv: raw_data.csvè·¯å¾„ï¼ˆé»˜è®¤ï¼šdata/raw_data.csvï¼‰
            models_config_path: models_config.jsonè·¯å¾„ï¼ˆé»˜è®¤ï¼šmutation/models_config.jsonï¼‰
            dry_run: æ˜¯å¦ä¸ºæµ‹è¯•è¿è¡Œï¼ˆä¸å®é™…å†™å…¥ï¼‰
            create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        self.session_dir = Path(session_dir)
        self.raw_data_csv = Path(raw_data_csv) if raw_data_csv else DEFAULT_RAW_DATA_CSV
        self.models_config_path = Path(models_config_path) if models_config_path else DEFAULT_MODELS_CONFIG
        self.dry_run = dry_run
        self.create_backup = create_backup
        self.verbose = verbose

        # åŠ è½½é…ç½®
        self.models_config = self._load_models_config()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_found': 0,
            'skipped_no_json': 0,
            'skipped_unknown_repo': 0,
            'skipped_duplicate': 0,
            'added': 0
        }

    def _log(self, message):
        """æ‰“å°æ—¥å¿—"""
        if self.verbose:
            print(message)

    def _load_models_config(self):
        """åŠ è½½models_config.json"""
        if not self.models_config_path.exists():
            raise FileNotFoundError(f"Models config not found: {self.models_config_path}")

        with open(self.models_config_path, 'r') as f:
            return json.load(f)['models']

    def _extract_performance_from_terminal_output(self, terminal_output_path, log_patterns):
        """ä»terminal_output.txtæå–æ€§èƒ½æŒ‡æ ‡"""
        if not terminal_output_path.exists():
            return {}

        try:
            with open(terminal_output_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            self._log(f"   âš ï¸  è¯»å–terminal_outputå¤±è´¥: {e}")
            return {}

        metrics = {}
        for metric_name, pattern in log_patterns.items():
            try:
                match = re.search(pattern, content)
                if match:
                    value = float(match.group(1))
                    metrics[f'perf_{metric_name}'] = value
            except (ValueError, IndexError, AttributeError):
                pass

        return metrics

    def _load_experiment_json(self, exp_dir):
        """åŠ è½½experiment.json"""
        json_path = exp_dir / 'experiment.json'
        if not json_path.exists():
            return None

        try:
            with open(json_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            self._log(f"   âš ï¸  åŠ è½½experiment.jsonå¤±è´¥: {e}")
            return None

    def _build_row_from_experiment(self, exp_data, perf_metrics, fieldnames):
        """ä»å®éªŒæ•°æ®æ„å»ºCSVè¡Œ"""
        # åˆå§‹åŒ–æ‰€æœ‰å­—æ®µä¸ºç©º
        row = {key: '' for key in fieldnames}

        # æ£€æŸ¥æ˜¯å¦ä¸ºå¹¶è¡Œæ¨¡å¼
        is_parallel = exp_data.get('mode') == 'parallel'

        if is_parallel:
            # å¹¶è¡Œæ¨¡å¼ï¼šä»foregroundä¸­æå–æ•°æ®
            fg_data = exp_data.get('foreground', {})

            # å¡«å……åŸºç¡€å­—æ®µ
            row['experiment_id'] = exp_data.get('experiment_id', '')
            row['timestamp'] = exp_data.get('timestamp', '')
            row['repository'] = fg_data.get('repository', '')
            row['model'] = fg_data.get('model', '')
            row['training_success'] = str(fg_data.get('training_success', ''))
            row['duration_seconds'] = str(fg_data.get('duration_seconds', ''))
            row['retries'] = str(fg_data.get('retries', 0))
            row['experiment_source'] = exp_data.get('experiment_source', '')  # é¡¶å±‚å­—æ®µ
            row['num_mutated_params'] = str(exp_data.get('num_mutated_params', ''))  # é¡¶å±‚å­—æ®µ
            row['mutated_param'] = exp_data.get('mutated_param', '')  # é¡¶å±‚å­—æ®µ
            row['mode'] = exp_data.get('mode', '')
            row['error_message'] = fg_data.get('error_message', '')

            # å¡«å……è¶…å‚æ•°ï¼ˆä»foregroundï¼‰
            hyperparams = fg_data.get('hyperparameters', {})
            for key, value in hyperparams.items():
                col_name = f'hyperparam_{key}'
                if col_name in fieldnames:
                    row[col_name] = str(value)

            # å¡«å……èƒ½è€—æ•°æ®ï¼ˆä»foreground.energy_metricsï¼‰
            energy = fg_data.get('energy_metrics', {})
            # æ˜ å°„energy_metricsçš„å­—æ®µååˆ°CSVåˆ—å
            energy_mapping = {
                'cpu_energy_pkg_joules': 'energy_cpu_pkg_joules',
                'cpu_energy_ram_joules': 'energy_cpu_ram_joules',
                'cpu_energy_total_joules': 'energy_cpu_total_joules',
                'gpu_power_avg_watts': 'energy_gpu_avg_watts',
                'gpu_power_max_watts': 'energy_gpu_max_watts',
                'gpu_power_min_watts': 'energy_gpu_min_watts',
                'gpu_energy_total_joules': 'energy_gpu_total_joules',
                'gpu_temp_avg_celsius': 'energy_gpu_temp_avg_celsius',
                'gpu_temp_max_celsius': 'energy_gpu_temp_max_celsius',
                'gpu_util_avg_percent': 'energy_gpu_util_avg_percent',
                'gpu_util_max_percent': 'energy_gpu_util_max_percent'
            }
            for src_key, dst_key in energy_mapping.items():
                if src_key in energy and dst_key in fieldnames:
                    row[dst_key] = str(energy[src_key])

            # å¡«å……æ€§èƒ½æ•°æ®ï¼ˆä»foreground.performance_metricså’Œä»terminal_outputæå–çš„ï¼‰
            fg_perf = fg_data.get('performance_metrics', {})
            # å…ˆå¡«å……foregroundä¸­çš„æ€§èƒ½æŒ‡æ ‡
            perf_mapping = {
                'eval_loss': 'perf_eval_loss',
                'final_training_loss': 'perf_final_training_loss',
                'eval_samples_per_second': 'perf_eval_samples_per_second',
                'accuracy': 'perf_accuracy',
                'precision': 'perf_precision',
                'recall': 'perf_recall',
                'f1': 'perf_f1',
                'top1_accuracy': 'perf_top1_accuracy',
                'top5_accuracy': 'perf_top5_accuracy',
                'top10_accuracy': 'perf_top10_accuracy',
                'top20_accuracy': 'perf_top20_accuracy',
                'test_accuracy': 'perf_test_accuracy',
                'test_error': 'perf_test_error',
                'train_error': 'perf_train_error'
            }
            for src_key, dst_key in perf_mapping.items():
                if src_key in fg_perf and dst_key in fieldnames:
                    row[dst_key] = str(fg_perf[src_key])

            # å†å¡«å……ä»terminal_outputæå–çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¯èƒ½ä¼šè¦†ç›–ï¼‰
            for key, value in perf_metrics.items():
                if key in fieldnames:
                    row[key] = str(value)

        else:
            # éå¹¶è¡Œæ¨¡å¼ï¼šç›´æ¥ä»é¡¶å±‚æå–æ•°æ®
            row['experiment_id'] = exp_data.get('experiment_id', '')
            row['timestamp'] = exp_data.get('timestamp', '')
            row['repository'] = exp_data.get('repository', '')
            row['model'] = exp_data.get('model', '')
            row['training_success'] = str(exp_data.get('training_success', ''))
            row['duration_seconds'] = str(exp_data.get('duration_seconds', ''))
            row['retries'] = str(exp_data.get('retries', 0))
            row['experiment_source'] = exp_data.get('experiment_source', '')
            row['num_mutated_params'] = str(exp_data.get('num_mutated_params', ''))
            row['mutated_param'] = exp_data.get('mutated_param', '')
            row['mode'] = exp_data.get('mode', '')
            row['error_message'] = exp_data.get('error_message', '')

            # å¡«å……è¶…å‚æ•°
            hyperparams = exp_data.get('hyperparameters', {})
            for key, value in hyperparams.items():
                col_name = f'hyperparam_{key}'
                if col_name in fieldnames:
                    row[col_name] = str(value)

            # å¡«å……èƒ½è€—æ•°æ®ï¼ˆä½¿ç”¨energy_metricsï¼‰
            energy = exp_data.get('energy_metrics', {})
            # æ˜ å°„energy_metricsçš„å­—æ®µååˆ°CSVåˆ—å
            energy_mapping = {
                'cpu_energy_pkg_joules': 'energy_cpu_pkg_joules',
                'cpu_energy_ram_joules': 'energy_cpu_ram_joules',
                'cpu_energy_total_joules': 'energy_cpu_total_joules',
                'gpu_power_avg_watts': 'energy_gpu_avg_watts',
                'gpu_power_max_watts': 'energy_gpu_max_watts',
                'gpu_power_min_watts': 'energy_gpu_min_watts',
                'gpu_energy_total_joules': 'energy_gpu_total_joules',
                'gpu_temp_avg_celsius': 'energy_gpu_temp_avg_celsius',
                'gpu_temp_max_celsius': 'energy_gpu_temp_max_celsius',
                'gpu_util_avg_percent': 'energy_gpu_util_avg_percent',
                'gpu_util_max_percent': 'energy_gpu_util_max_percent'
            }
            for src_key, dst_key in energy_mapping.items():
                if src_key in energy and dst_key in fieldnames:
                    row[dst_key] = str(energy[src_key])

            # å¡«å……æ€§èƒ½æ•°æ®ï¼ˆä»experiment.jsonçš„performance_metricså’Œterminal_outputï¼‰
            exp_perf = exp_data.get('performance_metrics', {})
            # å…ˆå¡«å……experiment.jsonä¸­çš„æ€§èƒ½æŒ‡æ ‡
            perf_mapping = {
                'eval_loss': 'perf_eval_loss',
                'final_training_loss': 'perf_final_training_loss',
                'eval_samples_per_second': 'perf_eval_samples_per_second',
                'accuracy': 'perf_accuracy',
                'precision': 'perf_precision',
                'recall': 'perf_recall',
                'f1': 'perf_f1',
                'top1_accuracy': 'perf_top1_accuracy',
                'top5_accuracy': 'perf_top5_accuracy',
                'top10_accuracy': 'perf_top10_accuracy',
                'top20_accuracy': 'perf_top20_accuracy',
                'test_accuracy': 'perf_test_accuracy',
                'test_error': 'perf_test_error',
                'train_error': 'perf_train_error'
            }
            for src_key, dst_key in perf_mapping.items():
                if src_key in exp_perf and dst_key in fieldnames:
                    row[dst_key] = str(exp_perf[src_key])

            # å†å¡«å……ä»terminal_outputæå–çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¯èƒ½ä¼šè¦†ç›–ï¼‰
            for key, value in perf_metrics.items():
                if key in fieldnames:
                    row[key] = str(value)

        return row

    def _is_duplicate(self, exp_data, existing_keys):
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å®éªŒ

        ä½¿ç”¨å¤åˆé”®ï¼šexperiment_id + timestamp
        è¿™æ ·å¯ä»¥é¿å…ä¸åŒæ‰¹æ¬¡äº§ç”Ÿç›¸åŒ experiment_id çš„é—®é¢˜

        Args:
            exp_data: å®éªŒæ•°æ®å­—å…¸
            existing_keys: ç°æœ‰å®éªŒçš„å¤åˆé”®é›†åˆ

        Returns:
            bool: æ˜¯å¦ä¸ºé‡å¤å®éªŒ
        """
        exp_id = exp_data.get('experiment_id', '')
        timestamp = exp_data.get('timestamp', '')

        # åˆ›å»ºå¤åˆé”®
        composite_key = f"{exp_id}|{timestamp}"

        return composite_key in existing_keys

    def extract_experiments(self):
        """ä»sessionç›®å½•æå–æ‰€æœ‰å®éªŒ"""
        if not self.session_dir.exists():
            raise FileNotFoundError(f"Session directory not found: {self.session_dir}")

        self._log('=' * 80)
        self._log(f'ä»Sessionæå–å®éªŒ: {self.session_dir.name}')
        self._log('=' * 80)
        self._log('')

        # è¯»å–ç°æœ‰raw_data.csv
        if not self.raw_data_csv.exists():
            raise FileNotFoundError(f"raw_data.csv not found: {self.raw_data_csv}")

        with open(self.raw_data_csv, 'r') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            existing_rows = list(reader)

        # æ„å»ºå¤åˆé”®é›†åˆï¼ˆexperiment_id + timestampï¼‰
        existing_keys = set()
        for row in existing_rows:
            exp_id = row.get('experiment_id', '')
            timestamp = row.get('timestamp', '')
            composite_key = f"{exp_id}|{timestamp}"
            existing_keys.add(composite_key)

        self._log(f'âœ… åŠ è½½ç°æœ‰æ•°æ®: {len(existing_rows)}è¡Œ')
        self._log(f'   ç°æœ‰å®éªŒå”¯ä¸€é”®: {len(existing_keys)}ä¸ª')
        self._log('')

        # éå†sessionç›®å½•
        new_experiments = []

        for exp_dir in sorted(self.session_dir.iterdir()):
            if not exp_dir.is_dir() or exp_dir.name in ['__pycache__', '.git']:
                continue

            self.stats['total_found'] += 1

            # åŠ è½½experiment.json
            exp_data = self._load_experiment_json(exp_dir)
            if not exp_data:
                self._log(f'âš ï¸  è·³è¿‡ {exp_dir.name}: æ— experiment.json')
                self.stats['skipped_no_json'] += 1
                continue

            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if self._is_duplicate(exp_data, existing_keys):
                exp_id = exp_data.get('experiment_id', '')
                self._log(f'âš ï¸  è·³è¿‡ {exp_dir.name}: é‡å¤å®éªŒ ({exp_id})')
                self.stats['skipped_duplicate'] += 1
                continue

            # è·å–log_patterns
            # å¯¹äºå¹¶è¡Œæ¨¡å¼ï¼Œrepositoryå’Œmodelåœ¨foregroundä¸­
            if exp_data.get('mode') == 'parallel':
                repo = exp_data.get('foreground', {}).get('repository')
                model = exp_data.get('foreground', {}).get('model')
            else:
                repo = exp_data.get('repository')
                model = exp_data.get('model')

            if repo not in self.models_config:
                self._log(f'âš ï¸  è·³è¿‡ {exp_dir.name}: ä»“åº“é…ç½®æœªæ‰¾åˆ° ({repo})')
                self.stats['skipped_unknown_repo'] += 1
                continue

            log_patterns = self.models_config[repo].get('performance_metrics', {}).get('log_patterns', {})

            # æå–æ€§èƒ½æ•°æ®
            terminal_output = exp_dir / 'terminal_output.txt'
            perf_metrics = self._extract_performance_from_terminal_output(terminal_output, log_patterns)

            # æ„å»ºè¡Œæ•°æ®
            row = self._build_row_from_experiment(exp_data, perf_metrics, fieldnames)
            new_experiments.append(row)
            self.stats['added'] += 1

            exp_id = exp_data.get('experiment_id', '')
            self._log(f'âœ… {exp_dir.name}:')
            self._log(f'   å®éªŒID: {exp_id}')
            self._log(f'   è®­ç»ƒæˆåŠŸ: {row["training_success"]}')
            self._log(f'   æ€§èƒ½æŒ‡æ ‡: {list(perf_metrics.keys()) if perf_metrics else "æ— "}')
            self._log('')

        return new_experiments, existing_rows, fieldnames

    def append_to_raw_data(self, new_experiments, existing_rows, fieldnames):
        """è¿½åŠ æ–°å®éªŒåˆ°raw_data.csv"""
        if not new_experiments:
            self._log('âš ï¸  æœªæ‰¾åˆ°æ–°å®éªŒï¼Œæ— éœ€æ›´æ–°')
            return False

        self._log(f'=== æ€»ç»“ ===')
        self._log(f'æ‰¾åˆ°ç›®å½•: {self.stats["total_found"]}ä¸ª')
        self._log(f'è·³è¿‡ï¼ˆæ— JSONï¼‰: {self.stats["skipped_no_json"]}ä¸ª')
        self._log(f'è·³è¿‡ï¼ˆæœªçŸ¥ä»“åº“ï¼‰: {self.stats["skipped_unknown_repo"]}ä¸ª')
        self._log(f'è·³è¿‡ï¼ˆé‡å¤ï¼‰: {self.stats["skipped_duplicate"]}ä¸ª')
        self._log(f'æ–°å¢å®éªŒ: {self.stats["added"]}ä¸ª')
        self._log('')

        if self.dry_run:
            self._log('ğŸ” [DRY RUN] æµ‹è¯•è¿è¡Œï¼Œä¸å®é™…å†™å…¥æ–‡ä»¶')
            self._log(f'   å°†æ·»åŠ  {len(new_experiments)} ä¸ªå®éªŒåˆ° raw_data.csv')
            return True

        # å¤‡ä»½
        if self.create_backup:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_path = self.raw_data_csv.parent / f'raw_data.csv.backup_{timestamp}'
            import shutil
            shutil.copy(self.raw_data_csv, backup_path)
            self._log(f'âœ… å·²å¤‡ä»½: {backup_path}')

        # è¿½åŠ æ–°å®éªŒ
        all_rows = existing_rows + new_experiments

        with open(self.raw_data_csv, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(all_rows)

        self._log(f'âœ… å·²æ›´æ–°: {self.raw_data_csv}')
        self._log(f'   åŸå§‹: {len(existing_rows)}è¡Œ')
        self._log(f'   æ–°å¢: {len(new_experiments)}è¡Œ')
        self._log(f'   æ€»è®¡: {len(all_rows)}è¡Œ')
        self._log('')

        # éªŒè¯
        with open(self.raw_data_csv, 'r') as f:
            reader = csv.DictReader(f)
            final_rows = list(reader)

        self._log(f'âœ… éªŒè¯: {len(final_rows)}è¡Œ (é¢„æœŸ{len(all_rows)}è¡Œ)')

        if len(final_rows) == len(all_rows):
            self._log('âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡')
            return True
        else:
            self._log('âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥')
            return False

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        try:
            new_experiments, existing_rows, fieldnames = self.extract_experiments()
            success = self.append_to_raw_data(new_experiments, existing_rows, fieldnames)

            self._log('')
            self._log('=' * 80)
            if success:
                self._log('âœ… å®Œæˆ')
            else:
                self._log('âš ï¸  å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰')
            self._log('=' * 80)

            return success

        except Exception as e:
            self._log(f'âŒ é”™è¯¯: {e}')
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä»sessionç›®å½•æå–å®éªŒæ•°æ®å¹¶è¿½åŠ åˆ°raw_data.csv',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # è¿½åŠ æœ€æ–°sessionçš„å®éªŒ
  python3 tools/data_management/append_session_to_raw_data.py results/run_20251213_100000

  # æµ‹è¯•è¿è¡Œï¼ˆä¸å®é™…å†™å…¥ï¼‰
  python3 tools/data_management/append_session_to_raw_data.py results/run_20251213_100000 --dry-run

  # ä¸åˆ›å»ºå¤‡ä»½
  python3 tools/data_management/append_session_to_raw_data.py results/run_20251213_100000 --no-backup

  # é™é»˜æ¨¡å¼
  python3 tools/data_management/append_session_to_raw_data.py results/run_20251213_100000 --quiet
"""
    )

    parser.add_argument('session_dir', type=str,
                        help='Sessionç›®å½•è·¯å¾„ (ä¾‹å¦‚: results/run_20251213_100000)')
    parser.add_argument('--raw-data-csv', type=str, default=None,
                        help=f'raw_data.csvè·¯å¾„ (é»˜è®¤: {DEFAULT_RAW_DATA_CSV})')
    parser.add_argument('--models-config', type=str, default=None,
                        help=f'models_config.jsonè·¯å¾„ (é»˜è®¤: {DEFAULT_MODELS_CONFIG})')
    parser.add_argument('--dry-run', action='store_true',
                        help='æµ‹è¯•è¿è¡Œï¼Œä¸å®é™…å†™å…¥æ–‡ä»¶')
    parser.add_argument('--no-backup', action='store_true',
                        help='ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--quiet', action='store_true',
                        help='é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º')

    args = parser.parse_args()

    # åˆ›å»ºè¿½åŠ å™¨
    appender = SessionDataAppender(
        session_dir=args.session_dir,
        raw_data_csv=args.raw_data_csv,
        models_config_path=args.models_config,
        dry_run=args.dry_run,
        create_backup=not args.no_backup,
        verbose=not args.quiet
    )

    # è¿è¡Œ
    success = appender.run()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
