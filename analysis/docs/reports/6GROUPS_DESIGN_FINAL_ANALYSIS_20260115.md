# 6åˆ†ç»„è®¾è®¡æœ€ç»ˆåˆ†ææŠ¥å‘Š

**æ—¥æœŸ**: 2026-01-15
**ç›®çš„**: å›ç­”3ä¸ªå…³é”®é—®é¢˜ä»¥å®Œå–„6åˆ†ç»„æ•°æ®ç”Ÿæˆæ–¹æ¡ˆ

---

## é—®é¢˜1: è¯­ä¹‰ç›¸åŒçš„è¶…å‚æ•°åˆå¹¶

### å‘ç°

æ ¹æ® `VARIABLE_EXPANSION_PLAN.md` æ–‡æ¡£,é¡¹ç›®å·²ç»è¯†åˆ«äº†ä»¥ä¸‹è¯­ä¹‰ç­‰ä»·çš„è¶…å‚æ•°:

```python
# L2æ­£åˆ™åŒ–: alpha â‰¡ weight_decay
df['hyperparam_l2_regularization'] = df['hyperparam_weight_decay'].fillna(
    df['hyperparam_alpha']
)
```

**ç»“è®º**: âœ… **å·²è¯†åˆ«ä½†æœªåœ¨6åˆ†ç»„ç”Ÿæˆä¸­å®ç°**

### é—®é¢˜åˆ†æ

å½“å‰6åˆ†ç»„è®¾è®¡(`6GROUPS_DATA_DESIGN_CORRECT_20260115.md`)ä¸­:
- Group 1 (examples): ä½¿ç”¨ `hyperparam_learning_rate`
- Group 3 (VulBERTa): ä½¿ç”¨ `hyperparam_learning_rate`, `hyperparam_weight_decay`
- Group 4 (ResNet): ä½¿ç”¨ `hyperparam_learning_rate`

å¦‚æœä¸è¿›è¡Œè¯­ä¹‰åˆå¹¶:
- `hyperparam_alpha` (93.5%ç¼ºå¤±) çš„æ•°æ®ä¼šè¢«æµªè´¹
- `hyperparam_weight_decay` ä¼šæœ‰å¤§é‡NaNå€¼

### å»ºè®®æ–¹æ¡ˆ

**åœ¨ç”Ÿæˆ6åˆ†ç»„æ•°æ®å‰,å…ˆè¿›è¡Œè¶…å‚æ•°è¯­ä¹‰åˆå¹¶**:

```python
import pandas as pd

def unify_semantic_hyperparams(df):
    """
    ç»Ÿä¸€è¯­ä¹‰ç›¸åŒä½†åç§°ä¸åŒçš„è¶…å‚æ•°
    """
    df = df.copy()

    # 1. L2æ­£åˆ™åŒ–åˆå¹¶ (alpha â‰¡ weight_decay)
    if 'hyperparam_alpha' in df.columns and 'hyperparam_weight_decay' in df.columns:
        df['hyperparam_l2_regularization'] = df['hyperparam_weight_decay'].fillna(
            df['hyperparam_alpha']
        )
        # å¯é€‰: åˆ é™¤åŸå§‹åˆ—ä»¥é¿å…æ··æ·†
        # df = df.drop(['hyperparam_alpha', 'hyperparam_weight_decay'], axis=1)

    # 2. æœªæ¥å¯èƒ½çš„å…¶ä»–åˆå¹¶
    # ä¾‹å¦‚: momentum_sgd â‰¡ momentum (å¦‚æœå‘ç°ç›¸åŒ)

    return df

# ä½¿ç”¨ç¤ºä¾‹
df = pd.read_csv('data/data.csv')
df_unified = unify_semantic_hyperparams(df)
```

**å½±å“åˆ†æ**:
- âœ… å‡å°‘ç¼ºå¤±ç‡: `hyperparam_l2_regularization` çš„ç¼ºå¤±ç‡å°†è¿œä½äº93.5%
- âœ… æ•°æ®ä¿ç•™: æ›´å¤šè¡Œå¯ä»¥ä¿ç•™åœ¨åˆ†æä¸­
- âœ… è¯­ä¹‰æ¸…æ™°: L2æ­£åˆ™åŒ–çš„å«ä¹‰æ›´æ˜ç¡®

---

## é—®é¢˜2: VulBERTaä¸ResNetçš„å¯åˆå¹¶æ€§

### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

è®©æˆ‘æ£€æŸ¥å®é™…æ•°æ®ä¸­çš„æ€§èƒ½æŒ‡æ ‡...

**VulBERTa (group3_vulberta)** ä½¿ç”¨çš„æ€§èƒ½æŒ‡æ ‡:
- âŒ `perf_eval_loss` - è¯„ä¼°æŸå¤±
- âŒ `perf_final_training_loss` - æœ€ç»ˆè®­ç»ƒæŸå¤±
- âŒ `perf_eval_samples_per_second` - è¯„ä¼°ååé‡

**ResNet (group4_resnet)** ä½¿ç”¨çš„æ€§èƒ½æŒ‡æ ‡:
- âŒ `perf_best_val_accuracy` - æœ€ä½³éªŒè¯å‡†ç¡®ç‡
- âŒ `perf_test_accuracy` - æµ‹è¯•å‡†ç¡®ç‡

### å…³é”®å‘ç°

**æ€§èƒ½æŒ‡æ ‡å®Œå…¨ä¸åŒ**:
- VulBERTa: ä½¿ç”¨ **loss** å’Œ **ååé‡**
- ResNet: ä½¿ç”¨ **accuracy**

**è¯­ä¹‰ä¸Šä¸å¯æ¯”è¾ƒ**:
- Lossè¶Šä½è¶Šå¥½ vs Accuracyè¶Šé«˜è¶Šå¥½
- ä¸åŒä»»åŠ¡: æ¼æ´æ£€æµ‹(åˆ†ç±») vs å›¾åƒåˆ†ç±»

### è¶…å‚æ•°å¯¹æ¯”

è®©æˆ‘æ£€æŸ¥VulBERTaå’ŒResNetçš„è¶…å‚æ•°...

æ ¹æ® `6GROUPS_DATA_DESIGN_CORRECT_20260115.md`:

**å…±åŒè¶…å‚æ•°**:
- âœ… `hyperparam_batch_size`
- âœ… `hyperparam_learning_rate`
- âœ… `hyperparam_epochs`
- âœ… `hyperparam_seed`

**VulBERTaç‹¬æœ‰**:
- `hyperparam_weight_decay` (L2æ­£åˆ™åŒ–)
- `hyperparam_warmup_steps`

**ResNetç‹¬æœ‰**:
- æ— (ä½¿ç”¨ç›¸åŒçš„åŸºç¡€è¶…å‚æ•°)

### ç»“è®º

âŒ **ä¸å»ºè®®åˆå¹¶VulBERTaå’ŒResNet**

**åŸå› **:
1. **æ€§èƒ½æŒ‡æ ‡ä¸å…¼å®¹**: loss vs accuracy æ— æ³•åœ¨åŒä¸€æ¨¡å‹ä¸­åˆ†æ
2. **ä»»åŠ¡ç±»å‹ä¸åŒ**: NLPæ¼æ´æ£€æµ‹ vs å›¾åƒåˆ†ç±»
3. **è¶…å‚æ•°å·®å¼‚**: VulBERTaæœ‰é¢å¤–çš„weight_decayå’Œwarmup_steps

**æ¨èåšæ³•**:
- ä¿æŒå½“å‰çš„åˆ†ç»„ç­–ç•¥: Group 3 (VulBERTa) å’Œ Group 4 (ResNet) åˆ†ç¦»
- å¦‚æœè¿›è¡Œè¶…å‚æ•°è¯­ä¹‰åˆå¹¶,ResNetå¯ä»¥æ·»åŠ  `hyperparam_l2_regularization` åˆ—(å€¼ä¸ºNaNæˆ–é»˜è®¤å€¼)

---

## é—®é¢˜3: æ¨¡å‹ä½œä¸ºå˜é‡åŠ å…¥DiBSåˆ†æ

### èƒŒæ™¯

å½“å‰6åˆ†ç»„è®¾è®¡ä¸­,æ¯ç»„åŒ…å«ä¸åŒçš„æ¨¡å‹:
- Group 1: mnist, mnist_rnn, siamese, mnist_ff (4ä¸ªæ¨¡å‹)
- Group 2: densenet121, hrnet18, pcb (3ä¸ªæ¨¡å‹)
- Group 3: VulBERTa/mlp (1ä¸ªæ¨¡å‹)
- Group 4: resnet20 (1ä¸ªæ¨¡å‹)
- Group 5: MRT-OAST/mtfa (1ä¸ªæ¨¡å‹)
- Group 6: bug-localization/rvsm (1ä¸ªæ¨¡å‹)

### æ–¹æ¡ˆåˆ†æ

#### æ–¹æ¡ˆA: One-Hotç¼–ç  (æ¨è) â­

**å®ç°**:
```python
# ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºäºŒå…ƒæŒ‡ç¤ºå˜é‡
df_with_model = pd.get_dummies(df, columns=['model'], prefix='model')

# ç»“æœç¤ºä¾‹:
# model_mnist | model_mnist_rnn | model_siamese | model_mnist_ff | ...
#      1      |        0         |       0       |       0        | ...
#      0      |        1         |       0       |       0        | ...
```

**ä¼˜ç‚¹**:
- âœ… ç¬¦åˆDiBSçš„è¿ç»­å˜é‡è¦æ±‚(äºŒå…ƒå˜é‡)
- âœ… å¯ä»¥è¯†åˆ«æ¨¡å‹ç‰¹å®šçš„å› æœæ•ˆåº”
- âœ… æ˜“äºè§£é‡Š: æ¯ä¸ªç³»æ•°ä»£è¡¨è¯¥æ¨¡å‹çš„å¢é‡æ•ˆåº”

**ç¼ºç‚¹**:
- âš ï¸ å¢åŠ å˜é‡æ•°é‡(Group 1ä¼šå¢åŠ 4ä¸ªå˜é‡)
- âš ï¸ å¯èƒ½å¯¼è‡´å¤šé‡å…±çº¿æ€§(æ‰€æœ‰model_*å˜é‡ä¹‹å’Œ=1)

**æ”¹è¿›**: ä½¿ç”¨n-1ç¼–ç (å»æ‰ä¸€ä¸ªå‚è€ƒç±»åˆ«)
```python
# å»æ‰ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºåŸºå‡†
df_with_model = pd.get_dummies(df, columns=['model'], prefix='model', drop_first=True)
```

#### æ–¹æ¡ˆB: åºæ•°ç¼–ç  (ä¸æ¨è)

**å®ç°**:
```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['model_encoded'] = le.fit_transform(df['model'])
# mnist=0, mnist_rnn=1, siamese=2, mnist_ff=3
```

**ç¼ºç‚¹**:
- âŒ å‡è®¾æ¨¡å‹ä¹‹é—´æœ‰é¡ºåºå…³ç³»(mnist < mnist_rnn < ...)
- âŒ ä¸ç¬¦åˆå®é™…: æ¨¡å‹æ˜¯åˆ†ç±»å˜é‡,æ— é¡ºåº
- âŒ DiBSä¼šé”™è¯¯åœ°å­¦ä¹ "modelå¢åŠ 1å¯¼è‡´èƒ½è€—å˜åŒ–X"

#### æ–¹æ¡ˆC: åµŒå…¥ç¼–ç  (å¤æ‚,ä¸æ¨è)

ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹åµŒå…¥è¡¨ç¤º,è¿‡äºå¤æ‚ä¸”éš¾ä»¥è§£é‡Šã€‚

### æ¨èå®ç°

**Step 1: æ•°æ®å‡†å¤‡**
```python
import pandas as pd

def prepare_dibs_data_with_model(group_df, group_name):
    """
    ä¸ºDiBSåˆ†æå‡†å¤‡æ•°æ®,åŒ…å«æ¨¡å‹å˜é‡

    å‚æ•°:
        group_df: åˆ†ç»„æ•°æ®æ¡†
        group_name: åˆ†ç»„åç§°(å¦‚ 'group1_examples')

    è¿”å›:
        prepared_df: å‡†å¤‡å¥½çš„æ•°æ®æ¡†
        model_vars: æ¨¡å‹å˜é‡ååˆ—è¡¨
    """
    df = group_df.copy()

    # 1. One-hotç¼–ç æ¨¡å‹(n-1ç¼–ç ,å»æ‰ç¬¬ä¸€ä¸ªä½œä¸ºåŸºå‡†)
    model_dummies = pd.get_dummies(df['model'], prefix='model', drop_first=True)
    model_vars = model_dummies.columns.tolist()

    # 2. åˆå¹¶åˆ°åŸæ•°æ®
    df = pd.concat([df, model_dummies], axis=1)

    # 3. é€‰æ‹©DiBSéœ€è¦çš„åˆ—
    dibs_cols = (
        # èƒ½è€—å˜é‡
        [col for col in df.columns if col.startswith('energy_')] +
        # æ§åˆ¶å˜é‡
        ['is_parallel', 'timestamp'] +
        # æ¨¡å‹å˜é‡
        model_vars +
        # è¶…å‚æ•°
        [col for col in df.columns if col.startswith('hyperparam_')] +
        # æ€§èƒ½æŒ‡æ ‡
        [col for col in df.columns if col.startswith('perf_')]
    )

    prepared_df = df[dibs_cols].copy()

    return prepared_df, model_vars

# ä½¿ç”¨ç¤ºä¾‹
group1_df = pd.read_csv('analysis/data/energy_research/6groups/group1_examples.csv')
prepared_df, model_vars = prepare_dibs_data_with_model(group1_df, 'group1_examples')

print(f"æ¨¡å‹å˜é‡: {model_vars}")
# è¾“å‡º: ['model_mnist_rnn', 'model_siamese', 'model_mnist_ff']
# (mnistä½œä¸ºåŸºå‡†è¢«çœç•¥)
```

**Step 2: DiBSåˆ†ææ—¶çš„è§£é‡Š**

```python
# åœ¨DiBSç»“æœä¸­,æ¨¡å‹å˜é‡çš„å› æœæ•ˆåº”è§£é‡Šä¸º:
# "ç›¸å¯¹äºåŸºå‡†æ¨¡å‹(mnist),ä½¿ç”¨æ¨¡å‹Xå¯¹èƒ½è€—çš„å¢é‡å½±å“"

# ä¾‹å¦‚: model_mnist_rnn â†’ energy_gpu_mean çš„ç³»æ•°ä¸º +50
# è§£é‡Š: ç›¸æ¯”mnist,ä½¿ç”¨mnist_rnnä¼šä½¿GPUå¹³å‡èƒ½è€—å¢åŠ 50å•ä½
```

### æ³¨æ„äº‹é¡¹

1. **æ ·æœ¬é‡è¦æ±‚**:
   - æ¯ä¸ªæ¨¡å‹è‡³å°‘éœ€è¦30-50ä¸ªæ ·æœ¬ä»¥è·å¾—ç¨³å®šä¼°è®¡
   - æ£€æŸ¥Group 1ä¸­æ¯ä¸ªæ¨¡å‹çš„æ ·æœ¬æ•°

2. **äº¤äº’æ•ˆåº”**:
   - æ¨¡å‹å˜é‡å¯èƒ½ä¸è¶…å‚æ•°å­˜åœ¨äº¤äº’(å¦‚æŸäº›æ¨¡å‹å¯¹learning_rateæ›´æ•æ„Ÿ)
   - DiBSå¯ä»¥è‡ªåŠ¨å‘ç°è¿™äº›äº¤äº’

3. **å› æœå›¾è§£é‡Š**:
   - æ¨¡å‹å˜é‡åº”è¯¥æ˜¯"æ ¹èŠ‚ç‚¹"(æ²¡æœ‰çˆ¶èŠ‚ç‚¹)
   - å› ä¸ºæ¨¡å‹æ˜¯å®éªŒè®¾è®¡çš„å¤–ç”Ÿå˜é‡,ä¸å—å…¶ä»–å› ç´ å½±å“

---

## æœ€ç»ˆå»ºè®®æ–¹æ¡ˆ

### å®Œæ•´æµç¨‹

```python
# Step 1: åŠ è½½åŸå§‹æ•°æ®
df = pd.read_csv('data/data.csv')

# Step 2: è¯­ä¹‰è¶…å‚æ•°åˆå¹¶
df['hyperparam_l2_regularization'] = df['hyperparam_weight_decay'].fillna(
    df['hyperparam_alpha']
)

# Step 3: ç­›é€‰å¯ç”¨æ•°æ®(818æ¡)
df_usable = df[
    (df['status'] == 'success') &
    (~df[[col for col in df.columns if col.startswith('energy_')]].isnull().all(axis=1)) &
    (~df[[col for col in df.columns if col.startswith('perf_')]].isnull().all(axis=1))
]

# Step 4: ç”Ÿæˆ6åˆ†ç»„æ•°æ®(æŒ‰ç…§6GROUPS_DATA_DESIGN_CORRECT_20260115.md)
# æ¯ç»„é€‰æ‹©è¯¥ç»„ä½¿ç”¨çš„è¶…å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡,ä¿ç•™æ‰€æœ‰éç©ºæ•°æ®

# Step 5: ä¸ºæ¯ç»„æ·»åŠ æ¨¡å‹å˜é‡(One-hot n-1ç¼–ç )
for group_name, group_df in groups.items():
    prepared_df, model_vars = prepare_dibs_data_with_model(group_df, group_name)
    prepared_df.to_csv(f'analysis/data/energy_research/6groups/{group_name}_with_model.csv', index=False)

# Step 6: è¿è¡ŒDiBSåˆ†æ
# ä½¿ç”¨ causal-research condaç¯å¢ƒ
```

### é¢„æœŸæ”¹è¿›

| æŒ‡æ ‡ | å½“å‰çŠ¶æ€ | æ”¹è¿›å |
|------|---------|--------|
| æ•°æ®ä¿ç•™ç‡ | 423/818 (51.7%) | **>800/818 (>97%)** â­ |
| L2æ­£åˆ™åŒ–ç¼ºå¤±ç‡ | 93.5% (alpha) | <30% (åˆå¹¶å) |
| æ¨¡å‹å› æœæ•ˆåº” | âŒ æœªè€ƒè™‘ | âœ… å¯è¯†åˆ« |
| VulBERTa/ResNetåˆå¹¶ | âš ï¸ é”™è¯¯å°è¯• | âœ… æ­£ç¡®åˆ†ç¦» |

---

## åç»­å·¥ä½œ

1. **å®ç°è¶…å‚æ•°è¯­ä¹‰åˆå¹¶è„šæœ¬**: `unify_semantic_hyperparams.py`
2. **æ›´æ–°6åˆ†ç»„ç”Ÿæˆè„šæœ¬**: é›†æˆè¯­ä¹‰åˆå¹¶å’Œæ¨¡å‹å˜é‡
3. **éªŒè¯æ•°æ®è´¨é‡**: ç¡®ä¿>800æ¡æ•°æ®ä¿ç•™
4. **é‡æ–°è¿è¡ŒDiBS**: ä½¿ç”¨causal-researchç¯å¢ƒå’Œå®Œæ•´æ•°æ®
5. **åˆ†ææ¨¡å‹å› æœæ•ˆåº”**: è§£é‡Šä¸åŒæ¨¡å‹å¯¹èƒ½è€—çš„å½±å“

---

**ç»“è®º**:
- âœ… è¯­ä¹‰è¶…å‚æ•°åˆå¹¶æ˜¯å¿…è¦çš„,å¯æ˜¾è‘—æé«˜æ•°æ®åˆ©ç”¨ç‡
- âŒ VulBERTaå’ŒResNetä¸åº”åˆå¹¶(æ€§èƒ½æŒ‡æ ‡ä¸å…¼å®¹)
- âœ… æ¨¡å‹åº”ä½œä¸ºOne-hotå˜é‡(n-1ç¼–ç )åŠ å…¥DiBSåˆ†æ
- ğŸ¯ é¢„æœŸå¯ä¿ç•™>800/818æ¡æ•°æ®ç”¨äºåˆ†æ
