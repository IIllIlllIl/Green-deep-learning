# èƒ½è€—å› æœåˆ†æ - æ•°æ®é‡æ–°æå–æ–¹æ¡ˆ

**æ—¥æœŸ**: 2025-12-24
**ç›®çš„**: ä»ä¸»å®éªŒJSONæ–‡ä»¶é‡æ–°æå–æ•°æ®ï¼Œè§£å†³ç¼ºå¤±å€¼é—®é¢˜
**ä¼˜å…ˆçº§**: ğŸ”´ **P0 - æœ€é«˜ä¼˜å…ˆçº§**

---

## æ‰§è¡Œæ‘˜è¦

### é—®é¢˜æ ¹æº

å½“å‰å› æœåˆ†ææ•°æ®çš„ç¼ºå¤±å€¼é—®é¢˜æºäºï¼š

1. **è¶…å‚æ•°å‘½åä¸ç»Ÿä¸€**: ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒå­—æ®µåï¼ˆ`learning_rate` vs `max_iter` vs `epochs`ï¼‰
2. **å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°ä¸ºç©º**: JSONä¸­å¹¶è¡Œå®éªŒçš„`foreground.hyperparameters`å­—æ®µä¸ºç©º âŒ
3. **æ•°æ®æå–é€»è¾‘ç®€å•**: å½“å‰è„šæœ¬ç›´æ¥æ˜ å°„JSONé”®åï¼Œæœªåšå­—æ®µç»Ÿä¸€

### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ**: ä»ä¸»å®éªŒçš„`experiment.json`æ–‡ä»¶é‡æ–°æå–æ•°æ®ï¼Œå®ç°ï¼š
1. **è¶…å‚æ•°å­—æ®µç»Ÿä¸€æ˜ å°„**ï¼ˆä¸åŒå‘½å â†’ ç»Ÿä¸€å˜é‡åï¼‰
2. **ä»é…ç½®æ–‡ä»¶å›æº¯è¶…å‚æ•°**ï¼ˆè§£å†³å¹¶è¡Œæ¨¡å¼hyperparametersä¸ºç©ºçš„é—®é¢˜ï¼‰
3. **å®Œæ•´çš„ç¼ºå¤±å€¼å¤„ç†**ï¼ˆåˆç†æ’è¡¥ + è¡Œåˆ é™¤ï¼‰

**é¢„æœŸæ”¹è¿›**:
- è¶…å‚æ•°ç¼ºå¤±ç‡: 32-100% â†’ **< 5%**
- å®Œå…¨æ— ç¼ºå¤±è¡Œ: 0-64.7% â†’ **> 90%**
- DiBSå› æœè¾¹æ•°: 0æ¡ â†’ **3-8æ¡/ä»»åŠ¡ç»„**

---

## ä¸€ã€ä¸»å®éªŒJSONç»“æ„åˆ†æ

### 1.1 éå¹¶è¡Œæ¨¡å¼JSONç»“æ„

**ç¤ºä¾‹**: VulBERTa_mlp_001

```json
{
  "experiment_id": "VulBERTa_mlp_001",
  "timestamp": "2025-12-17T22:26:33.012398",
  "repository": "VulBERTa",
  "model": "mlp",
  "hyperparameters": {
    "epochs": 14  // âœ… è¶…å‚æ•°åœ¨é¡¶å±‚
  },
  "energy_metrics": {
    "cpu_energy_total_joules": 137821.05,
    "gpu_power_avg_watts": 236.32,
    "gpu_util_avg_percent": 89.23,
    ...
  },
  "performance_metrics": {
    "eval_loss": 0.6955,  // âœ… æ€§èƒ½æŒ‡æ ‡å®Œæ•´
    "final_training_loss": 0.7908
  }
}
```

**ç‰¹å¾**:
- âœ… `hyperparameters`åœ¨é¡¶å±‚ï¼ŒåŒ…å«å®éªŒå‚æ•°
- âœ… `energy_metrics`å’Œ`performance_metrics`å®Œæ•´
- âš ï¸ è¶…å‚æ•°å­—æ®µåå› æ¨¡å‹è€Œå¼‚ï¼ˆ`epochs`, `learning_rate`, `max_iter`ï¼‰

---

### 1.2 å¹¶è¡Œæ¨¡å¼JSONç»“æ„

**ç¤ºä¾‹**: bug-localization-by-dnn-and-rvsm_default_001_parallel

```json
{
  "experiment_id": "bug-localization-by-dnn-and-rvsm_default_001_parallel",
  "timestamp": "2025-12-22T22:06:06.154204",
  "mode": "parallel",
  "foreground": {
    "repository": "bug-localization-by-dnn-and-rvsm",
    "model": "default",
    "hyperparameters": {},  // âŒ ç©ºçš„ï¼
    "energy_metrics": { ... },  // âœ… å®Œæ•´
    "performance_metrics": {
      "top1_accuracy": 0.382,  // âœ… å®Œæ•´
      "top5_accuracy": 0.629
    }
  },
  "background": {
    "repository": "examples",
    "model": "mnist"
  }
}
```

**ä¸¥é‡é—®é¢˜**:
- âŒ **`foreground.hyperparameters`ä¸ºç©ºå­—å…¸**
- è¿™æ˜¯ä¸ºä»€ä¹ˆå¹¶è¡Œæ¨¡å¼æ•°æ®æœ‰100%è¶…å‚æ•°ç¼ºå¤±ï¼

---

### 1.3 è¶…å‚æ•°å‘½åå·®å¼‚æ±‡æ€»

| æ¨¡å‹/æ•°æ®é›† | è¶…å‚æ•°å­—æ®µå | ç¤ºä¾‹å€¼ | åœ¨data.csvä¸­çš„åˆ—å |
|------------|-------------|--------|-------------------|
| **examples/mnist** | `learning_rate`, `batch_size` | 0.01, 64 | `hyperparam_learning_rate`, `hyperparam_batch_size` |
| **pytorch_resnet_cifar10/resnet20** | `epochs`, `lr` | 200, 0.1 | `hyperparam_epochs`, `hyperparam_lr` |
| **VulBERTa/mlp** | `epochs`, `learning_rate` | 14, 1e-5 | `hyperparam_epochs`, `hyperparam_learning_rate` |
| **bug-localization** | `max_iter` | 1209 | `hyperparam_max_iter` |
| **Person_reID** | `learning_rate`, `dropout` | 0.001, 0.5 | `hyperparam_learning_rate`, `hyperparam_dropout` |
| **MRT-OAST** | `learning_rate`, `num_iters` | 0.01, 100 | `hyperparam_learning_rate`, `hyperparam_num_iters` |

**é—®é¢˜**: åŒä¸€ä¸ªæ¦‚å¿µï¼ˆå¦‚"å­¦ä¹ ç‡"ï¼‰æœ‰å¤šä¸ªå­—æ®µåï¼š`learning_rate`, `lr`

---

## äºŒã€å½“å‰æ•°æ®æå–æµç¨‹åˆ†æ

### 2.1 append_session_to_raw_data.pyçš„æå–é€»è¾‘

**ä»£ç ** (`tools/data_management/append_session_to_raw_data.py:146-150,216-220`):

```python
# å¹¶è¡Œæ¨¡å¼
hyperparams = fg_data.get('hyperparameters', {})
for key, value in hyperparams.items():
    col_name = f'hyperparam_{key}'
    if col_name in fieldnames:
        row[col_name] = str(value)
```

**ç¼ºé™·**:

1. **ç›´æ¥æ˜ å°„å­—æ®µå**: `key` â†’ `hyperparam_{key}`ï¼Œæœªåšç»Ÿä¸€
   - `lr` â†’ `hyperparam_lr`ï¼ˆè€Œé`hyperparam_learning_rate`ï¼‰
   - `max_iter` â†’ `hyperparam_max_iter`ï¼ˆè€Œé`training_duration`ï¼‰

2. **ä¾èµ–JSONä¸­çš„hyperparameterså­—æ®µ**: å½“å­—æ®µä¸ºç©ºæ—¶æ— æ³•æå–
   - å¹¶è¡Œæ¨¡å¼çš„`hyperparameters`ä¸ºç©º â†’ 100%ç¼ºå¤±

3. **æ— å­—æ®µæ˜ å°„è¡¨**: ä¸åŒå‘½åæ— æ³•å½’ä¸€åŒ–

---

### 2.2 create_unified_data_csv.pyçš„å¤„ç†

**åŠŸèƒ½**: ä»`raw_data.csv`ç”Ÿæˆ`data.csv`

**ä»£ç é€»è¾‘**:
```python
# ç»Ÿä¸€è¶…å‚æ•°å­—æ®µï¼ˆå¹¶è¡Œ/éå¹¶è¡Œï¼‰
df['hyperparam_learning_rate'] = df['hyperparam_learning_rate'].fillna(df['fg_hyperparam_learning_rate'])
```

**ç¼ºé™·**:
- åªæ˜¯åˆå¹¶å¹¶è¡Œ/éå¹¶è¡Œçš„åŒåå­—æ®µ
- æ— æ³•è§£å†³å­—æ®µåä¸ç»Ÿä¸€çš„é—®é¢˜ï¼ˆ`lr` vs `learning_rate`ï¼‰
- æ— æ³•è§£å†³hyperparametersä¸ºç©ºçš„é—®é¢˜

---

### 2.3 analysis/scripts/stage0_data_validation.pyçš„å¤„ç†

**åŠŸèƒ½**: éªŒè¯`data.csv`è´¨é‡

**ä»£ç ** (`analysis/scripts/stage0_data_validation.py:85-90`):
```python
required_columns = {
    'è¶…å‚æ•°': ['hyperparam_learning_rate', 'hyperparam_batch_size', 'hyperparam_epochs'],
    ...
}
```

**ç¼ºé™·**:
- é¢„æœŸå›ºå®šåˆ—åï¼Œä½†å®é™…JSONå­—æ®µåä¸å›ºå®š
- åªèƒ½æ£€æµ‹ç¼ºå¤±ï¼Œæ— æ³•ä¿®å¤

---

## ä¸‰ã€é‡æ–°æå–æ–¹æ¡ˆè®¾è®¡

### 3.1 æ–¹æ¡ˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸»å®éªŒJSONæ–‡ä»¶ (results/run_*/*/experiment.json)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–°è„šæœ¬: extract_causal_analysis_data.py                     â”‚
â”‚  åŠŸèƒ½:                                                       â”‚
â”‚  1. éå†æ‰€æœ‰sessionç›®å½•                                    â”‚
â”‚  2. è¯»å–experiment.json                                     â”‚
â”‚  3. åº”ç”¨è¶…å‚æ•°å­—æ®µæ˜ å°„è¡¨                                   â”‚
â”‚  4. ä»é…ç½®æ–‡ä»¶å›æº¯å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°                           â”‚
â”‚  5. ç»Ÿä¸€å­—æ®µå‘½å                                           â”‚
â”‚  6. ç¼ºå¤±å€¼å¤„ç†ï¼ˆæ’è¡¥/åˆ é™¤ï¼‰                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å‡º: analysis/data/energy_research/training/              â”‚
â”‚  - training_data_image_classification.csv (é«˜è´¨é‡)         â”‚
â”‚  - training_data_person_reid.csv                           â”‚
â”‚  - training_data_vulberta.csv                              â”‚
â”‚  - training_data_bug_localization.csv                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.2 è¶…å‚æ•°å­—æ®µæ˜ å°„è¡¨

**æ ¸å¿ƒæ€æƒ³**: å°†ä¸åŒæ¨¡å‹çš„è¶…å‚æ•°å­—æ®µåç»Ÿä¸€åˆ°æ ‡å‡†å˜é‡å

**æ˜ å°„è¡¨** (`HYPERPARAM_FIELD_MAPPING`):

```python
HYPERPARAM_FIELD_MAPPING = {
    # å­¦ä¹ ç‡ç»Ÿä¸€
    'learning_rate': 'hyperparam_learning_rate',
    'lr': 'hyperparam_learning_rate',  # CIFAR-10ä½¿ç”¨
    'initial_lr': 'hyperparam_learning_rate',

    # è®­ç»ƒè¿­ä»£æ¬¡æ•°ç»Ÿä¸€
    'epochs': 'training_duration',  # VulBERTa, CIFAR-10
    'max_iter': 'training_duration',  # Bugå®šä½
    'num_iters': 'training_duration',  # MRT-OAST

    # æ‰¹é‡å¤§å°ç»Ÿä¸€
    'batch_size': 'hyperparam_batch_size',
    'train_batch_size': 'hyperparam_batch_size',

    # æ­£åˆ™åŒ–ç»Ÿä¸€
    'weight_decay': 'l2_regularization',  # å¤§å¤šæ•°æ¨¡å‹
    'alpha': 'l2_regularization',  # MRT-OAST

    # Dropout
    'dropout': 'hyperparam_dropout',
    'dropout_rate': 'hyperparam_dropout',

    # å…¶ä»–è¶…å‚æ•°ï¼ˆä¿æŒåŸåï¼‰
    'seed': 'seed',
    'momentum': 'hyperparam_momentum',
    'gamma': 'hyperparam_gamma',
}
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# JSONä¸­çš„å­—æ®µ: {"max_iter": 1209}
# æ˜ å°„å: {"training_duration": 1209}
# CSVåˆ—å: training_durationï¼ˆè€Œéhyperparam_max_iterï¼‰
```

---

### 3.3 å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°å›æº¯ç­–ç•¥

**é—®é¢˜**: å¹¶è¡Œæ¨¡å¼JSONä¸­`foreground.hyperparameters`ä¸ºç©º

**è§£å†³æ–¹æ¡ˆ**: ä»å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆsettings/*.jsonï¼‰ä¸­å›æº¯è¶…å‚æ•°

#### 3.3.1 å›æº¯æµç¨‹

```
1. è¯»å–experiment.json
   â”œâ”€ experiment_id: "bug-localization_default_001_parallel"
   â””â”€ foreground.hyperparameters: {}  // ç©ºçš„

2. è§£æexperiment_id
   â”œâ”€ æå–repo: "bug-localization-by-dnn-and-rvsm"
   â”œâ”€ æå–model: "default"
   â””â”€ æå–åºå·: 001

3. æŸ¥æ‰¾å¯¹åº”çš„é…ç½®æ–‡ä»¶
   â”œâ”€ æœç´¢: settings/stage*_*.json
   â””â”€ åŒ¹é…: "repo": "bug-localization", "mode": "parallel"

4. ä»é…ç½®æ–‡ä»¶æå–è¶…å‚æ•°
   â”œâ”€ é…ç½®ä¸­çš„ "mutate": ["max_iter"]
   â””â”€ é…ç½®ä¸­çš„è¶…å‚æ•°å€¼æˆ–é»˜è®¤å€¼

5. å¡«å……åˆ°data.csv
   â””â”€ training_duration: <ä»é…ç½®æå–çš„max_iterå€¼>
```

#### 3.3.2 ç¤ºä¾‹ä»£ç é€»è¾‘

```python
def get_parallel_hyperparams(experiment_id, repo, model):
    """
    ä»é…ç½®æ–‡ä»¶å›æº¯å¹¶è¡Œæ¨¡å¼çš„è¶…å‚æ•°

    Args:
        experiment_id: å®éªŒID
        repo: ä»“åº“å
        model: æ¨¡å‹å

    Returns:
        dict: è¶…å‚æ•°å­—å…¸
    """
    # 1. æŸ¥æ‰¾å¯¹åº”çš„é…ç½®æ–‡ä»¶
    config_files = glob.glob('settings/stage*.json') + glob.glob('settings/*parallel*.json')

    for config_file in config_files:
        with open(config_file) as f:
            configs = json.load(f)

        for config in configs:
            if (config.get('repo') == repo and
                config.get('model') == model and
                config.get('mode') == 'parallel'):

                # 2. æå–è¶…å‚æ•°
                hyperparams = {}

                # ä»foregroundä¸­æå–
                fg = config.get('foreground', {})
                if 'hyperparameters' in fg:
                    hyperparams.update(fg['hyperparameters'])

                # ä»mutateå‚æ•°æå–ï¼ˆå¦‚æœæ˜¯å˜å¼‚å®éªŒï¼‰
                if config.get('mode') == 'mutation':
                    mutate_params = config.get('mutate', [])
                    # éœ€è¦è¿›ä¸€æ­¥è§£æå˜å¼‚å€¼...

                return hyperparams

    # æœªæ‰¾åˆ°é…ç½®ï¼Œè¿”å›ç©ºå­—å…¸
    return {}
```

**æ³¨æ„**: è¿™ä¸ªæ–¹æ³•å¯èƒ½ä¸å®Œç¾ï¼Œå› ä¸ºï¼š
- é…ç½®æ–‡ä»¶ä¸­å¯èƒ½ä¹Ÿæ²¡æœ‰å…·ä½“çš„è¶…å‚æ•°å€¼ï¼ˆåªæœ‰å˜å¼‚æŒ‡ä»¤ï¼‰
- éœ€è¦ç»“åˆé»˜è®¤å€¼å’Œå˜å¼‚ç­–ç•¥æ¥æ¨æ–­

**æ›¿ä»£æ–¹æ¡ˆ**: å¦‚æœæ— æ³•å›æº¯ï¼Œä½¿ç”¨**ä¸­ä½æ•°æ’è¡¥**ï¼ˆåç»­å¤„ç†ï¼‰

---

### 3.4 ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥

#### 3.4.1 ä¼˜å…ˆçº§åˆ†å±‚

| åˆ—ç±»å‹ | å¤„ç†ç­–ç•¥ | åŸå›  |
|--------|----------|------|
| **è¶…å‚æ•°åˆ—** | ä¸­ä½æ•°æ’è¡¥ï¼ˆæŒ‰ä»»åŠ¡ç»„ï¼‰ | å…è®¸è½»å¾®åå·®ï¼Œä¿ç•™æ ·æœ¬é‡ |
| **æ€§èƒ½æŒ‡æ ‡åˆ—** | åˆ é™¤è¯¥è¡Œ | ç›®æ ‡å˜é‡ï¼Œæ’è¡¥ä¼šä¸¥é‡åå€šå› æœä¼°è®¡ |
| **èƒ½è€—æŒ‡æ ‡åˆ—** | åˆ é™¤è¯¥è¡Œ | ç›®æ ‡å˜é‡ï¼Œä¸èƒ½æ’è¡¥ |
| **å…ƒä¿¡æ¯åˆ—** | å¿…é¡»å®Œæ•´ | experiment_id, timestampç­‰ä¸èƒ½ç¼ºå¤± |

#### 3.4.2 è¶…å‚æ•°æ’è¡¥è§„åˆ™

**æŒ‰ä»»åŠ¡ç»„ Ã— One-Hotåˆ†ç»„æ’è¡¥**:

```python
# ç¤ºä¾‹ï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡ç»„
# MNISTæ¨¡å‹çš„learning_rateç¼ºå¤± â†’ ç”¨MNISTå…¶ä»–å®éªŒçš„ä¸­ä½æ•°
mnist_lr_median = df[(df['is_mnist']==1) & df['hyperparam_learning_rate'].notna()]['hyperparam_learning_rate'].median()

df.loc[(df['is_mnist']==1) & df['hyperparam_learning_rate'].isnull(),
       'hyperparam_learning_rate'] = mnist_lr_median
```

**åˆç†æ€§**:
- åŒä¸€æ¨¡å‹çš„è¶…å‚æ•°é€šå¸¸åœ¨ç›¸ä¼¼èŒƒå›´
- ä¸­ä½æ•°ä»£è¡¨å…¸å‹å€¼ï¼Œæ¯”å‡å€¼æ›´ç¨³å¥
- æŒ‰ä»»åŠ¡ç»„åˆ†ç»„é¿å…è·¨ä»»åŠ¡æ±¡æŸ“

#### 3.4.3 è¡Œåˆ é™¤è§„åˆ™

**åˆ é™¤æ¡ä»¶**:
1. æ€§èƒ½æŒ‡æ ‡**å…¨éƒ¨**ç¼ºå¤±ï¼ˆå¦‚VulBERTaçš„60è¡Œæ— eval_lossï¼‰
2. èƒ½è€—æŒ‡æ ‡**å…¨éƒ¨**ç¼ºå¤±ï¼ˆå¦‚å›¾åƒåˆ†ç±»çš„1è¡Œæ— èƒ½è€—æ•°æ®ï¼‰
3. å…ƒä¿¡æ¯ç¼ºå¤±ï¼ˆæå°‘è§ï¼‰

**é¢„æœŸå½±å“**:
- å›¾åƒåˆ†ç±»: 258 â†’ 257 (-1è¡Œï¼Œ-0.4%)
- Person_reID: 116 â†’ 116 (æ— å˜åŒ–)
- VulBERTa: 142 â†’ 82 (-60è¡Œï¼Œ-42.3%) âš ï¸
- Bugå®šä½: 132 â†’ 80 (-52è¡Œï¼Œ-39.4%) âš ï¸

**æ ·æœ¬é‡å……è¶³æ€§éªŒè¯**:
- DiBSæœ€ä½è¦æ±‚: 10æ ·æœ¬ âœ…
- æ‰€æœ‰ä»»åŠ¡ç»„åˆ é™¤åä» > 80æ ·æœ¬ âœ…

---

### 3.5 æ•°æ®è´¨é‡ç›®æ ‡

| ä»»åŠ¡ç»„ | å½“å‰ç¼ºå¤±ç‡ | ç›®æ ‡ç¼ºå¤±ç‡ | å½“å‰å®Œå…¨æ— ç¼ºå¤±è¡Œ | ç›®æ ‡å®Œå…¨æ— ç¼ºå¤±è¡Œ |
|--------|------------|------------|------------------|------------------|
| å›¾åƒåˆ†ç±» | 8.83% | **< 2%** | 48.4% | **> 95%** |
| Person_reID | 4.96% | **< 2%** | 64.7% | **> 95%** |
| VulBERTa | 28.87% | **< 3%** | 0% | **> 80%** |
| Bugå®šä½ | 24.38% | **< 3%** | 0% | **> 80%** |

**æ€»ä½“ç›®æ ‡**:
- âœ… æ‰€æœ‰è¶…å‚æ•°åˆ—å¡«å……ç‡ > 90%
- âœ… æ‰€æœ‰ä»»åŠ¡ç»„è‡³å°‘80ä¸ªå®Œå…¨æ— ç¼ºå¤±çš„è¡Œ
- âœ… ç›¸å…³æ€§çŸ©é˜µå¯è®¡ç®—ï¼ˆæ— nanï¼‰
- âœ… DiBSèƒ½å‘ç°å› æœè¾¹ï¼ˆé¢„æœŸ3-8æ¡/ä»»åŠ¡ç»„ï¼‰

---

## å››ã€å®æ–½è®¡åˆ’

### 4.1 ç¬¬ä¸€é˜¶æ®µï¼šéªŒè¯JSONå¯è®¿é—®æ€§ï¼ˆ30åˆ†é’Ÿï¼‰

**ç›®æ ‡**: ç¡®è®¤æ‰€æœ‰experiment.jsonæ–‡ä»¶å¯è¯»å–

**è„šæœ¬**: `scripts/validate_json_accessibility.py`

```python
#!/usr/bin/env python3
"""éªŒè¯æ‰€æœ‰experiment.jsonæ–‡ä»¶çš„å¯è®¿é—®æ€§å’Œå®Œæ•´æ€§"""

import json
from pathlib import Path
from collections import defaultdict

def validate_json_files():
    """éå†æ‰€æœ‰sessionç›®å½•ï¼ŒéªŒè¯JSONæ–‡ä»¶"""

    results_dir = Path('results')
    stats = {
        'total_sessions': 0,
        'total_experiments': 0,
        'json_found': 0,
        'json_parse_error': 0,
        'hyperparams_empty': 0,
        'hyperparams_nonempty': 0,
        'parallel_mode': 0,
        'non_parallel_mode': 0
    }

    repo_counts = defaultdict(int)
    hyperparam_fields = defaultdict(set)

    # éå†æ‰€æœ‰run_*ç›®å½•
    for session_dir in sorted(results_dir.glob('run_*')):
        if not session_dir.is_dir():
            continue

        stats['total_sessions'] += 1

        # éå†å®éªŒç›®å½•
        for exp_dir in session_dir.iterdir():
            if not exp_dir.is_dir() or exp_dir.name in ['__pycache__', '.git']:
                continue

            stats['total_experiments'] += 1

            json_file = exp_dir / 'experiment.json'
            if not json_file.exists():
                continue

            stats['json_found'] += 1

            try:
                with open(json_file) as f:
                    data = json.load(f)

                # æ£€æŸ¥æ¨¡å¼
                is_parallel = data.get('mode') == 'parallel'
                if is_parallel:
                    stats['parallel_mode'] += 1
                    repo = data.get('foreground', {}).get('repository')
                    hyperparams = data.get('foreground', {}).get('hyperparameters', {})
                else:
                    stats['non_parallel_mode'] += 1
                    repo = data.get('repository')
                    hyperparams = data.get('hyperparameters', {})

                repo_counts[repo] += 1

                # æ£€æŸ¥è¶…å‚æ•°
                if hyperparams:
                    stats['hyperparams_nonempty'] += 1
                    hyperparam_fields[repo].update(hyperparams.keys())
                else:
                    stats['hyperparams_empty'] += 1

            except Exception as e:
                stats['json_parse_error'] += 1
                print(f"  âŒ è§£æå¤±è´¥: {json_file}: {e}")

    # æ‰“å°ç»Ÿè®¡
    print("=" * 80)
    print("JSONæ–‡ä»¶éªŒè¯ç»Ÿè®¡")
    print("=" * 80)
    print(f"Sessionç›®å½•æ•°: {stats['total_sessions']}")
    print(f"å®éªŒç›®å½•æ•°: {stats['total_experiments']}")
    print(f"JSONæ–‡ä»¶æ‰¾åˆ°: {stats['json_found']} ({stats['json_found']/stats['total_experiments']*100:.1f}%)")
    print(f"JSONè§£æé”™è¯¯: {stats['json_parse_error']}")
    print()

    print(f"å¹¶è¡Œæ¨¡å¼å®éªŒ: {stats['parallel_mode']}")
    print(f"éå¹¶è¡Œæ¨¡å¼å®éªŒ: {stats['non_parallel_mode']}")
    print()

    print(f"è¶…å‚æ•°éç©º: {stats['hyperparams_nonempty']} ({stats['hyperparams_nonempty']/stats['json_found']*100:.1f}%)")
    print(f"è¶…å‚æ•°ä¸ºç©º: {stats['hyperparams_empty']} ({stats['hyperparams_empty']/stats['json_found']*100:.1f}%)")
    print()

    print("=" * 80)
    print("å„ä»“åº“å®éªŒæ•°é‡")
    print("=" * 80)
    for repo, count in sorted(repo_counts.items(), key=lambda x: -x[1]):
        print(f"{repo}: {count}")
    print()

    print("=" * 80)
    print("å„ä»“åº“è¶…å‚æ•°å­—æ®µ")
    print("=" * 80)
    for repo, fields in sorted(hyperparam_fields.items()):
        if fields:
            print(f"{repo}:")
            for field in sorted(fields):
                print(f"  - {field}")

    return stats

if __name__ == '__main__':
    stats = validate_json_files()
```

**è¿è¡Œ**:
```bash
cd /home/green/energy_dl/nightly
python3 scripts/validate_json_accessibility.py
```

**é¢„æœŸè¾“å‡º**:
- æ€»JSONæ–‡ä»¶æ•°: ~726
- è¶…å‚æ•°ä¸ºç©ºçš„æ¯”ä¾‹: ~5-10%ï¼ˆä¸»è¦æ˜¯å¹¶è¡Œæ¨¡å¼ï¼‰
- å„ä»“åº“çš„è¶…å‚æ•°å­—æ®µæ¸…å•ï¼ˆç”¨äºæ„å»ºæ˜ å°„è¡¨ï¼‰

---

### 4.2 ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºè¶…å‚æ•°æ˜ å°„è¡¨ï¼ˆ1å°æ—¶ï¼‰

**ç›®æ ‡**: åŸºäºç¬¬ä¸€é˜¶æ®µçš„å‘ç°ï¼Œå®Œå–„è¶…å‚æ•°å­—æ®µæ˜ å°„è¡¨

**ä»»åŠ¡**:
1. æ±‡æ€»æ‰€æœ‰å‡ºç°çš„è¶…å‚æ•°å­—æ®µå
2. æ ¹æ®è¯­ä¹‰å½’ç±»ï¼ˆå¦‚`lr`å’Œ`learning_rate`éƒ½æ˜¯å­¦ä¹ ç‡ï¼‰
3. ç¡®å®šç»Ÿä¸€çš„ç›®æ ‡å­—æ®µå
4. ç¼–å†™æ˜ å°„å­—å…¸

**è¾“å‡º**: `HYPERPARAM_FIELD_MAPPING` å­—å…¸ï¼ˆè§3.2èŠ‚ï¼‰

---

### 4.3 ç¬¬ä¸‰é˜¶æ®µï¼šå®ç°æ•°æ®æå–è„šæœ¬ï¼ˆ2-3å°æ—¶ï¼‰

**è„šæœ¬**: `analysis/scripts/extract_from_json_direct.py`

**åŠŸèƒ½æ¨¡å—**:

#### æ¨¡å—1: JSONéå†ä¸åŠ è½½
```python
def load_all_experiments(results_dir):
    """éå†æ‰€æœ‰sessionç›®å½•ï¼ŒåŠ è½½experiment.json"""
    experiments = []

    for session_dir in results_dir.glob('run_*'):
        for exp_dir in session_dir.iterdir():
            if not exp_dir.is_dir():
                continue

            json_file = exp_dir / 'experiment.json'
            if not json_file.exists():
                continue

            try:
                with open(json_file) as f:
                    data = json.load(f)
                experiments.append(data)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ {json_file}: {e}")

    return experiments
```

#### æ¨¡å—2: è¶…å‚æ•°æå–ä¸æ˜ å°„
```python
def extract_hyperparams(exp_data, field_mapping):
    """
    ä»experiment.jsonæå–è¶…å‚æ•°ï¼Œå¹¶åº”ç”¨å­—æ®µæ˜ å°„

    Args:
        exp_data: å®éªŒæ•°æ®å­—å…¸
        field_mapping: è¶…å‚æ•°å­—æ®µæ˜ å°„è¡¨

    Returns:
        dict: ç»Ÿä¸€åçš„è¶…å‚æ•°å­—å…¸
    """
    is_parallel = exp_data.get('mode') == 'parallel'

    if is_parallel:
        raw_hyperparams = exp_data.get('foreground', {}).get('hyperparameters', {})
    else:
        raw_hyperparams = exp_data.get('hyperparameters', {})

    # åº”ç”¨å­—æ®µæ˜ å°„
    unified_hyperparams = {}
    for raw_key, raw_value in raw_hyperparams.items():
        # æŸ¥æ‰¾æ˜ å°„
        if raw_key in field_mapping:
            unified_key = field_mapping[raw_key]
        else:
            # æœªæ˜ å°„çš„å­—æ®µä¿æŒåŸåï¼ˆåŠ hyperparam_å‰ç¼€ï¼‰
            unified_key = f'hyperparam_{raw_key}'

        unified_hyperparams[unified_key] = raw_value

    return unified_hyperparams
```

#### æ¨¡å—3: å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°å›æº¯ï¼ˆå¯é€‰ï¼‰
```python
def backfill_parallel_hyperparams(exp_data, config_files):
    """
    å°è¯•ä»é…ç½®æ–‡ä»¶å›æº¯å¹¶è¡Œæ¨¡å¼çš„è¶…å‚æ•°

    å¦‚æœå¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸ï¼ˆåç»­ç”¨ä¸­ä½æ•°æ’è¡¥ï¼‰
    """
    # å®ç°çœç•¥ï¼ˆå¤æ‚åº¦è¾ƒé«˜ï¼Œå¯ä½œä¸ºå¯é€‰å¢å¼ºï¼‰
    return {}
```

#### æ¨¡å—4: æ•°æ®è½¬æ¢ä¸ºDataFrame
```python
def experiments_to_dataframe(experiments, field_mapping):
    """
    å°†å®éªŒåˆ—è¡¨è½¬æ¢ä¸ºDataFrame

    Args:
        experiments: å®éªŒæ•°æ®åˆ—è¡¨
        field_mapping: è¶…å‚æ•°å­—æ®µæ˜ å°„è¡¨

    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å®éªŒçš„DataFrame
    """
    rows = []

    for exp in experiments:
        row = {}

        is_parallel = exp.get('mode') == 'parallel'

        # æå–åŸºç¡€å­—æ®µ
        if is_parallel:
            fg = exp.get('foreground', {})
            row['experiment_id'] = exp.get('experiment_id')
            row['timestamp'] = exp.get('timestamp')
            row['repository'] = fg.get('repository')
            row['model'] = fg.get('model')
            row['mode'] = 'parallel'
            row['is_parallel'] = 1

            # è¶…å‚æ•°
            hyperparams = extract_hyperparams(exp, field_mapping)
            row.update(hyperparams)

            # èƒ½è€—å’Œæ€§èƒ½
            row.update(extract_energy_metrics(fg.get('energy_metrics', {})))
            row.update(extract_performance_metrics(fg.get('performance_metrics', {})))
        else:
            row['experiment_id'] = exp.get('experiment_id')
            row['timestamp'] = exp.get('timestamp')
            row['repository'] = exp.get('repository')
            row['model'] = exp.get('model')
            row['mode'] = exp.get('mode', 'default')
            row['is_parallel'] = 0

            # è¶…å‚æ•°
            hyperparams = extract_hyperparams(exp, field_mapping)
            row.update(hyperparams)

            # èƒ½è€—å’Œæ€§èƒ½
            row.update(extract_energy_metrics(exp.get('energy_metrics', {})))
            row.update(extract_performance_metrics(exp.get('performance_metrics', {})))

        rows.append(row)

    df = pd.DataFrame(rows)
    return df
```

#### æ¨¡å—5: ç¼ºå¤±å€¼å¤„ç†
```python
def handle_missing_values(df):
    """
    å¤„ç†ç¼ºå¤±å€¼

    ç­–ç•¥:
    1. åˆ é™¤æ€§èƒ½æŒ‡æ ‡å…¨ç¼ºå¤±çš„è¡Œ
    2. åˆ é™¤èƒ½è€—æŒ‡æ ‡å…¨ç¼ºå¤±çš„è¡Œ
    3. å¯¹è¶…å‚æ•°åˆ—è¿›è¡Œä¸­ä½æ•°æ’è¡¥ï¼ˆæŒ‰ä»»åŠ¡ç»„åˆ†ç»„ï¼‰
    """
    # 1. åˆ é™¤æ€§èƒ½æŒ‡æ ‡å…¨ç¼ºå¤±çš„è¡Œ
    perf_cols = [c for c in df.columns if c.startswith('perf_')]
    df_clean = df[df[perf_cols].notna().any(axis=1)]
    print(f"  åˆ é™¤æ€§èƒ½å…¨ç¼ºå¤±: {len(df) - len(df_clean)} è¡Œ")

    # 2. åˆ é™¤èƒ½è€—æŒ‡æ ‡å…¨ç¼ºå¤±çš„è¡Œ
    energy_cols = [c for c in df.columns if c.startswith('energy_')]
    df_clean = df_clean[df_clean[energy_cols].notna().any(axis=1)]
    print(f"  åˆ é™¤èƒ½è€—å…¨ç¼ºå¤±: {len(df) - len(df_clean)} è¡Œ")

    # 3. è¶…å‚æ•°æ’è¡¥ï¼ˆæŒ‰ä»»åŠ¡ç»„ï¼‰
    # è¿™é‡Œéœ€è¦å…ˆåšä»»åŠ¡åˆ†ç»„...

    return df_clean
```

---

### 4.4 ç¬¬å››é˜¶æ®µï¼šæ•°æ®åˆ†å±‚ä¸ä¿å­˜ï¼ˆ1å°æ—¶ï¼‰

**ç›®æ ‡**: æŒ‰4ä¸ªä»»åŠ¡ç»„åˆ†å±‚ä¿å­˜æ•°æ®

**ä»»åŠ¡**:
1. æŒ‰`repository`åˆ†ç»„ï¼ˆexamples, Person_reID, VulBERTa, bug-localizationï¼‰
2. æ·»åŠ One-Hotç¼–ç åˆ—
3. é€‰æ‹©ä»»åŠ¡ç‰¹å®šçš„å˜é‡
4. ä¿å­˜ä¸º`training_data_{task}.csv`

**è¾“å‡º**:
- `analysis/data/energy_research/training/training_data_image_classification.csv`
- `analysis/data/energy_research/training/training_data_person_reid.csv`
- `analysis/data/energy_research/training/training_data_vulberta.csv`
- `analysis/data/energy_research/training/training_data_bug_localization.csv`

---

### 4.5 ç¬¬äº”é˜¶æ®µï¼šæ•°æ®è´¨é‡éªŒè¯ï¼ˆ30åˆ†é’Ÿï¼‰

**è„šæœ¬**: `analysis/scripts/validate_extracted_data.py`

**éªŒè¯é¡¹**:
1. âœ… ç¼ºå¤±ç‡ < ç›®æ ‡å€¼
2. âœ… å®Œå…¨æ— ç¼ºå¤±è¡Œæ¯”ä¾‹ > ç›®æ ‡å€¼
3. âœ… ç›¸å…³æ€§çŸ©é˜µå¯è®¡ç®—ï¼ˆæ— nanï¼‰
4. âœ… æ ·æœ¬é‡å……è¶³ï¼ˆæ¯ç»„ > 80ï¼‰
5. âœ… è¶…å‚æ•°å¡«å……ç‡ > 90%

**é€šè¿‡æ¡ä»¶**: æ‰€æœ‰éªŒè¯é¡¹é€šè¿‡

---

### 4.6 ç¬¬å…­é˜¶æ®µï¼šé‡æ–°è¿è¡ŒDiBSåˆ†æï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡**: ä½¿ç”¨æ–°æå–çš„æ•°æ®é‡æ–°è¿è¡Œå› æœåˆ†æ

**è„šæœ¬**: `analysis/scripts/experiments/run_energy_causal_analysis.sh`

**é¢„æœŸç»“æœ**:
- å›¾åƒåˆ†ç±»: å‘ç° **3-6æ¡å› æœè¾¹**
- Person_reID: å‘ç° **2-5æ¡å› æœè¾¹**
- VulBERTa: å‘ç° **1-3æ¡å› æœè¾¹**
- Bugå®šä½: å‘ç° **1-3æ¡å› æœè¾¹**

---

## äº”ã€æ—¶é—´è¡¨ä¸é‡Œç¨‹ç¢‘

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|----------|--------|------|
| **é˜¶æ®µ1** | éªŒè¯JSONå¯è®¿é—®æ€§ | 0.5å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **é˜¶æ®µ2** | æ„å»ºè¶…å‚æ•°æ˜ å°„è¡¨ | 1å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **é˜¶æ®µ3** | å®ç°æ•°æ®æå–è„šæœ¬ | 2-3å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **é˜¶æ®µ4** | æ•°æ®åˆ†å±‚ä¸ä¿å­˜ | 1å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **é˜¶æ®µ5** | æ•°æ®è´¨é‡éªŒè¯ | 0.5å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **é˜¶æ®µ6** | é‡æ–°è¿è¡ŒDiBSåˆ†æ | 2å°æ—¶ | Claude | â³ å¾…å¼€å§‹ |
| **æ€»è®¡** | - | **7-8å°æ—¶** | - | - |

**æœ€å¿«å®Œæˆæ—¶é—´**: 1ä¸ªå·¥ä½œæ—¥
**æ¨èå®Œæˆæ—¶é—´**: 2ä¸ªå·¥ä½œæ—¥ï¼ˆç•™å‡ºè°ƒè¯•å’ŒéªŒè¯æ—¶é—´ï¼‰

---

## å…­ã€é£é™©ä¸ç¼“è§£æªæ–½

### é£é™©1: å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°å›æº¯å¤±è´¥

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­ç­‰

**æè¿°**: ä»é…ç½®æ–‡ä»¶å›æº¯è¶…å‚æ•°å¯èƒ½å¤±è´¥ï¼ˆé…ç½®æ–‡ä»¶ç¼ºå¤±æˆ–æ ¼å¼ä¸å…¼å®¹ï¼‰

**å½±å“**: å¹¶è¡Œæ¨¡å¼å®éªŒä»æœ‰è¶…å‚æ•°ç¼ºå¤±

**ç¼“è§£æªæ–½**:
1. **ä¸»ç­–ç•¥**: ä½¿ç”¨ä¸­ä½æ•°æ’è¡¥ï¼ˆæŒ‰ä»»åŠ¡ç»„åˆ†ç»„ï¼‰
2. **å¤‡é€‰ç­–ç•¥**: æ‰‹åŠ¨å¡«å……ï¼ˆæŸ¥çœ‹é…ç½®æ–‡ä»¶æˆ–å®éªŒæ—¥å¿—ï¼‰
3. **æœ€åæƒ…å†µ**: åˆ é™¤è¯¥è¡Œï¼ˆæ ·æœ¬é‡å……è¶³ï¼Œå½±å“æœ‰é™ï¼‰

---

### é£é™©2: æ•°æ®è´¨é‡ä»ä¸è¾¾æ ‡

**é£é™©ç­‰çº§**: ğŸŸ¢ ä½

**æè¿°**: å³ä½¿é‡æ–°æå–ï¼Œæ•°æ®è´¨é‡å¯èƒ½ä»æœ‰é—®é¢˜

**å½±å“**: DiBSä»æ— æ³•å­¦ä¹ å› æœè¾¹

**ç¼“è§£æªæ–½**:
1. **è¯Šæ–­**: ä½¿ç”¨`validate_extracted_data.py`è¯¦ç»†æ£€æŸ¥
2. **è¿­ä»£**: æ ¹æ®éªŒè¯æŠ¥å‘Šè°ƒæ•´æå–é€»è¾‘
3. **é™çº§æ–¹æ¡ˆ**: ä»…ä½¿ç”¨å®Œå…¨æ— ç¼ºå¤±çš„è¡Œï¼ˆåˆ é™¤æ›´å¤šæ ·æœ¬ï¼‰

---

### é£é™©3: å®æ–½æ—¶é—´è¶…é¢„æœŸ

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­ç­‰

**æè¿°**: è„šæœ¬å¼€å‘å’Œè°ƒè¯•å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´

**å½±å“**: å»¶è¿Ÿå› æœåˆ†æç»“æœ

**ç¼“è§£æªæ–½**:
1. **åˆ†é˜¶æ®µéªŒè¯**: æ¯é˜¶æ®µå®Œæˆåç«‹å³æµ‹è¯•
2. **ä¼˜å…ˆçº§æ’åº**: å…ˆè§£å†³è¶…å‚æ•°ç¼ºå¤±ï¼ˆæœ€ä¸¥é‡é—®é¢˜ï¼‰
3. **æ¸è¿›å¼æ”¹è¿›**: ç¬¬ä¸€ç‰ˆå…ˆè¾¾åˆ°åŸºæœ¬å¯ç”¨ï¼Œåç»­è¿­ä»£ä¼˜åŒ–

---

## ä¸ƒã€æˆåŠŸæ ‡å‡†

### 7.1 æ•°æ®è´¨é‡æ ‡å‡†

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | éªŒè¯æ–¹æ³• |
|------|--------|--------|----------|
| **æ€»ä½“ç¼ºå¤±ç‡** | 8-28% | < 3% | `df.isnull().sum().sum()` |
| **è¶…å‚æ•°å¡«å……ç‡** | 32-100%ç¼ºå¤± | > 90% | æ¯åˆ—å•ç‹¬æ£€æŸ¥ |
| **å®Œå…¨æ— ç¼ºå¤±è¡Œ** | 0-64.7% | > 90% | `df.dropna()` |
| **ç›¸å…³æ€§å¯è®¡ç®—** | å¤±è´¥(nan) | æˆåŠŸ | `df.corr()` |
| **æ ·æœ¬é‡** | 80-258 | > 80 | `len(df)` |

---

### 7.2 å› æœåˆ†ææ ‡å‡†

| ä»»åŠ¡ç»„ | å½“å‰å› æœè¾¹æ•° | ç›®æ ‡å› æœè¾¹æ•° | DiBSè¿­ä»£æ¬¡æ•° |
|--------|--------------|--------------|--------------|
| å›¾åƒåˆ†ç±» | 0 | **3-6æ¡** | 3000 |
| Person_reID | 0 | **2-5æ¡** | 3000 |
| VulBERTa | 0 | **1-3æ¡** | 3000 |
| Bugå®šä½ | 0 | **1-3æ¡** | 3000 |

**å‚è€ƒ**: Adultæ•°æ®é›†ï¼ˆ10æ ·æœ¬ï¼‰å‘ç°6æ¡è¾¹ï¼Œèƒ½è€—æ•°æ®ï¼ˆ80-258æ ·æœ¬ï¼‰åº”èƒ½å‘ç°æ›´å¤š

---

### 7.3 å¯äº¤ä»˜æˆæœ

âœ… **ä»£ç **:
1. `scripts/validate_json_accessibility.py` - JSONéªŒè¯è„šæœ¬
2. `analysis/scripts/extract_from_json_direct.py` - æ•°æ®æå–è„šæœ¬
3. `analysis/scripts/validate_extracted_data.py` - æ•°æ®éªŒè¯è„šæœ¬

âœ… **æ•°æ®**:
4. `analysis/data/energy_research/training/training_data_*.csv` (4ä¸ªæ–‡ä»¶ï¼Œé«˜è´¨é‡)

âœ… **æŠ¥å‘Š**:
5. `analysis/docs/reports/DATA_REEXTRACTION_EXECUTION_REPORT.md` - æ‰§è¡ŒæŠ¥å‘Š
6. `analysis/docs/reports/DATA_QUALITY_COMPARISON_REPORT.md` - æ–°æ—§æ•°æ®å¯¹æ¯”

âœ… **å› æœåˆ†æç»“æœ**:
7. `analysis/results/energy_research/task_specific/*.npy` - DiBSå› æœå›¾
8. `analysis/results/energy_research/task_specific/*.pkl` - å› æœè¾¹å’Œæ•ˆåº”

---

## å…«ã€æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆA: é‡æ–°æå–ï¼ˆæ¨èï¼‰â­â­â­

**ä¼˜ç‚¹**:
- âœ… ä»æºå¤´è§£å†³é—®é¢˜ï¼Œæ•°æ®è´¨é‡æœ€é«˜
- âœ… å¯ä»¥å®Œå…¨æ§åˆ¶å­—æ®µæ˜ å°„å’Œç¼ºå¤±å€¼å¤„ç†
- âœ… æœªæ¥å¯å¤ç”¨ï¼ˆæ–°å®éªŒæ•°æ®ä¹Ÿèƒ½æ­£ç¡®æå–ï¼‰

**ç¼ºç‚¹**:
- âš ï¸ å¼€å‘æ—¶é—´è¾ƒé•¿ï¼ˆ7-8å°æ—¶ï¼‰
- âš ï¸ éœ€è¦æ·±å…¥ç†è§£JSONç»“æ„

**é€‚ç”¨åœºæ™¯**: å½“å‰æƒ…å†µï¼ˆæ•°æ®è´¨é‡ä¸¥é‡ä¸è¶³ï¼Œéœ€è¦å½»åº•ä¿®å¤ï¼‰

---

### æ–¹æ¡ˆB: ä»…ä½¿ç”¨å®Œå…¨æ— ç¼ºå¤±çš„è¡Œ

**ä¼˜ç‚¹**:
- âœ… å®æ–½ç®€å•ï¼ˆ30åˆ†é’Ÿï¼‰
- âœ… æ•°æ®è´¨é‡æœ‰ä¿è¯

**ç¼ºç‚¹**:
- âŒ æ ·æœ¬é‡å¤§å¹…å‡å°‘ï¼ˆå›¾åƒåˆ†ç±»: 258 â†’ 125, VulBERTa: 142 â†’ 0 âŒï¼‰
- âŒ å¯èƒ½å¼•å…¥é€‰æ‹©åå·®ï¼ˆåªæœ‰æŸäº›é…ç½®æ— ç¼ºå¤±ï¼‰
- âŒ VulBERTaå’ŒBugå®šä½**å®Œå…¨ä¸å¯ç”¨**ï¼ˆ0è¡Œæ— ç¼ºå¤±ï¼‰

**é€‚ç”¨åœºæ™¯**: ä»…ä½œä¸ºå¿«é€ŸéªŒè¯ï¼Œä¸é€‚åˆæ­£å¼åˆ†æ

---

### æ–¹æ¡ˆC: ç®€å•æ’è¡¥ï¼ˆæœªåšå­—æ®µæ˜ å°„ï¼‰

**ä¼˜ç‚¹**:
- âœ… å®æ–½è¾ƒå¿«ï¼ˆ2-3å°æ—¶ï¼‰
- âœ… ä¿ç•™æ‰€æœ‰æ ·æœ¬

**ç¼ºç‚¹**:
- âŒ æ— æ³•è§£å†³å­—æ®µå‘½åä¸ç»Ÿä¸€é—®é¢˜
- âŒ Bugå®šä½çš„`learning_rate`ä»100%ç¼ºå¤±ï¼ˆæ— æ³•æ’è¡¥ï¼‰
- âŒ å¹¶è¡Œæ¨¡å¼è¶…å‚æ•°ä»ä¸ºç©ºï¼ˆæ— æ³•æ’è¡¥ï¼‰

**é€‚ç”¨åœºæ™¯**: æ•°æ®è´¨é‡é—®é¢˜è¾ƒè½»å¾®çš„æƒ…å†µï¼ˆä½†å½“å‰ä¸é€‚ç”¨ï¼‰

---

## ä¹ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©ï¼‰

1. **ç”¨æˆ·ç¡®è®¤æ–¹æ¡ˆ** âœ…
   - ç¡®è®¤é‡‡ç”¨æ–¹æ¡ˆAï¼ˆé‡æ–°æå–ï¼‰
   - ç¡®è®¤æ—¶é—´é¢„ç®—ï¼ˆ7-8å°æ—¶å¯æ¥å—ï¼‰

2. **é˜¶æ®µ1: JSONéªŒè¯**ï¼ˆ30åˆ†é’Ÿï¼‰
   - è¿è¡Œ`validate_json_accessibility.py`
   - ç¡®è®¤æ‰€æœ‰JSONæ–‡ä»¶å¯è®¿é—®
   - æ±‡æ€»è¶…å‚æ•°å­—æ®µæ¸…å•

3. **é˜¶æ®µ2: æ„å»ºæ˜ å°„è¡¨**ï¼ˆ1å°æ—¶ï¼‰
   - åŸºäºé˜¶æ®µ1çš„å‘ç°å®Œå–„`HYPERPARAM_FIELD_MAPPING`
   - ç”¨æˆ·ç¡®è®¤æ˜ å°„è§„åˆ™

### æ˜å¤©è¡ŒåŠ¨

4. **é˜¶æ®µ3-4: å®ç°æå–è„šæœ¬**ï¼ˆ3-4å°æ—¶ï¼‰
   - ç¼–å†™`extract_from_json_direct.py`
   - å•å…ƒæµ‹è¯•
   - ç”Ÿæˆ4ä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶

5. **é˜¶æ®µ5-6: éªŒè¯ä¸åˆ†æ**ï¼ˆ2.5å°æ—¶ï¼‰
   - æ•°æ®è´¨é‡éªŒè¯
   - é‡æ–°è¿è¡ŒDiBS
   - ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

---

## åã€é™„å½•

### é™„å½•A: ä¸»è¦æ•°æ®æ–‡ä»¶æ¸…å•

| æ–‡ä»¶è·¯å¾„ | ç±»å‹ | è¡Œæ•° | åˆ—æ•° | ç”¨é€” |
|---------|------|------|------|------|
| `data/raw_data.csv` | ä¸»æ•°æ® | 726 | 87 | ä¸»é¡¹ç›®æ±‡æ€»æ•°æ® |
| `data/data.csv` | ç²¾ç®€ | 726 | 56 | ä¸»é¡¹ç›®ç²¾ç®€æ•°æ® |
| `analysis/data/energy_research/raw/energy_data_original.csv` | å‰¯æœ¬ | 726 | 56 | analysisæ¨¡å—åŸå§‹æ•°æ® |
| `analysis/data/energy_research/training/training_data_*.csv` | è®­ç»ƒ | 80-258 | 13-17 | **å¾…é‡æ–°ç”Ÿæˆ** âš ï¸ |

### é™„å½•B: å…³é”®é…ç½®æ–‡ä»¶ä½ç½®

- æ¨¡å‹é…ç½®: `mutation/models_config.json`
- å®éªŒé…ç½®: `settings/stage*.json`, `settings/*parallel*.json`
- DiBSé…ç½®: `analysis/config_energy.py`

### é™„å½•C: ç›¸å…³è„šæœ¬ä½ç½®

**ä¸»é¡¹ç›®**:
- `tools/data_management/append_session_to_raw_data.py` - ä»sessionè¿½åŠ æ•°æ®
- `tools/data_management/create_unified_data_csv.py` - ç”Ÿæˆdata.csv

**analysisæ¨¡å—**:
- `analysis/scripts/stage0_data_validation.py` - æ•°æ®éªŒè¯
- `analysis/scripts/stage1_hyperparam_unification.py` - è¶…å‚æ•°ç»Ÿä¸€
- `analysis/scripts/experiments/run_energy_causal_analysis.sh` - è¿è¡Œå› æœåˆ†æ

---

**æŠ¥å‘Šäºº**: Claude
**ç”Ÿæˆæ—¶é—´**: 2025-12-24
**çŠ¶æ€**: â³ ç­‰å¾…ç”¨æˆ·ç¡®è®¤æ–¹æ¡ˆ
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æœ€é«˜ä¼˜å…ˆçº§
