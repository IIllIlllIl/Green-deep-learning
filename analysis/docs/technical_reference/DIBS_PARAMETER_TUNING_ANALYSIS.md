# DiBS参数调整分析与测试方案

**创建日期**: 2026-01-04
**目的**: 分析DiBS所有可调参数，评估通过参数调整产生更多边的可行性
**状态**: 📋 方案设计完成，待测试

---

## 📋 执行摘要

**背景**: DiBS在能耗数据上产生**0条边**（三个版本v1/v2/v3都是完全0矩阵）

**已尝试的参数**:
- Alpha: 0.1 → 0.5 → 0.9（全失败）
- n_steps: 3000 → 10000（全失败）
- n_particles: 硬编码20（未测试其他值）

**本文档目标**:
1. 全面分析DiBS的所有可调参数
2. 识别可能影响边数量的关键参数
3. 设计系统性的参数调整实验
4. 预测成功概率

---

## 🎯 DiBS完整参数列表

### 一、核心推断参数（DiBS基类）

| 参数名 | 类型 | 默认值 | 当前值 | 含义 | 对边数量的影响 |
|-------|------|--------|--------|------|---------------|
| **alpha_linear** | float | 0.05 | 0.1/0.5/0.9 | Sigmoid温度调度斜率 | ⭐⭐⭐⭐⭐ **极高** |
| **beta_linear** | float | 1.0 | 1.0 | 约束惩罚调度斜率 | ⭐⭐⭐ 中等 |
| **tau** | float | 1.0 | 1.0 | Gumbel-softmax温度 | ⭐⭐ 低 |
| **n_grad_mc_samples** | int | 128 | 128 | 似然梯度MC样本数 | ⭐⭐ 低 |
| **n_acyclicity_mc_samples** | int | 32 | 32 | 无环约束MC样本数 | ⭐ 极低 |
| **grad_estimator_z** | str | 'reparam' | 'reparam' | 梯度估计器类型 | ⭐⭐ 低 |
| **score_function_baseline** | float | 0.0 | 0.0 | Score function基线 | ⭐ 极低 |
| **latent_prior_std** | float | 1/√k | 自动 | 潜在先验标准差 | ⭐⭐ 低 |

**注**: k = n_dim_particles（潜在维度）

### 二、SVGD采样参数

| 参数名 | 类型 | 默认值 | 当前值 | 含义 | 对边数量的影响 |
|-------|------|--------|--------|------|---------------|
| **n_particles** | int | - | 20 | SVGD粒子数量 | ⭐⭐⭐⭐ **高** |
| **steps** | int | - | 3000/10000 | SVGD迭代步数 | ⭐⭐⭐ 中等 |
| **n_dim_particles** | int | n_vars | n_vars | 潜在空间维度k | ⭐⭐ 低 |

### 三、图模型先验参数

| 参数名 | 类型 | 默认值 | 当前值 | 含义 | 对边数量的影响 |
|-------|------|--------|--------|------|---------------|
| **graph_prior_str** | str | 'er' | 'er' | 图先验类型 | ⭐⭐⭐⭐ 高 |
| **edge_prior_prob** | float | 0.5 | - | Erdős-Rényi边概率 | ⭐⭐⭐⭐⭐ **极高** |

**说明**: `graph_prior_str='er'`表示使用Erdős-Rényi先验，默认边概率可能较低

### 四、优化器参数（JointDiBS）

| 参数名 | 类型 | 默认值 | 当前值 | 含义 | 对边数量的影响 |
|-------|------|--------|--------|------|---------------|
| **optimizer** | str | - | 默认 | 优化器类型 | ⭐⭐ 低 |
| **optimizer_param** | dict | - | 默认 | 优化器参数 | ⭐⭐ 低 |

---

## 🔍 关键参数深度分析

### 参数1: `alpha_linear` ⭐⭐⭐⭐⭐ **最关键**

#### 作用机制
```python
# alpha(t) = alpha_linear * t
# 在步骤t时，边概率为:
p(edge) = sigmoid(alpha(t) * score)
```

**影响**:
- `alpha_linear` **越大** → `alpha(t)` 增长**越快** → sigmoid越接近阶跃函数 → 边概率越**极端**（0或1）
- `alpha_linear` **越小** → sigmoid越平滑 → 更多中间概率值 → 可能产生更多边

#### 已测试值
- v1: `alpha_linear=0.1` → 0边
- v2: `alpha_linear=0.5` → 0边
- v3: `alpha_linear=0.9` → 0边

#### 问题诊断
**矛盾**: alpha从0.1（弱）增加到0.9（强）应该产生**更稠密的图**，但都是0边

**可能原因**:
1. ❌ **数据中真的没有因果关系** → alpha无论多大都是0
2. ⚠️ **alpha值范围错误** → 应该使用更小的值（如0.001-0.05）
3. ⚠️ **参数理解错误** → alpha_linear在我们代码中的使用可能与论文不同

#### 推荐测试
```python
# 测试极小的alpha值
alpha_tests = [0.001, 0.01, 0.05, 0.1, 0.5, 0.9]
```

---

### 参数2: `n_particles` ⭐⭐⭐⭐ **用户指定的关键参数**

#### 作用机制
SVGD使用多个粒子来近似后验分布。粒子数量影响：
- **粒子越多** → 后验覆盖越广 → 更可能探索到**稀疏解和稠密解**
- **粒子越少** → 容易陷入局部最优 → 可能全部收敛到0边

####当前值
```python
# causal_discovery.py 第126行
n_particles = 20  # 硬编码
```

#### DiBS论文建议
- 小规模数据（<20变量）: n_particles = 10-20
- 中等规模（20-50变量）: n_particles = 20-50
- 大规模（>50变量）: n_particles = 50-100

#### 已测试值
- v1/v2/v3: 都使用`n_particles=20`（v1为10，v2/v3为20）

#### 推荐测试
```python
# 能耗数据：15变量（v3过滤后）
n_particles_tests = [10, 20, 50, 100, 200]
```

**预期**:
- ✅ **如果数据有因果结构**: n_particles增加→可能发现边
- ❌ **如果数据无因果结构**: n_particles增加→仍然0边

---

### 参数3: `beta_linear` ⭐⭐⭐

#### 作用机制
```python
# beta(t) = beta_linear * t
# 无环约束惩罚项
penalty = beta(t) * E[h(G)]
```

**影响**:
- `beta_linear` **越大** → 无环约束**越强** → 强制DAG → 可能**减少边**
- `beta_linear` **越小** → 约束松弛 → 允许更多边（甚至环）

#### 当前值
```python
beta_linear = 1.0  # 默认值
```

#### 推荐测试
```python
# 降低beta可能允许更多边
beta_tests = [0.1, 0.5, 1.0, 2.0]
```

**警告**: beta太小可能产生有环图（非DAG）

---

### 参数4: `tau` ⭐⭐

#### 作用机制
Gumbel-softmax温度参数：
```python
soft_g = sigmoid(tau * (eps + alpha(t) * score))
```

**影响**:
- `tau` **越大** → 梯度越平滑 → 优化越稳定
- `tau` **越小** → 接近硬采样 → 梯度不稳定

#### 当前值
```python
tau = 1.0  # 默认值
```

#### 推荐测试
```python
# 降低tau可能增加探索性
tau_tests = [0.1, 0.5, 1.0, 2.0]
```

---

### 参数5: `n_grad_mc_samples` ⭐⭐

#### 作用机制
蒙特卡洛梯度估计的样本数：
- **样本越多** → 梯度估计越准确 → 收敛更可靠
- **样本越少** → 梯度噪声大 → 可能跳出局部最优

#### 当前值
```python
n_grad_mc_samples = 128  # 默认值
```

#### 推荐测试
```python
# 增加样本数提高精度
n_grad_mc_tests = [64, 128, 256, 512]
```

---

### 参数6: `graph_prior_str` 和 `edge_prior_prob` ⭐⭐⭐⭐⭐ **潜在关键**

#### 作用机制
图先验定义了边的先验概率分布：
- `'er'` (Erdős-Rényi): 每条边独立以概率p存在
- `'sf'` (Scale-free): 幂律度分布
- `'sbm'` (Stochastic Block Model): 社区结构

#### 当前值
```python
# causal_discovery.py 第112行
graph_prior_str='er'  # 硬编码
# edge_prior_prob 未指定（使用默认值，可能为0.1或0.5）
```

#### 问题诊断
**关键发现**: Erdős-Rényi先验的默认边概率可能**非常低**（如0.1），导致先验就偏向稀疏图！

#### 推荐测试
```python
# 测试不同的边先验概率
edge_prior_prob_tests = [0.1, 0.3, 0.5, 0.7, 0.9]
```

**注意**: 这需要修改`make_linear_gaussian_model`调用，传入`edge_prior_prob`参数。

---

## 🧪 参数调整实验方案

### 方案A: 单参数敏感性测试（推荐） ⭐⭐⭐

**目标**: 识别哪个参数对边数量影响最大

**测试矩阵**:

| 实验ID | alpha_linear | n_particles | beta_linear | tau | 预期结果 |
|-------|-------------|-------------|-------------|-----|----------|
| **A1** | 0.001 | 20 | 1.0 | 1.0 | 测试极小alpha |
| **A2** | 0.01 | 20 | 1.0 | 1.0 | 测试小alpha |
| **A3** | 0.05 | 20 | 1.0 | 1.0 | 测试论文默认alpha |
| **A4** | 0.1 | 50 | 1.0 | 1.0 | 增加粒子数 |
| **A5** | 0.1 | 100 | 1.0 | 1.0 | 大幅增加粒子数 |
| **A6** | 0.1 | 20 | 0.1 | 1.0 | 降低约束 |
| **A7** | 0.1 | 20 | 0.5 | 1.0 | 中等约束 |

**测试任务**: examples (219样本, 15变量) - DiBS v3已处理好的数据

**评估标准**:
- 成功: 检测到≥1条边
- 部分成功: 图矩阵max > 0.01（虽然<阈值但非全0）
- 失败: 图矩阵完全为0

**预估时间**: 每个实验15-30分钟 × 7 = 2-4小时

---

### 方案B: 组合优化测试（如果方案A成功）

**前提**: 方案A中至少一个参数组合产生了边

**测试矩阵**（基于A的最佳参数）:

| 实验ID | alpha_linear | n_particles | beta_linear | tau | n_grad_mc | 预期 |
|-------|-------------|-------------|-------------|-----|-----------|------|
| **B1** | 最佳 | 最佳 | 最佳 | 1.0 | 128 | 基线 |
| **B2** | 最佳 | 最佳×2 | 最佳 | 1.0 | 256 | 增强精度 |
| **B3** | 最佳×0.5 | 最佳 | 最佳×0.5 | 0.5 | 128 | 松弛约束 |

---

### 方案C: 修改图先验（需要代码修改）

**修改位置**: `causal_discovery.py` 第108-113行

```python
# 当前代码
_, graph_model, likelihood_model = make_linear_gaussian_model(
    key=subk,
    n_vars=self.n_vars,
    n_observations=len(data_continuous),
    graph_prior_str='er'  # ← 硬编码
)

# 修改为
_, graph_model, likelihood_model = make_linear_gaussian_model(
    key=subk,
    n_vars=self.n_vars,
    n_observations=len(data_continuous),
    graph_prior_str='er',
    edge_prior_prob=0.7  # ← 新增：偏向稠密图
)
```

**测试**: `edge_prior_prob = [0.3, 0.5, 0.7, 0.9]`

---

## 📊 预测与风险评估

### 成功概率评估

| 场景 | 概率 | 说明 |
|------|------|------|
| **参数调整产生边** | **10-20%** | 如果数据真的有因果结构，参数优化可能有效 |
| **仍然0边** | **80-90%** | 更可能是数据本身缺乏因果结构 |

### 关键风险

1. **根本性风险** ⭐⭐⭐⭐⭐:
   - DiBS失败报告已明确指出：**数据缺乏明确的因果方向**
   - 即使优化参数，如果数据真的没有因果关系，仍然会失败

2. **参数理解风险** ⭐⭐⭐:
   - 我们对`alpha_linear`的理解可能与论文/代码实现不同
   - 默认值0.05 vs 我们测试的0.1/0.5/0.9可能不在正确范围

3. **计算成本风险** ⭐⭐:
   - n_particles=100, steps=10000 → 耗时可能>1小时/实验
   - 7个实验 → 总耗时可能7-10小时

---

## ✅ 推荐行动计划

### 阶段1: 快速验证（1-2小时）⭐⭐⭐

**目标**: 快速判断参数调整是否有希望

**行动**:
```bash
# 实验A1: 测试极小alpha
alpha=0.001, n_particles=20, steps=5000

# 实验A2: 测试小alpha
alpha=0.01, n_particles=20, steps=5000

# 实验A5: 大幅增加粒子数
alpha=0.1, n_particles=100, steps=5000
```

**决策标准**:
- ✅ **如果任一实验图矩阵max > 0**: 继续阶段2
- ❌ **如果全部仍为0**: 放弃DiBS，确认失败报告结论

---

### 阶段2: 系统测试（4-6小时）

**前提**: 阶段1至少一个实验有希望

**行动**: 运行完整的方案A（7个实验）

---

### 阶段3: 深度优化（可选）

**前提**: 阶段2发现了最佳参数组合

**行动**:
1. 运行方案B（组合优化）
2. 修改代码实现方案C（图先验优化）
3. 在所有6个任务组上测试

---

## 🎯 预期结论

### 最可能的结果（80%概率）

**即使优化所有参数，DiBS仍然产生0边**

**原因**:
1. 能耗数据缺乏明确的因果方向（干预→中间→结果链）
2. 变量间关系主要是共同因驱动的相关性，而非直接因果
3. DiBS的线性高斯假设不满足

**建议**:
- ✅ 确认DiBS失败报告的结论
- ✅ 采用回归分析（已验证R²=0.999成功）
- ❌ 不再投入时间在DiBS或其他因果发现算法上

---

### 次可能的结果（15%概率）

**参数优化后产生少量边（1-5条）**

**后续行动**:
1. 仔细检查这些边的合理性
2. 与相关性分析对比
3. 可能需要降低阈值才能看到边

**风险**: 边可能是假阳性（统计噪声）

---

### 最乐观的结果（5%概率）

**参数优化后产生合理数量的边（>10条）**

**后续行动**:
1. 🎉 DiBS成功！
2. 在所有6个任务组上运行
3. 生成完整的因果图分析报告
4. 与回归分析结果对比验证

---

## 📝 实施检查清单

### 代码准备
- [ ] 修改`causal_discovery.py`使`n_particles`可配置（当前硬编码）
- [ ] （可选）修改`make_linear_gaussian_model`调用以支持`edge_prior_prob`
- [ ] 创建参数扫描脚本

### 实验执行
- [ ] 运行实验A1（alpha=0.001）
- [ ] 运行实验A2（alpha=0.01）
- [ ] 运行实验A5（n_particles=100）
- [ ] 分析阶段1结果
- [ ] **决策点**: 继续或放弃？

### 结果分析
- [ ] 记录每个实验的图矩阵统计（min, max, mean, std）
- [ ] 记录边数（不同阈值下）
- [ ] 与Adult成功案例对比
- [ ] 生成最终测试报告

---

## 📚 参考资料

### DiBS论文
- 论文: "DiBS: Differentiable Bayesian Structure Learning" (NeurIPS 2021)
- GitHub: https://github.com/larslorch/dibs
- 参数说明: 见论文附录

### 相关文档
- DiBS失败报告: `/home/green/energy_dl/nightly/analysis/docs/reports/DIBS_FINAL_FAILURE_REPORT_20251226.md`
- 方法对比报告: `/home/green/energy_dl/nightly/analysis/docs/reports/METHOD_COMPARISON_REPORT_20251226.md`
- 回归分析方案: `/home/green/energy_dl/nightly/analysis/docs/QUESTION1_REGRESSION_ANALYSIS_PLAN.md`

---

## 🔬 关键洞察

### 为什么之前的alpha测试可能无效？

**问题**: 我们测试了alpha=0.1, 0.5, 0.9，但这些值可能**都太大了**！

**DiBS源代码证据** (`dibs.py`第56行):
```python
alpha_linear=0.05  # 默认值
```

**alpha的真实作用**:
```python
# 在步骤t，alpha(t) = alpha_linear * t
# 例如，alpha_linear=0.05, steps=10000:
# t=0: alpha(0) = 0
# t=5000: alpha(5000) = 0.05 * 5000 = 250
# t=10000: alpha(10000) = 0.05 * 10000 = 500
```

**关键发现**:
- `alpha_linear=0.9`, `steps=10000` → 最终`alpha(10000) = 9000` ⚠️⚠️⚠️ **巨大！**
- 这会导致sigmoid几乎完全是阶跃函数
- 可能过早地将所有边概率推向0或1，丢失了中间探索

**结论**: **我们之前测试的alpha值可能全都太大了**！

**正确的测试范围**应该是:
```python
alpha_linear_correct = [0.001, 0.01, 0.05, 0.1, 0.2]
# 而不是[0.1, 0.5, 0.9]
```

---

## 🎯 最终建议

### 如果用户坚持尝试DiBS

**按优先级排序的测试计划**:

1. **最高优先级** ⭐⭐⭐⭐⭐: 测试极小的alpha值
   ```python
   alpha_linear = 0.001, n_particles = 20, steps = 5000
   alpha_linear = 0.01, n_particles = 20, steps = 5000
   ```

2. **高优先级** ⭐⭐⭐⭐: 大幅增加粒子数
   ```python
   alpha_linear = 0.05, n_particles = 100, steps = 10000
   ```

3. **中等优先级** ⭐⭐⭐: 降低无环约束
   ```python
   alpha_linear = 0.05, n_particles = 20, beta_linear = 0.1
   ```

### 如果用户接受现实

✅ **直接采用回归分析方案**
- 已验证成功（R²=0.999）
- 速度快（<1秒 vs DiBS的15分钟）
- 结果可解释
- 可回答实际研究问题

---

---

## ✅ 测试结果更新（2026-01-05）

### 测试完成状态

**测试日期**: 2026-01-04 20:31 - 2026-01-05 02:34
**测试实验数**: 11个
**成功率**: 11/11 (100%)

### 关键发现验证

✅ **假设1完全正确**: Alpha值范围错误是失败根因
- 使用alpha=0.001-0.05后，**所有11个实验全部检测到边**
- 最优配置（alpha=0.05, beta=0.1）检测到**23条强边**(>0.3)

✅ **假设2部分正确**: 降低beta显著提升边检测
- Beta从1.0降低到0.1，强边从16条增加到23条（+44%）

❌ **假设3错误**: 增加粒子数降低了边强度
- 20粒子: 平均17.6条强边 ← 最佳
- 100粒子: 平均7.0条强边 ← 更差

### 最终推荐配置（已验证）

```python
OPTIMAL_VALIDATED = {
    "alpha_linear": 0.05,
    "beta_linear": 0.1,
    "n_particles": 20,
    "tau": 1.0,
    "n_steps": 5000,
    "n_grad_mc_samples": 128
}

# 实测结果:
# - 强边(>0.3): 23条
# - 总边(>0.01): 123条
# - 耗时: 10.6分钟
# - 成功率: 100%
```

### 详细结果报告

完整测试结果和分析详见:
- **成功报告**: `DIBS_PARAMETER_TUNING_SUCCESS_REPORT_20260105.md`
- **实验数据**: `results/dibs_parameter_sweep/20260104_203108/`

---

**文档状态**: ✅ 测试完成并验证
**维护者**: Claude
**最后更新**: 2026-01-05
