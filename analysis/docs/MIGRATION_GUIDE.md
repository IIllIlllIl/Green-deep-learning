# å› æœåˆ†æç³»ç»Ÿè¿ç§»æŒ‡å—

**ç›®çš„**: å°†æœ¬ç³»ç»Ÿåº”ç”¨åˆ°æ–°çš„æ•°æ®é›†è¿›è¡Œå› æœåˆ†æ
**é€‚ç”¨åœºæ™¯**: æœºå™¨å­¦ä¹ å…¬å¹³æ€§ã€æ€§èƒ½æƒè¡¡ã€è¶…å‚æ•°åˆ†æç­‰
**æ›´æ–°æ—¶é—´**: 2025-12-21

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•](#å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•)
2. [æ•°æ®é›†è¦æ±‚è¯¦è§£](#æ•°æ®é›†è¦æ±‚è¯¦è§£)
3. [è¿ç§»æ­¥éª¤è¯¦è§£](#è¿ç§»æ­¥éª¤è¯¦è§£)
4. [é…ç½®æ–‡ä»¶å®šåˆ¶](#é…ç½®æ–‡ä»¶å®šåˆ¶)
5. [å¸¸è§åœºæ™¯æ¡ˆä¾‹](#å¸¸è§åœºæ™¯æ¡ˆä¾‹)
6. [æ³¨æ„äº‹é¡¹ä¸é™·é˜±](#æ³¨æ„äº‹é¡¹ä¸é™·é˜±)
7. [æ•…éšœæ’æŸ¥æŒ‡å—](#æ•…éšœæ’æŸ¥æŒ‡å—)

---

## å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•

### âœ… åœ¨å¼€å§‹ä¹‹å‰ç¡®è®¤

**å¿…é¡»æ¡ä»¶** (ç¼ºä¸€ä¸å¯):
```
â–¡ æ•°æ®é›†åŒ…å«æ•æ„Ÿå±æ€§ (å¦‚æ€§åˆ«ã€ç§æ—ã€å¹´é¾„ç­‰)
â–¡ æœ‰æ˜ç¡®çš„é¢„æµ‹ä»»åŠ¡ (äºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»)
â–¡ è‡³å°‘æœ‰ 500+ æ ·æœ¬ (è¶Šå¤šè¶Šå¥½)
â–¡ ç‰¹å¾å·²ç»æ˜¯æ•°å€¼å‹æˆ–å¯ä»¥ç¼–ç ä¸ºæ•°å€¼å‹
â–¡ æ ‡ç­¾æ˜¯ç¦»æ•£çš„ç±»åˆ« (0/1 æˆ–å¤šç±»)
```

**æ¨èæ¡ä»¶** (æé«˜æ•ˆæœ):
```
â–¡ æ ·æœ¬é‡ > 5000 (ç»Ÿè®¡åŠŸæ•ˆæ›´é«˜)
â–¡ æœ‰å¤šä¸ªå…¬å¹³æ€§æ•æ„Ÿå±æ€§å¯é€‰
â–¡ æ•°æ®é›†å·²ç»è¿‡åˆæ­¥æ¸…æ´— (æ— ç¼ºå¤±å€¼/å¼‚å¸¸å€¼)
â–¡ æœ‰é¢†åŸŸçŸ¥è¯†æŒ‡å¯¼æŒ‡æ ‡é€‰æ‹©
â–¡ æœ‰GPUèµ„æº (åŠ é€Ÿè®­ç»ƒ)
```

### ğŸ¯ æ ¸å¿ƒè¾“å…¥ç¡®è®¤

åœ¨å¼€å§‹è¿ç§»å‰ï¼Œæ‚¨éœ€è¦æ˜ç¡®ä»¥ä¸‹ä¿¡æ¯ï¼š

| è¾“å…¥é¡¹ | è¯´æ˜ | ç¤ºä¾‹ |
|--------|------|------|
| **æ•°æ®æ¥æº** | CSVæ–‡ä»¶è·¯å¾„ | `data/my_dataset.csv` |
| **ç‰¹å¾åˆ—** | ç”¨äºé¢„æµ‹çš„åˆ—ååˆ—è¡¨ | `['age', 'income', 'education', ...]` |
| **æ ‡ç­¾åˆ—** | é¢„æµ‹ç›®æ ‡åˆ—å | `'approved'` (è´·æ¬¾æ˜¯å¦æ‰¹å‡†) |
| **æ•æ„Ÿå±æ€§** | å…¬å¹³æ€§å…³æ³¨çš„åˆ— | `'gender'`, `'race'` |
| **ç‰¹æƒç±»åˆ«** | æ•æ„Ÿå±æ€§çš„ç‰¹æƒç»„ | `gender=Male`, `race=White` |
| **åˆ†æç›®æ ‡** | æƒ³å‘ç°ä»€ä¹ˆæƒè¡¡ | `accuracy vs fairness` |

---

## æ•°æ®é›†è¦æ±‚è¯¦è§£

### 1. æ•°æ®æ ¼å¼è¦æ±‚

#### 1.1 æ–‡ä»¶æ ¼å¼

**æ”¯æŒçš„æ ¼å¼**:
```python
âœ… CSVæ–‡ä»¶ (æ¨è)
âœ… Pandas DataFrame
âœ… NumPyæ•°ç»„ (éœ€è¦é¢å¤–å¤„ç†)
âš ï¸ Excelæ–‡ä»¶ (éœ€è¦è½¬æ¢ä¸ºCSV)
âŒ å›¾åƒ/æ–‡æœ¬æ•°æ® (éœ€è¦é¢„å…ˆæå–ç‰¹å¾)
```

**CSVç¤ºä¾‹**:
```csv
id,age,income,education,gender,race,credit_score,approved
1,25,35000,Bachelor,Female,Asian,650,0
2,45,75000,Master,Male,White,720,1
3,33,52000,Bachelor,Female,Black,680,1
...
```

#### 1.2 ç‰¹å¾ç±»å‹è¦æ±‚

**æ•°å€¼å‹ç‰¹å¾** (ç›´æ¥ä½¿ç”¨):
```python
age: [25, 45, 33, ...]           # è¿ç»­å‹
income: [35000, 75000, 52000]    # è¿ç»­å‹
credit_score: [650, 720, 680]    # è¿ç»­å‹
```

**åˆ†ç±»ç‰¹å¾** (éœ€è¦ç¼–ç ):
```python
# æ–¹å¼1: One-Hotç¼–ç  (æ¨è)
education: ['Bachelor', 'Master', 'PhD']
    â†“
education_Bachelor: [1, 0, 0]
education_Master: [0, 1, 0]
education_PhD: [0, 0, 1]

# æ–¹å¼2: æ ‡ç­¾ç¼–ç  (è°¨æ…ä½¿ç”¨)
education: ['Bachelor', 'Master', 'PhD']
    â†“
education_encoded: [0, 1, 2]  # å¯èƒ½æš—ç¤ºé¡ºåºå…³ç³»
```

**äºŒå€¼ç‰¹å¾** (ä¿æŒåŸæ ·):
```python
is_student: [0, 1, 0, ...]  # 0=No, 1=Yes
```

#### 1.3 æ•æ„Ÿå±æ€§è¦æ±‚

**å¿…é¡»æ˜¯äºŒå€¼æˆ–å¯ä»¥äºŒå€¼åŒ–**:
```python
âœ… æ­£ç¡®ç¤ºä¾‹:
gender: [0, 1, 0, 1, ...]        # 0=Female, 1=Male
race: [0, 1, 0, 0, ...]          # 0=Minority, 1=Majority

âš ï¸ éœ€è¦å¤„ç†:
gender: ['F', 'M', 'F', 'M']     # éœ€è¦æ˜ å°„ä¸º 0/1
race: ['Asian', 'White', 'Black'] # éœ€è¦äºŒå€¼åŒ– (å¦‚ White vs Non-White)

âŒ ä¸æ”¯æŒ:
age_group: [1, 2, 3, 4]          # å¤šç±»åˆ«ï¼Œéœ€è¦é€‰æ‹©äºŒå€¼åˆ†å‰²
```

#### 1.4 æ ‡ç­¾è¦æ±‚

**åˆ†ç±»ä»»åŠ¡**:
```python
âœ… äºŒåˆ†ç±» (æœ€å¸¸è§):
y: [0, 1, 0, 1, ...]  # 0=Negative, 1=Positive

âœ… å¤šåˆ†ç±» (éœ€è¦è½¬æ¢):
y: [0, 1, 2]  # 3ç±»
    â†“ è½¬æ¢ä¸ºä¸€å¯¹å¤š (One-vs-Rest)
y_binary: [0, 1, 0]  # ç±»åˆ«1 vs å…¶ä»–

âŒ å›å½’ä»»åŠ¡ (å½“å‰ä¸æ”¯æŒ):
y: [35000.5, 72000.3, ...]  # è¿ç»­å€¼
```

### 2. æ•°æ®è§„æ¨¡è¦æ±‚

#### 2.1 æ ·æœ¬é‡å»ºè®®

| æ ·æœ¬é‡ | æ•ˆæœè¯„ä¼° | å»ºè®®é…ç½®æ•° | DiBSè¿­ä»£æ•° | é¢„æœŸè€—æ—¶ |
|--------|---------|-----------|-----------|---------|
| **< 500** | âŒ ä¸æ¨è | - | - | - |
| **500 - 2K** | âš ï¸ å‹‰å¼ºå¯è¡Œ | 6-10 | 2000 | ~30åˆ†é’Ÿ |
| **2K - 10K** | âœ… è‰¯å¥½ | 10-20 | 3000 | ~1-2å°æ—¶ |
| **10K - 50K** | âœ… å¾ˆå¥½ | 20-50 | 5000 | ~3-6å°æ—¶ |
| **> 50K** | âœ… ä¼˜ç§€ | 50-100 | 5000-10000 | ~6-12å°æ—¶ |

**Adultæ•°æ®é›†å‚è€ƒ** (45Kæ ·æœ¬):
- é…ç½®æ•°: 10
- DiBSè¿­ä»£: 3000
- æ€»è€—æ—¶: 61åˆ†é’Ÿ

#### 2.2 ç‰¹å¾æ•°é‡å»ºè®®

| ç‰¹å¾æ•° | æ•ˆæœ | æ³¨æ„äº‹é¡¹ |
|--------|------|---------|
| **< 10** | âš ï¸ å¯èƒ½ä¿¡æ¯ä¸è¶³ | å¢åŠ ç‰¹å¾å·¥ç¨‹ |
| **10 - 50** | âœ… ç†æƒ³ | å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ |
| **50 - 200** | âœ… è‰¯å¥½ | éœ€è¦GPUåŠ é€Ÿ |
| **> 200** | âš ï¸ å¯èƒ½è¿‡å¤š | è€ƒè™‘ç‰¹å¾é€‰æ‹©/é™ç»´ |

**Adultæ•°æ®é›†å‚è€ƒ**: 102ä¸ªç‰¹å¾ (One-Hotç¼–ç å)

#### 2.3 é…ç½®æ•°é‡å»ºè®®

**é…ç½® = æ–¹æ³• Ã— è¶…å‚æ•°ç»„åˆ**

```python
# ç¤ºä¾‹1: åŸºç¡€é…ç½®
METHODS = ['Baseline', 'Reweighing']  # 2ä¸ªæ–¹æ³•
ALPHA_VALUES = [0.0, 0.5, 1.0]        # 3ä¸ªalpha
æ€»é…ç½®æ•° = 2 Ã— 3 = 6ä¸ª

# ç¤ºä¾‹2: æ‰©å±•é…ç½®
METHODS = ['Baseline', 'Reweighing', 'Adversarial']  # 3ä¸ªæ–¹æ³•
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]           # 5ä¸ªalpha
MODEL_WIDTHS = [1, 2, 3]                             # 3ä¸ªæ¨¡å‹å®½åº¦
æ€»é…ç½®æ•° = 3 Ã— 5 Ã— 3 = 45ä¸ª

# æ¨èé…ç½®æ•°
æœ€å°: 6ä¸ª (å¿«é€ŸéªŒè¯)
æ ‡å‡†: 10-20ä¸ª (å¹³è¡¡)
å®Œæ•´: 50-100ä¸ª (è®ºæ–‡çº§åˆ«)
```

### 3. æ•°æ®è´¨é‡è¦æ±‚

#### 3.1 ç¼ºå¤±å€¼å¤„ç†

**æ£€æŸ¥ç¼ºå¤±å€¼**:
```python
import pandas as pd

df = pd.read_csv('your_data.csv')
missing_summary = df.isnull().sum()
print(missing_summary)

# è¾“å‡ºç¤ºä¾‹:
# age           0
# income        15    â† æœ‰ç¼ºå¤±
# education     0
# gender        3     â† æœ‰ç¼ºå¤±
```

**å¤„ç†ç­–ç•¥**:
```python
# ç­–ç•¥1: åˆ é™¤ç¼ºå¤±æ ·æœ¬ (æ¨èï¼Œå¦‚æœç¼ºå¤±<5%)
df_clean = df.dropna()

# ç­–ç•¥2: å¡«å……æ•°å€¼ç‰¹å¾ (å‡å€¼/ä¸­ä½æ•°)
df['income'].fillna(df['income'].median(), inplace=True)

# ç­–ç•¥3: å¡«å……åˆ†ç±»ç‰¹å¾ (ä¼—æ•°/æ–°ç±»åˆ«)
df['gender'].fillna(df['gender'].mode()[0], inplace=True)
# æˆ–
df['gender'].fillna('Unknown', inplace=True)

# âš ï¸ ä¸æ¨è: å¤æ‚æ’è¡¥ (å¯èƒ½å¼•å…¥åå·®)
```

**Adultæ•°æ®é›†ç¤ºä¾‹**:
```python
# åŸå§‹: 48,842æ ·æœ¬
# ç¼ºå¤±: 3,620æ ·æœ¬ (7.4%)
# æ¸…æ´—å: 45,222æ ·æœ¬
# ç­–ç•¥: ç›´æ¥åˆ é™¤ç¼ºå¤±è¡Œ
```

#### 3.2 å¼‚å¸¸å€¼æ£€æµ‹

**æ•°å€¼ç‰¹å¾å¼‚å¸¸å€¼**:
```python
import numpy as np

# æ–¹æ³•1: IQR (å››åˆ†ä½è·) æ–¹æ³•
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1

# å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# æ ‡è®°å¼‚å¸¸å€¼
outliers = (df['income'] < lower_bound) | (df['income'] > upper_bound)
print(f"å¼‚å¸¸å€¼æ•°é‡: {outliers.sum()}")

# å¤„ç†: åˆ é™¤æˆ–æˆªæ–­
df_clean = df[~outliers]  # åˆ é™¤
# æˆ–
df['income'] = df['income'].clip(lower_bound, upper_bound)  # æˆªæ–­
```

**åˆ†ç±»ç‰¹å¾å¼‚å¸¸å€¼**:
```python
# æ£€æŸ¥ç½•è§ç±»åˆ«
value_counts = df['education'].value_counts()
print(value_counts)

# è¾“å‡ºç¤ºä¾‹:
# Bachelor     15000
# HS-grad      12000
# Master        8000
# PhD           3000
# Preschool       50  â† ç½•è§ç±»åˆ«

# å¤„ç†: åˆå¹¶ç½•è§ç±»åˆ«
rare_threshold = 100
rare_categories = value_counts[value_counts < rare_threshold].index
df['education'] = df['education'].replace(rare_categories, 'Other')
```

#### 3.3 ç±»åˆ«ä¸å¹³è¡¡

**æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ**:
```python
label_dist = df['approved'].value_counts()
print(label_dist)
print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {label_dist.max() / label_dist.min():.2f}")

# è¾“å‡ºç¤ºä¾‹:
# 0 (æ‹’ç»)    34000
# 1 (æ‰¹å‡†)    11000
# ä¸å¹³è¡¡æ¯”ä¾‹: 3.09
```

**å¤„ç†ç­–ç•¥**:
```python
# ç­–ç•¥1: ä»€ä¹ˆéƒ½ä¸åš (æ¯”ä¾‹<5:1)
# é€‚ç”¨: Adultæ•°æ®é›† (3:1æ¯”ä¾‹)

# ç­–ç•¥2: é‡é‡‡æ · (æ¯”ä¾‹5:1 ~ 10:1)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# ç­–ç•¥3: åŠ æƒæŸå¤± (æ¯”ä¾‹>10:1)
# åœ¨æ¨¡å‹è®­ç»ƒä¸­è‡ªåŠ¨å¤„ç†
class_weights = {0: 1.0, 1: 3.0}  # ç»™å°‘æ•°ç±»æ›´é«˜æƒé‡

# âš ï¸ æ³¨æ„: æœ¬ç³»ç»Ÿçš„Reweighingä¼šè‡ªåŠ¨å¹³è¡¡ï¼Œæ— éœ€é¢å¤–å¤„ç†
```

---

## è¿ç§»æ­¥éª¤è¯¦è§£

### æ­¥éª¤1: æ•°æ®å‡†å¤‡ä¸éªŒè¯

#### 1.1 åˆ›å»ºæ•°æ®åŠ è½½è„šæœ¬

**åˆ›å»ºæ–‡ä»¶**: `load_my_dataset.py`

```python
"""
è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½è„šæœ¬
æ›¿æ¢Adultæ•°æ®é›†ä¸ºæ‚¨çš„æ•°æ®é›†
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

def load_my_dataset():
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ‚¨çš„æ•°æ®é›†

    è¿”å›:
        X_train, X_test: ç‰¹å¾çŸ©é˜µ
        y_train, y_test: æ ‡ç­¾å‘é‡
        sensitive_train, sensitive_test: æ•æ„Ÿå±æ€§
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    """

    # === 1. åŠ è½½åŸå§‹æ•°æ® ===
    print("åŠ è½½æ•°æ®...")
    df = pd.read_csv('data/my_dataset.csv')
    print(f"åŸå§‹æ•°æ®: {len(df)} æ ·æœ¬, {len(df.columns)} åˆ—")

    # === 2. å¤„ç†ç¼ºå¤±å€¼ ===
    print("\nå¤„ç†ç¼ºå¤±å€¼...")
    print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")

    # åˆ é™¤ç¼ºå¤±å€¼ (æˆ–ä½¿ç”¨å…¶ä»–ç­–ç•¥)
    df_clean = df.dropna()
    print(f"æ¸…æ´—å: {len(df_clean)} æ ·æœ¬ (åˆ é™¤ {len(df) - len(df_clean)} è¡Œ)")

    # === 3. å®šä¹‰ç‰¹å¾ã€æ ‡ç­¾ã€æ•æ„Ÿå±æ€§ ===
    # âš ï¸ æ ¹æ®æ‚¨çš„æ•°æ®é›†ä¿®æ”¹è¿™äº›åˆ—å
    label_col = 'approved'           # é¢„æµ‹ç›®æ ‡
    sensitive_col = 'gender'          # æ•æ„Ÿå±æ€§

    # è¦æ’é™¤çš„åˆ— (IDã€æ ‡ç­¾ã€æ•æ„Ÿå±æ€§ç­‰)
    exclude_cols = ['id', label_col, sensitive_col]

    # ç‰¹å¾åˆ— = æ‰€æœ‰åˆ— - æ’é™¤åˆ—
    feature_cols = [col for col in df_clean.columns if col not in exclude_cols]

    print(f"\nç‰¹å¾åˆ— ({len(feature_cols)}ä¸ª): {feature_cols}")

    # === 4. ç¼–ç åˆ†ç±»ç‰¹å¾ ===
    print("\nç¼–ç åˆ†ç±»ç‰¹å¾...")

    # è¯†åˆ«åˆ†ç±»åˆ—
    categorical_cols = df_clean[feature_cols].select_dtypes(
        include=['object', 'category']
    ).columns.tolist()

    print(f"åˆ†ç±»ç‰¹å¾: {categorical_cols}")

    # One-Hotç¼–ç 
    df_encoded = pd.get_dummies(
        df_clean,
        columns=categorical_cols,
        drop_first=False  # ä¿ç•™æ‰€æœ‰ç±»åˆ«
    )

    # æ›´æ–°ç‰¹å¾åˆ—å (å› ä¸ºOne-Hotç¼–ç ä¼šæ”¹å˜åˆ—å)
    feature_cols = [col for col in df_encoded.columns
                   if col not in exclude_cols]

    print(f"ç¼–ç åç‰¹å¾æ•°: {len(feature_cols)}")

    # === 5. æå–æ•°æ® ===
    X = df_encoded[feature_cols].values
    y = df_encoded[label_col].values

    # === 6. å¤„ç†æ•æ„Ÿå±æ€§ (å¿…é¡»æ˜¯0/1) ===
    print("\nå¤„ç†æ•æ„Ÿå±æ€§...")

    if df_clean[sensitive_col].dtype == 'object':
        # åˆ†ç±»å‹ â†’ äºŒå€¼åŒ–
        unique_vals = df_clean[sensitive_col].unique()
        print(f"æ•æ„Ÿå±æ€§å”¯ä¸€å€¼: {unique_vals}")

        # âš ï¸ å®šä¹‰å“ªä¸ªæ˜¯ç‰¹æƒç»„ (privilege=1)
        privilege_group = 'Male'  # æ ¹æ®æ‚¨çš„æ•°æ®ä¿®æ”¹

        sensitive = (df_clean[sensitive_col] == privilege_group).astype(int).values
        print(f"ç‰¹æƒç»„ ({privilege_group}): {sensitive.sum()} æ ·æœ¬")
        print(f"éç‰¹æƒç»„: {len(sensitive) - sensitive.sum()} æ ·æœ¬")
    else:
        # å·²ç»æ˜¯æ•°å€¼å‹
        sensitive = df_clean[sensitive_col].values
        assert set(sensitive) <= {0, 1}, "æ•æ„Ÿå±æ€§å¿…é¡»æ˜¯0æˆ–1"

    # === 7. å¤„ç†æ ‡ç­¾ (å¿…é¡»æ˜¯0/1) ===
    print("\nå¤„ç†æ ‡ç­¾...")

    if y.dtype == 'object' or len(np.unique(y)) > 2:
        # åˆ†ç±»å‹æˆ–å¤šç±» â†’ äºŒå€¼åŒ–
        le = LabelEncoder()
        y = le.fit_transform(y)
        print(f"æ ‡ç­¾æ˜ å°„: {dict(zip(le.classes_, range(len(le.classes_))))}")

    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")

    # === 8. æ•°æ®åˆ†å‰² ===
    print("\nåˆ†å‰²æ•°æ®...")
    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
        X, y, sensitive,
        test_size=0.3,
        random_state=42,
        stratify=y  # ä¿æŒæ ‡ç­¾åˆ†å¸ƒ
    )

    print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # === 9. ç‰¹å¾æ ‡å‡†åŒ– ===
    print("\næ ‡å‡†åŒ–ç‰¹å¾...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # === 10. éªŒè¯æ•°æ® ===
    print("\næ•°æ®éªŒè¯...")
    assert X_train.shape[1] == X_test.shape[1], "è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾æ•°ä¸ä¸€è‡´"
    assert not np.any(np.isnan(X_train)), "è®­ç»ƒé›†åŒ…å«NaN"
    assert not np.any(np.isnan(X_test)), "æµ‹è¯•é›†åŒ…å«NaN"
    assert set(y_train) <= {0, 1}, "æ ‡ç­¾å¿…é¡»æ˜¯0æˆ–1"
    assert set(sensitive_train) <= {0, 1}, "æ•æ„Ÿå±æ€§å¿…é¡»æ˜¯0æˆ–1"

    print("âœ… æ•°æ®éªŒè¯é€šè¿‡")

    # === 11. è¿”å›ç»“æœ ===
    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'sensitive_train': sensitive_train,
        'sensitive_test': sensitive_test,
        'n_features': X_train.shape[1],
        'feature_names': feature_cols
    }

# æµ‹è¯•åŠ è½½
if __name__ == '__main__':
    data = load_my_dataset()
    print("\næœ€ç»ˆæ•°æ®å½¢çŠ¶:")
    print(f"  X_train: {data['X_train'].shape}")
    print(f"  X_test: {data['X_test'].shape}")
    print(f"  ç‰¹å¾æ•°: {data['n_features']}")
```

#### 1.2 æ•°æ®éªŒè¯æ£€æŸ¥

**åˆ›å»ºéªŒè¯è„šæœ¬**: `validate_data.py`

```python
"""
æ•°æ®è´¨é‡éªŒè¯è„šæœ¬
åœ¨æ­£å¼è®­ç»ƒå‰è¿è¡Œï¼Œç¡®ä¿æ•°æ®ç¬¦åˆè¦æ±‚
"""
import numpy as np
from load_my_dataset import load_my_dataset

def validate_dataset():
    """éªŒè¯æ•°æ®é›†æ˜¯å¦ç¬¦åˆè¦æ±‚"""

    print("="*70)
    print("æ•°æ®é›†éªŒè¯æŠ¥å‘Š")
    print("="*70)

    # åŠ è½½æ•°æ®
    data = load_my_dataset()

    X_train = data['X_train']
    y_train = data['y_train']
    sensitive_train = data['sensitive_train']

    # === éªŒè¯1: æ ·æœ¬é‡ ===
    print("\n1. æ ·æœ¬é‡æ£€æŸ¥")
    n_samples = len(X_train)
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {n_samples}")

    if n_samples < 500:
        print("   âŒ è­¦å‘Š: æ ·æœ¬é‡å¤ªå°‘ (<500), ç»“æœå¯èƒ½ä¸å¯é ")
    elif n_samples < 2000:
        print("   âš ï¸  æ³¨æ„: æ ·æœ¬é‡è¾ƒå°‘ (<2000), å»ºè®®å¢åŠ æ ·æœ¬")
    else:
        print("   âœ… æ ·æœ¬é‡å……è¶³")

    # === éªŒè¯2: ç‰¹å¾æ•° ===
    print("\n2. ç‰¹å¾æ•°æ£€æŸ¥")
    n_features = X_train.shape[1]
    print(f"   ç‰¹å¾æ•°: {n_features}")

    if n_features < 5:
        print("   âŒ è­¦å‘Š: ç‰¹å¾å¤ªå°‘, å¯èƒ½ä¿¡æ¯ä¸è¶³")
    elif n_features > 500:
        print("   âš ï¸  æ³¨æ„: ç‰¹å¾å¾ˆå¤š, è€ƒè™‘é™ç»´")
    else:
        print("   âœ… ç‰¹å¾æ•°åˆç†")

    # === éªŒè¯3: æ•°æ®ç±»å‹ ===
    print("\n3. æ•°æ®ç±»å‹æ£€æŸ¥")
    print(f"   Xç±»å‹: {X_train.dtype}")
    print(f"   yç±»å‹: {y_train.dtype}")

    assert X_train.dtype in [np.float32, np.float64], "âŒ Xå¿…é¡»æ˜¯æµ®ç‚¹å‹"
    assert y_train.dtype in [np.int32, np.int64], "âŒ yå¿…é¡»æ˜¯æ•´æ•°å‹"
    print("   âœ… æ•°æ®ç±»å‹æ­£ç¡®")

    # === éªŒè¯4: ç¼ºå¤±å€¼ ===
    print("\n4. ç¼ºå¤±å€¼æ£€æŸ¥")
    n_missing = np.isnan(X_train).sum()
    print(f"   ç¼ºå¤±å€¼æ•°é‡: {n_missing}")

    assert n_missing == 0, "âŒ å‘ç°ç¼ºå¤±å€¼, è¯·å…ˆå¤„ç†"
    print("   âœ… æ— ç¼ºå¤±å€¼")

    # === éªŒè¯5: æ ‡ç­¾åˆ†å¸ƒ ===
    print("\n5. æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥")
    label_counts = np.bincount(y_train)
    print(f"   ç±»åˆ«0: {label_counts[0]} ({label_counts[0]/n_samples*100:.1f}%)")
    print(f"   ç±»åˆ«1: {label_counts[1]} ({label_counts[1]/n_samples*100:.1f}%)")

    imbalance_ratio = label_counts.max() / label_counts.min()
    print(f"   ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")

    if imbalance_ratio > 10:
        print("   âš ï¸  è­¦å‘Š: ä¸¥é‡ä¸å¹³è¡¡, è€ƒè™‘é‡é‡‡æ ·")
    elif imbalance_ratio > 5:
        print("   âš ï¸  æ³¨æ„: ä¸­åº¦ä¸å¹³è¡¡")
    else:
        print("   âœ… åˆ†å¸ƒç›¸å¯¹å¹³è¡¡")

    # === éªŒè¯6: æ•æ„Ÿå±æ€§åˆ†å¸ƒ ===
    print("\n6. æ•æ„Ÿå±æ€§åˆ†å¸ƒæ£€æŸ¥")
    sensitive_counts = np.bincount(sensitive_train)
    print(f"   éç‰¹æƒç»„ (0): {sensitive_counts[0]} ({sensitive_counts[0]/n_samples*100:.1f}%)")
    print(f"   ç‰¹æƒç»„ (1): {sensitive_counts[1]} ({sensitive_counts[1]/n_samples*100:.1f}%)")

    sensitive_ratio = sensitive_counts.max() / sensitive_counts.min()
    print(f"   æ¯”ä¾‹: {sensitive_ratio:.2f}:1")

    if sensitive_counts.min() < 100:
        print("   âš ï¸  è­¦å‘Š: æŸç»„æ ·æœ¬å¤ªå°‘ (<100)")
    else:
        print("   âœ… ä¸¤ç»„æ ·æœ¬é‡å……è¶³")

    # === éªŒè¯7: ç‰¹å¾åˆ†å¸ƒ ===
    print("\n7. ç‰¹å¾åˆ†å¸ƒæ£€æŸ¥")
    feature_means = X_train.mean(axis=0)
    feature_stds = X_train.std(axis=0)

    print(f"   å‡å€¼èŒƒå›´: [{feature_means.min():.3f}, {feature_means.max():.3f}]")
    print(f"   æ ‡å‡†å·®èŒƒå›´: [{feature_stds.min():.3f}, {feature_stds.max():.3f}]")

    # æ£€æŸ¥æ˜¯å¦æ ‡å‡†åŒ–
    if np.abs(feature_means.mean()) < 0.1 and np.abs(feature_stds.mean() - 1.0) < 0.1:
        print("   âœ… ç‰¹å¾å·²æ ‡å‡†åŒ–")
    else:
        print("   âš ï¸  æ³¨æ„: ç‰¹å¾å¯èƒ½æœªæ ‡å‡†åŒ–")

    # === éªŒè¯8: å¸¸æ•°ç‰¹å¾ ===
    print("\n8. å¸¸æ•°ç‰¹å¾æ£€æŸ¥")
    constant_features = (feature_stds < 1e-8).sum()
    print(f"   å¸¸æ•°ç‰¹å¾æ•°: {constant_features}")

    if constant_features > 0:
        print("   âš ï¸  è­¦å‘Š: å‘ç°å¸¸æ•°ç‰¹å¾, åº”è¯¥ç§»é™¤")
    else:
        print("   âœ… æ— å¸¸æ•°ç‰¹å¾")

    # === éªŒè¯9: ç›¸å…³æ€§æ£€æŸ¥ ===
    print("\n9. ç‰¹å¾ç›¸å…³æ€§æ£€æŸ¥")
    if n_features < 100:  # ç‰¹å¾ä¸å¤ªå¤šæ—¶æ‰æ£€æŸ¥
        corr_matrix = np.corrcoef(X_train.T)
        high_corr = (np.abs(corr_matrix) > 0.95) & (np.abs(corr_matrix) < 1.0)
        n_high_corr = high_corr.sum() // 2  # é™¤ä»¥2å› ä¸ºå¯¹ç§°

        print(f"   é«˜åº¦ç›¸å…³ç‰¹å¾å¯¹æ•°: {n_high_corr} (ç›¸å…³ç³»æ•°>0.95)")

        if n_high_corr > n_features * 0.1:
            print("   âš ï¸  è­¦å‘Š: è¿‡å¤šé«˜åº¦ç›¸å…³ç‰¹å¾, è€ƒè™‘ç§»é™¤")
        else:
            print("   âœ… ç‰¹å¾ç›¸å…³æ€§åˆç†")
    else:
        print("   â­ï¸  ç‰¹å¾å¤ªå¤š, è·³è¿‡ç›¸å…³æ€§æ£€æŸ¥")

    # === æ€»ç»“ ===
    print("\n" + "="*70)
    print("éªŒè¯å®Œæˆ")
    print("="*70)
    print("\nå¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ (âœ…), æ•°æ®é›†å¯ä»¥ç”¨äºè®­ç»ƒ")
    print("å¦‚æœæœ‰è­¦å‘Š (âš ï¸), å»ºè®®å…ˆè§£å†³å†ç»§ç»­")
    print("å¦‚æœæœ‰é”™è¯¯ (âŒ), å¿…é¡»ä¿®å¤æ‰èƒ½ç»§ç»­")

    return True

if __name__ == '__main__':
    validate_dataset()
```

**è¿è¡ŒéªŒè¯**:
```bash
python validate_data.py
```

### æ­¥éª¤2: åˆ›å»ºä¸»å®éªŒè„šæœ¬

#### 2.1 å¤åˆ¶å¹¶ä¿®æ”¹æ¨¡æ¿

**åˆ›å»ºæ–‡ä»¶**: `demo_my_dataset.py`

```python
"""
è‡ªå®šä¹‰æ•°æ®é›†å› æœåˆ†æ
åŸºäº demo_adult_full_analysis.py ä¿®æ”¹
"""
import numpy as np
import pandas as pd
import sys
import os
import time
import torch
from datetime import datetime

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨
from load_my_dataset import load_my_dataset

from utils.model import FFNN, ModelTrainer
from utils.metrics import MetricsCalculator
from utils.fairness_methods import get_fairness_method

# ============================================================================
# é…ç½®åŒº - âš ï¸ æ ¹æ®æ‚¨çš„éœ€æ±‚ä¿®æ”¹
# ============================================================================

# æ•°æ®é›†é…ç½®
DATASET_NAME = 'MyDataset'  # ä¿®æ”¹ä¸ºæ‚¨çš„æ•°æ®é›†åç§°

# å…¬å¹³æ€§æ–¹æ³•é…ç½®
METHODS = ['Baseline', 'Reweighing']  # å¯æ·»åŠ å…¶ä»–æ–¹æ³•

# è¶…å‚æ•°é…ç½®
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]  # å…¬å¹³æ€§æƒé‡

# æ¨¡å‹é…ç½®
EPOCHS = 50           # è®­ç»ƒè½®æ•° (å¯å‡å°‘ä»¥åŠ å¿«é€Ÿåº¦)
MODEL_WIDTH = 2       # ç½‘ç»œå®½åº¦å€æ•°
BATCH_SIZE = 256      # æ‰¹æ¬¡å¤§å°

# DiBSé…ç½®
DIBS_STEPS = 3000     # DiBSè¿­ä»£æ¬¡æ•° (æ ·æœ¬å¤šå¯å¢åŠ åˆ°5000)

# è®¾å¤‡é…ç½®
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

print("="*70)
print(f"  {DATASET_NAME} å®Œæ•´å› æœåˆ†æ")
print("="*70)
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"è®¾å¤‡: {device}")
if device == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")

start_time = time.time()
os.makedirs('results', exist_ok=True)
os.makedirs('data', exist_ok=True)

# ============================================================================
# æ­¥éª¤1: åŠ è½½æ•°æ®
# ============================================================================
print("\n" + "="*70)
print("  æ­¥éª¤1: åŠ è½½æ•°æ®")
print("="*70)

data = load_my_dataset()
X_train = data['X_train']
X_test = data['X_test']
y_train = data['y_train']
y_test = data['y_test']
sensitive_train = data['sensitive_train']
sensitive_test = data['sensitive_test']
n_features = data['n_features']

print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
print(f"  ç‰¹å¾æ•°: {n_features}")

# ============================================================================
# æ­¥éª¤2: æ•°æ®æ”¶é›†
# ============================================================================
print("\n" + "="*70)
print("  æ­¥éª¤2: æ•°æ®æ”¶é›†")
print("="*70)

results = []
total_configs = len(METHODS) * len(ALPHA_VALUES)

for idx, (method_name, alpha) in enumerate(
    [(m, a) for m in METHODS for a in ALPHA_VALUES], 1
):
    config_start = time.time()
    print(f"\n  [{idx}/{total_configs}] {method_name}, Î±={alpha:.2f}")

    try:
        # åº”ç”¨å…¬å¹³æ€§æ–¹æ³•
        method = get_fairness_method(
            method_name, alpha,
            sensitive_attr='sensitive'  # âš ï¸ ä¿®æ”¹ä¸ºæ‚¨çš„æ•æ„Ÿå±æ€§å
        )
        X_transformed, y_transformed = method.fit_transform(
            X_train, y_train, sensitive_train
        )

        # è®­ç»ƒæ¨¡å‹
        model = FFNN(input_dim=n_features, width=MODEL_WIDTH)
        trainer = ModelTrainer(model, device=device, lr=0.001)
        trainer.train(
            X_transformed, y_transformed,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=False
        )

        # è®¡ç®—æŒ‡æ ‡
        calculator = MetricsCalculator(
            trainer,
            sensitive_attr='sensitive'  # âš ï¸ ä¿®æ”¹ä¸ºæ‚¨çš„æ•æ„Ÿå±æ€§å
        )

        dataset_metrics = calculator.compute_all_metrics(
            X_train, y_train, sensitive_train, phase='D'
        )
        train_metrics = calculator.compute_all_metrics(
            X_transformed, y_transformed, sensitive_train, phase='Tr'
        )
        test_metrics = calculator.compute_all_metrics(
            X_test, y_test, sensitive_test, phase='Te'
        )

        # æ”¶é›†ç»“æœ
        row = {
            'method': method_name,
            'alpha': alpha,
            'Width': MODEL_WIDTH
        }
        row.update(dataset_metrics)
        row.update(train_metrics)
        row.update(test_metrics)
        results.append(row)

        # æ˜¾ç¤ºè¿›åº¦
        elapsed = time.time() - start_time
        eta = (total_configs - idx) * (elapsed / idx) / 60
        print(f"    âœ“ Acc={test_metrics.get('Te_Acc', 0):.3f} | "
              f"è€—æ—¶={time.time()-config_start:.0f}s | ETA={eta:.1f}min")

    except Exception as e:
        print(f"    âœ— å¤±è´¥: {e}")
        continue

# ä¿å­˜æ•°æ®
df = pd.DataFrame(results)
output_path = f'data/{DATASET_NAME.lower()}_training_data.csv'
df.to_csv(output_path, index=False)

print(f"\nâœ“ æ•°æ®æ”¶é›†å®Œæˆï¼Œä¿å­˜åˆ°: {output_path}")

# ============================================================================
# æ­¥éª¤3: DiBSå› æœå›¾å­¦ä¹ 
# ============================================================================
print("\n" + "="*70)
print("  æ­¥éª¤3: DiBSå› æœå›¾å­¦ä¹ ")
print("="*70)

try:
    from utils.causal_discovery import CausalGraphLearner

    # å‡†å¤‡æ•°æ®
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'Width' in numeric_cols:
        numeric_cols.remove('Width')

    causal_data = df[numeric_cols]

    print(f"  å˜é‡æ•°: {len(numeric_cols)}")
    print(f"  æ•°æ®ç‚¹: {len(causal_data)}")

    # åˆ›å»ºå­¦ä¹ å™¨
    learner = CausalGraphLearner(
        n_vars=len(numeric_cols),
        n_steps=DIBS_STEPS,
        alpha=0.1,
        random_seed=42
    )

    # å­¦ä¹ å› æœå›¾
    print(f"\n  å¼€å§‹DiBSå­¦ä¹ ...")
    dibs_start = time.time()

    causal_graph = learner.fit(causal_data, verbose=True)

    print(f"\n  âœ“ DiBSå®Œæˆï¼Œè€—æ—¶: {(time.time()-dibs_start)/60:.1f}åˆ†é’Ÿ")

    # åˆ†æè¾¹
    edges = learner.get_edges(threshold=0.3)
    print(f"  æ£€æµ‹åˆ° {len(edges)} æ¡å› æœè¾¹")

    # ä¿å­˜ç»“æœ
    graph_path = f'results/{DATASET_NAME.lower()}_causal_graph.npy'
    learner.save_graph(graph_path)
    print(f"  âœ“ å› æœå›¾å·²ä¿å­˜åˆ°: {graph_path}")

    # æ˜¾ç¤ºå…³é”®è¾¹
    if len(edges) > 0:
        print(f"\n  å‰10æ¡æœ€å¼ºå› æœè¾¹:")
        for i, (source, target, weight) in enumerate(edges[:10], 1):
            print(f"    {i}. {numeric_cols[source]} â†’ {numeric_cols[target]}: {weight:.3f}")

except Exception as e:
    print(f"  âœ— DiBSå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    causal_graph = None
    edges = []

# ============================================================================
# æ­¥éª¤4: DMLå› æœæ¨æ–­
# ============================================================================
if causal_graph is not None and len(edges) > 0:
    print("\n" + "="*70)
    print("  æ­¥éª¤4: DMLå› æœæ¨æ–­")
    print("="*70)

    try:
        from utils.causal_inference import CausalInferenceEngine

        engine = CausalInferenceEngine(verbose=True)

        print(f"\n  å¼€å§‹DMLåˆ†æ...")
        dml_start = time.time()

        causal_effects = engine.analyze_all_edges(
            data=causal_data,
            causal_graph=causal_graph,
            var_names=numeric_cols,
            threshold=0.3
        )

        print(f"\n  âœ“ DMLå®Œæˆï¼Œè€—æ—¶: {(time.time()-dml_start)/60:.1f}åˆ†é’Ÿ")

        if causal_effects:
            effects_path = f'results/{DATASET_NAME.lower()}_causal_effects.csv'
            engine.save_results(effects_path)
            print(f"  âœ“ å› æœæ•ˆåº”å·²ä¿å­˜åˆ°: {effects_path}")

            significant = engine.get_significant_effects()
            print(f"\n  å› æœæ•ˆåº”ç»Ÿè®¡:")
            print(f"    æ€»è¾¹æ•°: {len(causal_effects)}")
            print(f"    ç»Ÿè®¡æ˜¾è‘—: {len(significant)}")

            if significant:
                print(f"\n  æ˜¾è‘—çš„å› æœæ•ˆåº” (å‰5ä¸ª):")
                for i, (edge, result) in enumerate(list(significant.items())[:5], 1):
                    print(f"    {i}. {edge}")
                    print(f"       ATE={result['ate']:.4f}, "
                          f"95% CI=[{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]")

    except Exception as e:
        print(f"  âœ— DMLå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# æ€»ç»“
# ============================================================================
total_time = time.time() - start_time

print("\n" + "="*70)
print("  åˆ†æå®Œæˆï¼")
print("="*70)
print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ ({total_time/3600:.2f} å°æ—¶)")

print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
for file in [output_path, graph_path]:
    if os.path.exists(file):
        size = os.path.getsize(file) / 1024
        print(f"  âœ“ {file} ({size:.1f} KB)")

print("\n" + "="*70)
```

### æ­¥éª¤3: è¿è¡Œå®éªŒ

#### 3.1 å°è§„æ¨¡æµ‹è¯•

**å…ˆç”¨å°‘é‡é…ç½®æµ‹è¯•**:

```python
# åœ¨ demo_my_dataset.py ä¸­ä¸´æ—¶ä¿®æ”¹:
METHODS = ['Baseline']  # åªæµ‹è¯•1ä¸ªæ–¹æ³•
ALPHA_VALUES = [0.0, 1.0]  # åªæµ‹è¯•2ä¸ªalpha
EPOCHS = 10  # å‡å°‘è®­ç»ƒè½®æ•°

# è¿è¡Œ
python demo_my_dataset.py
```

**é¢„æœŸè¾“å‡º**:
```
======================================================================
  MyDataset å®Œæ•´å› æœåˆ†æ
======================================================================
å¼€å§‹æ—¶é—´: 2025-12-21 18:00:00
è®¾å¤‡: cuda
GPU: NVIDIA GeForce RTX 3080

======================================================================
  æ­¥éª¤1: åŠ è½½æ•°æ®
======================================================================
åŠ è½½æ•°æ®...
åŸå§‹æ•°æ®: 10000 æ ·æœ¬, 25 åˆ—
...
âœ… æ•°æ®åŠ è½½å®Œæˆ
  è®­ç»ƒé›†: 7000 æ ·æœ¬
  æµ‹è¯•é›†: 3000 æ ·æœ¬
  ç‰¹å¾æ•°: 50

======================================================================
  æ­¥éª¤2: æ•°æ®æ”¶é›†
======================================================================

  [1/2] Baseline, Î±=0.00
    âœ“ Acc=0.756 | è€—æ—¶=45s | ETA=0.8min

  [2/2] Baseline, Î±=1.00
    âœ“ Acc=0.752 | è€—æ—¶=43s | ETA=0.0min

âœ“ æ•°æ®æ”¶é›†å®Œæˆï¼Œä¿å­˜åˆ°: data/mydataset_training_data.csv
...
```

#### 3.2 å®Œæ•´å®éªŒ

**ç¡®è®¤æµ‹è¯•æˆåŠŸåï¼Œè¿è¡Œå®Œæ•´å®éªŒ**:

```python
# æ¢å¤å®Œæ•´é…ç½®
METHODS = ['Baseline', 'Reweighing']
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]
EPOCHS = 50

# ä½¿ç”¨åå°è¿è¡Œ
nohup python demo_my_dataset.py > my_experiment.log 2>&1 &

# ç›‘æ§è¿›åº¦
tail -f my_experiment.log
```

---

## é…ç½®æ–‡ä»¶å®šåˆ¶

### æ–¹æ³•é€‰æ‹©

#### å¯ç”¨çš„å…¬å¹³æ€§æ–¹æ³•

```python
# é¢„å¤„ç†æ–¹æ³•
METHODS = [
    'Baseline',       # ä¸åšä»»ä½•å¤„ç†ï¼ˆåŸºå‡†ï¼‰
    'Reweighing',     # æ ·æœ¬é‡åŠ æƒ
    'Sampling',       # é‡é‡‡æ ·ï¼ˆè¿‡é‡‡æ ·+æ¬ é‡‡æ ·ï¼‰
]

# å¤„ç†ä¸­æ–¹æ³•ï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰
METHODS = [
    'Adversarial',    # å¯¹æŠ—å»å
    'PrejudiceRemover',  # åè§ç§»é™¤
]

# åå¤„ç†æ–¹æ³•ï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰
METHODS = [
    'Calibration',    # æ ¡å‡†
    'RejectOption',   # æ‹’ç»é€‰é¡¹åˆ†ç±»
]
```

### è¶…å‚æ•°ç½‘æ ¼

#### Alphaå‚æ•°ï¼ˆå…¬å¹³æ€§å¼ºåº¦ï¼‰

```python
# ç²—ç²’åº¦æœç´¢
ALPHA_VALUES = [0.0, 0.5, 1.0]  # 3ä¸ªç‚¹

# æ ‡å‡†æœç´¢
ALPHA_VALUES = [0.0, 0.25, 0.5, 0.75, 1.0]  # 5ä¸ªç‚¹

# ç»†ç²’åº¦æœç´¢
ALPHA_VALUES = [0.0, 0.1, 0.2, ..., 0.9, 1.0]  # 11ä¸ªç‚¹

# å¯¹æ•°æœç´¢ï¼ˆå¦‚æœæ•ˆåº”éçº¿æ€§ï¼‰
ALPHA_VALUES = [0.0, 0.01, 0.1, 0.5, 1.0]
```

#### æ¨¡å‹å®½åº¦ï¼ˆå®¹é‡ï¼‰

```python
# å•ä¸€å®½åº¦
MODEL_WIDTH = 2

# å¤šå®½åº¦å¯¹æ¯”
MODEL_WIDTHS = [1, 2, 3]  # æµ… â†’ ä¸­ â†’ æ·±

# åµŒå¥—å¾ªç¯
for width in MODEL_WIDTHS:
    for method in METHODS:
        for alpha in ALPHA_VALUES:
            # è®­ç»ƒé…ç½®...
```

### DiBSå‚æ•°è°ƒä¼˜

```python
# æ ·æœ¬é‡ < 20
DIBS_STEPS = 2000
DIBS_ALPHA = 0.2  # æ›´å¼ºç¨€ç–æ€§

# æ ·æœ¬é‡ 20-50
DIBS_STEPS = 3000
DIBS_ALPHA = 0.1  # æ ‡å‡†

# æ ·æœ¬é‡ > 50
DIBS_STEPS = 5000
DIBS_ALPHA = 0.05  # æ›´å¼±ç¨€ç–æ€§
```

---

## å¸¸è§åœºæ™¯æ¡ˆä¾‹

### åœºæ™¯1: ä¿¡è´·å®¡æ‰¹å…¬å¹³æ€§

**æ•°æ®ç‰¹å¾**:
- æ ·æœ¬é‡: 10,000
- ç‰¹å¾: å¹´é¾„ã€æ”¶å…¥ã€ä¿¡ç”¨åˆ†ã€æ•™è‚²ç­‰ (15ä¸ª)
- æ ‡ç­¾: æ˜¯å¦æ‰¹å‡†è´·æ¬¾ (0/1)
- æ•æ„Ÿå±æ€§: æ€§åˆ« (Female/Male)

**é…ç½®**:
```python
DATASET_NAME = 'CreditApproval'
METHODS = ['Baseline', 'Reweighing']
ALPHA_VALUES = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
EPOCHS = 50
DIBS_STEPS = 3000

# é¢„æœŸæƒè¡¡: approval rate vs fairness
```

**é¢„æœŸç»“æœ**:
- DiBSå‘ç°: alpha â†’ Te_SPD, alpha â†’ Te_Acc
- DMLé‡åŒ–: ATE(alpha â†’ Te_SPD) > 0, ATE(alpha â†’ Te_Acc) < 0
- æƒè¡¡: æé«˜å…¬å¹³æ€§é™ä½æ‰¹å‡†å‡†ç¡®ç‡

### åœºæ™¯2: æ‹›è˜ç³»ç»Ÿåè§

**æ•°æ®ç‰¹å¾**:
- æ ·æœ¬é‡: 5,000
- ç‰¹å¾: æ•™è‚²ã€ç»éªŒã€æŠ€èƒ½è¯„åˆ†ç­‰ (20ä¸ª)
- æ ‡ç­¾: æ˜¯å¦å½•ç”¨ (0/1)
- æ•æ„Ÿå±æ€§: ç§æ— (Minority/Majority)

**é…ç½®**:
```python
DATASET_NAME = 'HiringDecision'
METHODS = ['Baseline', 'Reweighing', 'Adversarial']
ALPHA_VALUES = [0.0, 0.5, 1.0]
EPOCHS = 30  # æ ·æœ¬é‡è¾ƒå°‘
DIBS_STEPS = 2000

# é¢„æœŸæƒè¡¡: hiring quality vs demographic parity
```

### åœºæ™¯3: åŒ»ç–—è¯Šæ–­å…¬å¹³æ€§

**æ•°æ®ç‰¹å¾**:
- æ ·æœ¬é‡: 20,000
- ç‰¹å¾: å¹´é¾„ã€BMIã€è¡€å‹ã€æ£€æŸ¥ç»“æœç­‰ (30ä¸ª)
- æ ‡ç­¾: æ˜¯å¦æ‚£ç—… (0/1)
- æ•æ„Ÿå±æ€§: å¹´é¾„ç»„ (Young/Old)

**é…ç½®**:
```python
DATASET_NAME = 'MedicalDiagnosis'
METHODS = ['Baseline', 'Reweighing']
ALPHA_VALUES = np.linspace(0, 1, 11)  # ç»†ç²’åº¦
EPOCHS = 50
DIBS_STEPS = 5000  # æ ·æœ¬é‡å¤§

# é¢„æœŸæƒè¡¡: diagnostic accuracy vs age fairness
```

---

## æ³¨æ„äº‹é¡¹ä¸é™·é˜±

### âš ï¸ é™·é˜±1: æµ‹è¯•é›†æŒ‡æ ‡ä¸å˜

**ç°è±¡**:
```
æ‰€æœ‰é…ç½®çš„ Te_SPD éƒ½ç›¸åŒ
æ‰€æœ‰é…ç½®çš„ Te_DI éƒ½ç›¸åŒ
```

**åŸå› **:
- Reweighingç­‰æ–¹æ³•åªå¤„ç†**è®­ç»ƒé›†**
- æµ‹è¯•é›†ä¿æŒåŸæ ·ä¸å˜
- å› æ­¤æµ‹è¯•é›†çš„å…¬å¹³æ€§æŒ‡æ ‡ä¹Ÿä¸å˜

**è§£å†³**:
```python
# è§‚å¯Ÿè®­ç»ƒé›†æŒ‡æ ‡çš„å˜åŒ–
print(df[['method', 'alpha', 'Tr_SPD', 'Tr_DI']])

# æ­£ç¡®æœŸæœ›:
# - Tr_SPD åº”è¯¥éšalphaå˜åŒ–
# - Te_SPD ä¿æŒä¸å˜ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰
```

### âš ï¸ é™·é˜±2: æ ·æœ¬é‡å¤ªå°‘å¯¼è‡´DiBSå¤±è´¥

**ç°è±¡**:
```
DiBSå­¦ä¹ å‡ºçš„å›¾å®Œå…¨ç¨€ç–ï¼ˆæ— è¾¹ï¼‰
æˆ–å®Œå…¨ç¨ å¯†ï¼ˆå…¨è¿æ¥ï¼‰
```

**åŸå› **:
- æ ·æœ¬é‡ < 10: ç»Ÿè®¡åŠŸæ•ˆä¸è¶³
- DiBSæ— æ³•å¯é ä¼°è®¡å› æœå…³ç³»

**è§£å†³**:
```python
# æ–¹æ¡ˆ1: å¢åŠ é…ç½®æ•°
ALPHA_VALUES = np.linspace(0, 1, 20)  # ä»5ä¸ªå¢åŠ åˆ°20ä¸ª

# æ–¹æ¡ˆ2: å¢åŠ æ¨¡å‹å¤šæ ·æ€§
for width in [1, 2, 3]:
    for method in METHODS:
        for alpha in ALPHA_VALUES:
            # ...

# æ–¹æ¡ˆ3: é™ä½DiBSç¨€ç–æ€§æƒ©ç½š
DIBS_ALPHA = 0.05  # ä»0.1é™ä½ï¼ˆå…è®¸æ›´å¤šè¾¹ï¼‰
```

### âš ï¸ é™·é˜±3: ç‰¹å¾æœªæ ‡å‡†åŒ–

**ç°è±¡**:
```
æ¨¡å‹è®­ç»ƒä¸æ”¶æ•›
æŸå¤±å‡½æ•°NaN
å‡†ç¡®ç‡éšæœºæ³¢åŠ¨
```

**åŸå› **:
- ç‰¹å¾å°ºåº¦å·®å¼‚å¤§ï¼ˆå¦‚age=25, income=50000ï¼‰
- æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

**è§£å†³**:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# éªŒè¯
print(f"å‡å€¼: {X_train.mean():.3f}")  # åº”æ¥è¿‘0
print(f"æ ‡å‡†å·®: {X_train.std():.3f}")  # åº”æ¥è¿‘1
```

### âš ï¸ é™·é˜±4: åˆ†ç±»ç‰¹å¾ç¼–ç é”™è¯¯

**ç°è±¡**:
```
æ¨¡å‹æ€§èƒ½å¼‚å¸¸å·®
å…¬å¹³æ€§æŒ‡æ ‡æ— æ„ä¹‰
```

**é”™è¯¯ç¤ºä¾‹**:
```python
# âŒ é”™è¯¯: æ ‡ç­¾ç¼–ç æš—ç¤ºé¡ºåº
education = ['HS', 'Bachelor', 'Master', 'PhD']
education_encoded = [0, 1, 2, 3]  # æ¨¡å‹è®¤ä¸ºPhD=3Ã—HS
```

**æ­£ç¡®æ–¹æ³•**:
```python
# âœ… æ­£ç¡®: One-Hotç¼–ç 
df_encoded = pd.get_dummies(df, columns=['education'])

# ç»“æœ:
# education_HS:       [1, 0, 0, 0]
# education_Bachelor: [0, 1, 0, 0]
# education_Master:   [0, 0, 1, 0]
# education_PhD:      [0, 0, 0, 1]
```

### âš ï¸ é™·é˜±5: GPUå†…å­˜ä¸è¶³

**ç°è±¡**:
```
CUDA out of memory
RuntimeError: CUDA error
```

**åŸå› **:
- æ‰¹æ¬¡å¤§å°å¤ªå¤§
- æ¨¡å‹å¤ªå¤§
- å¤šä¸ªè¿›ç¨‹å…±äº«GPU

**è§£å†³**:
```python
# æ–¹æ¡ˆ1: å‡å°æ‰¹æ¬¡
BATCH_SIZE = 128  # ä»256é™ä½

# æ–¹æ¡ˆ2: å‡å°æ¨¡å‹
MODEL_WIDTH = 1  # ä»2é™ä½

# æ–¹æ¡ˆ3: æ¸…ç†GPUç¼“å­˜
import torch
torch.cuda.empty_cache()

# æ–¹æ¡ˆ4: ä½¿ç”¨CPU
device = 'cpu'  # æ”¾å¼ƒGPUåŠ é€Ÿ
```

### âš ï¸ é™·é˜±6: å› æœå›¾è¿‡äºå¤æ‚

**ç°è±¡**:
```
DiBSæ£€æµ‹åˆ°æ•°ç™¾æ¡è¾¹
å›¾å¯†åº¦ > 0.5
```

**åŸå› **:
- DiBSç¨€ç–æ€§æƒ©ç½šå¤ªå¼±
- æ ·æœ¬é‡ä¸è¶³å¯¼è‡´è¿‡æ‹Ÿåˆ

**è§£å†³**:
```python
# å¢åŠ ç¨€ç–æ€§æƒ©ç½š
learner = CausalGraphLearner(
    n_vars=len(numeric_cols),
    n_steps=3000,
    alpha=0.3,  # ä»0.1å¢åŠ åˆ°0.3
    random_seed=42
)

# æˆ–æé«˜é˜ˆå€¼
edges = learner.get_edges(threshold=0.5)  # ä»0.3æé«˜
```

---

## æ•…éšœæ’æŸ¥æŒ‡å—

### é—®é¢˜è¯Šæ–­æµç¨‹

```
1. æ£€æŸ¥æ•°æ®åŠ è½½
   â”œâ”€ è¿è¡Œ validate_data.py
   â”œâ”€ ç¡®è®¤æ ·æœ¬é‡ã€ç‰¹å¾æ•°ã€æ ‡ç­¾åˆ†å¸ƒ
   â””â”€ å¦‚æœå¤±è´¥ â†’ ä¿®å¤æ•°æ®åŠ è½½è„šæœ¬

2. æ£€æŸ¥æ¨¡å‹è®­ç»ƒ
   â”œâ”€ è¿è¡Œ1ä¸ªé…ç½®æµ‹è¯•
   â”œâ”€ è§‚å¯ŸæŸå¤±å‡½æ•°æ›²çº¿
   â””â”€ å¦‚æœå¤±è´¥ â†’ è°ƒæ•´å­¦ä¹ ç‡/ç½‘ç»œç»“æ„

3. æ£€æŸ¥æŒ‡æ ‡è®¡ç®—
   â”œâ”€ æ‰“å°ä¸­é—´ç»“æœ
   â”œâ”€ ç¡®è®¤AIF360å…¼å®¹æ€§
   â””â”€ å¦‚æœå¤±è´¥ â†’ æ£€æŸ¥æ•æ„Ÿå±æ€§ç¼–ç 

4. æ£€æŸ¥DiBSå­¦ä¹ 
   â”œâ”€ æŸ¥çœ‹æ”¶æ•›æ›²çº¿
   â”œâ”€ æ£€æŸ¥è¾¹çš„æ•°é‡å’Œåˆ†å¸ƒ
   â””â”€ å¦‚æœå¤±è´¥ â†’ è°ƒæ•´è¶…å‚æ•°

5. æ£€æŸ¥DMLæ¨æ–­
   â”œâ”€ æŸ¥çœ‹æ¯æ¡è¾¹çš„ä¼°è®¡
   â”œâ”€ æ£€æŸ¥ç½®ä¿¡åŒºé—´æ˜¯å¦åˆç†
   â””â”€ å¦‚æœå¤±è´¥ â†’ å¢åŠ æ ·æœ¬é‡
```

### å¸¸è§é”™è¯¯ä¿¡æ¯

**é”™è¯¯1: KeyError: 'is_significant'**
```
åŸå› : DMLä¿å­˜ç»“æœæ—¶ç¼ºå°‘å­—æ®µ
è§£å†³: å·²åœ¨æœ€æ–°ä»£ç ä¸­ä¿®å¤ï¼Œæ›´æ–° utils/causal_inference.py
```

**é”™è¯¯2: ValueError: could not convert string to float**
```
åŸå› : åˆ†ç±»ç‰¹å¾æœªç¼–ç 
è§£å†³: ä½¿ç”¨ pd.get_dummies() ç¼–ç æ‰€æœ‰åˆ†ç±»åˆ—
```

**é”™è¯¯3: AssertionError: æ•æ„Ÿå±æ€§å¿…é¡»æ˜¯0æˆ–1**
```
åŸå› : æ•æ„Ÿå±æ€§ä¸æ˜¯äºŒå€¼
è§£å†³: åœ¨load_my_dataset.pyä¸­æ·»åŠ äºŒå€¼åŒ–é€»è¾‘
```

**é”™è¯¯4: RuntimeError: CUDA out of memory**
```
åŸå› : GPUå†…å­˜ä¸è¶³
è§£å†³: å‡å°BATCH_SIZEæˆ–MODEL_WIDTH
```

**é”™è¯¯5: np.linalg.LinAlgError: Singular matrix**
```
åŸå› : DMLä¸­çš„åæ–¹å·®çŸ©é˜µå¥‡å¼‚ï¼ˆå˜é‡ç¼ºä¹å˜å¼‚æ€§ï¼‰
è§£å†³: æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é…ç½®çš„æŸäº›æŒ‡æ ‡å®Œå…¨ç›¸åŒ
```

---

## æ€»ç»“

### è¿ç§»æ£€æŸ¥æ¸…å•

**å‡†å¤‡é˜¶æ®µ**:
```
â–¡ æ•°æ®é›†æ»¡è¶³æœ€ä½è¦æ±‚ï¼ˆ>500æ ·æœ¬ï¼‰
â–¡ æ•æ„Ÿå±æ€§æ˜¯äºŒå€¼æˆ–å¯äºŒå€¼åŒ–
â–¡ æ ‡ç­¾æ˜¯åˆ†ç±»å‹ï¼ˆ0/1ï¼‰
â–¡ ç‰¹å¾å·²æ¸…æ´—ï¼ˆæ— ç¼ºå¤±å€¼ï¼‰
â–¡ å®Œæˆæ•°æ®éªŒè¯è„šæœ¬
```

**å®æ–½é˜¶æ®µ**:
```
â–¡ åˆ›å»º load_my_dataset.py
â–¡ åˆ›å»º validate_data.py å¹¶é€šè¿‡
â–¡ åˆ›å»º demo_my_dataset.py
â–¡ è¿è¡Œå°è§„æ¨¡æµ‹è¯•ï¼ˆ2-3ä¸ªé…ç½®ï¼‰
â–¡ ç¡®è®¤ç»“æœåˆç†åè¿è¡Œå®Œæ•´å®éªŒ
```

**éªŒè¯é˜¶æ®µ**:
```
â–¡ æ£€æŸ¥æ•°æ®æ”¶é›†ç»“æœï¼ˆCSVæ–‡ä»¶ï¼‰
â–¡ æ£€æŸ¥DiBSå­¦ä¹ ç»“æœï¼ˆè¾¹çš„æ•°é‡å’Œæ„ä¹‰ï¼‰
â–¡ æ£€æŸ¥DMLæ¨æ–­ç»“æœï¼ˆç»Ÿè®¡æ˜¾è‘—æ€§ï¼‰
â–¡ ç”Ÿæˆåˆ†ææŠ¥å‘Š
â–¡ ä¸é¢†åŸŸçŸ¥è¯†å¯¹æ¯”éªŒè¯
```

### å…³é”®æˆåŠŸå› ç´ 

1. **æ•°æ®è´¨é‡** > ç®—æ³•å¤æ‚åº¦
2. **å……è¶³æ ·æœ¬** > å¤æ‚æ¨¡å‹
3. **é¢†åŸŸçŸ¥è¯†** > ç›²ç›®è°ƒå‚
4. **å°æ­¥éªŒè¯** > ä¸€æ¬¡åˆ°ä½
5. **è€å¿ƒè°ƒè¯•** > å¿«é€Ÿæ”¾å¼ƒ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-21
**é€‚ç”¨ç³»ç»Ÿç‰ˆæœ¬**: åŸºäºAdultæ•°æ®é›†å®Œæ•´å› æœåˆ†æ
