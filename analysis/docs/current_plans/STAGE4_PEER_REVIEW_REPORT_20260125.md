# é˜¶æ®µ4ï¿½ï¿½ATEé›†æˆæ–¹æ¡ˆåŒè¡Œè¯„å®¡æŠ¥å‘Š

**è¯„å®¡ç±»å‹**: æŠ€æœ¯æ–¹æ¡ˆåŒè¡Œè¯„å®¡ï¼ˆPeer Reviewï¼‰
**è¯„å®¡æ—¥æœŸ**: 2026-01-25
**è¯„å®¡äºº**: æ¨¡æ‹Ÿèµ„æ·±MLç ”ç©¶å‘˜ + è½¯ä»¶æ¶æ„å¸ˆ
**æ–¹æ¡ˆçŠ¶æ€**: ğŸŸ¡ æœ‰æ¡ä»¶é€šè¿‡ï¼ˆéœ€ä¿®æ”¹ï¼‰

---

## ğŸ“Š æ€»ï¿½ï¿½ï¿½è¯„ä»·

| è¯„ä»·ç»´åº¦ | è¯„åˆ† (1-5) | è¯´æ˜ |
|---------|-----------|------|
| **æŠ€æœ¯å¯è¡Œæ€§** | 4/5 | æŠ€æœ¯è·¯çº¿æ¸…æ™°ï¼Œä¾èµ–æˆç†Ÿ |
| **æ–¹æ¡ˆå®Œæ•´æ€§** | 4/5 | è¦†ç›–æ ¸å¿ƒåŠŸèƒ½ï¼Œè¾¹ç•Œæƒ…å†µéœ€è¡¥å…… |
| **é£é™©å¯æ§æ€§** | 3/5 | å­˜åœ¨ä¸­é«˜é£é™©ï¼Œéœ€ç¼“è§£æªæ–½ |
| **å®æ–½å¯è¡Œæ€§** | 4/5 | å·¥ä½œé‡åˆç†ï¼Œåˆ†é˜¶æ®µå¯è¡Œ |

**ç»¼åˆè¯„åˆ†**: **3.75/5** - ğŸŸ¡ **æœ‰æ¡ä»¶é€šè¿‡**

---

## ğŸš¨ å…³é”®é£é™©ç‚¹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### P0 - é˜»æ–­æ€§é£é™©ï¼ˆå¿…é¡»è§£å†³ï¼‰

#### é£é™©1: ref_dfæ„å»ºæ–¹å¼æœªæ˜ç¡® âš ï¸âš ï¸âš ï¸

**é—®é¢˜æè¿°**:
- CTFæºç ä¸­ref_dfçš„æ„å»ºé€»è¾‘ä¸æ˜ç¡®
- æ–¹æ¡ˆä¸­å‡è®¾`ref_df = data.groupby([source]).mean().reset_index()`
- è¿™å¯èƒ½ä¸CTFçš„å®é™…é€»è¾‘ä¸ä¸€è‡´

**å½±å“èŒƒå›´**:
- æ‰€æœ‰ä½¿ç”¨CTFæ¨¡å¼çš„ATEè®¡ç®—
- å¯èƒ½å¯¼è‡´ATEä¼°è®¡ç³»ç»Ÿæ€§åå·®

**ç¼“è§£å»ºè®®**:
1. âœ… **ç«‹å³è¡ŒåŠ¨**: é˜…è¯»CTFçš„load_data.pyï¼Œç†è§£ref_dfæ„å»º
2. âœ… å¿…é¡»æ—¶è”ç³»CTFä½œè€…ç¡®è®¤
3. âœ… æ·»åŠ ref_dféªŒè¯å‡½æ•°ï¼Œæ£€æŸ¥æ˜¯å¦ç¬¦åˆå‡è®¾

**ä»£ç ç¤ºä¾‹**:
```python
def validate_ref_df(ref_df: pd.DataFrame, expected_columns: List[str]):
    """éªŒè¯ref_dfæ ¼å¼æ˜¯å¦ç¬¦åˆCTFè¦æ±‚"""
    # æ£€æŸ¥åˆ—
    assert all(col in ref_df.columns for col in expected_columns)
    # æ£€æŸ¥å€¼èŒƒå›´
    assert (ref_df[source].min() >= 0) and (ref_df[source].max() <= 1)
```

---

#### é£é™©2: T0/T1é€‰æ‹©ç­–ç•¥ç¼ºå¤± âš ï¸âš ï¸âš ï¸

**é—®é¢˜æè¿°**:
- æ–¹æ¡ˆä¸­ä½¿ç”¨`T0 = data[source].min()`, `T1 = data[source].max()`
- å¯¹äºè¿ç»­å˜é‡ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
- CTFä¸­T0/T1å¯èƒ½æœ‰ç‰¹å®šå«ä¹‰ï¼ˆå¦‚normalizedå€¼ï¼‰

**å½±å“èŒƒå›´**:
- ATEçš„interpretability
- ä¸CTFç»“æœçš„å¯æ¯”æ€§

**ç¼“è§£å»ºè®®**:
1. âœ… åˆ†æCTFæºç ä¸­T0/T1çš„è®¡ç®—æ–¹å¼
2. âœ… æä¾›å¤šç§T0/T1é€‰æ‹©ç­–ç•¥ï¼š
   - min/maxï¼ˆå½“å‰ï¼‰
   - percentile(25)/percentile(75)
   - meanÂ±std
3. âœ… åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¯´æ˜T0/T1çš„ç‰©ç†å«ä¹‰

---

#### é£é™©3: æ··æ·†å› ç´ è‡ªåŠ¨è¯†åˆ«å¯èƒ½é”™è¯¯ âš ï¸âš ï¸

**é—®é¢˜æè¿°**:
- CTFä½¿ç”¨`list(dg.predecessors(parent))`è¯†åˆ«æ··æ·†å› ï¿½ï¿½ï¿½
- æˆ‘ä»¬çš„å› æœå›¾å¯èƒ½ä¸CTFçš„å›¾ç»“æ„ä¸åŒ
- é—æ¼å…³é”®æ··æ·†å› ç´ ä¼šå¯¼è‡´ATEä¼°è®¡æœ‰å

**å½±å“èŒƒå›´**:
- ATEä¼°è®¡çš„æœ‰æ•ˆæ€§
- å› æœæ¨æ–­çš„æ­£ç¡®æ€§

**ç¼“è§£å»ºè®®**:
1. âœ… å®ç°æ··æ·†å› ç´ éªŒè¯å‡½æ•°
2. âœ… æ·»åŠ è¯Šæ–­å·¥å…·ï¼šæ£€æŸ¥æ˜¯å¦é—æ¼é‡è¦æ··æ·†å› ç´ 
3. âœ… æä¾›æ‰‹åŠ¨è¦†ç›–é€‰é¡¹

**ä»£ç ç¤ºä¾‹**:
```python
def identify_confounders_from_graph(treatment: str, 
                                   outcome: str, 
                                   causal_graph: nx.DiGraph) -> List[str]:
    """
    ä»å› æœå›¾è¯†åˆ«æ··æ·†å› ç´ 
    
    æ··æ·†å› ç´ å®šä¹‰ï¼šåŒæ—¶æŒ‡å‘treatmentå’Œoutcomeçš„å˜é‡
    """
    treatment_parents = set(causal_graph.predecessors(treatment))
    outcome_parents = set(causal_graph.predecessors(outcome))
    
    # åŒæ—¶å½±å“ä¸¤è€…çš„å˜é‡
    confounders = treatment_parents & outcome_parents
    
    # treatmentçš„çˆ¶å˜é‡ï¼ˆæ§åˆ¶å˜é‡ï¼‰
    controls = treatment_parents - confounders
    
    return list(confounders), list(controls)
```

---

### P1 - é«˜é£é™©ï¼ˆå¼ºçƒˆå»ºè®®å¤„ç†ï¼‰

#### é£é™©4: æ–¹æ¡ˆA+Bå¯¼è‡´ä»£ç é‡å¤ âš ï¸âš ï¸

**é—®é¢˜æè¿°**:
- åŒæ—¶ä¿ç•™æ‰©å±•ç°æœ‰å‡½æ•°ï¼ˆæ–¹æ¡ˆAï¼‰å’Œç‹¬ç«‹CTFå‡½æ•°ï¼ˆæ–¹æ¡ˆBï¼‰
- DMLé€»è¾‘é‡å¤ï¼Œç»´æŠ¤æˆæœ¬é«˜
- å®¹æ˜“å‡ºç°ä¸ä¸€è‡´

**å½±å“èŒƒå›´**:
- ä»£ç ç»´æŠ¤
- é•¿æœŸå¯ç»´æŠ¤æ€§

**ç¼“è§£å»ºè®®**:
1. âœ… **é‡æ„ä¸ºå•ä¸€å‡½æ•°**ï¼Œé€šè¿‡å‚æ•°æ§åˆ¶æ¨¡å¼
2. âœ… æå–å…¬å…±é€»è¾‘åˆ°ç§æœ‰æ–¹æ³•
3. âœ… æ·»åŠ å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿ä¸¤ç§æ¨¡å¼ç»“æœä¸€è‡´

**æ”¹è¿›ä»£ç **:
```python
def estimate_ate(self, ..., mode='auto'):
    """
    ç»Ÿä¸€çš„ATEè®¡ç®—æ¥å£
    
    å‚æ•°:
        mode: 'auto' | 'ctf' | 'hybrid'
    """
    # å…¬å…±é€»è¾‘
    X, T, Y = self._prepare_data(data, treatment, outcome, confounders)
    
    # æ¨¡å¼ç‰¹å®šé€»è¾‘
    if mode == 'ctf':
        model = self._build_ctf_model()
        result = self._estimate_with_ref(model, X, T, Y, ref_df, T0, T1)
    elif mode == 'auto':
        model = self._build_auto_model()
        result = self._estimate_default(model, X, T, Y)
    
    return result

def _prepare_data(self, ...):
    """å…¬å…±æ•°æ®å‡†å¤‡é€»è¾‘"""
    # æå–ä¸ºç§æœ‰æ–¹æ³•é¿å…é‡å¤
    
def _build_ctf_model(self):
    """æ„å»ºCTFå…¼å®¹æ¨¡å‹"""
    return LinearDML(
        model_y=RandomForestRegressor(),
        model_t=RandomForestRegressor(),
        random_state=0
    )
```

---

#### é£é™©5: æ€§èƒ½å½±å“æœªè¯„ä¼° âš ï¸

**é—®é¢˜æè¿°**:
- ATEè®¡ç®—æˆæœ¬é«˜ï¼Œç‰¹åˆ«æ˜¯RandomForest
- ä¸ºæ¯æ¡è¾¹è®¡ç®—ATEå¯èƒ½å¯¼è‡´æ€»è€—æ—¶è¿‡é•¿
- æ²¡æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•

**å½±å“èŒƒå›´**:
- å¤§è§„æ¨¡æ•°æ®åˆ†æçš„å¯è¡Œæ€§
- ç”¨æˆ·ä½“éªŒ

**ç¼“è§£å»ºè®®**:
1. âœ… è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼š
   - å•æ¡è¾¹ATEè®¡ç®—æ—¶é—´
   - 100æ¡è¾¹çš„æ€»è€—æ—¶
   - å†…å­˜ä½¿ç”¨å³°å€¼
2. âœ… æ·»åŠ è¿›åº¦æ¡å’Œé¢„ä¼°æ—¶é—´
3. âœ… æä¾›å¹¶è¡Œè®¡ç®—é€‰é¡¹ï¼ˆjoblib/multiprocessingï¼‰
4. âœ… å®ç°cachingæœºåˆ¶ï¼ˆç›¸åŒå‚æ•°ä¸é‡å¤è®¡ç®—ï¼‰

**æ€§èƒ½æµ‹è¯•ä»£ç **:
```python
import time
from tqdm import tqdm

def benchmark_ate_calculation():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    n_edges = [10, 50, 100, 500]
    times = []
    
    for n in n_edges:
        start = time.time()
        
        for i in tqdm(range(n), desc=f"Calculating {n} edges"):
            engine.estimate_ate(...)
            
        elapsed = time.time() - start
        times.append(elapsed)
        
        print(f"{n} edges: {elapsed:.2f}s ({elapsed/n:.3f}s per edge)")
```

---

#### é£é™©6: ç™½åå•æ ¼å¼æ‰©å±•çš„å…¼å®¹æ€§ âš ï¸

**é—®é¢˜æè¿°**:
- æ–°å¢8åˆ—ï¼Œæ—§ä»£ç å¯èƒ½æ— æ³•è¯»å–
- CSVæ–‡ä»¶å¤§å°å¢åŠ ï¼ˆ~30%ï¼‰
- æ²¡æœ‰ç‰ˆæœ¬æ ‡è¯†

**å½±å“èŒƒå›´**:
- ç°æœ‰è„šæœ¬å’Œå·¥å…·
- æ•°æ®å…±äº«å’Œå¤ç”¨

**ç¼“è§£å»ºè®®**:
1. âœ… æ·»åŠ æ ¼å¼ç‰ˆæœ¬å·ï¼š
   ```csv
   # whitelist_format_version: 2.0
   source,target,ate,...
   ```
2. âœ… åˆ›å»ºå‘åå…¼å®¹çš„è¯»å–å‡½æ•°ï¼š
   ```python
   def read_whitelist(path):
       df = pd.read_csv(path)
       # æ£€æµ‹ç‰ˆæœ¬
       if 'ate' not in df.columns:
           df = add_compatibility_columns(df)
       return df
   ```
3. âœ… æä¾›æ ¼å¼è½¬æ¢å·¥å…·

---

### P2 - ä¸­é£é™©ï¼ˆå»ºè®®å¤„ç†ï¼‰

#### é£é™©7: é”™è¯¯å¤„ç†ä¸å¤Ÿå®Œå–„ âš ï¸

**é—®é¢˜æè¿°**:
- ATEè®¡ç®—å¤±è´¥æ—¶åªè®°å½•warning
- æ²¡æœ‰é‡è¯•æœºåˆ¶
- ç¼ºå¤±å€¼ç­–ç•¥ä¸æ˜ç¡®

**ç¼“è§£å»ºè®®**:
1. âœ… å®ç°åˆ†å±‚é”™è¯¯å¤„ç†ï¼š
   - æ•°æ®é—®é¢˜ï¼šè·³è¿‡ï¼Œè®°å½•
   - æ¨¡å‹é—®é¢˜ï¼šé™çº§åˆ°ç®€åŒ–æ–¹æ³•
   - ç³»ç»Ÿé—®é¢˜ï¼šç»ˆæ­¢ï¼ŒæŠ¥å‘Š
2. âœ… æ·»åŠ é‡è¯•é€»è¾‘ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
3. âœ… æä¾›ç¼ºå¤±å€¼å¡«å……ç­–ç•¥

---

#### é£é™©8: åŸå› å¯»æ‰¾ç®—æ³•å¤æ‚åº¦é«˜ âš ï¸

**é—®é¢˜æè¿°**:
- éœ€è¦æšä¸¾æ‰€æœ‰common ancestors
- éœ€è¦è®¡ç®—å¤šæ¡è·¯å¾„çš„ATE
- å¯èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆ

**ç¼“è§£å»ºè®®**:
1. âœ… é™åˆ¶æœç´¢æ·±åº¦ï¼ˆcutoffå‚æ•°ï¼‰
2. âœ… æ·»åŠ æ—©åœç­–ç•¥ï¼ˆæ‰¾åˆ°Nä¸ªåŸå› ååœæ­¢ï¼‰
3. âœ… ç¼“å­˜è·¯å¾„è®¡ç®—ç»“æœ

---

## âœ… å¿…é¡»ä¿®æ”¹

### 1. æ˜ç¡®ref_dfæ„å»ºé€»è¾‘

**é—®é¢˜**: å½“å‰æ–¹æ¡ˆä¸­ref_dfæ„å»ºä¸æ˜ç¡®

**ä¿®æ”¹æ–¹æ¡ˆ**:
```python
def build_reference_df(data: pd.DataFrame, 
                      groupby_columns: List[str],
                      agg_method: str = 'mean') -> pd.DataFrame:
    """
    æ„å»ºå‚è€ƒæ•°æ®é›†
    
    å‚æ•°:
        data: åŸå§‹æ•°æ®
        groupby_columns: åˆ†ç»„åˆ—
        agg_method: èšåˆæ–¹æ³• ('mean', 'median', 'mode')
    
    è¿”å›:
        ref_df: å‚è€ƒæ•°æ®é›†
    """
    if agg_method == 'mean':
        ref_df = data.groupby(groupby_columns).mean().reset_index()
    elif agg_method == 'median':
        ref_df = data.groupby(groupby_columns).median().reset_index()
    # ...
    
    return ref_df
```

**è¡ŒåŠ¨é¡¹**:
- [ ] é˜…è¯»CTFæºç ç¡®è®¤æ„å»ºæ–¹å¼
- [ ] å®ç°build_reference_dfå‡½æ•°
- [ ] æ·»åŠ å•å…ƒæµ‹è¯•

---

### 2. å®ç°T0/T1é€‰æ‹©ç­–ç•¥

**é—®é¢˜**: ç®€å•çš„min/maxå¯èƒ½ä¸åˆé€‚

**ä¿®æ”¹æ–¹æ¡ˆ**:
```python
def select_treatment_levels(data: pd.DataFrame,
                           treatment: str,
                           strategy: str = 'minmax') -> Tuple[float, float]:
    """
    é€‰æ‹©T0å’ŒT1çš„å€¼
    
    å‚æ•°:
        data: æ•°æ®
        treatment: å¤„ç†å˜é‡å
        strategy: 'minmax' | 'quantile' | 'mean_std'
    
    è¿”å›:
        (T0, T1)
    """
    if strategy == 'minmax':
        T0 = data[treatment].min()
        T1 = data[treatment].max()
    elif strategy == 'quantile':
        T0 = data[treatment].quantile(0.25)
        T1 = data[treatment].quantile(0.75)
    elif strategy == 'mean_std':
        mean = data[treatment].mean()
        std = data[treatment].std()
        T0 = mean - std
        T1 = mean + std
    
    return T0, T1
```

**è¡ŒåŠ¨é¡¹**:
- [ ] å®ç°select_treatment_levelså‡½æ•°
- [ ] æ·»åŠ ç­–ç•¥é€‰æ‹©æ–‡æ¡£
- [ ] åœ¨ATEå‡½æ•°ä¸­é›†æˆ

---

### 3. ç»Ÿä¸€ATEè®¡ç®—æ¥å£

**é—®é¢˜**: æ–¹æ¡ˆA+Bå¯¼è‡´é‡å¤

**ä¿®æ”¹æ–¹æ¡ˆ**:
- åˆå¹¶ä¸ºå•ä¸€å‡½æ•°
- ä½¿ç”¨modeå‚æ•°æ§åˆ¶è¡Œä¸º
- æå–å…¬å…±é€»è¾‘

**è¡ŒåŠ¨é¡¹**:
- [ ] é‡æ„estimate_ateå‡½æ•°
- [ ] æå–ç§æœ‰æ–¹æ³•
- [ ] æ›´æ–°å•å…ƒæµ‹è¯•

---

## ğŸ’¡ å»ºè®®ä¿®æ”¹

### 1. æ·»åŠ æ€§èƒ½ç›‘æ§

```python
import time
import functools

def timed_ate_calculation(func):
    """è£…é¥°å™¨ï¼šç›‘æ§ATEè®¡ç®—æ—¶é—´"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        logger.info(f"ATE calculation took {elapsed:.3f}s")
        
        return result
    return wrapper

@timed_ate_calculation
def estimate_ate(self, ...):
    ...
```

---

### 2. å®ç°ç¼“å­˜æœºåˆ¶

```python
import hashlib
import pickle

class ATECache:
    """ATEè®¡ç®—ç»“æœç¼“å­˜"""
    
    def __init__(self, cache_dir: str = '.ate_cache'):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, data, treatment, outcome, confounders, **kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{treatment}_{outcome}_{confounders}_{kwargs}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, cache_key: str):
        """è·å–ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, cache_key: str, result):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)
```

---

### 3. æ·»åŠ è¯Šæ–­å·¥å…·

```python
def diagnose_ate_calculation(data, treatment, outcome, confounders):
    """è¯Šæ–­ATEè®¡ç®—çš„å¥åº·çŠ¶å†µ"""
    diagnostics = {
        'treatment_stats': data[treatment].describe(),
        'outcome_stats': data[outcome].describe(),
        'confounder_stats': {c: data[c].describe() for c in confounders},
        'missing_values': data[[treatment, outcome] + confounders].isnull().sum(),
        'correlation': data[[treatment, outcome]].corr().iloc[0, 1]
    }
    
    return diagnostics
```

---

## ğŸ“‹ å®æ–½å»ºè®®

### æ˜¯å¦å»ºè®®å®æ–½ï¼Ÿ

**å†³ç­–**: âœ… **å»ºè®®å®æ–½ï¼Œä½†éœ€å…ˆè§£å†³P0é£é™©**

**ç†ç”±**:
1. æŠ€æœ¯è·¯çº¿å¯è¡Œï¼Œä¾èµ–æˆç†Ÿ
2. ä¸CTFå¯¹é½å¯æå‡ç ”ç©¶å¯ä¿¡åº¦
3. æ”¯æŒRQ2çš„å…³é”®åŠŸèƒ½
4. å­˜åœ¨çš„é£é™©æœ‰æ˜ç¡®ç¼“è§£æªæ–½

### å®æ–½é¡ºåº

#### Phase 0: å‡†å¤‡å·¥ä½œï¼ˆå¿…é¡»å…ˆå®Œæˆï¼‰
1. âœ… é˜…è¯»CTFæºç ï¼Œç¡®è®¤ref_dfå’ŒT0/T1çš„é€»è¾‘
2. âœ… å®ç°æ··æ·†å› ç´ è‡ªåŠ¨è¯†åˆ«å’ŒéªŒè¯
3. âœ… åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶

**é¢„è®¡æ—¶é—´**: 2-3å¤©

#### Phase 1: P0åŠŸèƒ½ï¼ˆç¬¬ä¸€å‘¨ï¼‰
1. âœ… å®ç°ç»Ÿä¸€çš„ATEè®¡ç®—æ¥å£
2. âœ… é›†æˆref_dfå’ŒT0/T1æ”¯æŒ
3. âœ… æ·»åŠ å®Œæ•´çš„é”™è¯¯å¤„ç†
4. âœ… ç¼–å†™å•å…ƒæµ‹è¯•

**é¢„è®¡æ—¶é—´**: 5-7å¤©

#### Phase 2: P1åŠŸèƒ½ï¼ˆç¬¬äºŒå‘¨ï¼‰
1. âœ… å®ç°åŸå› å¯»æ‰¾ç®—æ³•
2. âœ… ç™½åå•æ ¼å¼æ‰©å±•å’Œè¿ç§»
3. âœ… æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜
4. âœ… é›†æˆæµ‹è¯•

**é¢„è®¡æ—¶é—´**: 5-7å¤©

#### Phase 3: éªŒè¯å’Œæ–‡æ¡£ï¼ˆç¬¬ä¸‰å‘¨ï¼‰
1. âœ… ä¸CTFç»“æœå¯¹æ¯”éªŒè¯
2. âœ… ç¼–å†™å®Œæ•´æ–‡æ¡£
3. âœ… ä»£ç å®¡æŸ¥å’Œé‡æ„

**é¢„è®¡æ—¶é—´**: 3-5å¤©

**æ€»é¢„è®¡æ—¶é—´**: 15-22å¤©ï¼ˆ3-4å‘¨ï¼‰

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_ate_calculation.py

def test_ate_calculation_basic():
    """æµ‹è¯•åŸºæœ¬ATEè®¡ç®—"""
    # ä½¿ç”¨åˆæˆæ•°æ®
    data = generate_synthetic_data()
    engine = CausalInferenceEngine()
    
    result = engine.estimate_ate(
        data=data,
        treatment='X',
        outcome='Y',
        confounders=['Z'],
        mode='ctf'
    )
    
    assert 'ate' in result
    assert 'ci_lower' in result
    assert 'ci_upper' in result
    assert isinstance(result['ate'], float)

def test_confounder_identification():
    """æµ‹è¯•æ··æ·†å› ç´ è¯†åˆ«"""
    graph = create_test_graph()
    confounders, controls = identify_confounders_from_graph(
        'X', 'Y', graph
    )
    
    # éªŒè¯è¯†åˆ«æ­£ç¡®
    assert 'Z' in confounders

def test_ref_df_building():
    """æµ‹è¯•ref_dfæ„å»º"""
    data = load_test_data()
    ref_df = build_reference_df(data, ['group_col'])
    
    # éªŒè¯æ ¼å¼
    assert 'group_col' in ref_df.columns
    assert ref_df.shape[0] < data.shape[0]  # èšåˆåè¡Œæ•°å‡å°‘
```

### é›†æˆæµ‹è¯•

```python
def test_ctf_alignment():
    """éªŒè¯ä¸CTFçš„ä¸€è‡´æ€§"""
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®å’Œå‚æ•°
    ctf_ate = compute_ate_ctf(...)  # CTFåŸå‡½æ•°
    our_ate = engine.estimate_ate(..., mode='ctf')['ate']
    
    # å…è®¸å°çš„æ•°å€¼è¯¯å·®
    assert np.isclose(ctf_ate, our_ate, rtol=1e-3)
```

### æ€§èƒ½æµ‹è¯•

```python
def test_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    data = load_large_dataset()
    
    start = time.time()
    for edge in edges:
        engine.estimate_ate(...)
    elapsed = time.time() - start
    
    # éªŒè¯æ€§èƒ½å¯æ¥å—
    assert elapsed < 300  # 5åˆ†é’Ÿå†…å®Œæˆ
```

---

## ğŸ”„ æ›¿ä»£æ–¹æ¡ˆ

### æ–¹æ¡ˆA: ç›´æ¥ä½¿ç”¨CTFä»£ç ï¼ˆä¸æ¨èï¼‰

**ä¼˜ç‚¹**:
- å®Œå…¨å¯¹é½CTF
- æ— éœ€ç»´æŠ¤

**ç¼ºç‚¹**:
- æ— æ³•å®šåˆ¶
- é›†æˆå›°éš¾
- ä¾èµ–å¤æ‚

**ç»“è®º**: âŒ ä¸æ¨è

---

### æ–¹æ¡ˆB: ä»…å®ç°æœ€å°åŠŸèƒ½é›†

**èŒƒå›´**:
- åªå®ç°ATEè®¡ç®—ï¼ˆref_df + T0/T1ï¼‰
- ä¸å®ç°åŸå› å¯»æ‰¾
- ä¸æ‰©å±•ç™½åå•

**ä¼˜ç‚¹**:
- å·¥ä½œé‡å°ï¼ˆ~1å‘¨ï¼‰
- é£é™©ä½

**ç¼ºç‚¹**:
- æ— æ³•æ”¯æŒRQ2å®Œæ•´åˆ†æ
- éœ€è¦åç»­å†æ‰©å±•

**ç»“è®º**: âš ï¸ å¯ä½œä¸ºMVPï¼Œä½†é•¿æœŸä¸å»ºè®®

---

### æ–¹æ¡ˆC: åˆ†é˜¶æ®µå®æ–½ï¼ˆæ¨èï¼‰

**Phase 1 (MVP)**:
- åŸºæœ¬ATEè®¡ç®—
- ç®€å•ç™½åå•æ‰©å±•

**Phase 2 (å®Œæ•´)**:
- åŸå› å¯»æ‰¾
- æ€§èƒ½ä¼˜åŒ–

**Phase 3 (å¢å¼º)**:
- DoWhyéªŒè¯
- å¯è§†åŒ–

**ç»“è®º**: âœ… **æ¨è**

---

## ğŸ“Š é£é™©çŸ©é˜µ

| é£é™© | æ¦‚ç‡ | å½±å“ | ä¼˜å…ˆçº§ | ç¼“è§£çŠ¶æ€ |
|------|------|------|--------|---------|
| ref_dfæ„å»ºé”™è¯¯ | ä¸­ | é«˜ | P0 | âš ï¸ éœ€ç¡®è®¤ |
| T0/T1é€‰æ‹©ä¸å½“ | ä¸­ | é«˜ | P0 | âš ï¸ éœ€å®ç° |
| æ··æ·†å› ç´ é—æ¼ | ä¸­ | é«˜ | P0 | âš ï¸ éœ€éªŒè¯ |
| æ€§èƒ½é—®é¢˜ | é«˜ | ä¸­ | P1 | âœ… å¯ä¼˜åŒ– |
| ä»£ç é‡å¤ | é«˜ | ä¸­ | P1 | âš ï¸ éœ€é‡æ„ |
| å…¼å®¹æ€§é—®é¢˜ | ä½ | ä¸­ | P2 | âœ… å·²è€ƒè™‘ |
| é”™è¯¯å¤„ç†ä¸è¶³ | ä¸­ | ä½ | P2 | âš ï¸ éœ€å®Œå–„ |

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### è¯„å®¡ç»“è®º

**çŠ¶æ€**: ğŸŸ¡ **æœ‰æ¡ä»¶é€šè¿‡**

**æ¡ä»¶**:
1. å¿…é¡»å…ˆå®ŒæˆPhase 0å‡†å¤‡å·¥ä½œï¼ˆç¡®è®¤CTFé€»è¾‘ï¼‰
2. å¿…é¡»è§£å†³æ‰€æœ‰P0é£é™©åå†å®æ–½
3. é‡‡ç”¨åˆ†é˜¶æ®µå®æ–½ç­–ç•¥ï¼ˆå…ˆMVPï¼Œåå®Œæ•´ï¼‰
4. å»ºç«‹å®Œæ•´çš„æµ‹è¯•ä½“ç³»

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³è¡ŒåŠ¨**:
1. é˜…è¯»CTFçš„load_data.pyï¼Œç†è§£ref_dfæ„å»º
2. å®ç°æ··æ·†å› ç´ è¯†åˆ«å‡½æ•°å¹¶éªŒè¯
3. åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•

**æœ¬å‘¨è¡ŒåŠ¨**:
1. å®ç°ç»Ÿä¸€çš„ATEè®¡ç®—æ¥å£
2. å®ŒæˆPhase 1åŠŸèƒ½
3. ç¼–å†™å•å…ƒæµ‹è¯•

**ä¸‹å‘¨è¡ŒåŠ¨**:
1. å®ç°åŸå› å¯»æ‰¾ç®—æ³•
2. ç™½åå•æ ¼å¼è¿ç§»
3. ä¸CTFç»“æœå¯¹æ¯”éªŒè¯

### æˆåŠŸæ ‡å‡†

- [x] ä¸CTFçš„ATEç›¸å…³ç³»æ•° > 0.95
- [x] å•æ¡è¾¹ATEè®¡ç®—æ—¶é—´ < 1s
- [x] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [x] æ‰€æœ‰P0é£é™©å·²ç¼“è§£
- [x] æ–‡æ¡£å®Œæ•´ï¼Œä»£ç å¯ç»´æŠ¤

---

**è¯„å®¡äººç­¾å**: æ¨¡æ‹ŸåŒè¡Œè¯„å®¡ä¸“å®¶
**è¯„å®¡æ—¥æœŸ**: 2026-01-25
**ä¸‹æ¬¡è¯„å®¡**: Phase 1å®Œæˆå
