# ATEé›†æˆåˆ°å› æœåˆ†æç™½åå•ï¼šå®Œæ•´æŠ€æœ¯æ–¹æ¡ˆ

**ç‰ˆæœ¬**: v1.0
**çŠ¶æ€**: ğŸŸ¡ æœ‰æ¡ä»¶é€šè¿‡ï¼ˆéœ€è§£å†³P0é£é™©ï¼‰
**è¯„å®¡æ—¥æœŸ**: 2026-01-25
**é¢„è®¡å·¥æœŸ**: 15-22å¤©ï¼ˆ3-4å‘¨ï¼‰

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

### é¡¹ç›®ç›®æ ‡

å°†CTFè®ºæ–‡ï¼ˆCausality-Aided Trade-off Analysis for Machine Learning Fairnessï¼‰çš„ATEï¼ˆAverage Treatment Effectï¼‰è®¡ç®—æ–¹æ³•é›†æˆåˆ°æ·±åº¦å­¦ä¹ èƒ½è€—ç ”ç©¶çš„å› æœåˆ†ææµç¨‹ä¸­ï¼Œæ‰©å±•ç™½åå•æ•°æ®æ ¼å¼ä»¥æ”¯æŒRQ2çš„trade-offåˆ†æã€‚

### æ–¹æ¡ˆçŠ¶æ€

- **æŠ€æœ¯å¯è¡Œæ€§**: 4/5 â­â­â­â­
- **æ–¹æ¡ˆå®Œæ•´æ€§**: 4/5 â­â­â­â­
- **é£é™©å¯æ§æ€§**: 3/5 â­â­â­
- **ç»¼åˆè¯„åˆ†**: **3.75/5** - ğŸŸ¡ æœ‰æ¡ä»¶é€šè¿‡

### å…³é”®å†³ç­–

âœ… **å»ºè®®å®æ–½**ï¼Œä½†å¿…é¡»ï¼š
1. å…ˆå®ŒæˆPhase 0å‡†å¤‡å·¥ä½œï¼ˆç¡®è®¤CTFé€»è¾‘ï¼‰
2. è§£å†³æ‰€æœ‰P0é£é™©
3. é‡‡ç”¨åˆ†é˜¶æ®µå®æ–½ç­–ç•¥
4. å»ºç«‹å®Œæ•´æµ‹è¯•ä½“ç³»

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

### é˜¶æ®µæ€§æ–‡æ¡£

| é˜¶æ®µ | æ–‡æ¡£ | å†…å®¹ |
|------|------|------|
| é˜¶æ®µ1 | `STAGE1_SOURCE_CODE_VERIFICATION_20260125.md` | CTFæºç å®Œæ•´æ€§éªŒè¯ |
| é˜¶æ®µ2 | `STAGE2_CODE_COMPARISON_20260125.md` | ä»£ç å·®å¼‚å¯¹æ¯”åˆ†æ |
| é˜¶æ®µ3 | `STAGE3_ATE_INTEGRATION_PLAN_20260125.md` | ATEé›†æˆæ–¹æ¡ˆè®¾è®¡ |
| é˜¶æ®µ4 | `STAGE4_PEER_REVIEW_REPORT_20260125.md` | åŒè¡Œè¯„å®¡é£é™©è¯„ä¼° |
| **æ€»è§ˆ** | **`ATE_INTEGRATION_COMPLETE_PLAN_20260125.md`** | **æœ¬æ–‡æ¡£ - å®Œæ•´æ–¹æ¡ˆ** |

### å‚è€ƒæ–‡æ¡£

- `CTF_SOURCE_CODE_COMPARISON_20260125.md` - è¯¦ç»†ä»£ç å¯¹æ¯”
- `CAUSAL_EDGE_WHITELIST_DESIGN.md` - ç™½åå•è®¾è®¡è§„èŒƒ
- `CLAUDE_FULL_REFERENCE.md` - é¡¹ç›®å®Œæ•´å‚è€ƒ

---

## ğŸ” èƒŒæ™¯ä¸åŠ¨æœº

### ç ”ç©¶é—®é¢˜

**RQ1**: To what extent do hyperparameters affect GPU energy consumption?
**RQ2**: What is the trade-off relationship between energy consumption and performance?

### ä¸ºä»€ä¹ˆéœ€è¦ATEï¼Ÿ

1. **é‡åŒ–å› æœæ•ˆåº”** - ATEæä¾›æ ‡å‡†åŒ–çš„å› æœæ•ˆåº”åº¦é‡
2. **æ”¯æŒtrade-offåˆ†æ** - RQ2éœ€è¦ATEæ¥æ£€æµ‹èƒ½è€—-æ€§èƒ½æƒè¡¡
3. **å¯¹é½CTFæ–¹æ³•** - æå‡ç ”ç©¶å¯ä¿¡åº¦å’Œå¯å¤ç°æ€§
4. **æ‰©å±•ç™½åå•** - å½“å‰ç™½åå•ç¼ºå°‘å› æœæ¨æ–­ä¿¡æ¯

### CTFè®ºæ–‡ç®€ä»‹

**è®ºæ–‡**: Causality-Aided Trade-off Analysis for Machine Learning Fairness
**ä»“åº“**: https://anonymous.4open.science/r/CTF-47BF
**æ ¸å¿ƒè´¡çŒ®**: 
- ä½¿ç”¨å› æœæ¨æ–­åˆ†æfairness trade-offs
- Algorithm 1: åŸºäºå› æœå›¾çš„trade-offæ£€æµ‹
- é›†æˆDiBSå› æœå‘ç° + DMLå› æœæ¨æ–­

---

## ğŸ“Š å½“å‰çŠ¶å†µåˆ†æ

### CTFæºç å®Œæ•´æ€§

âœ… **å·²éªŒè¯** - `CTF_original/` ä»“åº“å®Œæ•´

**æ ¸å¿ƒæ–‡ä»¶**:
- `src/inf.py` (337è¡Œ) - ATEè®¡ç®—å’Œtrade-offæ£€æµ‹
- `src/collect.py` (1418è¡Œ) - æ•°æ®æ”¶é›†
- `src/fairness/in_p.py` (466è¡Œ) - Fairnessæ–¹æ³•

**å…³é”®å‡½æ•°**:
```python
# CTF_original/src/inf.py:78
def compute_ate(parent, child, data_df, ref_df, dg, T0, T1):
    """ä½¿ç”¨LinearDML + RandomForestè®¡ç®—ATE"""
```

### æˆ‘ä»¬çš„ä»£ç ç°çŠ¶

**ç°æœ‰å®ç°**:
- `utils/causal_inference.py` - CausalInferenceEngineç±»ï¼ˆéƒ¨åˆ†å®ç°ï¼‰
- `utils/tradeoff_detection.py` - TradeoffDetectorç±»ï¼ˆç¼ºå°‘åŸå› å¯»æ‰¾ï¼‰
- `results/energy_research/data/interaction/whitelist/*.csv` - ç™½åå•æ•°æ®ï¼ˆæ— ATEåˆ—ï¼‰

**åŠŸèƒ½å®Œæ•´æ€§**: çº¦60%

### äº”å¤§å…³é”®å·®å¼‚

| åŠŸèƒ½ | CTF | æˆ‘ä»¬ | å½±å“ |
|------|-----|------|------|
| ATEè®¡ç®— | ref_df + T0/T1 | åŸæ•°æ® | âš ï¸ å…³é”® |
| æ··æ·†å› ç´  | è‡ªåŠ¨è¯†åˆ« | æ‰‹åŠ¨ä¼ å…¥ | âš ï¸ å…³é”® |
| åŸå› å¯»æ‰¾ | æ·±åº¦åˆ†æ | ä¸æ”¯æŒ | âš ï¸ å…³é”® |
| æ¨¡å‹é€‰æ‹© | RandomForest | 'auto' | âš ï¸ ä¸­ç­‰ |
| DoWhy | é›†æˆ | æ—  | âš ï¸ ä¸­ç­‰ |

---

## ğŸ¯ æŠ€æœ¯æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ATEè®¡ç®—å‡½æ•°æ‰©å±•

#### è®¾è®¡åŸåˆ™

1. **å‘åå…¼å®¹** - é»˜è®¤ä¿æŒåŸæœ‰è¡Œä¸º
2. **CTFå¯¹é½** - å¯é€‰å¼€å¯CTFå…¼å®¹æ¨¡å¼
3. **ç»Ÿä¸€æ¥å£** - é¿å…ä»£ç é‡å¤
4. **ç»“æ„åŒ–è¿”å›** - æ˜“äºæ‰©å±•å’ŒéªŒè¯

#### æ¥å£è®¾è®¡

```python
def estimate_ate(self,
                 data: pd.DataFrame,
                 treatment: str,
                 outcome: str,
                 confounders: Optional[List[str]] = None,
                 controls: Optional[List[str]] = None,
                 ref_df: Optional[pd.DataFrame] = None,
                 T0: Optional[float] = None,
                 T1: Optional[float] = None,
                 mode: str = 'auto',
                 verbose: bool = False) -> Dict[str, Any]:
    """
    ä¼°è®¡å¹³å‡å¤„ç†æ•ˆåº”(ATE)
    
    å‚æ•°:
        data: å®éªŒæ•°æ®
        treatment: å¤„ç†å˜é‡ï¼ˆå¹²é¢„èŠ‚ç‚¹ï¼‰
        outcome: ç»“æœå˜é‡ï¼ˆç›®æ ‡èŠ‚ç‚¹ï¼‰
        confounders: æ··æ·†å› ç´ åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œmode='ctf'æ—¶è‡ªåŠ¨è¯†åˆ«ï¼‰
        controls: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        ref_df: å‚è€ƒæ•°æ®é›†ï¼ˆCTFæ¨¡å¼ï¼‰
        T0: å¯¹ç…§å€¼ï¼ˆCTFæ¨¡å¼ï¼‰
        T1: å¤„ç†å€¼ï¼ˆCTFæ¨¡å¼ï¼‰
        mode: è®¡ç®—æ¨¡å¼
            - 'auto': è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
            - 'ctf': CTFå…¼å®¹æ¨¡å¼ï¼ˆRandomForest + ref_dfï¼‰
            - 'hybrid': æ··åˆæ¨¡å¼
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
        {
            'ate': float,              # å¹³å‡å¤„ç†æ•ˆåº”
            'ci_lower': float,         # ç½®ä¿¡åŒºé—´ä¸‹ç•Œ
            'ci_upper': float,         # ç½®ä¿¡åŒºé—´ä¸Šç•Œ
            'is_significant': bool,    # æ˜¯å¦ç»Ÿè®¡æ˜¾è‘—
            'T0': float,               # å¯¹ç…§å€¼
            'T1': float,               # å¤„ç†å€¼
            'ref_mean': float,         # å‚è€ƒå‡å€¼
            'method': str,             # è®¡ç®—æ–¹æ³•
            'confounders': List[str],  # ä½¿ç”¨çš„æ··æ·†å› ç´ 
            'n_samples': int           # æ ·æœ¬æ•°
        }
    """
```

#### å®ç°ç»“æ„

```python
class CausalInferenceEngine:
    """å› æœæ¨æ–­å¼•æ“"""
    
    def estimate_ate(self, ...):
        """ç»Ÿä¸€ATEè®¡ç®—æ¥å£"""
        # 1. å‡†å¤‡æ•°æ®
        X, T, Y, W = self._prepare_data(...)
        
        # 2. è¯†åˆ«æ··æ·†å› ç´ ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if confounders is None and mode == 'ctf':
            confounders, controls = self._identify_confounders_from_graph(...)
        
        # 3. æ„å»ºæ¨¡å‹
        if mode == 'ctf':
            model = self._build_ctf_model()
        else:
            model = self._build_auto_model()
        
        # 4. æ‹Ÿåˆæ¨¡å‹
        model.fit(Y, T, X=X, W=W)
        
        # 5. è®¡ç®—ATE
        if T0 is not None and T1 is not None:
            ate = model.ate(X=X_eval, T0=T0, T1=T1)
        else:
            ate = model.ate(X=X_eval)
        
        # 6. è®¡ç®—ç½®ä¿¡åŒºé—´
        ci = self._compute_confidence_interval(model, X_eval)
        
        return self._format_result(ate, ci, ...)
    
    def _prepare_data(self, ...):
        """å…¬å…±æ•°æ®å‡†å¤‡é€»è¾‘"""
        # æå–ä¸ºç§æœ‰æ–¹æ³•ï¼Œé¿å…é‡å¤
        
    def _identify_confounders_from_graph(self, treatment, outcome, causal_graph):
        """ä»å› æœå›¾è¯†åˆ«æ··æ·†å› ç´ """
        # å®ç°CTFçš„é€»è¾‘
        
    def _build_ctf_model(self):
        """æ„å»ºCTFå…¼å®¹æ¨¡å‹"""
        from sklearn.ensemble import RandomForestRegressor
        return LinearDML(
            model_y=RandomForestRegressor(),
            model_t=RandomForestRegressor(),
            random_state=0
        )
    
    def _build_auto_model(self):
        """æ„å»ºè‡ªåŠ¨æ¨¡å‹"""
        return LinearDML(
            model_y='auto',
            model_t='auto',
            random_state=42
        )
```

### æ–¹æ¡ˆ2: ç™½åå•æ ¼å¼æ‰©å±•

#### æ–°å¢åˆ—å®šä¹‰

| åˆ—å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| ate | float | NaN | å¹³å‡å¤„ç†æ•ˆåº” |
| ate_ci_lower | float | NaN | 95%ç½®ä¿¡åŒºé—´ä¸‹ç•Œ |
| ate_ci_upper | float | NaN | 95%ç½®ä¿¡åŒºé—´ä¸Šç•Œ |
| ate_is_significant | bool | False | æ˜¯å¦ç»Ÿè®¡æ˜¾è‘— |
| T0 | float | NaN | å¯¹ç…§å€¼ |
| T1 | float | NaN | å¤„ç†å€¼ |
| ref_mean | float | NaN | å‚è€ƒå‡å€¼ |
| ate_method | str | 'N/A' | è®¡ç®—æ–¹æ³•æ ‡è¯† |

#### CSVæ ¼å¼ç¤ºä¾‹

```csv
# whitelist_format_version: 2.0
source,target,strength,edge_type,is_significant,strength_level,source_category,target_category,question_relevance,interpretation,ate,ate_ci_lower,ate_ci_upper,ate_is_significant,T0,T1,ref_mean,ate_method
hyperparam_batch_size,energy_gpu_min_watts,0.95,moderation,yes,very_strong,hyperparam,energy,other,å¹¶è¡Œæ¨¡å¼è°ƒèŠ‚batch_sizeå¯¹gpu_min_wattsçš„æ•ˆåº”,0.123,0.089,0.157,true,0.0,1.0,0.5,DML_CTF
```

#### è¿ç§»è„šæœ¬

**æ–‡ä»¶**: `tools/data_management/add_ate_to_whitelist.py`

```python
def add_ate_to_whitelist(whitelist_path: str,
                        data: pd.DataFrame,
                        causal_graph: nx.DiGraph,
                        mode: str = 'ctf') -> pd.DataFrame:
    """
    ä¸ºç™½åå•CSVæ·»åŠ ATEåˆ—
    
    å‚æ•°:
        whitelist_path: ç™½åå•CSVè·¯å¾„
        data: åŸå§‹æ•°æ®
        causal_graph: å› æœå›¾
        mode: ATEè®¡ç®—æ¨¡å¼
    
    è¿”å›:
        æ·»åŠ äº†ATEåˆ—çš„DataFrame
    """
    # 1. è¯»å–ç™½åå•
    df = pd.read_csv(whitelist_path)
    
    # 2. åˆå§‹åŒ–ATEåˆ—
    ate_columns = ['ate', 'ate_ci_lower', 'ate_ci_upper', 
                   'ate_is_significant', 'T0', 'T1', 'ref_mean', 'ate_method']
    for col in ate_columns:
        if col not in df.columns:
            df[col] = np.nan if col != 'ate_method' else 'N/A'
            if col == 'ate_is_significant':
                df[col] = False
    
    # 3. åˆ›å»ºATEè®¡ç®—å¼•æ“
    engine = CausalInferenceEngine(verbose=True)
    
    # 4. ä¸ºæ¯æ¡è¾¹è®¡ç®—ATE
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="è®¡ç®—ATE"):
        source = row['source']
        target = row['target']
        
        try:
            # æ„å»ºref_dfï¼ˆCTFæ¨¡å¼ï¼‰
            if mode == 'ctf':
                ref_df = build_reference_df(data, [source])
                T0 = data[source].min()
                T1 = data[source].max()
            else:
                ref_df, T0, T1 = None, None, None
            
            # è®¡ç®—ATE
            result = engine.estimate_ate(
                data=data,
                treatment=source,
                outcome=target,
                confounders=None,  # è‡ªåŠ¨è¯†åˆ«
                ref_df=ref_df,
                T0=T0,
                T1=T1,
                mode=mode
            )
            
            # å¡«å……ç»“æœ
            df.loc[idx, 'ate'] = result['ate']
            df.loc[idx, 'ate_ci_lower'] = result['ci_lower']
            df.loc[idx, 'ate_ci_upper'] = result['ci_upper']
            df.loc[idx, 'ate_is_significant'] = result['is_significant']
            df.loc[idx, 'T0'] = result['T0']
            df.loc[idx, 'T1'] = result['T1']
            df.loc[idx, 'ref_mean'] = result['ref_mean']
            df.loc[idx, 'ate_method'] = result['method']
            
        except Exception as e:
            warnings.warn(f"è®¡ç®—ATEå¤±è´¥ ({source}->{target}): {e}")
            continue
    
    # 5. æ·»åŠ æ ¼å¼ç‰ˆæœ¬
    df.attrs['whitelist_format_version'] = '2.0'
    
    return df
```

### æ–¹æ¡ˆ3: åŸå› å¯»æ‰¾ç®—æ³•

#### ç®—æ³•æµç¨‹

```python
class TradeoffDetector:
    
    def find_causes(self,
                    metric_A: str,
                    metric_B: str,
                    intervention: str,
                    causal_graph: nx.DiGraph,
                    data_df: pd.DataFrame,
                    ref_df: pd.DataFrame,
                    rules: Dict[str, str],
                    ate_engine: CausalInferenceEngine) -> List[str]:
        """
        å¯»æ‰¾trade-offçš„æ ¹æœ¬åŸå› 
        
        å®ç°CTFè®ºæ–‡çš„åŸå› å¯»æ‰¾ç®—æ³•ï¼š
        1. æ‰¾common ancestors
        2. åˆ†æè·¯å¾„ä¾èµ–
        3. å¯¹æ¯ä¸ªæ½œåœ¨åŸå› è®¡ç®—ATE
        4. åˆ¤æ–­æ˜¯å¦ä¹Ÿäº§ç”Ÿtrade-off
        """
        # æ­¥éª¤1: æ‰¾common ancestors
        ancestors_A = set(nx.ancestors(causal_graph, metric_A))
        ancestors_B = set(nx.ancestors(causal_graph, metric_B))
        common_ancestors = ancestors_A & ancestors_B
        common_ancestors.discard(intervention)
        
        if not common_ancestors:
            return []
        
        # æ­¥éª¤2: åˆ†æè·¯å¾„ä¾èµ–ï¼Œè¿‡æ»¤å†—ä½™åŸå› 
        explored_step = set()
        potential_causes = self._filter_potential_causes(
            common_ancestors, causal_graph, 
            metric_A, metric_B, explored_step
        )
        
        # æ­¥éª¤3: å¯¹æ¯ä¸ªæ½œåœ¨åŸå› è®¡ç®—ATE
        causes = []
        for pc in potential_causes:
            if pc not in data_df.columns:
                continue
            
            # è®¡ç®—ATE
            T0 = ref_df[pc].mean()
            T1 = data_df[data_df[intervention] == 1][pc].mean()
            
            ate_A = ate_engine.estimate_ate(
                data=data_df, treatment=pc, outcome=metric_A,
                ref_df=ref_df, T0=T0, T1=T1, mode='ctf'
            )['ate']
            
            ate_B = ate_engine.estimate_ate(
                data=data_df, treatment=pc, outcome=metric_B,
                ref_df=ref_df, T0=T0, T1=T1, mode='ctf'
            )['ate']
            
            # åˆ¤æ–­æ–¹å‘
            cf_direction_A = '+' if ate_A > 0 else '-'
            cf_direction_B = '+' if ate_B > 0 else '-'
            
            cf_improve_A = (cf_direction_A == rules.get(metric_A, '+'))
            cf_improve_B = (cf_direction_B == rules.get(metric_B, '+'))
            
            # å¦‚æœä¹Ÿäº§ç”Ÿå†²çªï¼Œåˆ™æ˜¯æ ¹æœ¬åŸå› 
            if cf_improve_A != cf_improve_B:
                causes.append(pc)
        
        return causes
    
    def _filter_potential_causes(self, common_ancestors, causal_graph, 
                                 metric_A, metric_B, explored_step):
        """è¿‡æ»¤æ½œåœ¨åŸå› ï¼Œç§»é™¤å†—ä½™"""
        potential_causes = set(common_ancestors)
        ca_last_step = {}
        
        for ca in common_ancestors:
            toX_paths = list(nx.all_simple_paths(
                causal_graph, ca, metric_A, cutoff=5
            ))
            toY_paths = list(nx.all_simple_paths(
                causal_graph, ca, metric_B, cutoff=5
            ))
            
            toX_last_step = set([x[-2] for x in toX_paths if len(x) > 1])
            toY_last_step = set([y[-2] for y in toY_paths if len(y) > 1])
            
            ca_last_step[ca] = (toX_last_step, toY_last_step)
        
        # æŒ‰æ‹“æ‰‘æ’åºï¼ˆä»æœ€è¿œçš„åŸå› å¼€å§‹ï¼‰
        sorted_nodes = list(nx.topological_sort(causal_graph))
        ca_last_step = dict(sorted(
            ca_last_step.items(),
            key=lambda x: sorted_nodes.index(x[0]),
            reverse=True
        ))
        
        # è¿‡æ»¤å·²æ¢ç´¢çš„åŸå› 
        for ca, (toX_last_step, toY_last_step) in ca_last_step.items():
            if toX_last_step.issubset(explored_step) and \
               toY_last_step.issubset(explored_step):
                potential_causes.remove(ca)
            else:
                explored_step.update(toX_last_step)
                explored_step.update(toY_last_step)
        
        return potential_causes
```

---

## âš ï¸ é£é™©ä¸ç¼“è§£

### é£é™©çŸ©é˜µ

| é£é™© | æ¦‚ç‡ | å½±å“ | ä¼˜å…ˆçº§ | ç¼“è§£æªæ–½ |
|------|------|------|--------|---------|
| ref_dfæ„å»ºé”™è¯¯ | ä¸­ | é«˜ | P0 | ç¡®è®¤CTFé€»è¾‘ |
| T0/T1é€‰æ‹©ä¸å½“ | ä¸­ | é«˜ | P0 | å¤šç§ç­–ç•¥ |
| æ··æ·†å› ç´ é—æ¼ | ä¸­ | é«˜ | P0 | éªŒè¯å‡½æ•° |
| æ€§èƒ½é—®é¢˜ | é«˜ | ä¸­ | P1 | ç¼“å­˜+å¹¶è¡Œ |
| ä»£ç é‡å¤ | é«˜ | ä¸­ | P1 | é‡æ„ |
| å…¼å®¹æ€§é—®é¢˜ | ä½ | ä¸­ | P2 | ç‰ˆæœ¬æ§åˆ¶ |

### P0é£é™©ç¼“è§£æ–¹æ¡ˆ

#### 1. ref_dfæ„å»ºé€»è¾‘

```python
def build_reference_df(data: pd.DataFrame,
                      groupby_columns: List[str],
                      agg_method: str = 'mean') -> pd.DataFrame:
    """
    æ„å»ºå‚è€ƒæ•°æ®é›†ï¼ˆéœ€ç¡®è®¤CTFé€»è¾‘ï¼‰
    
    âš ï¸ é‡è¦ï¼šéœ€è¦å…ˆé˜…è¯»CTFçš„load_data.pyç¡®è®¤æ­£ç¡®æ–¹å¼
    """
    if agg_method == 'mean':
        ref_df = data.groupby(groupby_columns).mean().reset_index()
    elif agg_method == 'median':
        ref_df = data.groupby(groupby_columns).median().reset_index()
    else:
        raise ValueError(f"Unknown agg_method: {agg_method}")
    
    return ref_df
```

**è¡ŒåŠ¨é¡¹**:
- [ ] é˜…è¯»CTF_original/src/load_data.py
- [ ] ç¡®è®¤ref_dfçš„ç¡®åˆ‡æ„å»ºæ–¹å¼
- [ ] å®ç°å¹¶æµ‹è¯•

#### 2. T0/T1é€‰æ‹©ç­–ç•¥

```python
def select_treatment_levels(data: pd.DataFrame,
                           treatment: str,
                           strategy: str = 'minmax') -> Tuple[float, float]:
    """
    é€‰æ‹©T0å’ŒT1çš„å€¼
    
    ç­–ç•¥:
    - 'minmax': data[treatment].min(), data[treatment].max()
    - 'quantile': quantile(0.25), quantile(0.75)
    - 'mean_std': mean - std, mean + std
    """
    if strategy == 'minmax':
        T0 = data[treatment].min()
        T1 = data[treatment].max()
    elif strategy == 'quantile':
        T0 = data[treatment].quantile(0.25)
        T1 = data[treatment].quantile(0.75)
    elif strategy == 'mean_std':
        mean = data[treatment].mean()
        std = data[treatment].std()
        T0 = mean - std
        T1 = mean + std
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    return T0, T1
```

#### 3. æ··æ·†å› ç´ éªŒè¯

```python
def identify_confounders_from_graph(treatment: str,
                                   outcome: str,
                                   causal_graph: nx.DiGraph) -> Tuple[List[str], List[str]]:
    """
    ä»å› æœå›¾è¯†åˆ«æ··æ·†å› ç´ 
    
    è¿”å›:
        (confounders, controls)
        - confounders: åŒæ—¶å½±å“treatmentå’Œoutcomeçš„å˜é‡
        - controls: åªå½±å“treatmentçš„å˜é‡
    """
    treatment_parents = set(causal_graph.predecessors(treatment))
    outcome_parents = set(causal_graph.predecessors(outcome))
    
    # æ··æ·†å› ç´ ï¼šåŒæ—¶æŒ‡å‘ä¸¤è€…
    confounders = treatment_parents & outcome_parents
    
    # æ§åˆ¶å˜é‡ï¼šåªæŒ‡å‘treatment
    controls = treatment_parents - confounders
    
    return list(confounders), list(controls)

def validate_confounders(data, treatment, outcome, confounders):
    """éªŒè¯æ··æ·†å› ç´ çš„æœ‰æ•ˆæ€§"""
    diagnostics = {
        'missing_values': data[confounders].isnull().sum(),
        'variance': data[confounders].var(),
        'correlation_treatment': data[confounders].corrwith(data[treatment]),
        'correlation_outcome': data[confounders].corrwith(data[outcome])
    }
    return diagnostics
```

---

## ğŸ“‹ å®æ–½è®¡åˆ’

### Phase 0: å‡†å¤‡å·¥ä½œï¼ˆ2-3å¤©ï¼‰âš ï¸ å¿…é¡»å…ˆå®Œæˆ

**ç›®æ ‡**: ç¡®è®¤CTFé€»è¾‘ï¼Œå»ºç«‹æµ‹è¯•æ¡†æ¶

**ä»»åŠ¡**:
1. [ ] é˜…è¯»CTFæºç ç¡®è®¤å…³é”®é€»è¾‘
   - `CTF_original/src/inf.py` - ref_dfå’ŒT0/T1çš„ä½¿ç”¨
   - `CTF_original/src/load_data.py` - æ•°æ®åŠ è½½å’Œref_dfæ„å»º
   
2. [ ] å®ç°æ··æ·†å› ç´ è¯†åˆ«å‡½æ•°
   - `identify_confounders_from_graph()`
   - `validate_confounders()`
   - å•å…ƒæµ‹è¯•
   
3. [ ] åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
   - `tests/benchmark/test_ate_performance.py`
   - å®šä¹‰æ€§èƒ½æŒ‡æ ‡

**äº§å‡º**:
- CTFé€»è¾‘ç†è§£æ–‡æ¡£
- æ··æ·†å› ç´ è¯†åˆ«æ¨¡å—
- æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶

---

### Phase 1: P0åŠŸèƒ½å®ç°ï¼ˆ5-7å¤©ï¼‰

**ç›®æ ‡**: å®ç°æ ¸å¿ƒATEè®¡ç®—åŠŸèƒ½

**ä»»åŠ¡**:
1. [ ] é‡æ„CausalInferenceEngine
   - ç»Ÿä¸€estimate_ateæ¥å£
   - å®ç°mode='ctf'é€»è¾‘
   - æå–å…¬å…±æ–¹æ³•
   
2. [ ] å®ç°è¾…åŠ©å‡½æ•°
   - `build_reference_df()`
   - `select_treatment_levels()`
   - `identify_confounders_from_graph()`
   
3. [ ] é”™è¯¯å¤„ç†å’ŒéªŒè¯
   - æ•°æ®éªŒè¯
   - æ¨¡å‹éªŒè¯
   - ç»“æœéªŒè¯
   
4. [ ] å•å…ƒæµ‹è¯•
   - `tests/test_ate_calculation.py`
   - è¦†ç›–ç‡ > 80%

**äº§å‡º**:
- é‡æ„åçš„causal_inference.py
- å®Œæ•´çš„å•å…ƒæµ‹è¯•
- æµ‹è¯•æŠ¥å‘Š

---

### Phase 2: P1åŠŸèƒ½å®ç°ï¼ˆ5-7å¤©ï¼‰

**ç›®æ ‡**: å®ç°åŸå› å¯»æ‰¾å’Œç™½åå•æ‰©å±•

**ä»»åŠ¡**:
1. [ ] å®ç°åŸå› å¯»æ‰¾ç®—æ³•
   - `TradeoffDetector.find_causes()`
   - è·¯å¾„åˆ†æé€»è¾‘
   - å•å…ƒæµ‹è¯•
   
2. [ ] ç™½åå•æ ¼å¼æ‰©å±•
   - åˆ›å»º`add_ate_to_whitelist.py`
   - æ‰¹é‡å¤„ç†è„šæœ¬
   - æ ¼å¼éªŒè¯
   
3. [ ] æ€§èƒ½ä¼˜åŒ–
   - å®ç°ç¼“å­˜æœºåˆ¶
   - å¹¶è¡Œè®¡ç®—æ”¯æŒ
   - è¿›åº¦æ¡å’Œæ—¥å¿—
   
4. [ ] é›†æˆæµ‹è¯•
   - ç«¯åˆ°ç«¯æµ‹è¯•
   - ä¸CTFå¯¹æ¯”éªŒè¯

**äº§å‡º**:
- åŸå› å¯»æ‰¾ç®—æ³•
- ç™½åå•æ‰©å±•å·¥å…·
- æ€§èƒ½ä¼˜åŒ–ä»£ç 
- é›†æˆæµ‹è¯•æŠ¥å‘Š

---

### Phase 3: éªŒè¯å’Œæ–‡æ¡£ï¼ˆ3-5å¤©ï¼‰

**ç›®æ ‡**: å®Œå–„æ–‡æ¡£å’ŒéªŒè¯ç»“æœ

**ä»»åŠ¡**:
1. [ ] ä¸CTFç»“æœå¯¹æ¯”
   - ä½¿ç”¨ç›¸åŒæ•°æ®
   - è®¡ç®—ç›¸å…³ç³»æ•°
   - åˆ†æå·®å¼‚åŸå› 
   
2. [ ] ç¼–å†™å®Œæ•´æ–‡æ¡£
   - APIæ–‡æ¡£
   - ä½¿ç”¨æŒ‡å—
   - æŠ€æœ¯æŠ¥å‘Š
   
3. [ ] ä»£ç å®¡æŸ¥å’Œé‡æ„
   - ä»£ç å®¡æŸ¥
   - é‡æ„ä¼˜åŒ–
   - æ–‡æ¡£è¡¥å……

**äº§å‡º**:
- å¯¹æ¯”éªŒè¯æŠ¥å‘Š
- å®Œæ•´æ–‡æ¡£
- ä¼˜åŒ–åçš„ä»£ç 

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_ate_calculation.py

def test_ate_basic():
    """åŸºæœ¬ATEè®¡ç®—æµ‹è¯•"""
    data = generate_synthetic_data()
    engine = CausalInferenceEngine()
    result = engine.estimate_ate(
        data=data, treatment='X', outcome='Y',
        confounders=['Z'], mode='ctf'
    )
    assert 'ate' in result
    assert isinstance(result['ate'], float)

def test_confounder_identification():
    """æ··æ·†å› ç´ è¯†åˆ«æµ‹è¯•"""
    graph = create_test_causal_graph()
    confounders, controls = identify_confounders_from_graph(
        'X', 'Y', graph
    )
    assert 'Z' in confounders

def test_ref_df_building():
    """ref_dfæ„å»ºæµ‹è¯•"""
    data = load_test_data()
    ref_df = build_reference_df(data, ['group_col'])
    assert 'group_col' in ref_df.columns
```

### é›†æˆæµ‹è¯•

```python
# tests/test_integration.py

def test_ctf_alignment():
    """ä¸CTFå¯¹é½æµ‹è¯•"""
    # ä½¿ç”¨CTFç›¸åŒçš„æ•°æ®å’Œå‚æ•°
    ctf_ate = compute_with_ctf(...)
    our_ate = engine.estimate_ate(..., mode='ctf')['ate']
    
    # å…è®¸1%çš„ç›¸å¯¹è¯¯å·®
    assert np.isclose(ctf_ate, our_ate, rtol=1e-2)

def test_end_to_end():
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    # åŠ è½½æ•°æ®
    data = load_energy_data()
    graph = load_causal_graph()
    
    # è®¡ç®—ATE
    engine = CausalInferenceEngine()
    result = engine.estimate_ate(
        data=data, treatment='batch_size',
        outcome='gpu_energy', causal_graph=graph, mode='ctf'
    )
    
    # éªŒè¯ç»“æœ
    assert result['is_significant'] == True
    assert result['ate'] > 0
```

### æ€§èƒ½æµ‹è¯•

```python
# tests/benchmark/test_performance.py

def test_ate_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    data = load_large_dataset(n_samples=10000)
    edges = get_test_edges(n=100)
    
    engine = CausalInferenceEngine()
    times = []
    
    for source, target in edges:
        start = time.time()
        engine.estimate_ate(
            data=data, treatment=source,
            outcome=target, mode='ctf'
        )
        times.append(time.time() - start)
    
    avg_time = np.mean(times)
    assert avg_time < 1.0, f"ATEè®¡ç®—è¿‡æ…¢: {avg_time:.2f}s"
```

---

## ğŸ“¦ äº¤ä»˜ç‰©æ¸…å•

### ä»£ç æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ | çŠ¶æ€ |
|------|------|------|
| `utils/causal_inference.py` | ATEè®¡ç®—å¼•æ“ | å¾…é‡æ„ |
| `utils/tradeoff_detection.py` | Trade-offæ£€æµ‹ï¼ˆå«åŸå› å¯»æ‰¾ï¼‰ | å¾…æ‰©å±• |
| `tools/data_management/add_ate_to_whitelist.py` | ç™½åå•æ‰©å±•å·¥å…· | æ–°å»º |
| `utils/ref_df_builder.py` | ref_dfæ„å»ºå·¥å…· | æ–°å»º |
| `utils/confounder_identifier.py` | æ··æ·†å› ç´ è¯†åˆ« | æ–°å»º |

### æµ‹è¯•æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `tests/test_ate_calculation.py` | ATEè®¡ç®—å•å…ƒæµ‹è¯• |
| `tests/test_confounder_identification.py` | æ··æ·†å› ç´ æµ‹è¯• |
| `tests/test_tradeoff_detection.py` | Trade-offæ£€æµ‹æµ‹è¯• |
| `tests/test_integration.py` | é›†æˆæµ‹è¯• |
| `tests/benchmark/test_performance.py` | æ€§èƒ½æµ‹è¯• |

### æ–‡æ¡£æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `docs/ATE_INTEGRATION_API.md` | APIæ–‡æ¡£ |
| `docs/ATE_INTEGRATION_GUIDE.md` | ä½¿ç”¨æŒ‡å— |
| `docs/CTF_ALIGNMENT_REPORT.md` | ä¸CTFå¯¹æ¯”æŠ¥å‘Š |
| `docs/ATE_INTEGRATION_COMPLETE_PLAN_20260125.md` | æœ¬æ–‡æ¡£ |

### æ•°æ®æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `results/energy_research/data/interaction/whitelist/*_with_ate.csv` | æ‰©å±•åçš„ç™½åå• |

---

## ğŸ“ˆ æˆåŠŸæ ‡å‡†

### æŠ€æœ¯æŒ‡æ ‡

- [ ] ä¸CTFçš„ATEç›¸å…³ç³»æ•° > 0.95
- [ ] å•æ¡è¾¹ATEè®¡ç®—æ—¶é—´ < 1s
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] æ‰€æœ‰P0é£é™©å·²ç¼“è§£
- [ ] ä¸CTFç»“æœè¯¯å·® < 5%

### åŠŸèƒ½å®Œæ•´æ€§

- [ ] æ”¯æŒCTFå…¼å®¹æ¨¡å¼
- [ ] è‡ªåŠ¨è¯†åˆ«æ··æ·†å› ç´ 
- [ ] å®ç°åŸå› å¯»æ‰¾ç®—æ³•
- [ ] ç™½åå•æ ¼å¼æ‰©å±•
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜/å¹¶è¡Œï¼‰

### æ–‡æ¡£å’Œè´¨é‡

- [ ] APIæ–‡æ¡£å®Œæ•´
- [ ] ä½¿ç”¨æŒ‡å—æ¸…æ™°
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡
- [ ] å¯å¤ç°æ€§éªŒè¯

---

## ğŸ”„ åç»­å·¥ä½œ

### çŸ­æœŸï¼ˆPhaseå®Œæˆåï¼‰

1. **åº”ç”¨åˆ°å®é™…æ•°æ®**
   - ä¸ºæ‰€æœ‰6ä¸ªä»»åŠ¡ç»„è®¡ç®—ATE
   - ç”Ÿæˆæ‰©å±•ç™½åå•
   - åˆ†ætrade-offæ¨¡å¼

2. **RQ2åˆ†æ**
   - ä½¿ç”¨æ‰©å±•ç™½åå•è¿›è¡Œtrade-offåˆ†æ
   - è¯†åˆ«èƒ½è€—-æ€§èƒ½æƒè¡¡
   - ç”Ÿæˆç ”ç©¶æŠ¥å‘Š

### ä¸­æœŸï¼ˆ1-2ä¸ªæœˆï¼‰

1. **æ€§èƒ½ä¼˜åŒ–**
   - å®ç°åˆ†å¸ƒå¼è®¡ç®—
   - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
   - æ·»åŠ å¢é‡æ›´æ–°

2. **åŠŸèƒ½å¢å¼º**
   - æ”¯æŒæ›´å¤šå› æœæ¨æ–­æ–¹æ³•
   - å¯è§†åŒ–ATEç»“æœ
   - äº¤äº’å¼æ¢ç´¢å·¥å…·

### é•¿æœŸï¼ˆ3-6ä¸ªæœˆï¼‰

1. **æ–¹æ³•æ”¹è¿›**
   - ç ”ç©¶æ›´å¥½çš„ATEä¼°è®¡æ–¹æ³•
   - æ¢ç´¢å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ï¼ˆHTEï¼‰
   - å› æœæœºåˆ¶å­¦ä¹ 

2. **å·¥å…·å¼€æº**
   - æ•´ç†ä»£ç åº“
   - ç¼–å†™å®Œæ•´æ–‡æ¡£
   - å‘å¸ƒä¸ºå¼€æºå·¥å…·

---

## ğŸ“ è”ç³»å’Œæ”¯æŒ

### é—®é¢˜åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥é˜…ç›¸å…³æ–‡æ¡£ï¼ˆè§æ–‡æ¡£å¯¼èˆªï¼‰
2. æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹
3. è”ç³»é¡¹ç›®è´Ÿè´£äºº

### æ–‡æ¡£ç»´æŠ¤

- **ç»´æŠ¤è€…**: Green
- **ç‰ˆæœ¬**: v1.0
- **æœ€åæ›´æ–°**: 2026-01-25
- **ä¸‹æ¬¡å®¡æŸ¥**: Phase 1å®Œæˆå

---

## é™„å½•

### A. å‚è€ƒèµ„æº

**è®ºæ–‡**:
- CTF: Causality-Aided Trade-off Analysis for ML Fairness
- Chernozhukov et al. (2018): Double/Debiased Machine Learning

**å·¥å…·**:
- EconML: https://github.com/py-why/EconML
- DoWhy: https://github.com/py-why/dowhy
- NetworkX: https://networkx.org/

**é¡¹ç›®æ–‡æ¡£**:
- CLAUDE_FULL_REFERENCE.md
- CAUSAL_EDGE_WHITELIST_DESIGN.md
- QUESTIONS_2_3_DIBS_ANALYSIS_PLAN.md

### B. æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è¯´æ˜ |
|------|------|------|
| ATE | Average Treatment Effect | å¹³å‡å¤„ç†æ•ˆåº” |
| DML | Double Machine Learning | åŒé‡æœºå™¨å­¦ä¹  |
| ref_df | Reference DataFrame | å‚è€ƒæ•°æ®é›† |
| T0/T1 | Treatment levels | å¯¹ç…§/å¤„ç†å€¼ |
| Trade-off | æƒè¡¡ | ä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„å†²çª |
| Confounder | æ··æ·†å› ç´  | åŒæ—¶å½±å“å¹²é¢„å’Œç»“æœçš„å˜é‡ |

### C. å˜æ›´æ—¥å¿—

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´ |
|------|------|------|
| v1.0 | 2026-01-25 | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´æ–¹æ¡ˆ |

---

**æ–‡æ¡£ç»“æŸ**
