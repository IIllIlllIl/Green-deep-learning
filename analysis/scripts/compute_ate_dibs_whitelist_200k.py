#!/usr/bin/env python3
"""
åŸºäºDiBSç™½åå•å› æœå›¾çš„å…¨å±€æ ‡å‡†åŒ–æ•°æ®ATEè®¡ç®—è„šæœ¬ï¼ˆæ–¹æ¡ˆCï¼Œ200kæ­¥ï¼‰

åŸºäºç™½åå•è¿‡æ»¤åçš„DiBSå› æœå›¾è®¡ç®—å…¨å±€æ ‡å‡†åŒ–æ•°æ®çš„ATE
ä½¿ç”¨å·²éªŒæ”¶çš„CausalInferenceEngineï¼ˆCTFé£æ ¼DMLæ–¹æ³•ï¼‰

æ•°æ®æº:
  - å…¨å±€æ ‡å‡†åŒ–æ•°æ®: data/energy_research/6groups_global_std/
  - DiBSç™½åå•è¾¹: results/energy_research/data/global_std_whitelist_200k/

è¾“å‡º:
  - ATEç»“æœæ–‡ä»¶ï¼ˆæ¯æ¡è¾¹çš„ATEä¼°è®¡ã€ç½®ä¿¡åŒºé—´ã€æ˜¾è‘—æ€§ï¼‰
  - ä¿å­˜åˆ°: results/energy_research/data/global_std_dibs_ate_200k/

ä½¿ç”¨æ–¹æ³•:
    # Dry run (æµ‹è¯•æ¨¡å¼)
    python compute_ate_dibs_whitelist_200k.py --dry-run

    # å®é™…è¿è¡Œï¼ˆæ›´æ–°æ‰€æœ‰ç»„ï¼‰
    python compute_ate_dibs_whitelist_200k.py

    # åªå¤„ç†ç‰¹å®šgroup
    python compute_ate_dibs_whitelist_200k.py --group 1

ä¾èµ–:
    - analysis/utils/causal_inference.py (å·²éªŒæ”¶çš„CTFé£æ ¼ATE)
    - EconML 0.14.1 (å·²å®‰è£…åˆ°causal-researchç¯å¢ƒ)
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import warnings
import time
import json
from pathlib import Path
import shutil

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from utils.causal_inference import CausalInferenceEngine
    ECONML_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥CausalInferenceEngine: {e}")
    print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
    ECONML_AVAILABLE = False


def build_causal_graph_from_whitelist(
    whitelist_df: pd.DataFrame,
    var_names: List[str],
    default_strength: float = 0.5
) -> np.ndarray:
    """
    ä»ç™½åå•æ„å»ºå› æœå›¾é‚»æ¥çŸ©é˜µ

    å‚æ•°:
        whitelist_df: ç™½åå•DataFrameï¼ŒåŒ…å«sourceå’Œtargetåˆ—
        var_names: æ‰€æœ‰å˜é‡ååˆ—è¡¨
        default_strength: é»˜è®¤è¾¹å¼ºåº¦

    è¿”å›:
        causal_graph: é‚»æ¥çŸ©é˜µï¼Œshape (n_vars, n_vars)
    """
    n_vars = len(var_names)
    causal_graph = np.zeros((n_vars, n_vars))

    # åˆ›å»ºå˜é‡ååˆ°ç´¢å¼•çš„æ˜ å°„
    var_to_idx = {var: idx for idx, var in enumerate(var_names)}

    # å¡«å……é‚»æ¥çŸ©é˜µ
    for _, row in whitelist_df.iterrows():
        source = row['source']
        target = row['target']

        if source in var_to_idx and target in var_to_idx:
            source_idx = var_to_idx[source]
            target_idx = var_to_idx[target]

            # ä½¿ç”¨strengthåˆ—çš„å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            strength = row.get('strength', default_strength)
            if pd.isna(strength):
                strength = default_strength
            causal_graph[source_idx, target_idx] = strength

    return causal_graph


def load_whitelist_edges(whitelist_file: str) -> pd.DataFrame:
    """
    åŠ è½½ç™½åå•è¾¹CSVæ–‡ä»¶

    å‚æ•°:
        whitelist_file: ç™½åå•è¾¹CSVæ–‡ä»¶è·¯å¾„

    è¿”å›:
        whitelist_df: DataFrameåŒ…å«æ‰€æœ‰è¾¹ä¿¡æ¯
    """
    if not os.path.exists(whitelist_file):
        raise FileNotFoundError(f"ç™½åå•æ–‡ä»¶ä¸å­˜åœ¨: {whitelist_file}")

    # è¯»å–CSVæ–‡ä»¶
    whitelist_df = pd.read_csv(whitelist_file)

    # éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['source', 'target']
    for col in required_cols:
        if col not in whitelist_df.columns:
            raise ValueError(f"ç™½åå•æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ— '{col}'")

    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤è¾¹
    edge_pairs = whitelist_df[['source', 'target']].drop_duplicates()
    if len(edge_pairs) < len(whitelist_df):
        print(f"âš  è­¦å‘Š: ç™½åå•æ–‡ä»¶ä¸­æœ‰é‡å¤è¾¹ï¼Œå·²è‡ªåŠ¨å»é‡")
        whitelist_df = whitelist_df.drop_duplicates(subset=['source', 'target'])

    return whitelist_df


def compute_ate_for_group(
    group_num: int,
    global_std_dir: str,
    whitelist_dir: str,
    output_dir: str,
    dry_run: bool = False,
    threshold: float = 0.3
) -> Dict:
    """
    ä¸ºå•ä¸ªç»„è®¡ç®—åŸºäºç™½åå•è¾¹çš„å…¨å±€æ ‡å‡†åŒ–ATE

    è¿”å›:
        results: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
    """
    # ç»„åæ˜ å°„
    group_mapping = {
        1: "group1_examples",
        2: "group2_vulberta",
        3: "group3_person_reid",
        4: "group4_bug_localization",
        5: "group5_mrt_oast",
        6: "group6_resnet"
    }

    group_id = group_mapping.get(group_num, f"group{group_num}")

    print(f"\n{'='*80}")
    print(f"å¤„ç†ç»„ {group_num}: {group_id}")
    print(f"{'='*80}")

    # 1. æ„å»ºæ–‡ä»¶è·¯å¾„
    global_std_file = os.path.join(global_std_dir, f"{group_id}_global_std.csv")
    whitelist_file = os.path.join(whitelist_dir, group_id, f"{group_id}_dibs_edges_whitelist.csv")
    output_file = os.path.join(output_dir, f"{group_id}_dibs_whitelist_200k_ate.csv")

    print(f"   å…¨å±€æ ‡å‡†åŒ–æ•°æ®: {global_std_file}")
    print(f"   ç™½åå•è¾¹æ–‡ä»¶: {whitelist_file}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(global_std_file):
        print(f"âŒ å…¨å±€æ ‡å‡†åŒ–æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {global_std_file}")
        return {"success": False, "error": f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {global_std_file}"}

    if not os.path.exists(whitelist_file):
        print(f"âŒ ç™½åå•è¾¹æ–‡ä»¶ä¸å­˜åœ¨: {whitelist_file}")
        return {"success": False, "error": f"ç™½åå•è¾¹æ–‡ä»¶ä¸å­˜åœ¨: {whitelist_file}"}

    # 2. è¯»å–æ•°æ®
    print(f"   è¯»å–æ•°æ®...")
    try:
        data_df = pd.read_csv(global_std_file)
        # åŠ è½½ç™½åå•è¾¹
        whitelist_df = load_whitelist_edges(whitelist_file)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {"success": False, "error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"}

    print(f"   âœ… è¯»å–æˆåŠŸ: {len(data_df)} æ¡æ•°æ®, {len(whitelist_df)} æ¡ç™½åå•è¾¹")

    if dry_run:
        print(f"   ğŸ§ª Dry runæ¨¡å¼ - åªæ£€æŸ¥æ•°æ®ï¼Œä¸è®¡ç®—ATE")
        # ç»Ÿè®¡è¾¹ä¿¡æ¯
        unique_sources = whitelist_df['source'].nunique()
        unique_targets = whitelist_df['target'].nunique()
        all_vars = set(whitelist_df['source']).union(set(whitelist_df['target']))

        # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨ç™½åå•å˜é‡
        missing_vars = []
        for var in all_vars:
            if var not in data_df.columns:
                missing_vars.append(var)

        if missing_vars:
            print(f"   âš  è­¦å‘Š: {len(missing_vars)} ä¸ªå˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨:")
            for var in missing_vars[:5]:
                print(f"      {var}")
            if len(missing_vars) > 5:
                print(f"      ... å…±{len(missing_vars)}ä¸ªå˜é‡")

        return {"success": True, "dry_run": True, "data_rows": len(data_df),
                "whitelist_edges": len(whitelist_df), "unique_sources": unique_sources,
                "unique_targets": unique_targets, "missing_vars": len(missing_vars)}

    # 3. æ•°æ®æ¸…æ´—ï¼šå¤„ç†NaNå€¼
    print(f"   æ•°æ®æ¸…æ´—...")
    original_rows = len(data_df)
    original_nan = data_df.isna().sum().sum()

    # é¦–å…ˆåˆ é™¤å…¨ä¸ºNaNçš„æ•°å€¼åˆ—ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
    all_nan_numeric_cols = []
    numeric_cols = data_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if data_df[col].isna().all():
            all_nan_numeric_cols.append(col)

    if all_nan_numeric_cols:
        print(f"   åˆ é™¤å…¨NaNæ•°å€¼åˆ—: {len(all_nan_numeric_cols)} ä¸ª")
        for col in all_nan_numeric_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"      {col}: å…¨ä¸ºNaNï¼Œåˆ é™¤")
        if len(all_nan_numeric_cols) > 5:
            print(f"      ... å…±{len(all_nan_numeric_cols)}ä¸ªåˆ—")
        data_df = data_df.drop(columns=all_nan_numeric_cols)
        numeric_cols = data_df.select_dtypes(include=[np.number]).columns

    # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
    for col in numeric_cols:
        if data_df[col].isna().sum() > 0:
            median_val = data_df[col].median()
            # æ£€æŸ¥median_valæ˜¯å¦ä¸ºNaNï¼ˆå…¨NaNåˆ—åº”è¯¥å·²è¢«åˆ é™¤ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
            if pd.isna(median_val):
                print(f"      âš  {col}: ä¸­ä½æ•°ä¸ºNaNï¼Œç”¨0å¡«å……")
                data_df[col] = data_df[col].fillna(0)
            else:
                data_df[col] = data_df[col].fillna(median_val)
                na_count = data_df[col].isna().sum()
                if na_count == 0:
                    print(f"      {col}: ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……")

    # å¸ƒå°”åˆ—ï¼šç”¨Falseå¡«å……
    bool_cols = data_df.select_dtypes(include=[bool]).columns
    for col in bool_cols:
        if data_df[col].isna().sum() > 0:
            data_df[col] = data_df[col].fillna(False)
            print(f"      {col}: ç”¨Falseå¡«å……")

    # å…¶ä»–åˆ—ï¼šç”¨ä¼—æ•°å¡«å……
    other_cols = [col for col in data_df.columns
                  if col not in numeric_cols and col not in bool_cols]
    for col in other_cols:
        if data_df[col].isna().sum() > 0:
            mode_val = data_df[col].mode()
            if len(mode_val) > 0:
                data_df[col] = data_df[col].fillna(mode_val.iloc[0])
                print(f"      {col}: ç”¨ä¼—æ•° '{mode_val.iloc[0]}' å¡«å……")
            else:
                # å¦‚æœä¼—æ•°ä¸å­˜åœ¨ï¼ˆå…¨NaNï¼‰ï¼Œç”¨ç¬¬ä¸€ä¸ªéNaNå€¼æˆ–é»˜è®¤å€¼
                non_na_vals = data_df[col].dropna()
                if len(non_na_vals) > 0:
                    data_df[col] = data_df[col].fillna(non_na_vals.iloc[0])
                    print(f"      {col}: ç”¨ç¬¬ä¸€ä¸ªéNaNå€¼å¡«å……")
                else:
                    # å¦‚æœå…¨NaNï¼Œåˆ é™¤è¯¥åˆ—
                    print(f"      {col}: å…¨ä¸ºNaNï¼Œåˆ é™¤è¯¥åˆ—")
                    data_df = data_df.drop(columns=[col])

    cleaned_nan = data_df.isna().sum().sum()
    print(f"   âœ… æ¸…æ´—å®Œæˆ: åŸå§‹NaN {original_nan} â†’ å‰©ä½™NaN {cleaned_nan}")
    if cleaned_nan > 0:
        print(f"   âš  è­¦å‘Š: ä»æœ‰ {cleaned_nan} ä¸ªNaNï¼Œå¯èƒ½ä¼šå½±å“ATEè®¡ç®—")

    # 4. å‡†å¤‡å˜é‡åˆ—è¡¨
    # ä»ç™½åå•è¾¹ä¸­è·å–æ‰€æœ‰å˜é‡
    all_vars_in_edges = set(whitelist_df['source']).union(set(whitelist_df['target']))

    # ä»æ•°æ®ä¸­è·å–æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤éç‰¹å¾åˆ—ï¼‰
    exclude_cols = ['timestamp', 'experiment_id', 'session_id']
    data_feature_cols = [col for col in data_df.columns if col not in exclude_cols]

    # æ‰¾å‡ºç™½åå•è¾¹å’Œæ•°æ®å…±æœ‰çš„å˜é‡
    common_vars = [var for var in all_vars_in_edges if var in data_feature_cols]
    edge_only_vars = [var for var in all_vars_in_edges if var not in data_feature_cols]
    data_only_vars = [var for var in data_feature_cols if var not in all_vars_in_edges]

    print(f"   å˜é‡åŒ¹é…åˆ†æ:")
    print(f"     - ç™½åå•è¾¹ä¸­å˜é‡: {len(all_vars_in_edges)} ä¸ª")
    print(f"     - æ•°æ®ä¸­ç‰¹å¾å˜é‡: {len(data_feature_cols)} ä¸ª")
    print(f"     - å…±æœ‰å˜é‡: {len(common_vars)} ä¸ª")
    if edge_only_vars:
        print(f"     - ä»…ç™½åå•ä¸­æœ‰: {len(edge_only_vars)} ä¸ª (ä¾‹å¦‚: {', '.join(edge_only_vars[:3])}{'...' if len(edge_only_vars) > 3 else ''})")
    if data_only_vars:
        print(f"     - ä»…æ•°æ®ä¸­æœ‰: {len(data_only_vars)} ä¸ª (ä»…ç»Ÿè®¡å‰3ä¸ª...)")

    # å¦‚æœå…±æœ‰çš„å˜é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„ATEè®¡ç®—
    if len(common_vars) < 5:
        print(f"âŒ å…±æœ‰å˜é‡å¤ªå°‘: ä»…{len(common_vars)}ä¸ªï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„ATEè®¡ç®—")
        return {"success": False, "error": "å…±æœ‰å˜é‡å¤ªå°‘"}

    # 5. è¿‡æ»¤ç™½åå•è¾¹ï¼Œåªä¿ç•™å…±æœ‰å˜é‡çš„è¾¹
    # é¦–å…ˆå¤åˆ¶åŸå§‹ç™½åå•æ•°æ®ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®
    filtered_whitelist = whitelist_df.copy()

    # è¿‡æ»¤æ‰åŒ…å«éå…±æœ‰å˜é‡çš„è¾¹
    original_edge_count = len(filtered_whitelist)
    filtered_whitelist = filtered_whitelist[
        filtered_whitelist['source'].isin(common_vars) &
        filtered_whitelist['target'].isin(common_vars)
    ]
    filtered_edge_count = len(filtered_whitelist)

    removed_edge_count = original_edge_count - filtered_edge_count
    if removed_edge_count > 0:
        print(f"   âš  è¿‡æ»¤æ‰ {removed_edge_count} æ¡åŒ…å«éå…±æœ‰å˜é‡çš„è¾¹")
        print(f"   âœ… è¿‡æ»¤åä¿ç•™ {filtered_edge_count} æ¡è¾¹")

    if filtered_edge_count == 0:
        print(f"âŒ è¿‡æ»¤åæ— æœ‰æ•ˆè¾¹")
        return {"success": False, "error": "è¿‡æ»¤åæ— æœ‰æ•ˆè¾¹"}

    # 6. æ„å»ºå› æœå›¾é‚»æ¥çŸ©é˜µ
    print(f"   æ„å»ºå› æœå›¾é‚»æ¥çŸ©é˜µ...")
    # ä½¿ç”¨sortedç¡®ä¿å˜é‡é¡ºåºä¸€è‡´
    sorted_common_vars = sorted(common_vars)
    causal_graph = build_causal_graph_from_whitelist(
        filtered_whitelist,
        sorted_common_vars,
        default_strength=0.5
    )

    # éªŒè¯é‚»æ¥çŸ©é˜µçš„éé›¶è¾¹æ•°åº”ä¸ç™½åå•è¾¹æ•°ä¸€è‡´
    non_zero_edges = np.sum(causal_graph > 0)
    print(f"   âœ… æ„å»ºå®Œæˆ: {causal_graph.shape[0]}Ã—{causal_graph.shape[1]} çŸ©é˜µ, {non_zero_edges} æ¡éé›¶è¾¹")

    if non_zero_edges != filtered_edge_count:
        print(f"   âš  è­¦å‘Š: é‚»æ¥çŸ©é˜µéé›¶è¾¹æ•°({non_zero_edges})ä¸ç™½åå•è¾¹æ•°({filtered_edge_count})ä¸ä¸€è‡´")
        # è¿™å¯èƒ½æ˜¯ç”±äºé‡å¤è¾¹æˆ–åŒä¸€å¯¹å˜é‡æœ‰å¤šæ¡è¾¹å¯¼è‡´çš„ï¼Œç»§ç»­æ‰§è¡Œ

    # 7. æ·»åŠ ATEåˆ—åˆ°ç™½åå•DataFrame
    ate_cols = [
        'ate_whitelist_200k',
        'ate_whitelist_200k_ci_lower',
        'ate_whitelist_200k_ci_upper',
        'ate_whitelist_200k_is_significant',
        'ate_whitelist_200k_confounders_count'
    ]

    for col in ate_cols:
        if col not in filtered_whitelist.columns:
            if col == 'ate_whitelist_200k_confounders_count':
                filtered_whitelist[col] = 0  # æ•´æ•°
            elif col == 'ate_whitelist_200k_is_significant':
                filtered_whitelist[col] = False  # å¸ƒå°”å€¼
            else:
                filtered_whitelist[col] = np.nan  # æµ®ç‚¹æ•°

    # 8. åˆå§‹åŒ–å› æœæ¨æ–­å¼•æ“
    engine = CausalInferenceEngine(verbose=True)

    # 9. è®¡ç®—æ¯æ¡è¾¹çš„ATEï¼ˆå…¨å±€æ ‡å‡†åŒ–æ•°æ®ï¼‰
    print(f"   å¼€å§‹è®¡ç®—ç™½åå•ATE...")
    start_time = time.time()

    try:
        results = engine.analyze_all_edges_ctf_style(
            data=data_df,
            causal_graph=causal_graph,
            var_names=sorted_common_vars,
            threshold=0,  # ä½¿ç”¨0ï¼Œå› ä¸ºç™½åå•è¾¹å·²ç»ç»è¿‡é˜ˆå€¼è¿‡æ»¤
            ref_df=None,  # ä½¿ç”¨CTFé£æ ¼ï¼šè‡ªåŠ¨åˆ›å»ºæ•°æ®å‡å€¼å‘é‡
            t_strategy='quantile'  # ä½¿ç”¨CTFé£æ ¼ï¼š25/75åˆ†ä½æ•°T0/T1
        )

        elapsed_time = time.time() - start_time
        print(f"   âœ… ATEè®¡ç®—å®Œæˆï¼è€—æ—¶: {elapsed_time:.1f}ç§’")

        # 10. æ›´æ–°ç™½åå•DataFrame
        print(f"   æ›´æ–°ç™½åå•æ•°æ®...")

        # æ£€æŸ¥resultsçš„ç±»å‹
        if isinstance(results, dict):
            # resultsæ˜¯å­—å…¸çš„å­—å…¸ï¼Œé”®æ˜¯"source->target"æ ¼å¼
            print(f"   resultsç±»å‹: å­—å…¸ (åŒ…å« {len(results)} ä¸ªé”®å€¼å¯¹)")

            # åˆ›å»ºç»“æœæ˜ å°„å­—å…¸
            results_dict = {}
            for edge_key, result in results.items():
                if isinstance(result, dict) and 'ate' in result:
                    # è§£æè¾¹é”® "source->target"
                    if '->' in edge_key:
                        source, target = edge_key.split('->', 1)
                        key = (source, target)
                        results_dict[key] = result
                    else:
                        print(f"   âš  è­¦å‘Š: è·³è¿‡æ— æ•ˆè¾¹é”®æ ¼å¼: {edge_key}")
                else:
                    print(f"   âš  è­¦å‘Š: è·³è¿‡æ— æ•ˆç»“æœ: {type(result)}")

            valid_results = len(results_dict)
            print(f"   æœ‰æ•ˆç»“æœæ•°: {valid_results}/{len(results)}")

        else:
            print(f"   âŒ é”™è¯¯: resultsç±»å‹æœªçŸ¥: {type(results)}")
            results_dict = {}
            valid_results = 0

        # æ›´æ–°æ¯æ¡è¾¹çš„ATEä¿¡æ¯
        updated_count = 0
        for idx, row in filtered_whitelist.iterrows():
            source = row['source']
            target = row['target']
            key = (source, target)

            if key in results_dict:
                result = results_dict[key]

                # æ›´æ–°ç™½åå•ATEåˆ—
                filtered_whitelist.at[idx, 'ate_whitelist_200k'] = result.get('ate', np.nan)
                filtered_whitelist.at[idx, 'ate_whitelist_200k_ci_lower'] = result.get('ci_lower', np.nan)
                filtered_whitelist.at[idx, 'ate_whitelist_200k_ci_upper'] = result.get('ci_upper', np.nan)
                filtered_whitelist.at[idx, 'ate_whitelist_200k_is_significant'] = result.get('is_significant', False)

                # å¤„ç†æ··æ·†å› ç´ è®¡æ•°
                confounders = result.get('confounders', [])
                if isinstance(confounders, list):
                    filtered_whitelist.at[idx, 'ate_whitelist_200k_confounders_count'] = len(confounders)
                else:
                    filtered_whitelist.at[idx, 'ate_whitelist_200k_confounders_count'] = 0

                updated_count += 1

        print(f"   âœ… æ›´æ–°å®Œæˆ: {updated_count}/{len(filtered_whitelist)} æ¡è¾¹å·²æ›´æ–°")

        # 11. ä¿å­˜ç»“æœ
        print(f"   ä¿å­˜ç»“æœ...")
        os.makedirs(output_dir, exist_ok=True)
        filtered_whitelist.to_csv(output_file, index=False)
        print(f"   âœ… ç»“æœå·²ä¿å­˜: {output_file}")

        # 12. ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        ate_values = filtered_whitelist['ate_whitelist_200k'].dropna()
        significant_count = filtered_whitelist['ate_whitelist_200k_is_significant'].sum()

        summary = {
            "group_id": group_id,
            "group_num": group_num,
            "data_rows": len(data_df),
            "whitelist_edges_original": len(whitelist_df),
            "whitelist_edges_filtered": len(filtered_whitelist),
            "edges_removed": removed_edge_count,
            "common_vars": len(common_vars),
            "ate_computed": len(ate_values),
            "ate_significant": int(significant_count),
            "ate_mean": float(ate_values.mean()) if len(ate_values) > 0 else np.nan,
            "ate_std": float(ate_values.std()) if len(ate_values) > 0 else np.nan,
            "ate_min": float(ate_values.min()) if len(ate_values) > 0 else np.nan,
            "ate_max": float(ate_values.max()) if len(ate_values) > 0 else np.nan,
            "elapsed_seconds": elapsed_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿å­˜æ‘˜è¦
        summary_file = os.path.join(output_dir, f"{group_id}_ate_whitelist_200k_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"   âœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")

        return {
            "success": True,
            "summary": summary,
            "output_file": output_file,
            "summary_file": summary_file
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"âŒ ATEè®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        return {
            "success": False,
            "error": str(e),
            "elapsed_seconds": elapsed_time
        }


def main():
    parser = argparse.ArgumentParser(description="åŸºäºDiBSç™½åå•å› æœå›¾è®¡ç®—å…¨å±€æ ‡å‡†åŒ–æ•°æ®çš„ATEï¼ˆæ–¹æ¡ˆCï¼Œ200kæ­¥ï¼‰")
    parser.add_argument("--dry-run", action="store_true",
                       help="åªæµ‹è¯•ä¸å†™å…¥æ–‡ä»¶")
    parser.add_argument("--group", type=int, choices=range(1, 7),
                       help="åªå¤„ç†æŒ‡å®šç»„ï¼ˆ1-6ï¼‰")
    parser.add_argument("--global-std-dir", type=str,
                       default="data/energy_research/6groups_global_std",
                       help="å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç›®å½•")
    parser.add_argument("--whitelist-dir", type=str,
                       default="results/energy_research/data/global_std_whitelist_200k",
                       help="ç™½åå•è¾¹ç›®å½•")
    parser.add_argument("--output-dir", type=str,
                       default="results/energy_research/data/global_std_dibs_ate_200k",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--threshold", type=float, default=0.0,
                       help="è¾¹å¼ºåº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.0ï¼Œç™½åå•å·²è¿‡æ»¤ï¼‰")

    args = parser.parse_args()

    print("=" * 80)
    print("DiBSç™½åå•å› æœå›¾ATEè®¡ç®—ï¼ˆæ–¹æ¡ˆCï¼Œ200kæ­¥ï¼‰")
    print("=" * 80)

    print(f"\né…ç½®:")
    print(f"  å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç›®å½•: {args.global_std_dir}")
    print(f"  ç™½åå•è¾¹ç›®å½•: {args.whitelist_dir}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  è¾¹å¼ºåº¦é˜ˆå€¼: {args.threshold}")
    print(f"  Dry runæ¨¡å¼: {args.dry_run}")

    # æ£€æŸ¥EconMLæ˜¯å¦å¯ç”¨
    if not ECONML_AVAILABLE:
        print("\nâŒ æ— æ³•å¯¼å…¥CausalInferenceEngine")
        print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
        return 1

    # ç¡®å®šè¦å¤„ç†çš„ç»„
    if args.group:
        groups_to_process = [args.group]
        print(f"\nå¤„ç†æŒ‡å®šç»„: {args.group}")
    else:
        groups_to_process = list(range(1, 7))
        print(f"\nå¤„ç†æ‰€æœ‰ç»„: 1-6")

    # å¤„ç†æ¯ä¸ªç»„
    all_results = []

    for group_num in groups_to_process:
        result = compute_ate_for_group(
            group_num=group_num,
            global_std_dir=args.global_std_dir,
            whitelist_dir=args.whitelist_dir,
            output_dir=args.output_dir,
            dry_run=args.dry_run,
            threshold=args.threshold
        )

        all_results.append({
            "group": group_num,
            **result
        })

    # ç”Ÿæˆæ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ç™½åå•ATEè®¡ç®—æ€»æŠ¥å‘Š")
    print(f"{'='*80}")

    successful_groups = [r for r in all_results if r.get('success', False)]
    failed_groups = [r for r in all_results if not r.get('success', False)]

    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  æˆåŠŸç»„æ•°: {len(successful_groups)}")
    print(f"  å¤±è´¥ç»„æ•°: {len(failed_groups)}")

    if successful_groups:
        print(f"\næˆåŠŸç»„è¯¦æƒ…:")
        for result in successful_groups:
            if result.get('dry_run', False):
                print(f"  ç»„ {result['group']}: Dry runå®Œæˆ - {result.get('data_rows', 0)}è¡Œæ•°æ®, {result.get('whitelist_edges', 0)}æ¡è¾¹")
            elif 'summary' in result:
                summary = result['summary']
                print(f"  ç»„ {result['group']}: {summary['ate_computed']}/{summary['whitelist_edges_filtered']}æ¡è¾¹è®¡ç®—ATE")
                print(f"      ATEæ˜¾è‘—: {summary['ate_significant']}æ¡")
                print(f"      ATEå‡å€¼: {summary['ate_mean']:.4f}, æ ‡å‡†å·®: {summary['ate_std']:.4f}")

    if failed_groups:
        print(f"\nå¤±è´¥ç»„è¯¦æƒ…:")
        for result in failed_groups:
            print(f"  ç»„ {result['group']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # ä¿å­˜æ€»æŠ¥å‘Š
    if not args.dry_run and successful_groups:
        total_report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "config": {
                "global_std_dir": args.global_std_dir,
                "whitelist_dir": args.whitelist_dir,
                "output_dir": args.output_dir,
                "threshold": args.threshold,
                "dry_run": args.dry_run
            },
            "results": all_results,
            "summary": {
                "total_groups": len(all_results),
                "successful_groups": len(successful_groups),
                "failed_groups": len(failed_groups)
            }
        }

        total_report_file = os.path.join(args.output_dir, "ate_whitelist_200k_total_report.json")
        os.makedirs(args.output_dir, exist_ok=True)
        with open(total_report_file, 'w') as f:
            json.dump(total_report, f, indent=2)

        print(f"\nâœ… æ€»æŠ¥å‘Šå·²ä¿å­˜: {total_report_file}")

    print(f"\n{'='*80}")
    print("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
    print(f"{'='*80}")

    return 0 if len(failed_groups) == 0 else 1


if __name__ == "__main__":
    sys.exit(main())