#!/usr/bin/env python3
"""
ä¸ºç™½åå•å› æœè¾¹è®¡ç®—ATE (Average Treatment Effect)
ä½¿ç”¨å·²éªŒæ”¶çš„CausalInferenceEngineï¼ˆCTFé£æ ¼DMLæ–¹æ³•ï¼‰

è¯»å–ç™½åå•CSVæ–‡ä»¶ï¼Œä¸ºæ¯æ¡å› æœè¾¹è®¡ç®—ATEå’Œç½®ä¿¡åŒºé—´ï¼Œ
å¹¶å°†ç»“æœæ·»åŠ åˆ°CSVä¸­ã€‚

ä½¿ç”¨æ–¹æ³•:
    # Dry run (æµ‹è¯•æ¨¡å¼)
    python compute_ate_whitelist.py --dry-run

    # å®é™…è¿è¡Œï¼ˆæ›´æ–°æ‰€æœ‰ç™½åå•æ–‡ä»¶ï¼‰
    python compute_ate_whitelist.py

    # åªå¤„ç†ç‰¹å®šgroup
    python compute_ate_whitelist.py --group 1

ä¾èµ–:
    - analysis/utils/causal_inference.py (å·²éªŒæ”¶çš„CTFé£æ ¼ATE)
    - EconML 0.14.1 (å·²å®‰è£…åˆ°causal-researchç¯å¢ƒ)
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import warnings
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from utils.causal_inference import CausalInferenceEngine
    ECONML_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥CausalInferenceEngine: {e}")
    print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
    ECONML_AVAILABLE = False


def build_causal_graph_from_whitelist(
    whitelist_df: pd.DataFrame,
    var_names: List[str],
    default_strength: float = 0.5
) -> np.ndarray:
    """
    ä»ç™½åå•DataFrameæ„å»ºå› æœå›¾é‚»æ¥çŸ©é˜µ

    å‚æ•°:
        whitelist_df: ç™½åå•DataFrameï¼ŒåŒ…å«source, target, strengthåˆ—
        var_names: æ‰€æœ‰å˜é‡ååˆ—è¡¨
        default_strength: é»˜è®¤è¾¹æƒé‡ï¼ˆç”¨äºä¸åœ¨ç™½åå•ä¸­çš„è¾¹ï¼Œè®¾ä¸º0ï¼‰

    è¿”å›:
        causal_graph: (n_vars, n_vars)é‚»æ¥çŸ©é˜µ
    """
    n_vars = len(var_names)
    causal_graph = np.zeros((n_vars, n_vars))

    # åˆ›å»ºå˜é‡ååˆ°ç´¢å¼•çš„æ˜ å°„
    var_to_idx = {var: i for i, var in enumerate(var_names)}

    # å¡«å……ç™½åå•ä¸­çš„è¾¹
    for _, row in whitelist_df.iterrows():
        source = row['source']
        target = row['target']
        strength = row.get('strength', default_strength)

        if source in var_to_idx and target in var_to_idx:
            source_idx = var_to_idx[source]
            target_idx = var_to_idx[target]
            causal_graph[source_idx, target_idx] = strength

    return causal_graph


def process_whitelist_file(
    whitelist_path: str,
    data_path: str,
    dry_run: bool = False,
    threshold: float = 0.3
) -> pd.DataFrame:
    """
    å¤„ç†å•ä¸ªç™½åå•æ–‡ä»¶

    å‚æ•°:
        whitelist_path: ç™½åå•CSVè·¯å¾„
        data_path: æ•°æ®CSVè·¯å¾„
        dry_run: æ˜¯å¦åªæµ‹è¯•ä¸ä¿å­˜
        threshold: è¾¹æƒé‡é˜ˆå€¼

    è¿”å›:
        updated_df: æ›´æ–°åçš„DataFrameï¼ˆåŒ…å«ATEç»“æœï¼‰
    """
    print(f"\nğŸ“‚ å¤„ç†ç™½åå•: {os.path.basename(whitelist_path)}")
    print(f"   æ•°æ®æ–‡ä»¶: {os.path.basename(data_path)}")

    # è¯»å–æ•°æ®
    try:
        whitelist_df = pd.read_csv(whitelist_path)
        data_df = pd.read_csv(data_path)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None

    print(f"   âœ… è¯»å–æˆåŠŸ: {len(whitelist_df)} æ¡è¾¹, {len(data_df)} æ¡æ•°æ®")

    # æ•°æ®æ¸…æ´—ï¼šå¤„ç†NaNå€¼
    print(f"   æ•°æ®æ¸…æ´—...")
    original_rows = len(data_df)
    original_nan = data_df.isna().sum().sum()

    # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
    numeric_cols = data_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if data_df[col].isna().sum() > 0:
            median_val = data_df[col].median()
            data_df[col] = data_df[col].fillna(median_val)
            na_count = data_df[col].isna().sum()
            if na_count == 0:
                print(f"      {col}: ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……")

    # å¸ƒå°”åˆ—ï¼šç”¨Falseå¡«å……
    bool_cols = data_df.select_dtypes(include=[bool]).columns
    for col in bool_cols:
        if data_df[col].isna().sum() > 0:
            data_df[col] = data_df[col].fillna(False)
            print(f"      {col}: ç”¨Falseå¡«å……")

    # å…¶ä»–åˆ—ï¼šç”¨ä¼—æ•°å¡«å……
    other_cols = [col for col in data_df.columns
                  if col not in numeric_cols and col not in bool_cols]
    for col in other_cols:
        if data_df[col].isna().sum() > 0:
            mode_val = data_df[col].mode()
            if len(mode_val) > 0:
                data_df[col] = data_df[col].fillna(mode_val.iloc[0])
                print(f"      {col}: ç”¨ä¼—æ•° '{mode_val.iloc[0]}' å¡«å……")
            else:
                # å¦‚æœä¼—æ•°ä¸å­˜åœ¨ï¼ˆå…¨NaNï¼‰ï¼Œç”¨ç¬¬ä¸€ä¸ªéNaNå€¼æˆ–é»˜è®¤å€¼
                non_na_vals = data_df[col].dropna()
                if len(non_na_vals) > 0:
                    data_df[col] = data_df[col].fillna(non_na_vals.iloc[0])
                    print(f"      {col}: ç”¨ç¬¬ä¸€ä¸ªéNaNå€¼å¡«å……")
                else:
                    # å¦‚æœå…¨NaNï¼Œåˆ é™¤è¯¥åˆ—
                    print(f"      {col}: å…¨ä¸ºNaNï¼Œåˆ é™¤è¯¥åˆ—")
                    data_df = data_df.drop(columns=[col])

    cleaned_nan = data_df.isna().sum().sum()
    print(f"   âœ… æ¸…æ´—å®Œæˆ: åŸå§‹NaN {original_nan} â†’ å‰©ä½™NaN {cleaned_nan}")
    if cleaned_nan > 0:
        print(f"   âš  è­¦å‘Š: ä»æœ‰ {cleaned_nan} ä¸ªNaNï¼Œå¯èƒ½ä¼šå½±å“ATEè®¡ç®—")

    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_whitelist_cols = ['source', 'target']
    missing_cols = [col for col in required_whitelist_cols if col not in whitelist_df.columns]
    if missing_cols:
        print(f"âŒ ç™½åå•ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        return None

    # æ·»åŠ ATEåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    ate_cols = ['ate', 'ate_ci_lower', 'ate_ci_upper', 'ate_is_significant', 'ate_confounders_count']
    for col in ate_cols:
        if col not in whitelist_df.columns:
            if col == 'ate_confounders_count':
                whitelist_df[col] = 0  # æ•´æ•°
            elif col == 'ate_is_significant':
                whitelist_df[col] = False  # å¸ƒå°”å€¼ï¼Œé¿å…dtypeè­¦å‘Š
            else:
                whitelist_df[col] = np.nan  # æµ®ç‚¹æ•°

    # è·å–æ•°æ®ä¸­çš„æ‰€æœ‰å˜é‡åï¼ˆæ’é™¤éç‰¹å¾åˆ—ï¼‰
    exclude_cols = ['timestamp', 'experiment_id', 'session_id']  # å¸¸è§éç‰¹å¾åˆ—
    feature_cols = [col for col in data_df.columns if col not in exclude_cols]

    # æ„å»ºå› æœå›¾
    print(f"   æ„å»ºå› æœå›¾...")
    causal_graph = build_causal_graph_from_whitelist(whitelist_df, feature_cols)

    # åˆå§‹åŒ–å› æœæ¨æ–­å¼•æ“
    engine = CausalInferenceEngine(verbose=True)

    # è®¡ç®—æ¯æ¡è¾¹çš„ATE
    print(f"   å¼€å§‹è®¡ç®—ATE...")
    start_time = time.time()

    results = engine.analyze_all_edges_ctf_style(
        data=data_df,
        causal_graph=causal_graph,
        var_names=feature_cols,
        threshold=threshold,
        ref_df=None,  # æš‚æ—¶ä¸ä½¿ç”¨ref_df
        t_strategy=None  # æš‚æ—¶ä½¿ç”¨é»˜è®¤T0/T1
    )

    elapsed_time = time.time() - start_time
    print(f"   âœ… ATEè®¡ç®—å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f}ç§’")
    print(f"   æˆåŠŸåˆ†æ: {len(results)} æ¡è¾¹")

    # æ›´æ–°ç™½åå•DataFrame
    updated_count = 0
    for i, row in whitelist_df.iterrows():
        source = row['source']
        target = row['target']
        edge_key = f"{source}->{target}"

        if edge_key in results:
            result = results[edge_key]
            whitelist_df.at[i, 'ate'] = result['ate']
            whitelist_df.at[i, 'ate_ci_lower'] = result['ci_lower']
            whitelist_df.at[i, 'ate_ci_upper'] = result['ci_upper']
            whitelist_df.at[i, 'ate_is_significant'] = result['is_significant']
            whitelist_df.at[i, 'ate_confounders_count'] = len(result['confounders'])
            updated_count += 1

    print(f"   æ›´æ–°äº† {updated_count}/{len(whitelist_df)} æ¡è¾¹çš„ATEç»“æœ")

    # å¦‚æœédry runï¼Œä¿å­˜ç»“æœ
    if not dry_run:
        output_path = whitelist_path  # è¦†ç›–åŸæ–‡ä»¶
        whitelist_df.to_csv(output_path, index=False)
        print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    else:
        print(f"   ğŸ§ª Dry runæ¨¡å¼ï¼Œæœªä¿å­˜æ–‡ä»¶")

        # æ˜¾ç¤ºå‰å‡ æ¡ç»“æœä½œä¸ºç¤ºä¾‹
        if updated_count > 0:
            print(f"\n   ç¤ºä¾‹ç»“æœ (å‰3æ¡):")
            sample_df = whitelist_df[['source', 'target', 'ate', 'ate_is_significant']].head(3)
            for _, row in sample_df.iterrows():
                if not pd.isna(row['ate']):
                    print(f"     {row['source']} â†’ {row['target']}: ATE={row['ate']:.3f}, æ˜¾è‘—={row['ate_is_significant']}")

    return whitelist_df


def main():
    parser = argparse.ArgumentParser(description='ä¸ºç™½åå•å› æœè¾¹è®¡ç®—ATE')
    parser.add_argument('--dry-run', action='store_true',
                       help='æµ‹è¯•æ¨¡å¼ï¼Œä¸ä¿å­˜æ–‡ä»¶')
    parser.add_argument('--group', type=int, choices=range(1, 7),
                       help='åªå¤„ç†ç‰¹å®šgroup (1-6)')
    parser.add_argument('--threshold', type=float, default=0.3,
                       help='è¾¹æƒé‡é˜ˆå€¼ (é»˜è®¤: 0.3)')

    args = parser.parse_args()

    if not ECONML_AVAILABLE:
        print("âŒ æ— æ³•å¯¼å…¥CausalInferenceEngineï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        print("   ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
        print("   å¹¶å·²å®‰è£…EconML: pip install econml==0.14.1")
        sys.exit(1)

    # å®šä¹‰ç™½åå•å’Œæ•°æ®æ–‡ä»¶è·¯å¾„
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    whitelist_dir = os.path.join(
        base_dir, 'results', 'energy_research', 'data', 'interaction', 'whitelist'
    )

    data_dir = os.path.join(
        base_dir, 'data', 'energy_research', '6groups_interaction'
    )

    # å®šä¹‰groupæ˜ å°„ï¼ˆä½¿ç”¨äº¤äº’é¡¹æ•°æ®é›†ï¼‰
    groups = {
        1: ('group1_examples', 'group1_examples_interaction.csv'),
        2: ('group2_vulberta', 'group2_vulberta_interaction.csv'),
        3: ('group3_person_reid', 'group3_person_reid_interaction.csv'),
        4: ('group4_bug_localization', 'group4_bug_localization_interaction.csv'),
        5: ('group5_mrt_oast', 'group5_mrt_oast_interaction.csv'),
        6: ('group6_resnet', 'group6_resnet_interaction.csv')
    }

    # ç¡®å®šè¦å¤„ç†çš„groups
    if args.group:
        groups_to_process = {args.group: groups[args.group]}
    else:
        groups_to_process = groups

    print(f"ğŸš€ å¼€å§‹å¤„ç†ç™½åå•ATEè®¡ç®—")
    print(f"   æ¨¡å¼: {'ğŸ§ª Dry Run' if args.dry_run else 'ğŸš€ å®é™…è¿è¡Œ'}")
    print(f"   é˜ˆå€¼: {args.threshold}")
    print(f"   å¤„ç† {len(groups_to_process)} ä¸ªgroup")

    total_start = time.time()
    results = {}

    for group_num, (whitelist_prefix, data_file) in groups_to_process.items():
        whitelist_path = os.path.join(whitelist_dir, f"{whitelist_prefix}_causal_edges_whitelist.csv")
        data_path = os.path.join(data_dir, data_file)

        if not os.path.exists(whitelist_path):
            print(f"âŒ ç™½åå•æ–‡ä»¶ä¸å­˜åœ¨: {whitelist_path}")
            continue

        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            continue

        result_df = process_whitelist_file(
            whitelist_path, data_path,
            dry_run=args.dry_run,
            threshold=args.threshold
        )

        if result_df is not None:
            results[group_num] = result_df

    total_time = time.time() - total_start

    print(f"\n{'='*60}")
    print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’")
    print(f"   æˆåŠŸå¤„ç†: {len(results)}/{len(groups_to_process)} ä¸ªgroup")

    if args.dry_run:
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"   1. æ£€æŸ¥ä¸Šè¿°è¾“å‡ºæ˜¯å¦æ­£å¸¸")
        print(f"   2. ç¡®è®¤ATEè®¡ç®—ç»“æœç¬¦åˆé¢„æœŸ")
        print(f"   3. ç§»é™¤--dry-runå‚æ•°å®é™…è¿è¡Œ")
    else:
        print(f"\nâœ… æ‰€æœ‰ç™½åå•æ–‡ä»¶å·²æ›´æ–°ATEç»“æœ")

    return 0


if __name__ == '__main__':
    sys.exit(main())