#!/usr/bin/env python3
"""
DiBSå‚æ•°æ‰«ææµ‹è¯•è„šæœ¬

ç›®çš„: ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œå¯»æ‰¾èƒ½äº§ç”Ÿè¾¹çš„æœ€ä½³é…ç½®
åˆ›å»ºæ—¥æœŸ: 2026-01-04
"""

import numpy as np
import pandas as pd
import sys
import os
import time
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.causal_discovery import CausalGraphLearner

# æµ‹è¯•å‚æ•°é…ç½®
EXPERIMENTS = [
    # é˜¶æ®µ1: æå°alphaå€¼æµ‹è¯•ï¼ˆå…³é”®ï¼ä¹‹å‰ä»æœªæµ‹è¯•è¿‡ï¼‰
    {
        "id": "A1",
        "name": "æå°alpha (0.001)",
        "alpha": 0.001,
        "n_particles": 20,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 1
    },
    {
        "id": "A2",
        "name": "å°alpha (0.01)",
        "alpha": 0.01,
        "n_particles": 20,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 1
    },
    {
        "id": "A3",
        "name": "DiBSé»˜è®¤alpha (0.05)",
        "alpha": 0.05,
        "n_particles": 20,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 1
    },

    # é˜¶æ®µ2: å¢åŠ ç²’å­æ•°
    {
        "id": "A4",
        "name": "ä¸­ç­‰ç²’å­æ•° (50)",
        "alpha": 0.05,
        "n_particles": 50,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 2
    },
    {
        "id": "A5",
        "name": "å¤§ç²’å­æ•° (100)",
        "alpha": 0.05,
        "n_particles": 100,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 2
    },

    # é˜¶æ®µ3: é™ä½çº¦æŸ
    {
        "id": "A6",
        "name": "ä½betaçº¦æŸ (0.1)",
        "alpha": 0.05,
        "n_particles": 20,
        "beta": 0.1,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 3
    },
    {
        "id": "A7",
        "name": "ä¸­betaçº¦æŸ (0.5)",
        "alpha": 0.05,
        "n_particles": 20,
        "beta": 0.5,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 3
    },

    # é˜¶æ®µ4: è°ƒæ•´tau
    {
        "id": "A8",
        "name": "ä½tauæ¸©åº¦ (0.5)",
        "alpha": 0.05,
        "n_particles": 20,
        "beta": 1.0,
        "tau": 0.5,
        "n_steps": 5000,
        "n_grad_mc_samples": 128,
        "priority": 4
    },

    # é˜¶æ®µ5: å¢åŠ MCæ ·æœ¬æ•°
    {
        "id": "A9",
        "name": "é«˜MCæ ·æœ¬æ•° (256)",
        "alpha": 0.05,
        "n_particles": 20,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 5000,
        "n_grad_mc_samples": 256,
        "priority": 4
    },

    # é˜¶æ®µ6: ç»„åˆä¼˜åŒ–ï¼ˆåŸºäºé˜¶æ®µ1-5çš„æœ€ä½³ç»“æœï¼‰
    {
        "id": "B1",
        "name": "ç»„åˆä¼˜åŒ–: æå°alpha + å¤§ç²’å­æ•°",
        "alpha": 0.001,
        "n_particles": 100,
        "beta": 1.0,
        "tau": 1.0,
        "n_steps": 10000,
        "n_grad_mc_samples": 128,
        "priority": 5
    },
    {
        "id": "B2",
        "name": "ç»„åˆä¼˜åŒ–: å°alpha + ä½çº¦æŸ",
        "alpha": 0.01,
        "n_particles": 50,
        "beta": 0.1,
        "tau": 1.0,
        "n_steps": 10000,
        "n_grad_mc_samples": 256,
        "priority": 5
    },
]

def load_test_data():
    """
    åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨examplesä»»åŠ¡ç»„ï¼Œ219æ ·æœ¬ï¼Œ15å˜é‡ï¼‰

    æ³¨æ„: è¿™é‡Œéœ€è¦å‡†å¤‡å¥½å¤„ç†è¿‡çš„æ•°æ®æ–‡ä»¶
    """
    data_file = Path(__file__).parent.parent / "data" / "energy_research" / "processed" / "examples_for_dibs.csv"

    if not data_file.exists():
        # å¦‚æœå¤„ç†åçš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®å¹¶è¿›è¡Œå¿«é€Ÿå¤„ç†
        print("è­¦å‘Š: å¤„ç†åçš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
        raw_file = Path(__file__).parent.parent / "data" / "energy_research" / "raw" / "energy_data_original.csv"

        if not raw_file.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}")

        # åŠ è½½å¹¶è¿‡æ»¤examplesæ•°æ®
        df = pd.read_csv(raw_file)

        # è¿‡æ»¤examplesä»»åŠ¡
        examples_repos = ['examples']
        df = df[df['repository'].isin(examples_repos)].copy()

        # é€‰æ‹©æ•°å€¼å‹åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        df = df[numeric_cols]

        # ç§»é™¤å…¨NaNåˆ—
        df = df.dropna(axis=1, how='all')

        # ç§»é™¤ç¼ºå¤±ç‡>30%çš„åˆ—
        missing_per_col = df.isna().sum() / len(df)
        cols_to_keep = missing_per_col[missing_per_col <= 0.30].index.tolist()
        df = df[cols_to_keep]

        # å¡«å……å‰©ä½™ç¼ºå¤±å€¼
        df = df.fillna(df.mean())

        # æ ‡å‡†åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        df_scaled = pd.DataFrame(
            scaler.fit_transform(df),
            columns=df.columns
        )

        return df_scaled

    return pd.read_csv(data_file)

def run_experiment(exp_config, data, output_dir):
    """
    è¿è¡Œå•ä¸ªå®éªŒ

    å‚æ•°:
        exp_config: å®éªŒé…ç½®å­—å…¸
        data: æµ‹è¯•æ•°æ®
        output_dir: è¾“å‡ºç›®å½•

    è¿”å›:
        result_dict: å®éªŒç»“æœå­—å…¸
    """
    exp_id = exp_config["id"]
    exp_name = exp_config["name"]

    print("\n" + "=" * 80)
    print(f"å®éªŒ {exp_id}: {exp_name}")
    print("=" * 80)

    # æ‰“å°å‚æ•°
    print(f"å‚æ•°é…ç½®:")
    print(f"  alpha_linear: {exp_config['alpha']}")
    print(f"  n_particles: {exp_config['n_particles']}")
    print(f"  beta_linear: {exp_config['beta']}")
    print(f"  tau: {exp_config['tau']}")
    print(f"  n_steps: {exp_config['n_steps']}")
    print(f"  n_grad_mc_samples: {exp_config['n_grad_mc_samples']}")

    # åˆ›å»ºlearner
    learner = CausalGraphLearner(
        n_vars=len(data.columns),
        alpha=exp_config['alpha'],
        n_particles=exp_config['n_particles'],
        beta=exp_config['beta'],
        tau=exp_config['tau'],
        n_steps=exp_config['n_steps'],
        n_grad_mc_samples=exp_config['n_grad_mc_samples'],
        n_acyclicity_mc_samples=32,
        random_seed=42
    )

    # è¿è¡ŒDiBS
    start_time = time.time()
    try:
        causal_graph = learner.fit(data, verbose=True)
        elapsed_time = time.time() - start_time
        success = True
        error_msg = None
    except Exception as e:
        elapsed_time = time.time() - start_time
        causal_graph = None
        success = False
        error_msg = str(e)
        print(f"\nâŒ å®éªŒå¤±è´¥: {error_msg}")

    # åˆ†æç»“æœ
    if success and causal_graph is not None:
        graph_min = float(causal_graph.min())
        graph_max = float(causal_graph.max())
        graph_mean = float(causal_graph.mean())
        graph_std = float(causal_graph.std())

        # ä¸åŒé˜ˆå€¼ä¸‹çš„è¾¹æ•°
        edges_001 = int(np.sum(causal_graph > 0.01))
        edges_01 = int(np.sum(causal_graph > 0.1))
        edges_02 = int(np.sum(causal_graph > 0.2))
        edges_03 = int(np.sum(causal_graph > 0.3))
        edges_05 = int(np.sum(causal_graph > 0.5))

        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
        if edges_001 > 0:
            status = "æˆåŠŸ" if edges_03 > 0 else "éƒ¨åˆ†æˆåŠŸ"
        else:
            status = "å¤±è´¥"

        print(f"\n{'='*80}")
        print(f"å®éªŒç»“æœ:")
        print(f"  çŠ¶æ€: {status}")
        print(f"  è€—æ—¶: {elapsed_time/60:.2f}åˆ†é’Ÿ")
        print(f"  å›¾çŸ©é˜µç»Ÿè®¡:")
        print(f"    Min: {graph_min:.6f}")
        print(f"    Max: {graph_max:.6f}")
        print(f"    Mean: {graph_mean:.6f}")
        print(f"    Std: {graph_std:.6f}")
        print(f"  è¾¹æ•°ç»Ÿè®¡:")
        print(f"    >0.01: {edges_001}")
        print(f"    >0.1: {edges_01}")
        print(f"    >0.2: {edges_02}")
        print(f"    >0.3: {edges_03}")
        print(f"    >0.5: {edges_05}")

        if edges_03 > 0:
            print(f"\nğŸ‰ğŸ‰ğŸ‰ æˆåŠŸæ£€æµ‹åˆ°{edges_03}æ¡è¾¹ï¼")
        elif edges_001 > 0:
            print(f"\nâš ï¸  æ£€æµ‹åˆ°{edges_001}æ¡å¼±è¾¹ï¼ˆéœ€é™ä½é˜ˆå€¼ï¼‰")
        else:
            print(f"\nâŒ å›¾çŸ©é˜µä»ç„¶å…¨ä¸º0")

        # ä¿å­˜å›¾çŸ©é˜µ
        graph_file = output_dir / f"{exp_id}_graph.npy"
        np.save(graph_file, causal_graph)

    else:
        graph_min = graph_max = graph_mean = graph_std = None
        edges_001 = edges_01 = edges_02 = edges_03 = edges_05 = 0
        status = "é”™è¯¯"

    # æ„å»ºç»“æœå­—å…¸
    result = {
        "exp_id": exp_id,
        "exp_name": exp_name,
        "status": status,
        "success": success,
        "elapsed_time_minutes": elapsed_time / 60,
        "parameters": {
            "alpha": exp_config['alpha'],
            "n_particles": exp_config['n_particles'],
            "beta": exp_config['beta'],
            "tau": exp_config['tau'],
            "n_steps": exp_config['n_steps'],
            "n_grad_mc_samples": exp_config['n_grad_mc_samples']
        },
        "graph_stats": {
            "min": graph_min,
            "max": graph_max,
            "mean": graph_mean,
            "std": graph_std
        },
        "edges": {
            "threshold_0.01": edges_001,
            "threshold_0.1": edges_01,
            "threshold_0.2": edges_02,
            "threshold_0.3": edges_03,
            "threshold_0.5": edges_05
        },
        "error_message": error_msg
    }

    # ä¿å­˜å•ä¸ªå®éªŒç»“æœ
    result_file = output_dir / f"{exp_id}_result.json"
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)

    return result

def generate_summary_report(all_results, output_dir):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""

    # ä¿å­˜å®Œæ•´ç»“æœJSON
    full_results_file = output_dir / "all_results.json"
    with open(full_results_file, 'w') as f:
        json.dump(all_results, f, indent=2)

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_file = output_dir / "PARAMETER_SWEEP_REPORT.md"

    with open(report_file, 'w') as f:
        f.write("# DiBSå‚æ•°æ‰«ææµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æµ‹è¯•æ•°é‡**: {len(all_results)}ä¸ªå®éªŒ\n\n")

        # æ€»ä½“ç»Ÿè®¡
        f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
        successful = sum(1 for r in all_results if r['status'] in ['æˆåŠŸ', 'éƒ¨åˆ†æˆåŠŸ'])
        failed = sum(1 for r in all_results if r['status'] == 'å¤±è´¥')
        errors = sum(1 for r in all_results if r['status'] == 'é”™è¯¯')

        f.write(f"- æˆåŠŸ/éƒ¨åˆ†æˆåŠŸ: {successful}/{len(all_results)}\n")
        f.write(f"- å®Œå…¨å¤±è´¥ï¼ˆ0è¾¹ï¼‰: {failed}/{len(all_results)}\n")
        f.write(f"- è¿è¡Œé”™è¯¯: {errors}/{len(all_results)}\n")
        f.write(f"- æ€»è€—æ—¶: {sum(r['elapsed_time_minutes'] for r in all_results):.1f}åˆ†é’Ÿ\n\n")

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        f.write("## å®éªŒç»“æœè¯¦æƒ…\n\n")
        f.write("| ID | åç§° | çŠ¶æ€ | è€—æ—¶(åˆ†) | å›¾Max | è¾¹æ•°(>0.3) | è¾¹æ•°(>0.01) | Alpha | Particles |\n")
        f.write("|-------|------|------|---------|--------|-----------|------------|-------|--------|\n")

        for r in all_results:
            graph_max = r['graph_stats']['max']
            graph_max_str = f"{graph_max:.6f}" if graph_max is not None else "N/A"
            f.write(f"| {r['exp_id']} | {r['exp_name'][:30]} | {r['status']} | "
                   f"{r['elapsed_time_minutes']:.1f} | {graph_max_str} | "
                   f"{r['edges']['threshold_0.3']} | {r['edges']['threshold_0.01']} | "
                   f"{r['parameters']['alpha']} | {r['parameters']['n_particles']} |\n")

        # å…³é”®å‘ç°
        f.write("\n## å…³é”®å‘ç°\n\n")

        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        best_result = max(all_results, key=lambda r: r['edges']['threshold_0.01'] if r['success'] else 0)

        if best_result['edges']['threshold_0.01'] > 0:
            f.write("### âœ… æˆåŠŸå®éªŒ\n\n")
            f.write(f"**æœ€ä½³å®éªŒ**: {best_result['exp_id']} - {best_result['exp_name']}\n\n")
            f.write(f"- æ£€æµ‹åˆ°è¾¹æ•°(>0.01): {best_result['edges']['threshold_0.01']}\n")
            f.write(f"- æ£€æµ‹åˆ°è¾¹æ•°(>0.3): {best_result['edges']['threshold_0.3']}\n")
            f.write(f"- å›¾çŸ©é˜µæœ€å¤§å€¼: {best_result['graph_stats']['max']:.6f}\n")
            f.write(f"- å‚æ•°é…ç½®:\n")
            for k, v in best_result['parameters'].items():
                f.write(f"  - {k}: {v}\n")
        else:
            f.write("### âŒ æ‰€æœ‰å®éªŒå‡å¤±è´¥\n\n")
            f.write("æ‰€æœ‰å®éªŒçš„å›¾çŸ©é˜µå‡ä¸º0æˆ–æ¥è¿‘0ï¼ŒDiBSæ— æ³•åœ¨èƒ½è€—æ•°æ®ä¸Šäº§ç”Ÿå› æœè¾¹ã€‚\n\n")
            f.write("**ç»“è®º**: ç¡®è®¤DiBSä¸é€‚ç”¨äºèƒ½è€—æ•°æ®é›†ã€‚å»ºè®®é‡‡ç”¨å›å½’åˆ†æç­‰æ›¿ä»£æ–¹æ³•ã€‚\n")

        # å‚æ•°å½±å“åˆ†æ
        f.write("\n## å‚æ•°å½±å“åˆ†æ\n\n")
        f.write("### Alphaå‚æ•°å½±å“\n\n")
        alpha_results = {}
        for r in all_results:
            alpha = r['parameters']['alpha']
            if alpha not in alpha_results:
                alpha_results[alpha] = []
            alpha_results[alpha].append(r['edges']['threshold_0.01'])

        f.write("| Alpha | å¹³å‡è¾¹æ•°(>0.01) | æœ€å¤§è¾¹æ•° |\n")
        f.write("|-------|----------------|----------|\n")
        for alpha in sorted(alpha_results.keys()):
            avg_edges = np.mean(alpha_results[alpha])
            max_edges = max(alpha_results[alpha])
            f.write(f"| {alpha} | {avg_edges:.1f} | {max_edges} |\n")

        f.write("\n### N_particleså‚æ•°å½±å“\n\n")
        particles_results = {}
        for r in all_results:
            n_p = r['parameters']['n_particles']
            if n_p not in particles_results:
                particles_results[n_p] = []
            particles_results[n_p].append(r['edges']['threshold_0.01'])

        f.write("| N_particles | å¹³å‡è¾¹æ•°(>0.01) | æœ€å¤§è¾¹æ•° |\n")
        f.write("|-------------|----------------|----------|\n")
        for n_p in sorted(particles_results.keys()):
            avg_edges = np.mean(particles_results[n_p])
            max_edges = max(particles_results[n_p])
            f.write(f"| {n_p} | {avg_edges:.1f} | {max_edges} |\n")

        f.write("\n---\n\n")
        f.write("**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "\n")

    print(f"\nâœ… æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report_file

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("DiBSå‚æ•°æ‰«ææµ‹è¯•")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æµ‹è¯•å®éªŒæ•°: {len(EXPERIMENTS)}")
    print("="*80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent.parent / "results" / "dibs_parameter_sweep" / datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    try:
        data = load_test_data()
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(data)}è¡Œ Ã— {len(data.columns)}åˆ—")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_results = []

    for i, exp_config in enumerate(EXPERIMENTS, 1):
        print(f"\nè¿›åº¦: {i}/{len(EXPERIMENTS)}")

        try:
            result = run_experiment(exp_config, data, output_dir)
            all_results.append(result)

            # å®æ—¶å†³ç­–ï¼šå¦‚æœå‰3ä¸ªå®éªŒéƒ½å¤±è´¥ï¼Œå¯ä»¥æå‰ç»ˆæ­¢
            if i == 3 and all(r['edges']['threshold_0.01'] == 0 for r in all_results):
                print("\nâš ï¸ å‰3ä¸ªå®éªŒå‡äº§ç”Ÿ0è¾¹ï¼Œç»§ç»­æµ‹è¯•å¯èƒ½æ„ä¹‰ä¸å¤§")
                user_input = input("æ˜¯å¦ç»§ç»­æµ‹è¯•å‰©ä½™å®éªŒ? (y/n): ")
                if user_input.lower() != 'y':
                    print("ç”¨æˆ·é€‰æ‹©æå‰ç»ˆæ­¢æµ‹è¯•")
                    break

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
            break
        except Exception as e:
            print(f"\nâŒ å®éªŒè¿è¡Œå¼‚å¸¸: {e}")
            # è®°å½•é”™è¯¯å¹¶ç»§ç»­ä¸‹ä¸€ä¸ª
            result = {
                "exp_id": exp_config["id"],
                "exp_name": exp_config["name"],
                "status": "é”™è¯¯",
                "success": False,
                "elapsed_time_minutes": 0,
                "parameters": exp_config,
                "graph_stats": {"min": None, "max": None, "mean": None, "std": None},
                "edges": {"threshold_0.01": 0, "threshold_0.1": 0, "threshold_0.2": 0,
                         "threshold_0.3": 0, "threshold_0.5": 0},
                "error_message": str(e)
            }
            all_results.append(result)

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*80)
    print("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
    print("="*80)

    if all_results:
        report_file = generate_summary_report(all_results, output_dir)
        print(f"\nâœ… å‚æ•°æ‰«ææµ‹è¯•å®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"æ€»ç»“æŠ¥å‘Š: {report_file}")
    else:
        print(f"\nâŒ æ²¡æœ‰å®Œæˆä»»ä½•å®éªŒ")

    print("\n" + "="*80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

if __name__ == "__main__":
    main()
