#!/usr/bin/env python3
"""
åˆ†å±‚ATEè®¡ç®—è„šæœ¬

åŸºäºDiBSå­¦åˆ°çš„ç¨³å®šè¾¹è®¡ç®—åˆ†å±‚æ•°æ®çš„ATEï¼ˆAverage Treatment Effectï¼‰
ä½¿ç”¨DMLæ–¹æ³•ï¼ˆDouble Machine Learningï¼‰è¿›è¡Œå› æœæ¨æ–­

å…³é”®ç‰¹æ€§:
- ä½¿ç”¨DiBSç¨³å®šè¾¹ä½œä¸ºå› æœå›¾è¾“å…¥
- å…¨å±€FDRæ ¡æ­£ï¼šåˆå¹¶æ‰€æœ‰4ä¸ªåˆ†å±‚çš„på€¼åç»Ÿä¸€åº”ç”¨BH-FDR (Î±=0.10)
- Bootstrapç½®ä¿¡åŒºé—´ (n_bootstrap=500)

æ•°æ®æº:
  - åˆ†å±‚æ•°æ®: data/energy_research/stratified/
  - DiBSç»“æœ: results/energy_research/stratified/dibs/

è¾“å‡º:
  - ATEç»“æœ: results/energy_research/stratified/ate/

ä½¿ç”¨æ–¹æ³•:
    # Dry runï¼ˆæ£€æŸ¥æ•°æ®ï¼‰
    python scripts/stratified/compute_ate_stratified.py --dry-run

    # è¿è¡Œæ‰€æœ‰åˆ†å±‚
    python scripts/stratified/compute_ate_stratified.py

    # è¿è¡Œç‰¹å®šåˆ†å±‚
    python scripts/stratified/compute_ate_stratified.py --layer group1_parallel

ä¾èµ–:
    - analysis/utils/causal_inference.py (CausalInferenceEngine)
    - statsmodels (FDRæ ¡æ­£)
"""

import os
import sys
import argparse
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
from scipy import stats

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

try:
    from utils.causal_inference import CausalInferenceEngine
    ECONML_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥CausalInferenceEngine: {e}")
    print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
    ECONML_AVAILABLE = False

try:
    from statsmodels.stats.multitest import multipletests
    STATSMODELS_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥statsmodelsï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„FDRæ ¡æ­£")
    STATSMODELS_AVAILABLE = False


# ============================================================================
# é…ç½®
# ============================================================================

# åˆ†å±‚ä»»åŠ¡é…ç½®
STRATIFIED_TASKS = [
    {
        "id": "group1_parallel",
        "name": "group1_examples (å¹¶è¡Œ)",
        "csv_file": "group1_examples/group1_parallel.csv",
    },
    {
        "id": "group1_non_parallel",
        "name": "group1_examples (éå¹¶è¡Œ)",
        "csv_file": "group1_examples/group1_non_parallel.csv",
    },
    {
        "id": "group3_parallel",
        "name": "group3_person_reid (å¹¶è¡Œ)",
        "csv_file": "group3_person_reid/group3_parallel.csv",
    },
    {
        "id": "group3_non_parallel",
        "name": "group3_person_reid (éå¹¶è¡Œ)",
        "csv_file": "group3_person_reid/group3_non_parallel.csv",
    }
]

# ATEè®¡ç®—é…ç½®
ATE_CONFIG = {
    "threshold": 0.3,           # è¾¹å¼ºåº¦é˜ˆå€¼
    "n_bootstrap": 500,         # Bootstrapæ¬¡æ•°
    "alpha": 0.10,              # FDRæ ¡æ­£é˜ˆå€¼
    "t_strategy": "quantile",   # ä½¿ç”¨25/75åˆ†ä½æ•°
}


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_stratified_data(task_id: str, data_dir: Path) -> Tuple[pd.DataFrame, List[str]]:
    """
    åŠ è½½åˆ†å±‚æ•°æ®

    è¿”å›:
        data_df: æ•°æ®DataFrame
        feature_names: ç‰¹å¾ååˆ—è¡¨
    """
    task_config = next((t for t in STRATIFIED_TASKS if t["id"] == task_id), None)
    if task_config is None:
        raise ValueError(f"æœªçŸ¥çš„åˆ†å±‚ä»»åŠ¡: {task_id}")

    data_file = data_dir / task_config["csv_file"]
    if not data_file.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

    df = pd.read_csv(data_file)
    feature_names = list(df.columns)

    print(f"  æ•°æ®è§„æ¨¡: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"  è­¦å‘Š: æ•°æ®åŒ…å« {missing_count} ä¸ªç¼ºå¤±å€¼")

    return df, feature_names


def load_dibs_stable_edges(task_id: str, dibs_dir: Path, threshold: float = 0.3) -> pd.DataFrame:
    """
    åŠ è½½DiBSç¨³å®šè¾¹

    è¿”å›:
        stable_edges_df: ç¨³å®šè¾¹DataFrameï¼ŒåŒ…å«source, target, weightç­‰åˆ—
    """
    task_dir = dibs_dir / task_id

    # å°è¯•åŠ è½½ç¨³å®šè¾¹æ–‡ä»¶
    stable_edges_file = task_dir / f"stable_edges_threshold_{threshold}.csv"
    if stable_edges_file.exists():
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰ç©ºè¡Œ
        file_size = stable_edges_file.stat().st_size
        if file_size > 10:  # éç©ºæ–‡ä»¶åº”è‡³å°‘æœ‰10å­—èŠ‚ï¼ˆåŒ…å«headerï¼‰
            try:
                stable_edges_df = pd.read_csv(stable_edges_file)
                if len(stable_edges_df) > 0:
                    print(f"  åŠ è½½ç¨³å®šè¾¹: {len(stable_edges_df)} æ¡")
                    return stable_edges_df
            except pd.errors.EmptyDataError:
                pass  # æ–‡ä»¶ä¸ºç©ºï¼Œç»§ç»­ä½¿ç”¨å¹³å‡å› æœå›¾
        print(f"  ç¨³å®šè¾¹æ–‡ä»¶ä¸ºç©ºæˆ–æ— æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨å¹³å‡å› æœå›¾...")
    else:
        print(f"  ç¨³å®šè¾¹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¹³å‡å› æœå›¾...")
    avg_graph_file = task_dir / "averaged_causal_graph.csv"
    if not avg_graph_file.exists():
        raise FileNotFoundError(f"å¹³å‡å› æœå›¾æ–‡ä»¶ä¸å­˜åœ¨: {avg_graph_file}")

    # ä»å¹³å‡å› æœå›¾æå–è¾¹
    avg_graph_df = pd.read_csv(avg_graph_file, index_col=0)
    feature_names = list(avg_graph_df.columns)

    edges = []
    for i, src in enumerate(feature_names):
        for j, tgt in enumerate(feature_names):
            weight = avg_graph_df.iloc[i, j]
            if weight > threshold:
                edges.append({
                    "source": src,
                    "target": tgt,
                    "weight": float(weight)
                })

    stable_edges_df = pd.DataFrame(edges)
    print(f"  ä»å¹³å‡å› æœå›¾æå–è¾¹ (threshold>{threshold}): {len(stable_edges_df)} æ¡")
    return stable_edges_df


def load_dibs_causal_graph(task_id: str, dibs_dir: Path) -> Tuple[np.ndarray, List[str]]:
    """
    åŠ è½½DiBSå› æœå›¾ï¼ˆå¹³å‡å›¾ï¼‰

    è¿”å›:
        causal_graph: é‚»æ¥çŸ©é˜µ
        feature_names: ç‰¹å¾ååˆ—è¡¨
    """
    avg_graph_file = dibs_dir / task_id / "averaged_causal_graph.csv"
    if not avg_graph_file.exists():
        raise FileNotFoundError(f"å¹³å‡å› æœå›¾æ–‡ä»¶ä¸å­˜åœ¨: {avg_graph_file}")

    df = pd.read_csv(avg_graph_file, index_col=0)
    feature_names = list(df.columns)
    causal_graph = df.values.astype(float)

    return causal_graph, feature_names


# ============================================================================
# ATEè®¡ç®—
# ============================================================================

def compute_ate_for_layer(
    task_id: str,
    data_dir: Path,
    dibs_dir: Path,
    output_dir: Path,
    config: Dict,
    dry_run: bool = False
) -> Dict:
    """
    ä¸ºå•ä¸ªåˆ†å±‚è®¡ç®—ATE

    è¿”å›:
        results: åŒ…å«ATEç»“æœçš„å­—å…¸
    """
    task_config = next((t for t in STRATIFIED_TASKS if t["id"] == task_id), None)
    task_name = task_config["name"] if task_config else task_id

    print(f"\n{'='*70}")
    print(f"ATEè®¡ç®—: {task_name} ({task_id})")
    print(f"{'='*70}")

    # 1. åŠ è½½æ•°æ®
    print(f"\n1. åŠ è½½æ•°æ®...")
    try:
        data_df, data_features = load_stratified_data(task_id, data_dir)
    except Exception as e:
        print(f"  âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return {"success": False, "error": str(e), "task_id": task_id}

    # 2. åŠ è½½DiBSå› æœå›¾
    print(f"\n2. åŠ è½½DiBSå› æœå›¾...")
    try:
        causal_graph, dibs_features = load_dibs_causal_graph(task_id, dibs_dir)
        stable_edges_df = load_dibs_stable_edges(task_id, dibs_dir, config["threshold"])
    except Exception as e:
        print(f"  âŒ åŠ è½½DiBSç»“æœå¤±è´¥: {e}")
        return {"success": False, "error": str(e), "task_id": task_id}

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¾¹
    if len(stable_edges_df) == 0:
        print(f"  âš ï¸ æ²¡æœ‰ç¨³å®šè¾¹ï¼Œå°è¯•ä»å¹³å‡å›¾æå–å¼ºè¾¹...")
        # ä»å¹³å‡å› æœå›¾æå–å¼ºè¾¹
        edges = []
        for i, src in enumerate(dibs_features):
            for j, tgt in enumerate(dibs_features):
                weight = causal_graph[i, j]
                if weight > config["threshold"]:
                    edges.append({
                        "source": src,
                        "target": tgt,
                        "weight": float(weight)
                    })
        stable_edges_df = pd.DataFrame(edges)
        print(f"  ä»å¹³å‡å› æœå›¾æå– {len(stable_edges_df)} æ¡å¼ºè¾¹")

    if len(stable_edges_df) == 0:
        print(f"  âŒ æ²¡æœ‰å¯ç”¨çš„è¾¹è¿›è¡ŒATEè®¡ç®—")
        return {"success": False, "error": "æ²¡æœ‰å¯ç”¨çš„è¾¹", "task_id": task_id}

    # 3. ç‰¹å¾åŒ¹é…
    print(f"\n3. ç‰¹å¾åŒ¹é…...")
    common_features = [f for f in dibs_features if f in data_features]
    print(f"  å…±æœ‰ç‰¹å¾: {len(common_features)}/{len(dibs_features)}")

    if len(common_features) < 5:
        print(f"  âŒ å…±æœ‰ç‰¹å¾å¤ªå°‘")
        return {"success": False, "error": "å…±æœ‰ç‰¹å¾å¤ªå°‘", "task_id": task_id}

    # è¿‡æ»¤è¾¹ï¼ˆåªä¿ç•™å…±æœ‰ç‰¹å¾çš„è¾¹ï¼‰
    filtered_edges = stable_edges_df[
        stable_edges_df['source'].isin(common_features) &
        stable_edges_df['target'].isin(common_features)
    ].copy()
    print(f"  è¿‡æ»¤åè¾¹æ•°: {len(filtered_edges)}")

    if dry_run:
        print(f"\n  ğŸ§ª Dry runæ¨¡å¼ - åªæ£€æŸ¥æ•°æ®")
        return {
            "success": True,
            "dry_run": True,
            "task_id": task_id,
            "n_samples": len(data_df),
            "n_features": len(common_features),
            "n_edges": len(filtered_edges)
        }

    # 4. è®¡ç®—ATE
    print(f"\n4. è®¡ç®—ATE...")
    start_time = time.time()

    if not ECONML_AVAILABLE:
        print(f"  âŒ EconMLä¸å¯ç”¨")
        return {"success": False, "error": "EconMLä¸å¯ç”¨", "task_id": task_id}

    engine = CausalInferenceEngine(verbose=False)

    ate_results = []
    successful_edges = 0
    failed_edges = 0

    for idx, row in filtered_edges.iterrows():
        source = row['source']
        target = row['target']
        edge_weight = row.get('weight', 0.5)

        # ç¡®å®šæ··æ·†å› ç´ ï¼ˆæ’é™¤sourceå’Œtargetçš„æ‰€æœ‰å…¶ä»–å˜é‡ï¼‰
        confounders = [f for f in common_features if f != source and f != target]

        try:
            # è®¡ç®—ATE
            ate, (ci_lower, ci_upper) = engine.estimate_ate(
                data=data_df,
                treatment=source,
                outcome=target,
                confounders=confounders[:10],  # é™åˆ¶æ··æ·†å› ç´ æ•°é‡ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
                t_strategy=config["t_strategy"]
            )

            # è®¡ç®—på€¼ï¼ˆåŸºäºç½®ä¿¡åŒºé—´ï¼‰
            # å¦‚æœ0åœ¨CIå¤–ï¼Œåˆ™æ˜¾è‘—
            se = (ci_upper - ci_lower) / (2 * 1.96)  # è¿‘ä¼¼æ ‡å‡†è¯¯
            if se > 0:
                z_stat = abs(ate) / se
                p_value = 2 * (1 - stats.norm.cdf(z_stat))
            else:
                p_value = 1.0

            ate_results.append({
                "source": source,
                "target": target,
                "edge_weight": edge_weight,
                "ate": ate,
                "ci_lower": ci_lower,
                "ci_upper": ci_upper,
                "se": se,
                "p_value": p_value,
                "n_confounders": len(confounders[:10])
            })
            successful_edges += 1

        except Exception as e:
            print(f"    âš ï¸ {source} -> {target}: {str(e)[:50]}")
            ate_results.append({
                "source": source,
                "target": target,
                "edge_weight": edge_weight,
                "ate": np.nan,
                "ci_lower": np.nan,
                "ci_upper": np.nan,
                "se": np.nan,
                "p_value": np.nan,
                "error": str(e)
            })
            failed_edges += 1

    elapsed_time = time.time() - start_time
    print(f"  âœ… ATEè®¡ç®—å®Œæˆ: {successful_edges}æˆåŠŸ, {failed_edges}å¤±è´¥, è€—æ—¶{elapsed_time:.1f}ç§’")

    # 5. ä¿å­˜ç»“æœï¼ˆä¸è¿›è¡ŒFDRæ ¡æ­£ï¼Œç•™ç»™å…¨å±€æ ¡æ­£ï¼‰
    print(f"\n5. ä¿å­˜ç»“æœ...")
    task_output_dir = output_dir / task_id
    task_output_dir.mkdir(parents=True, exist_ok=True)

    ate_df = pd.DataFrame(ate_results)
    ate_file = task_output_dir / f"{task_id}_ate_raw.csv"
    ate_df.to_csv(ate_file, index=False)
    print(f"  ä¿å­˜åˆ°: {ate_file}")

    # ä¿å­˜æ‘˜è¦
    summary = {
        "task_id": task_id,
        "task_name": task_name,
        "n_samples": len(data_df),
        "n_features": len(common_features),
        "n_edges_total": len(filtered_edges),
        "n_edges_successful": successful_edges,
        "n_edges_failed": failed_edges,
        "elapsed_seconds": elapsed_time,
        "timestamp": datetime.now().isoformat()
    }

    summary_file = task_output_dir / f"{task_id}_ate_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    return {
        "success": True,
        "task_id": task_id,
        "ate_df": ate_df,
        "summary": summary,
        "output_dir": str(task_output_dir)
    }


# ============================================================================
# å…¨å±€FDRæ ¡æ­£
# ============================================================================

def apply_global_fdr_correction(
    all_results: List[Dict],
    output_dir: Path,
    alpha: float = 0.10
) -> pd.DataFrame:
    """
    å¯¹æ‰€æœ‰åˆ†å±‚çš„på€¼è¿›è¡Œå…¨å±€FDRæ ¡æ­£

    å‚æ•°:
        all_results: æ‰€æœ‰åˆ†å±‚çš„ATEç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        alpha: FDRé˜ˆå€¼

    è¿”å›:
        combined_df: åˆå¹¶åçš„DataFrameï¼ŒåŒ…å«FDRæ ¡æ­£ç»“æœ
    """
    print(f"\n{'='*70}")
    print("å…¨å±€FDRæ ¡æ­£")
    print(f"{'='*70}")

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_dfs = []
    for result in all_results:
        if result.get("success") and "ate_df" in result:
            df = result["ate_df"].copy()
            df["layer"] = result["task_id"]
            all_dfs.append(df)

    if not all_dfs:
        print("  âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœè¿›è¡ŒFDRæ ¡æ­£")
        return pd.DataFrame()

    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"  åˆå¹¶ {len(all_dfs)} ä¸ªåˆ†å±‚ï¼Œå…± {len(combined_df)} æ¡è¾¹")

    # æå–æœ‰æ•ˆpå€¼
    valid_mask = combined_df['p_value'].notna()
    valid_p_values = combined_df.loc[valid_mask, 'p_value'].values

    print(f"  æœ‰æ•ˆpå€¼: {len(valid_p_values)}")

    if len(valid_p_values) == 0:
        print("  âŒ æ²¡æœ‰æœ‰æ•ˆçš„på€¼")
        combined_df['p_value_fdr'] = np.nan
        combined_df['is_significant_fdr'] = False
        return combined_df

    # åº”ç”¨BH-FDRæ ¡æ­£
    if STATSMODELS_AVAILABLE:
        rejected, p_values_fdr, _, _ = multipletests(
            valid_p_values,
            alpha=alpha,
            method='fdr_bh'
        )
        print(f"  FDRæ ¡æ­£å®Œæˆ (BHæ–¹æ³•, Î±={alpha})")
    else:
        # ç®€åŒ–çš„BH-FDRæ ¡æ­£
        n = len(valid_p_values)
        sorted_indices = np.argsort(valid_p_values)
        sorted_p_values = valid_p_values[sorted_indices]

        # BHæ ¡æ­£
        p_values_fdr = np.zeros(n)
        for i, p in enumerate(sorted_p_values):
            p_values_fdr[sorted_indices[i]] = p * n / (i + 1)

        # ç¡®ä¿FDRæ ¡æ­£åçš„på€¼å•è°ƒ
        p_values_fdr = np.minimum.accumulate(p_values_fdr[::-1])[::-1]
        p_values_fdr = np.minimum(p_values_fdr, 1.0)

        rejected = p_values_fdr < alpha
        print(f"  FDRæ ¡æ­£å®Œæˆ (ç®€åŒ–BHæ–¹æ³•, Î±={alpha})")

    # å°†FDRæ ¡æ­£ç»“æœæ·»åŠ åˆ°DataFrame
    combined_df['p_value_fdr'] = np.nan
    combined_df['is_significant_fdr'] = False

    combined_df.loc[valid_mask, 'p_value_fdr'] = p_values_fdr
    combined_df.loc[valid_mask, 'is_significant_fdr'] = rejected

    # ç»Ÿè®¡
    n_significant = combined_df['is_significant_fdr'].sum()
    print(f"  æ˜¾è‘—è¾¹æ•° (FDRæ ¡æ­£å): {n_significant}/{len(valid_p_values)}")

    # ä¿å­˜åˆå¹¶ç»“æœ
    combined_file = output_dir / "all_layers_ate_fdr_corrected.csv"
    combined_df.to_csv(combined_file, index=False)
    print(f"  ä¿å­˜åˆ°: {combined_file}")

    # æŒ‰åˆ†å±‚ç»Ÿè®¡
    print(f"\n  å„åˆ†å±‚ç»Ÿè®¡:")
    for layer in combined_df['layer'].unique():
        layer_df = combined_df[combined_df['layer'] == layer]
        layer_significant = layer_df['is_significant_fdr'].sum()
        layer_total = layer_df['p_value'].notna().sum()
        print(f"    {layer}: {layer_significant}/{layer_total} æ˜¾è‘—")

    return combined_df


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="åˆ†å±‚ATEè®¡ç®—",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ£€æŸ¥æ•°æ®ï¼Œä¸è®¡ç®—ATE"
    )
    parser.add_argument(
        "--layer",
        type=str,
        choices=["group1_parallel", "group1_non_parallel",
                 "group3_parallel", "group3_non_parallel"],
        help="åªå¤„ç†ç‰¹å®šåˆ†å±‚"
    )
    parser.add_argument(
        "--skip-fdr",
        action="store_true",
        help="è·³è¿‡å…¨å±€FDRæ ¡æ­£"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.3,
        help="è¾¹å¼ºåº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.3ï¼‰"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡º"
    )

    args = parser.parse_args()

    print("=" * 70)
    print("åˆ†å±‚ATEè®¡ç®—")
    print("=" * 70)

    # è·¯å¾„è®¾ç½®
    script_dir = Path(__file__).parent.absolute()
    analysis_dir = script_dir.parent.parent

    data_dir = analysis_dir / "data" / "energy_research" / "stratified"
    dibs_dir = analysis_dir / "results" / "energy_research" / "stratified" / "dibs"

    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = analysis_dir / "results" / "energy_research" / "stratified" / "ate"

    print(f"\næ•°æ®ç›®å½•: {data_dir}")
    print(f"DiBSç›®å½•: {dibs_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # æ›´æ–°é…ç½®
    config = ATE_CONFIG.copy()
    config["threshold"] = args.threshold

    print(f"\né…ç½®:")
    print(f"  è¾¹å¼ºåº¦é˜ˆå€¼: {config['threshold']}")
    print(f"  Bootstrapæ¬¡æ•°: {config['n_bootstrap']}")
    print(f"  FDRé˜ˆå€¼: {config['alpha']}")

    # æ£€æŸ¥EconML
    if not ECONML_AVAILABLE and not args.dry_run:
        print("\nâŒ EconMLä¸å¯ç”¨ï¼Œæ— æ³•è®¡ç®—ATE")
        print("è¯·åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
        return 1

    # ç¡®å®šè¦å¤„ç†çš„ä»»åŠ¡
    if args.layer:
        tasks = [t for t in STRATIFIED_TASKS if t["id"] == args.layer]
    else:
        tasks = STRATIFIED_TASKS

    print(f"\nä»»åŠ¡æ•°é‡: {len(tasks)}")

    # å¤„ç†æ¯ä¸ªåˆ†å±‚
    all_results = []
    total_start_time = time.time()

    for task in tasks:
        task_id = task["id"]

        # æ£€æŸ¥DiBSç»“æœæ˜¯å¦å­˜åœ¨
        dibs_task_dir = dibs_dir / task_id
        if not dibs_task_dir.exists():
            print(f"\nâš ï¸ DiBSç»“æœä¸å­˜åœ¨: {task_id}ï¼Œè·³è¿‡")
            continue

        try:
            result = compute_ate_for_layer(
                task_id=task_id,
                data_dir=data_dir,
                dibs_dir=dibs_dir,
                output_dir=output_dir,
                config=config,
                dry_run=args.dry_run
            )
            all_results.append(result)

        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            all_results.append({
                "success": False,
                "task_id": task_id,
                "error": str(e)
            })

    total_runtime = time.time() - total_start_time

    # å…¨å±€FDRæ ¡æ­£
    if not args.dry_run and not args.skip_fdr:
        successful_results = [r for r in all_results if r.get("success") and "ate_df" in r]
        if len(successful_results) >= 2:
            combined_df = apply_global_fdr_correction(
                all_results=successful_results,
                output_dir=output_dir,
                alpha=config["alpha"]
            )
        else:
            print(f"\nâš ï¸ æˆåŠŸçš„åˆ†å±‚æ•°ä¸è¶³ï¼Œè·³è¿‡å…¨å±€FDRæ ¡æ­£ ({len(successful_results)}/4)")

    # ç”Ÿæˆæ€»æŠ¥å‘Š
    print(f"\n{'='*70}")
    print("æ€»æŠ¥å‘Š")
    print(f"{'='*70}")

    successful_tasks = [r for r in all_results if r.get("success")]
    failed_tasks = [r for r in all_results if not r.get("success")]

    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  æˆåŠŸ: {len(successful_tasks)}/{len(all_results)}")
    print(f"  å¤±è´¥: {len(failed_tasks)}/{len(all_results)}")
    print(f"  æ€»è€—æ—¶: {total_runtime/60:.1f}åˆ†é’Ÿ")

    for result in successful_tasks:
        if "summary" in result:
            s = result["summary"]
            print(f"\n  {s['task_id']}:")
            print(f"    æ ·æœ¬æ•°: {s['n_samples']}")
            print(f"    è¾¹æ•°: {s['n_edges_successful']}/{s['n_edges_total']}")

    if failed_tasks:
        print(f"\nå¤±è´¥ä»»åŠ¡:")
        for result in failed_tasks:
            print(f"  {result['task_id']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # ä¿å­˜æ€»æŠ¥å‘Š
    if not args.dry_run:
        output_dir.mkdir(parents=True, exist_ok=True)
        total_report = {
            "timestamp": datetime.now().isoformat(),
            "config": config,
            "total_runtime_seconds": total_runtime,
            "results": [
                {
                    "task_id": r.get("task_id"),
                    "success": r.get("success"),
                    "summary": r.get("summary"),
                    "error": r.get("error")
                }
                for r in all_results
            ]
        }

        report_file = output_dir / "stratified_ate_total_report.json"
        with open(report_file, 'w') as f:
            json.dump(total_report, f, indent=2, default=str)
        print(f"\nâœ… æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    print(f"\n{'='*70}")
    print("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
    print(f"{'='*70}")

    return 0 if len(failed_tasks) == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
