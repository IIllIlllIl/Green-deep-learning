#!/usr/bin/env python3
"""
åˆ†å±‚å› æœåˆ†æ - æ•°æ®åˆ†å±‚è„šæœ¬

æŒ‰is_parallelå˜é‡å°†å…¨å±€æ ‡å‡†åŒ–æ•°æ®åˆ†å‰²ä¸ºå¹¶è¡Œ/éå¹¶è¡Œå­é›†ã€‚

è¾“å…¥ (ä½¿ç”¨DiBSå°±ç»ªæ•°æ®ï¼Œæ— ç¼ºå¤±å€¼):
  - data/energy_research/6groups_dibs_ready/group1_examples_dibs_ready.csv
  - data/energy_research/6groups_dibs_ready/group3_person_reid_dibs_ready.csv

è¾“å‡º:
  - data/energy_research/stratified/group1_examples/group1_parallel.csv
  - data/energy_research/stratified/group1_examples/group1_non_parallel.csv
  - data/energy_research/stratified/group3_person_reid/group3_parallel.csv
  - data/energy_research/stratified/group3_person_reid/group3_non_parallel.csv
  - data/energy_research/stratified/stratification_report.json

ä½¿ç”¨æ–¹æ³•:
    # Dry run (æµ‹è¯•æ¨¡å¼)
    python prepare_stratified_data.py --dry-run

    # å®é™…è¿è¡Œ
    python prepare_stratified_data.py

    # å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
    python prepare_stratified_data.py --force

ä¾èµ–:
    - pandas
    - numpy
"""

import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import pandas as pd
import numpy as np


# ============================================================================
# é…ç½®
# ============================================================================

# åˆ†å±‚é…ç½® (ä½¿ç”¨DiBSå°±ç»ªæ•°æ®)
STRATIFIED_CONFIG = {
    "group1_examples": {
        "input_file": "group1_examples_dibs_ready.csv",
        "expected_parallel": 178,
        "expected_non_parallel": 126,
        "tolerance": 0.05  # 5%å®¹å·®
    },
    "group3_person_reid": {
        "input_file": "group3_person_reid_dibs_ready.csv",
        "expected_parallel": 113,
        "expected_non_parallel": 93,
        "tolerance": 0.05
    }
}

# è¦ç§»é™¤çš„åˆ—æ¨¡å¼
COLUMNS_TO_REMOVE = [
    "is_parallel",      # åˆ†å±‚åä¸ºå¸¸é‡
    "timestamp",        # éå› æœå˜é‡
]

# äº¤äº’é¡¹åˆ—åç¼€ï¼ˆåˆ†å±‚åå†—ä½™ï¼‰
INTERACTION_SUFFIX = "_x_is_parallel"


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def get_columns_to_remove(df: pd.DataFrame) -> List[str]:
    """è·å–éœ€è¦ç§»é™¤çš„åˆ—åˆ—è¡¨"""
    cols_to_remove = []

    for col in df.columns:
        # ç²¾ç¡®åŒ¹é…
        if col in COLUMNS_TO_REMOVE:
            cols_to_remove.append(col)
        # äº¤äº’é¡¹åˆ—
        elif col.endswith(INTERACTION_SUFFIX):
            cols_to_remove.append(col)

    return cols_to_remove


def check_data_quality(df: pd.DataFrame, layer_name: str) -> Dict:
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    quality_report = {
        "layer": layer_name,
        "n_samples": len(df),
        "n_features": len(df.columns),
        "missing_values": {},
        "constant_columns": [],
        "issues": []
    }

    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing = df.isnull().sum()
    missing_cols = missing[missing > 0]
    if len(missing_cols) > 0:
        quality_report["missing_values"] = missing_cols.to_dict()
        quality_report["issues"].append(f"å‘ç° {len(missing_cols)} åˆ—åŒ…å«ç¼ºå¤±å€¼")

    # æ£€æŸ¥å¸¸é‡åˆ—
    for col in df.columns:
        if df[col].nunique() <= 1:
            quality_report["constant_columns"].append(col)

    if quality_report["constant_columns"]:
        quality_report["issues"].append(
            f"å‘ç° {len(quality_report['constant_columns'])} ä¸ªå¸¸é‡åˆ—"
        )

    return quality_report


def validate_sample_count(actual: int, expected: int, tolerance: float) -> Tuple[bool, str]:
    """éªŒè¯æ ·æœ¬æ•°æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…"""
    lower = int(expected * (1 - tolerance))
    upper = int(expected * (1 + tolerance))

    if lower <= actual <= upper:
        return True, f"âœ… æ ·æœ¬æ•° {actual} åœ¨é¢„æœŸèŒƒå›´ [{lower}, {upper}] å†…"
    else:
        return False, f"âŒ æ ·æœ¬æ•° {actual} ä¸åœ¨é¢„æœŸèŒƒå›´ [{lower}, {upper}] å†…"


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def stratify_data(
    input_dir: Path,
    output_dir: Path,
    dry_run: bool = False,
    force: bool = False
) -> Dict:
    """
    æ‰§è¡Œæ•°æ®åˆ†å±‚

    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½• (6groups_global_std)
        output_dir: è¾“å‡ºç›®å½• (stratified)
        dry_run: æ˜¯å¦åªæ£€æŸ¥ä¸æ‰§è¡Œ
        force: æ˜¯å¦å¼ºåˆ¶è¦†ç›–

    è¿”å›:
        åˆ†å±‚æŠ¥å‘Šå­—å…¸
    """
    report = {
        "timestamp": datetime.now().isoformat(),
        "dry_run": dry_run,
        "input_dir": str(input_dir),
        "output_dir": str(output_dir),
        "groups": {},
        "summary": {
            "total_input_samples": 0,
            "total_output_samples": 0,
            "validation_passed": True
        }
    }

    print("=" * 70)
    print("åˆ†å±‚å› æœåˆ†æ - æ•°æ®åˆ†å±‚")
    print("=" * 70)
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ¨¡å¼: {'Dry Run (ä»…æ£€æŸ¥)' if dry_run else 'å®é™…æ‰§è¡Œ'}")
    print()

    for group_name, config in STRATIFIED_CONFIG.items():
        print(f"\n{'='*60}")
        print(f"å¤„ç†ç»„: {group_name}")
        print(f"{'='*60}")

        group_report = {
            "input_file": config["input_file"],
            "layers": {},
            "columns_removed": [],
            "validation": {}
        }

        # 1. è¯»å–æ•°æ®
        input_file = input_dir / config["input_file"]
        if not input_file.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            group_report["error"] = f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            report["groups"][group_name] = group_report
            report["summary"]["validation_passed"] = False
            continue

        print(f"è¯»å–æ–‡ä»¶: {input_file}")
        df = pd.read_csv(input_file)
        print(f"   åŸå§‹æ•°æ®: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
        report["summary"]["total_input_samples"] += len(df)

        # 2. æ£€æŸ¥is_parallelåˆ—
        if "is_parallel" not in df.columns:
            print(f"âŒ ç¼ºå°‘is_parallelåˆ—")
            group_report["error"] = "ç¼ºå°‘is_parallelåˆ—"
            report["groups"][group_name] = group_report
            report["summary"]["validation_passed"] = False
            continue

        # 3. åˆ†å±‚
        # å¤„ç†is_parallelåˆ—çš„ä¸åŒè¡¨ç¤ºæ–¹å¼
        if df["is_parallel"].dtype == bool:
            parallel_mask = df["is_parallel"]
        else:
            # å¯èƒ½æ˜¯å­—ç¬¦ä¸² "True"/"False"
            parallel_mask = df["is_parallel"].astype(str).str.lower() == "true"

        df_parallel = df[parallel_mask].copy()
        df_non_parallel = df[~parallel_mask].copy()

        print(f"   å¹¶è¡Œæ ·æœ¬: {len(df_parallel)}")
        print(f"   éå¹¶è¡Œæ ·æœ¬: {len(df_non_parallel)}")

        # 4. éªŒè¯æ ·æœ¬æ•°
        valid_p, msg_p = validate_sample_count(
            len(df_parallel), config["expected_parallel"], config["tolerance"]
        )
        valid_np, msg_np = validate_sample_count(
            len(df_non_parallel), config["expected_non_parallel"], config["tolerance"]
        )

        print(f"   {msg_p}")
        print(f"   {msg_np}")

        group_report["validation"]["parallel"] = {
            "actual": len(df_parallel),
            "expected": config["expected_parallel"],
            "passed": valid_p
        }
        group_report["validation"]["non_parallel"] = {
            "actual": len(df_non_parallel),
            "expected": config["expected_non_parallel"],
            "passed": valid_np
        }

        if not (valid_p and valid_np):
            report["summary"]["validation_passed"] = False

        # 5. ç§»é™¤åˆ—
        cols_to_remove = get_columns_to_remove(df_parallel)
        print(f"\n   ç§»é™¤åˆ— ({len(cols_to_remove)} ä¸ª):")
        for col in cols_to_remove[:5]:
            print(f"      - {col}")
        if len(cols_to_remove) > 5:
            print(f"      ... ç­‰ {len(cols_to_remove) - 5} ä¸ª")

        group_report["columns_removed"] = cols_to_remove

        # å®é™…ç§»é™¤åˆ—
        df_parallel_clean = df_parallel.drop(columns=cols_to_remove, errors='ignore')
        df_non_parallel_clean = df_non_parallel.drop(columns=cols_to_remove, errors='ignore')

        print(f"\n   æ¸…ç†åæ•°æ®:")
        print(f"      å¹¶è¡Œ: {len(df_parallel_clean)} è¡Œ Ã— {len(df_parallel_clean.columns)} åˆ—")
        print(f"      éå¹¶è¡Œ: {len(df_non_parallel_clean)} è¡Œ Ã— {len(df_non_parallel_clean.columns)} åˆ—")

        # 6. æ•°æ®è´¨é‡æ£€æŸ¥
        quality_p = check_data_quality(df_parallel_clean, f"{group_name}_parallel")
        quality_np = check_data_quality(df_non_parallel_clean, f"{group_name}_non_parallel")

        group_report["layers"]["parallel"] = {
            "n_samples": len(df_parallel_clean),
            "n_features": len(df_parallel_clean.columns),
            "quality": quality_p
        }
        group_report["layers"]["non_parallel"] = {
            "n_samples": len(df_non_parallel_clean),
            "n_features": len(df_non_parallel_clean.columns),
            "quality": quality_np
        }

        report["summary"]["total_output_samples"] += len(df_parallel_clean) + len(df_non_parallel_clean)

        if quality_p["issues"]:
            print(f"\n   âš ï¸ å¹¶è¡Œå±‚è´¨é‡é—®é¢˜:")
            for issue in quality_p["issues"]:
                print(f"      - {issue}")

        if quality_np["issues"]:
            print(f"\n   âš ï¸ éå¹¶è¡Œå±‚è´¨é‡é—®é¢˜:")
            for issue in quality_np["issues"]:
                print(f"      - {issue}")

        # 7. ä¿å­˜æ•°æ®
        if not dry_run:
            group_output_dir = output_dir / group_name
            group_output_dir.mkdir(parents=True, exist_ok=True)

            # çŸ­åç§° (å»æ‰group1_/group3_å‰ç¼€)
            short_name = group_name.split("_", 1)[0]  # group1 or group3

            parallel_file = group_output_dir / f"{short_name}_parallel.csv"
            non_parallel_file = group_output_dir / f"{short_name}_non_parallel.csv"

            if parallel_file.exists() and not force:
                print(f"\n   âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {parallel_file}")
                print(f"      ä½¿ç”¨ --force å¼ºåˆ¶è¦†ç›–")
            else:
                df_parallel_clean.to_csv(parallel_file, index=False)
                print(f"\n   âœ… å·²ä¿å­˜: {parallel_file}")

            if non_parallel_file.exists() and not force:
                print(f"   âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {non_parallel_file}")
            else:
                df_non_parallel_clean.to_csv(non_parallel_file, index=False)
                print(f"   âœ… å·²ä¿å­˜: {non_parallel_file}")

            group_report["output_files"] = {
                "parallel": str(parallel_file),
                "non_parallel": str(non_parallel_file)
            }

        report["groups"][group_name] = group_report

    # 8. ä¿å­˜æŠ¥å‘Š
    if not dry_run:
        report_file = output_dir / "stratification_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # 9. æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("æ€»ç»“")
    print("=" * 70)
    print(f"è¾“å…¥æ ·æœ¬æ€»æ•°: {report['summary']['total_input_samples']}")
    print(f"è¾“å‡ºæ ·æœ¬æ€»æ•°: {report['summary']['total_output_samples']}")
    print(f"éªŒè¯çŠ¶æ€: {'âœ… å…¨éƒ¨é€šè¿‡' if report['summary']['validation_passed'] else 'âŒ å­˜åœ¨é—®é¢˜'}")

    return report


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†å±‚å› æœåˆ†æ - æ•°æ®åˆ†å±‚è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Dry runæ¨¡å¼ï¼Œåªæ£€æŸ¥æ•°æ®ä¸å®é™…ä¿å­˜"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶"
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default=None,
        help="è¾“å…¥ç›®å½•è·¯å¾„ (é»˜è®¤: data/energy_research/6groups_dibs_ready)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: data/energy_research/stratified)"
    )

    args = parser.parse_args()

    # ç¡®å®šè·¯å¾„
    script_dir = Path(__file__).parent.absolute()
    analysis_dir = script_dir.parent.parent  # analysis/

    if args.input_dir:
        input_dir = Path(args.input_dir)
    else:
        input_dir = analysis_dir / "data" / "energy_research" / "6groups_dibs_ready"

    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = analysis_dir / "data" / "energy_research" / "stratified"

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not input_dir.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        sys.exit(1)

    # æ‰§è¡Œåˆ†å±‚
    report = stratify_data(
        input_dir=input_dir,
        output_dir=output_dir,
        dry_run=args.dry_run,
        force=args.force
    )

    # è¿”å›çŠ¶æ€
    if report["summary"]["validation_passed"]:
        print("\nğŸ‰ æ•°æ®åˆ†å±‚å®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸ æ•°æ®åˆ†å±‚å®Œæˆï¼Œä½†å­˜åœ¨éªŒè¯é—®é¢˜ï¼Œè¯·æ£€æŸ¥æŠ¥å‘Š")
        sys.exit(1)


if __name__ == "__main__":
    main()
