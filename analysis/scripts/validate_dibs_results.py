#!/usr/bin/env python3
"""
éªŒè¯DiBSå› æœå­¦ä¹ ç»“æœçš„è„šæœ¬

éªŒæ”¶æ ‡å‡†:
1. å› æœå›¾å¤§å°ä¸æ•°æ®ç‰¹å¾æ•°å®Œå…¨åŒ¹é…
2. æ¯ç»„æˆåŠŸç”Ÿæˆå› æœå›¾æ–‡ä»¶ï¼ˆæ­£ç¡®ç»´åº¦ï¼‰
3. æ£€æŸ¥å› æœå›¾çš„è´¨é‡æŒ‡æ ‡ï¼ˆè¾¹å¼ºåº¦åˆ†å¸ƒã€æ”¶æ•›æ€§ï¼‰
4. éªŒè¯æ ·æœ¬æ•°æ­£ç¡®ï¼ˆç‰¹åˆ«æ˜¯group5ä¸º60æ ·æœ¬ï¼‰

ç”¨æ³•:
python validate_dibs_results.py --group group5_mrt_oast
python validate_dibs_results.py --all
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import sys

def validate_group_results(group_id, results_dir):
    """éªŒè¯å•ä¸ªç»„çš„DiBSç»“æœ"""
    print(f"\néªŒè¯ç»„: {group_id}")
    print("-" * 40)

    group_dir = results_dir / group_id
    if not group_dir.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {group_dir}")
        return False

    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    required_files = [
        f"{group_id}_dibs_causal_graph.csv",
        f"{group_id}_dibs_summary.json",
        f"{group_id}_feature_names.json",
        f"{group_id}_dibs_config.json"
    ]

    missing_files = []
    for file in required_files:
        if not (group_dir / file).exists():
            missing_files.append(file)

    if missing_files:
        print(f"âŒ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
        return False

    print("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶å­˜åœ¨")

    # 1. éªŒè¯å› æœå›¾å¤§å°
    try:
        causal_graph_file = group_dir / f"{group_id}_dibs_causal_graph.csv"
        causal_graph_df = pd.read_csv(causal_graph_file, index_col=0)

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–¹é˜µ
        n_rows, n_cols = causal_graph_df.shape
        if n_rows != n_cols:
            print(f"âŒ å› æœå›¾ä¸æ˜¯æ–¹é˜µ: {n_rows}è¡Œ Ã— {n_cols}åˆ—")
            return False

        print(f"âœ… å› æœå›¾å¤§å°: {n_rows}Ã—{n_cols} (æ–¹é˜µ)")

        # æ£€æŸ¥ç‰¹å¾åç§°ä¸€è‡´æ€§
        features_file = group_dir / f"{group_id}_feature_names.json"
        with open(features_file, 'r') as f:
            feature_names = json.load(f)

        if len(feature_names) != n_rows:
            print(f"âŒ ç‰¹å¾æ•°ä¸åŒ¹é…: ç‰¹å¾åç§°{len(feature_names)}ä¸ª, å› æœå›¾{n_rows}Ã—{n_cols}")
            return False

        # æ£€æŸ¥åˆ—ååŒ¹é…
        if list(causal_graph_df.columns) != feature_names:
            print("âŒ å› æœå›¾åˆ—åä¸ç‰¹å¾åç§°ä¸åŒ¹é…")
            return False

        print(f"âœ… ç‰¹å¾åç§°ä¸€è‡´: {len(feature_names)}ä¸ªç‰¹å¾")

    except Exception as e:
        print(f"âŒ éªŒè¯å› æœå›¾æ—¶å‡ºé”™: {e}")
        return False

    # 2. éªŒè¯æ‘˜è¦ä¿¡æ¯
    try:
        summary_file = group_dir / f"{group_id}_dibs_summary.json"
        with open(summary_file, 'r') as f:
            summary = json.load(f)

        # æ£€æŸ¥å…³é”®å­—æ®µ
        required_fields = ['samples', 'features', 'edges_gt_0.3', 'strong_edge_percentage']
        for field in required_fields:
            if field not in summary:
                print(f"âŒ æ‘˜è¦ä¸­ç¼ºå¤±å­—æ®µ: {field}")
                return False

        print(f"âœ… æ‘˜è¦ä¿¡æ¯å®Œæ•´:")
        print(f"   æ ·æœ¬æ•°: {summary['samples']}")
        print(f"   ç‰¹å¾æ•°: {summary['features']}")
        print(f"   å¼ºè¾¹æ•°(>0.3): {summary['edges_gt_0.3']}")
        print(f"   å¼ºè¾¹æ¯”ä¾‹: {summary['strong_edge_percentage']:.1f}%")

        # éªŒè¯æ ·æœ¬æ•°ï¼ˆç‰¹åˆ«æ£€æŸ¥group5ï¼‰
        if group_id == "group5_mrt_oast":
            if summary['samples'] != 60:
                print(f"âŒ group5æ ·æœ¬æ•°ä¸æ­£ç¡®: {summary['samples']} (åº”ä¸º60)")
                return False
            else:
                print(f"âœ… group5æ ·æœ¬æ•°æ­£ç¡®: {summary['samples']}æ ·æœ¬")

        # éªŒè¯ç‰¹å¾æ•°ä¸å› æœå›¾ä¸€è‡´
        if summary['features'] != n_rows:
            print(f"âŒ ç‰¹å¾æ•°ä¸åŒ¹é…: æ‘˜è¦ä¸­{summary['features']}, å› æœå›¾ä¸­{n_rows}")
            return False

        print(f"âœ… ç‰¹å¾æ•°ä¸€è‡´: {summary['features']}")

    except Exception as e:
        print(f"âŒ éªŒè¯æ‘˜è¦æ—¶å‡ºé”™: {e}")
        return False

    # 3. éªŒè¯å› æœå›¾è´¨é‡æŒ‡æ ‡
    try:
        # æ£€æŸ¥å› æœå›¾æ•°å€¼èŒƒå›´
        causal_matrix = causal_graph_df.values
        min_val = np.min(causal_matrix)
        max_val = np.max(causal_matrix)
        mean_val = np.mean(causal_matrix)
        std_val = np.std(causal_matrix)

        print(f"âœ… å› æœå›¾æ•°å€¼ç»Ÿè®¡:")
        print(f"   æœ€å°å€¼: {min_val:.6f}")
        print(f"   æœ€å¤§å€¼: {max_val:.6f}")
        print(f"   å¹³å‡å€¼: {mean_val:.6f}")
        print(f"   æ ‡å‡†å·®: {std_val:.6f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.any(np.isnan(causal_matrix)):
            print("âŒ å› æœå›¾ä¸­åŒ…å«NaNå€¼")
            return False

        if np.any(np.isinf(causal_matrix)):
            print("âŒ å› æœå›¾ä¸­åŒ…å«æ— ç©·å¤§å€¼")
            return False

        print("âœ… å› æœå›¾æ— NaN/Infå€¼")

        # æ£€æŸ¥è¾¹å¼ºåº¦åˆ†å¸ƒ
        edges_001 = np.sum(causal_matrix > 0.01)
        edges_01 = np.sum(causal_matrix > 0.1)
        edges_03 = np.sum(causal_matrix > 0.3)
        edges_05 = np.sum(causal_matrix > 0.5)

        total_possible_edges = n_rows * (n_rows - 1)

        print(f"âœ… è¾¹å¼ºåº¦åˆ†å¸ƒ:")
        print(f"   >0.01: {edges_001}æ¡ ({edges_001/total_possible_edges*100:.1f}%)")
        print(f"   >0.1:  {edges_01}æ¡ ({edges_01/total_possible_edges*100:.1f}%)")
        print(f"   >0.3:  {edges_03}æ¡ ({edges_03/total_possible_edges*100:.1f}%)")
        print(f"   >0.5:  {edges_05}æ¡ ({edges_05/total_possible_edges*100:.1f}%)")

        # æ£€æŸ¥å¼ºè¾¹æ¯”ä¾‹æ˜¯å¦åˆç†ï¼ˆé€šå¸¸åœ¨1-10%ä¹‹é—´ï¼‰
        strong_edge_pct = edges_03 / total_possible_edges * 100
        if strong_edge_pct < 0.1 or strong_edge_pct > 30:
            print(f"âš ï¸  å¼ºè¾¹æ¯”ä¾‹å¼‚å¸¸: {strong_edge_pct:.1f}% (é€šå¸¸1-10%)")
            # ä¸è§†ä¸ºå¤±è´¥ï¼Œåªæ˜¯è­¦å‘Š

    except Exception as e:
        print(f"âŒ éªŒè¯å› æœå›¾è´¨é‡æ—¶å‡ºé”™: {e}")
        return False

    print(f"\nâœ… ç»„ {group_id} éªŒè¯é€šè¿‡!")
    return True

def main():
    parser = argparse.ArgumentParser(description="éªŒè¯DiBSå› æœå­¦ä¹ ç»“æœ")
    parser.add_argument("--group", type=str, help="éªŒè¯ç‰¹å®šç»„ï¼ˆå¦‚: group5_mrt_oastï¼‰")
    parser.add_argument("--all", action="store_true", help="éªŒè¯æ‰€æœ‰ç»„")
    parser.add_argument("--results-dir", type=str,
                       default="results/energy_research/data/global_std",
                       help="ç»“æœç›®å½•")

    args = parser.parse_args()

    if not args.group and not args.all:
        print("è¯·æŒ‡å®š --group æˆ– --all")
        parser.print_help()
        return

    results_dir = Path(args.results_dir)
    if not results_dir.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return

    print("=" * 80)
    print("DiBSç»“æœéªŒè¯")
    print("=" * 80)

    if args.all:
        # éªŒè¯æ‰€æœ‰ç»„
        groups = [
            "group1_examples", "group2_vulberta", "group3_person_reid",
            "group4_bug_localization", "group5_mrt_oast", "group6_resnet"
        ]

        validation_results = []
        for group_id in groups:
            success = validate_group_results(group_id, results_dir)
            validation_results.append((group_id, success))

        # æ±‡æ€»ç»“æœ
        print(f"\n{'='*80}")
        print("éªŒè¯æ±‡æ€»")
        print(f"{'='*80}")

        total_groups = len(validation_results)
        passed_groups = sum(1 for _, success in validation_results if success)

        for group_id, success in validation_results:
            status = "âœ… PASS" if success else "âŒ FAIL"
            print(f"{group_id}: {status}")

        print(f"\næ€»ç»„æ•°: {total_groups}")
        print(f"é€šè¿‡ç»„æ•°: {passed_groups}")
        print(f"å¤±è´¥ç»„æ•°: {total_groups - passed_groups}")

        if passed_groups == total_groups:
            print(f"\nğŸ‰ æ‰€æœ‰ç»„éªŒè¯é€šè¿‡ï¼")
        else:
            print(f"\nâš ï¸  {total_groups - passed_groups}ä¸ªç»„éªŒè¯å¤±è´¥")
            sys.exit(1)

    else:
        # éªŒè¯ç‰¹å®šç»„
        success = validate_group_results(args.group, results_dir)
        if success:
            print(f"\nğŸ‰ ç»„ {args.group} éªŒè¯é€šè¿‡ï¼")
        else:
            print(f"\nâŒ ç»„ {args.group} éªŒè¯å¤±è´¥")
            sys.exit(1)

if __name__ == "__main__":
    main()