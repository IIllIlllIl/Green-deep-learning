#!/usr/bin/env python3
"""
ä¸º6ä¸ªä»»åŠ¡ç»„ç”ŸæˆDiBSè®­ç»ƒæ•°æ®

åŸºäº836è¡Œå®Œæ•´æ•°æ®ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ç»„ç”Ÿæˆæ ‡å‡†åŒ–çš„DiBSè¾“å…¥æ•°æ®

åˆ›å»ºæ—¥æœŸ: 2026-01-05
æ•°æ®æº: data/energy_research/raw/energy_data_original.csv (836è¡Œ)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import json
from datetime import datetime

# 6ä¸ªä»»åŠ¡ç»„å®šä¹‰
TASK_GROUPS = [
    {
        'id': 'group1_examples',
        'name': 'examplesï¼ˆå›¾åƒåˆ†ç±»-å°å‹ï¼‰',
        'repositories': ['examples'],
        'description': '4ä¸ªå°å‹å›¾åƒåˆ†ç±»æ¨¡å‹ï¼ˆMNISTç­‰ï¼‰',
        'expected_samples': 259
    },
    {
        'id': 'group2_vulberta',
        'name': 'VulBERTaï¼ˆä»£ç æ¼æ´æ£€æµ‹ï¼‰',
        'repositories': ['VulBERTa'],
        'description': 'åŸºäºBERTçš„æ¼æ´æ£€æµ‹æ¨¡å‹',
        'expected_samples': 152
    },
    {
        'id': 'group3_person_reid',
        'name': 'Person_reIDï¼ˆè¡Œäººé‡è¯†åˆ«ï¼‰',
        'repositories': ['Person_reID_baseline_pytorch'],
        'description': 'è¡Œäººé‡è¯†åˆ«åŸºçº¿æ¨¡å‹',
        'expected_samples': 146
    },
    {
        'id': 'group4_bug_localization',
        'name': 'bug-localizationï¼ˆç¼ºé™·å®šä½ï¼‰',
        'repositories': ['bug-localization-by-dnn-and-rvsm'],
        'description': 'DNN+RVSMç¼ºé™·å®šä½æ¨¡å‹',
        'expected_samples': 142
    },
    {
        'id': 'group5_mrt_oast',
        'name': 'MRT-OASTï¼ˆç¼ºé™·å®šä½ï¼‰',
        'repositories': ['MRT-OAST'],
        'description': 'MRT-OASTç¼ºé™·å®šä½æ¨¡å‹',
        'expected_samples': 88
    },
    {
        'id': 'group6_resnet',
        'name': 'pytorch_resnetï¼ˆå›¾åƒåˆ†ç±»-ResNetï¼‰',
        'repositories': ['pytorch_resnet_cifar10'],
        'description': 'ResNet CIFAR-10åˆ†ç±»',
        'expected_samples': 49
    }
]

# æ•°æ®å¤„ç†å‚æ•°
MAX_MISSING_RATE = 0.40  # åˆ—æœ€å¤§ç¼ºå¤±ç‡é˜ˆå€¼ï¼ˆè°ƒæ•´ä¸º40%ä»¥ä¿ç•™æ›´å¤šæ€§èƒ½æŒ‡æ ‡ï¼‰
MIN_SAMPLES = 10  # æœ€å°æ ·æœ¬é‡

def process_task_group(df_full, task_group, verbose=True):
    """
    å¤„ç†å•ä¸ªä»»åŠ¡ç»„æ•°æ®

    å‚æ•°:
        df_full: å®Œæ•´æ•°æ®ï¼ˆ836è¡Œï¼‰
        task_group: ä»»åŠ¡ç»„é…ç½®å­—å…¸
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯

    è¿”å›:
        df_processed: å¤„ç†åçš„æ•°æ®
        stats: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    group_id = task_group['id']
    group_name = task_group['name']
    repos = task_group['repositories']

    if verbose:
        print(f"\n{'='*80}")
        print(f"å¤„ç†ä»»åŠ¡ç»„: {group_name}")
        print(f"ID: {group_id}")
        print(f"Repository: {repos}")
        print(f"{'='*80}")

    # 1. è¿‡æ»¤å¯¹åº”ä»»åŠ¡ç»„çš„æ•°æ®
    df = df_full[df_full['repository'].isin(repos)].copy()
    n_samples_raw = len(df)

    if verbose:
        print(f"\n[æ­¥éª¤1] è¿‡æ»¤repository")
        print(f"  åŸå§‹æ•°æ®: {len(df_full)}è¡Œ")
        print(f"  è¿‡æ»¤å: {n_samples_raw}è¡Œ")
        print(f"  é¢„æœŸ: {task_group['expected_samples']}è¡Œ")
        if n_samples_raw != task_group['expected_samples']:
            print(f"  âš ï¸  è­¦å‘Š: å®é™…æ ·æœ¬æ•°ä¸é¢„æœŸä¸ç¬¦ï¼")

    # æ£€æŸ¥æœ€å°æ ·æœ¬é‡
    if n_samples_raw < MIN_SAMPLES:
        raise ValueError(f"æ ·æœ¬é‡ä¸è¶³: {n_samples_raw} < {MIN_SAMPLES}")

    # 2. é€‰æ‹©æ•°å€¼å‹åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_numeric = df[numeric_cols].copy()

    if verbose:
        print(f"\n[æ­¥éª¤2] é€‰æ‹©æ•°å€¼å‹åˆ—")
        print(f"  æ€»åˆ—æ•°: {len(df.columns)}")
        print(f"  æ•°å€¼åˆ—æ•°: {len(numeric_cols)}")

    # 3. ç§»é™¤å…¨NaNåˆ—
    df_no_all_nan = df_numeric.dropna(axis=1, how='all')
    n_cols_removed_all_nan = len(df_numeric.columns) - len(df_no_all_nan.columns)

    if verbose:
        print(f"\n[æ­¥éª¤3] ç§»é™¤å…¨NaNåˆ—")
        print(f"  ç§»é™¤åˆ—æ•°: {n_cols_removed_all_nan}")
        print(f"  ä¿ç•™åˆ—æ•°: {len(df_no_all_nan.columns)}")

    # 4. è®¡ç®—ç¼ºå¤±ç‡å¹¶ç§»é™¤é«˜ç¼ºå¤±ç‡åˆ—
    missing_rate = df_no_all_nan.isna().sum() / len(df_no_all_nan)
    cols_high_missing = missing_rate[missing_rate > MAX_MISSING_RATE].index.tolist()
    cols_to_keep = missing_rate[missing_rate <= MAX_MISSING_RATE].index.tolist()

    df_low_missing = df_no_all_nan[cols_to_keep].copy()

    if verbose:
        print(f"\n[æ­¥éª¤4] ç§»é™¤ç¼ºå¤±ç‡>{MAX_MISSING_RATE*100}%çš„åˆ—")
        print(f"  é«˜ç¼ºå¤±ç‡åˆ—æ•°: {len(cols_high_missing)}")
        if len(cols_high_missing) > 0 and len(cols_high_missing) <= 10:
            print(f"  é«˜ç¼ºå¤±ç‡åˆ—: {cols_high_missing}")
        print(f"  ä¿ç•™åˆ—æ•°: {len(cols_to_keep)}")

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¶³å¤Ÿçš„ç‰¹å¾
    if len(cols_to_keep) < 3:
        raise ValueError(f"ç‰¹å¾æ•°ä¸è¶³: {len(cols_to_keep)} < 3")

    # 5. ç§»é™¤é›¶æ–¹å·®åˆ—ï¼ˆå¸¸æ•°åˆ—ï¼‰
    var_per_col = df_low_missing.var()
    zero_var_cols = var_per_col[var_per_col == 0].index.tolist()
    cols_with_var = var_per_col[var_per_col > 0].index.tolist()

    df_with_var = df_low_missing[cols_with_var].copy()

    if verbose:
        print(f"\n[æ­¥éª¤5] ç§»é™¤é›¶æ–¹å·®åˆ—ï¼ˆå¸¸æ•°åˆ—ï¼‰")
        print(f"  é›¶æ–¹å·®åˆ—æ•°: {len(zero_var_cols)}")
        if len(zero_var_cols) > 0 and len(zero_var_cols) <= 10:
            print(f"  é›¶æ–¹å·®åˆ—: {zero_var_cols}")
        print(f"  ä¿ç•™åˆ—æ•°: {len(cols_with_var)}")

    # 6. å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨å‡å€¼ï¼‰
    df_filled = df_with_var.fillna(df_with_var.mean())

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaN
    remaining_nan = df_filled.isna().sum().sum()
    if remaining_nan > 0:
        # å¦‚æœè¿˜æœ‰NaNï¼ˆå¯èƒ½æŸåˆ—å…¨NaNä½†æ²¡è¢«æ£€æµ‹åˆ°ï¼‰ï¼Œç”¨0å¡«å……
        df_filled = df_filled.fillna(0)
        if verbose:
            print(f"  âš ï¸  è­¦å‘Š: å¡«å……åä»æœ‰{remaining_nan}ä¸ªNaNï¼Œå·²ç”¨0å¡«å……")

    if verbose:
        print(f"\n[æ­¥éª¤6] å¡«å……ç¼ºå¤±å€¼ï¼ˆå‡å€¼å¡«å……ï¼‰")
        print(f"  å¡«å……å‰ç¼ºå¤±å€¼: {df_with_var.isna().sum().sum()}")
        print(f"  å¡«å……åç¼ºå¤±å€¼: {df_filled.isna().sum().sum()}")

    # 7. æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
    scaler = StandardScaler()
    df_scaled_values = scaler.fit_transform(df_filled)
    df_scaled = pd.DataFrame(
        df_scaled_values,
        columns=df_filled.columns,
        index=df_filled.index
    )

    if verbose:
        print(f"\n[æ­¥éª¤7] æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰")
        print(f"  å‡å€¼: {df_scaled.mean().mean():.6f} (åº”æ¥è¿‘0)")
        print(f"  æ ‡å‡†å·®: {df_scaled.std().mean():.6f} (åº”æ¥è¿‘1)")

    # 8. ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'group_id': group_id,
        'group_name': group_name,
        'repositories': repos,
        'n_samples_raw': n_samples_raw,
        'n_samples_final': len(df_scaled),
        'n_features_raw': len(numeric_cols),
        'n_features_final': len(df_scaled.columns),
        'n_cols_removed_all_nan': n_cols_removed_all_nan,
        'n_cols_removed_high_missing': len(cols_high_missing),
        'n_cols_removed_zero_var': len(zero_var_cols),
        'feature_names': df_scaled.columns.tolist(),
        'missing_rate_before_fill': float((df_with_var.isna().sum().sum() / (df_with_var.shape[0] * df_with_var.shape[1]))),
        'processing_success': True
    }

    if verbose:
        print(f"\n[å®Œæˆ] ä»»åŠ¡ç»„ {group_name}")
        print(f"  æœ€ç»ˆæ•°æ®: {stats['n_samples_final']}è¡Œ Ã— {stats['n_features_final']}åˆ—")
        print(f"  ç‰¹å¾ä¿ç•™ç‡: {stats['n_features_final']/stats['n_features_raw']*100:.1f}%")
        print(f"  ç¼ºå¤±ç‡ï¼ˆå¡«å……å‰ï¼‰: {stats['missing_rate_before_fill']*100:.2f}%")

    return df_scaled, stats

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ä¸º6ä¸ªä»»åŠ¡ç»„ç”ŸæˆDiBSè®­ç»ƒæ•°æ®")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # è¾“å…¥/è¾“å‡ºè·¯å¾„
    base_dir = Path(__file__).parent.parent
    input_file = base_dir / 'data' / 'energy_research' / 'raw' / 'energy_data_original.csv'
    output_dir = base_dir / 'data' / 'energy_research' / 'dibs_training'
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nè¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # åŠ è½½æ•°æ®
    print(f"\n{'='*80}")
    print("åŠ è½½å®Œæ•´æ•°æ®...")
    print(f"{'='*80}")

    if not input_file.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

    df_full = pd.read_csv(input_file)
    print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ: {len(df_full)}è¡Œ Ã— {len(df_full.columns)}åˆ—")

    # éªŒè¯æ•°æ®è¡Œæ•°
    if len(df_full) != 836:
        print(f"âš ï¸  è­¦å‘Š: æ•°æ®è¡Œæ•°ä¸º{len(df_full)}ï¼Œé¢„æœŸ836è¡Œ")

    # å¤„ç†æ‰€æœ‰ä»»åŠ¡ç»„
    all_stats = []
    success_count = 0

    for task_group in TASK_GROUPS:
        try:
            df_processed, stats = process_task_group(df_full, task_group, verbose=True)

            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            output_file = output_dir / f"{task_group['id']}.csv"
            df_processed.to_csv(output_file, index=False)
            print(f"\nâœ“ æ•°æ®å·²ä¿å­˜: {output_file}")

            stats['output_file'] = str(output_file)
            all_stats.append(stats)
            success_count += 1

        except Exception as e:
            print(f"\nâœ— ä»»åŠ¡ç»„ {task_group['name']} å¤„ç†å¤±è´¥: {e}")
            stats = {
                'group_id': task_group['id'],
                'group_name': task_group['name'],
                'processing_success': False,
                'error_message': str(e)
            }
            all_stats.append(stats)

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("æ•°æ®ç”Ÿæˆæ€»ç»“")
    print(f"{'='*80}")

    print(f"\næˆåŠŸç‡: {success_count}/{len(TASK_GROUPS)} ({success_count/len(TASK_GROUPS)*100:.0f}%)")

    # åˆ›å»ºæ€»ç»“è¡¨æ ¼
    print(f"\nä»»åŠ¡ç»„æ•°æ®æ€»ç»“:")
    print(f"{'ID':<25} {'æ ·æœ¬æ•°':<10} {'ç‰¹å¾æ•°':<10} {'çŠ¶æ€':<10}")
    print("-"*80)

    for stats in all_stats:
        if stats['processing_success']:
            status = "âœ“ æˆåŠŸ"
            samples = f"{stats['n_samples_final']}"
            features = f"{stats['n_features_final']}"
        else:
            status = "âœ— å¤±è´¥"
            samples = "N/A"
            features = "N/A"

        print(f"{stats['group_id']:<25} {samples:<10} {features:<10} {status:<10}")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯JSON
    stats_file = output_dir / 'generation_stats.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'input_file': str(input_file),
            'output_dir': str(output_dir),
            'total_tasks': len(TASK_GROUPS),
            'successful_tasks': success_count,
            'task_stats': all_stats
        }, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_file = output_dir / 'DATA_GENERATION_REPORT.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# 6ä»»åŠ¡ç»„DiBSè®­ç»ƒæ•°æ®ç”ŸæˆæŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æº**: {input_file}\n")
        f.write(f"**æ€»æ ·æœ¬æ•°**: {len(df_full)}è¡Œ\n")
        f.write(f"**æˆåŠŸç‡**: {success_count}/{len(TASK_GROUPS)} ({success_count/len(TASK_GROUPS)*100:.0f}%)\n\n")

        f.write("## ä»»åŠ¡ç»„è¯¦æƒ…\n\n")
        f.write("| ä»»åŠ¡ç»„ | æ ·æœ¬æ•° | ç‰¹å¾æ•° | ä¿ç•™ç‡ | ç¼ºå¤±ç‡ | çŠ¶æ€ |\n")
        f.write("|--------|--------|--------|--------|--------|------|\n")

        for stats in all_stats:
            if stats['processing_success']:
                name = stats['group_name']
                samples = stats['n_samples_final']
                features = stats['n_features_final']
                retention = f"{stats['n_features_final']/stats['n_features_raw']*100:.1f}%"
                missing = f"{stats['missing_rate_before_fill']*100:.2f}%"
                status = "âœ“"
            else:
                name = stats['group_name']
                samples = "N/A"
                features = "N/A"
                retention = "N/A"
                missing = "N/A"
                status = "âœ—"

            f.write(f"| {name} | {samples} | {features} | {retention} | {missing} | {status} |\n")

        f.write("\n## æ•°æ®å¤„ç†æµç¨‹\n\n")
        f.write("1. æŒ‰repositoryè¿‡æ»¤ä»»åŠ¡ç»„æ•°æ®\n")
        f.write("2. é€‰æ‹©æ•°å€¼å‹åˆ—\n")
        f.write("3. ç§»é™¤å…¨NaNåˆ—\n")
        f.write(f"4. ç§»é™¤ç¼ºå¤±ç‡>{MAX_MISSING_RATE*100}%çš„åˆ—\n")
        f.write("5. ç§»é™¤é›¶æ–¹å·®åˆ—ï¼ˆå¸¸æ•°åˆ—ï¼‰\n")
        f.write("6. å¡«å……ç¼ºå¤±å€¼ï¼ˆå‡å€¼å¡«å……ï¼‰\n")
        f.write("7. æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰\n\n")

        f.write("## è¾“å‡ºæ–‡ä»¶\n\n")
        for stats in all_stats:
            if stats['processing_success']:
                f.write(f"- `{stats['group_id']}.csv` - {stats['n_samples_final']}è¡Œ Ã— {stats['n_features_final']}åˆ—\n")

        f.write(f"\n---\n\n")
        f.write(f"**æŠ¥å‘Šç”Ÿæˆ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    print(f"\n{'='*80}")
    print("æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")

    if success_count == len(TASK_GROUPS):
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡ç»„æ•°æ®ç”ŸæˆæˆåŠŸï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹ç”ŸæˆæŠ¥å‘Š: cat data/energy_research/dibs_training/DATA_GENERATION_REPORT.md")
        print("  2. è¿è¡ŒDiBSåˆ†æ: ä½¿ç”¨ç”Ÿæˆçš„6ä¸ªCSVæ–‡ä»¶")
        return 0
    else:
        print(f"\nâš ï¸  {len(TASK_GROUPS) - success_count} ä¸ªä»»åŠ¡ç»„å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())
