#!/usr/bin/env python3
"""
6åˆ†ç»„æ•°æ®éªŒè¯è„šæœ¬ - ç‹¬ç«‹è´¨é‡æ£€æŸ¥

æ£€æŸ¥å†…å®¹:
1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (6ç»„æ•°æ®æ˜¯å¦éƒ½å·²ç”Ÿæˆ)
2. åˆ†ç»„æ­£ç¡®æ€§æ£€æŸ¥ (repositoryå’Œmodelæ˜¯å¦æ­£ç¡®)
3. å­—æ®µæ­£ç¡®æ€§æ£€æŸ¥ (è¶…å‚æ•°ã€æ€§èƒ½æŒ‡æ ‡ã€èƒ½è€—åˆ—)
4. æ¨¡å‹å˜é‡ç¼–ç æ£€æŸ¥ (One-hot n-1ç¼–ç )
5. æ•°æ®è´¨é‡æ£€æŸ¥ (ç¼ºå¤±ç‡ã€å¼‚å¸¸å€¼)
6. ç‰¹æ®Šæƒ…å†µæ£€æŸ¥ (L2æ­£åˆ™åŒ–ã€å¹¶è¡Œæ¨¡å¼)

åˆ›å»ºæ—¥æœŸ: 2026-01-15
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from collections import defaultdict
import json

# æ•°æ®ç›®å½•
DATA_DIR = '/home/green/energy_dl/nightly/analysis/data/energy_research/6groups_final'
SOURCE_DATA = '/home/green/energy_dl/nightly/data/data.csv'

# æ ¹æ®è®¾è®¡æ–‡æ¡£å®šä¹‰çš„6ç»„é…ç½®
GROUP_SPECS = {
    'group1_examples': {
        'name': 'å›¾åƒåˆ†ç±»-å°å‹æ¨¡å‹ç»„',
        'repo': 'examples',
        'models': ['mnist', 'mnist_ff', 'mnist_rnn', 'siamese'],
        'expected_rows': 304,
        'hyperparams': ['hyperparam_batch_size', 'hyperparam_learning_rate',
                       'hyperparam_epochs', 'hyperparam_seed'],
        'performance': ['perf_test_accuracy'],
        'model_vars_count': 3  # n-1 encoding for 4 models
    },
    'group2_vulberta': {
        'name': 'ä»£ç æ¼æ´æ£€æµ‹ç»„',
        'repo': 'VulBERTa',
        'models': ['mlp'],
        'expected_rows': 72,
        'hyperparams': ['hyperparam_learning_rate', 'hyperparam_epochs',
                       'hyperparam_seed', 'hyperparam_l2_regularization'],
        'performance': ['perf_eval_loss', 'perf_final_training_loss',
                       'perf_eval_samples_per_second'],
        'model_vars_count': 0  # single model, no encoding needed
    },
    'group3_person_reid': {
        'name': 'è¡Œäººé‡è¯†åˆ«ç»„',
        'repo': 'Person_reID_baseline_pytorch',
        'models': ['densenet121', 'hrnet18', 'pcb'],
        'expected_rows': 206,
        'hyperparams': ['hyperparam_dropout', 'hyperparam_learning_rate',
                       'hyperparam_epochs', 'hyperparam_seed'],
        'performance': ['perf_map', 'perf_rank1', 'perf_rank5'],
        'model_vars_count': 2  # n-1 encoding for 3 models
    },
    'group4_bug_localization': {
        'name': 'ç¼ºé™·å®šä½ç»„',
        'repo': 'bug-localization-by-dnn-and-rvsm',
        'models': ['default'],
        'expected_rows': 90,
        'hyperparams': ['hyperparam_alpha', 'hyperparam_kfold',
                       'hyperparam_max_iter', 'hyperparam_seed'],
        'performance': ['perf_top1_accuracy', 'perf_top5_accuracy',
                       'perf_top10_accuracy', 'perf_top20_accuracy'],
        'model_vars_count': 0  # single model
    },
    'group5_mrt_oast': {
        'name': 'å¤šç›®æ ‡ä¼˜åŒ–ç»„',
        'repo': 'MRT-OAST',
        'models': ['default'],
        'expected_rows': 72,
        'hyperparams': ['hyperparam_dropout', 'hyperparam_learning_rate',
                       'hyperparam_epochs', 'hyperparam_seed',
                       'hyperparam_l2_regularization'],
        'performance': ['perf_accuracy', 'perf_precision', 'perf_recall'],
        'model_vars_count': 0  # single model
    },
    'group6_resnet': {
        'name': 'å›¾åƒåˆ†ç±»-ResNetç»„',
        'repo': 'pytorch_resnet_cifar10',
        'models': ['resnet20'],
        'expected_rows': 74,
        'hyperparams': ['hyperparam_learning_rate', 'hyperparam_epochs',
                       'hyperparam_seed', 'hyperparam_l2_regularization'],
        'performance': ['perf_best_val_accuracy', 'perf_test_accuracy'],
        'model_vars_count': 0  # single model
    }
}

# èƒ½è€—åˆ—ï¼ˆæ‰€æœ‰ç»„å…±ç”¨ï¼‰
ENERGY_COLS = [
    'energy_gpu_avg', 'energy_cpu_avg', 'energy_ram_avg',
    'energy_gpu_total', 'energy_cpu_total', 'energy_ram_total',
    'energy_total_avg'
]

# æ§åˆ¶å˜é‡ï¼ˆæ‰€æœ‰ç»„å…±ç”¨ï¼‰
CONTROL_COLS = ['is_parallel', 'timestamp', 'duration_seconds', 'num_mutated_params']

# å…ƒæ•°æ®åˆ—
META_COLS = ['experiment_id', 'repository', 'model']


class ValidationReport:
    """éªŒè¯æŠ¥å‘Šç±»"""

    def __init__(self):
        self.checks = []
        self.issues = []
        self.warnings = []
        self.stats = {}

    def add_check(self, name, passed, details=None):
        """æ·»åŠ æ£€æŸ¥é¡¹"""
        self.checks.append({
            'name': name,
            'passed': passed,
            'details': details
        })

    def add_issue(self, severity, description):
        """æ·»åŠ é—®é¢˜"""
        issue = {'severity': severity, 'description': description}
        if severity == 'ERROR':
            self.issues.append(issue)
        else:
            self.warnings.append(issue)

    def add_stat(self, key, value):
        """æ·»åŠ ç»Ÿè®¡ä¿¡æ¯"""
        self.stats[key] = value

    def get_summary(self):
        """è·å–æ‘˜è¦"""
        total_checks = len(self.checks)
        passed_checks = sum(1 for c in self.checks if c['passed'])
        return {
            'total_checks': total_checks,
            'passed_checks': passed_checks,
            'failed_checks': total_checks - passed_checks,
            'errors': len(self.issues),
            'warnings': len(self.warnings),
            'overall_score': passed_checks / total_checks * 100 if total_checks > 0 else 0
        }


def check_data_completeness(report):
    """æ£€æŸ¥1: æ•°æ®å®Œæ•´æ€§"""
    print("\n" + "="*60)
    print("æ£€æŸ¥1: æ•°æ®å®Œæ•´æ€§")
    print("="*60)

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    csv_files = list(Path(DATA_DIR).glob('*.csv'))
    print(f"\næ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")
    for f in sorted(csv_files):
        print(f"  - {f.name}")

    # æ£€æŸ¥æ˜¯å¦æœ‰6ä¸ªç»„
    group_files = [f for f in csv_files if f.stem.startswith('group')]
    expected_groups = set(GROUP_SPECS.keys())
    actual_groups = set([f.stem for f in group_files])

    missing_groups = expected_groups - actual_groups
    extra_groups = actual_groups - expected_groups

    if missing_groups:
        report.add_issue('ERROR', f"ç¼ºå¤±ç»„: {missing_groups}")
        print(f"\nâŒ ç¼ºå¤±ç»„: {missing_groups}")

    if extra_groups:
        report.add_issue('WARNING', f"é¢å¤–æ–‡ä»¶: {extra_groups}")
        print(f"\nâš ï¸  é¢å¤–æ–‡ä»¶: {extra_groups}")

    # æ£€æŸ¥æ€»è¡Œæ•°
    total_rows = 0
    group_data = {}
    for group_id in GROUP_SPECS:
        file_path = Path(DATA_DIR) / f"{group_id}.csv"
        if file_path.exists():
            df = pd.read_csv(file_path)
            group_data[group_id] = df
            total_rows += len(df)
            expected = GROUP_SPECS[group_id]['expected_rows']
            if len(df) == expected:
                print(f"  âœ… {group_id}: {len(df)} è¡Œ (ç¬¦åˆé¢„æœŸ)")
            else:
                print(f"  âŒ {group_id}: {len(df)} è¡Œ (é¢„æœŸ {expected})")
                report.add_issue('ERROR', f"{group_id}: è¡Œæ•°ä¸åŒ¹é… ({len(df)} vs {expected})")
        else:
            print(f"  âŒ {group_id}: æ–‡ä»¶ä¸å­˜åœ¨")
            report.add_issue('ERROR', f"{group_id}: æ–‡ä»¶ä¸å­˜åœ¨")

    print(f"\næ€»æ•°æ®è¡Œæ•°: {total_rows} (é¢„æœŸ: 818)")
    if total_rows == 818:
        print("âœ… æ•°æ®å®Œæ•´æ€§: 100% åˆ©ç”¨ç‡")
        report.add_check('æ•°æ®å®Œæ•´æ€§', True, f"æ€»è¡Œæ•°: {total_rows}/818")
    else:
        print(f"âŒ æ•°æ®å®Œæ•´æ€§: {total_rows/818*100:.1f}% åˆ©ç”¨ç‡")
        report.add_check('æ•°æ®å®Œæ•´æ€§', False, f"æ€»è¡Œæ•°: {total_rows}/818")
        report.add_issue('ERROR', f"æ€»è¡Œæ•°ä¸åŒ¹é…: {total_rows} (é¢„æœŸ 818)")

    report.add_stat('total_rows', total_rows)
    report.add_stat('expected_rows', 818)

    return group_data


def check_timestamp_uniqueness(group_data, report):
    """æ£€æŸ¥timestampå”¯ä¸€æ€§"""
    print("\n" + "="*60)
    print("æ£€æŸ¥2: Timestampå”¯ä¸€æ€§")
    print("="*60)

    all_timestamps = []
    for group_id, df in group_data.items():
        if 'timestamp' not in df.columns:
            print(f"  âŒ {group_id}: ç¼ºå°‘timestampåˆ—")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå°‘timestampåˆ—")
            continue

        # æ£€æŸ¥ç»„å†…å”¯ä¸€æ€§
        duplicates = df['timestamp'].duplicated().sum()
        if duplicates > 0:
            print(f"  âŒ {group_id}: å‘ç° {duplicates} ä¸ªé‡å¤timestamp")
            report.add_issue('ERROR', f"{group_id}: {duplicates}ä¸ªé‡å¤timestamp")
        else:
            print(f"  âœ… {group_id}: timestampå”¯ä¸€ ({len(df)}æ¡)")

        all_timestamps.extend(df['timestamp'].tolist())

    # æ£€æŸ¥è·¨ç»„å”¯ä¸€æ€§
    all_timestamps_series = pd.Series(all_timestamps)
    cross_duplicates = all_timestamps_series.duplicated().sum()

    if cross_duplicates > 0:
        print(f"\nâŒ è·¨ç»„é‡å¤: å‘ç° {cross_duplicates} ä¸ªé‡å¤timestamp")
        report.add_issue('ERROR', f"è·¨ç»„é‡å¤: {cross_duplicates}ä¸ªtimestamp")
        report.add_check('Timestampå”¯ä¸€æ€§', False)
    else:
        print(f"\nâœ… æ‰€æœ‰timestampå”¯ä¸€ (æ€»è®¡ {len(all_timestamps)}æ¡)")
        report.add_check('Timestampå”¯ä¸€æ€§', True)


def check_grouping_correctness(group_data, report):
    """æ£€æŸ¥3: åˆ†ç»„æ­£ç¡®æ€§"""
    print("\n" + "="*60)
    print("æ£€æŸ¥3: åˆ†ç»„æ­£ç¡®æ€§ (Repository & Model)")
    print("="*60)

    all_passed = True

    for group_id, df in group_data.items():
        if group_id not in GROUP_SPECS:
            continue

        spec = GROUP_SPECS[group_id]
        print(f"\n{group_id} ({spec['name']}):")

        # æ£€æŸ¥repository
        if 'repository' in df.columns:
            repos = df['repository'].unique()
            expected_repo = spec['repo']
            if len(repos) == 1 and repos[0] == expected_repo:
                print(f"  âœ… Repository: {repos[0]}")
            else:
                print(f"  âŒ Repositoryä¸åŒ¹é…: {repos} (é¢„æœŸ: {expected_repo})")
                report.add_issue('ERROR', f"{group_id}: Repositoryä¸åŒ¹é…")
                all_passed = False
        else:
            print(f"  âŒ ç¼ºå°‘repositoryåˆ—")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå°‘repositoryåˆ—")
            all_passed = False

        # æ£€æŸ¥model
        if 'model' in df.columns:
            models = sorted(df['model'].unique())
            expected_models = sorted(spec['models'])
            if models == expected_models:
                print(f"  âœ… Models: {models}")
            else:
                print(f"  âŒ Modelsä¸åŒ¹é…:")
                print(f"     å®é™…: {models}")
                print(f"     é¢„æœŸ: {expected_models}")
                report.add_issue('ERROR', f"{group_id}: Modelsä¸åŒ¹é…")
                all_passed = False
        else:
            print(f"  âŒ ç¼ºå°‘modelåˆ—")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå°‘modelåˆ—")
            all_passed = False

    report.add_check('åˆ†ç»„æ­£ç¡®æ€§', all_passed)


def check_field_correctness(group_data, report):
    """æ£€æŸ¥4: å­—æ®µæ­£ç¡®æ€§"""
    print("\n" + "="*60)
    print("æ£€æŸ¥4: å­—æ®µæ­£ç¡®æ€§")
    print("="*60)

    all_passed = True

    for group_id, df in group_data.items():
        if group_id not in GROUP_SPECS:
            continue

        spec = GROUP_SPECS[group_id]
        print(f"\n{group_id}:")

        # æ£€æŸ¥èƒ½è€—åˆ—
        missing_energy = [col for col in ENERGY_COLS if col not in df.columns]
        if missing_energy:
            print(f"  âŒ ç¼ºå¤±èƒ½è€—åˆ—: {missing_energy}")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå¤±èƒ½è€—åˆ— {missing_energy}")
            all_passed = False
        else:
            print(f"  âœ… èƒ½è€—åˆ—å®Œæ•´ ({len(ENERGY_COLS)}åˆ—)")

        # æ£€æŸ¥æ§åˆ¶å˜é‡
        missing_control = [col for col in CONTROL_COLS if col not in df.columns]
        if missing_control:
            print(f"  âŒ ç¼ºå¤±æ§åˆ¶å˜é‡: {missing_control}")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå¤±æ§åˆ¶å˜é‡ {missing_control}")
            all_passed = False
        else:
            print(f"  âœ… æ§åˆ¶å˜é‡å®Œæ•´ ({len(CONTROL_COLS)}åˆ—)")

        # æ£€æŸ¥è¶…å‚æ•°åˆ—
        missing_hyper = [col for col in spec['hyperparams'] if col not in df.columns]
        if missing_hyper:
            print(f"  âŒ ç¼ºå¤±è¶…å‚æ•°: {missing_hyper}")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå¤±è¶…å‚æ•° {missing_hyper}")
            all_passed = False
        else:
            print(f"  âœ… è¶…å‚æ•°å®Œæ•´ ({len(spec['hyperparams'])}åˆ—)")

        # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        missing_perf = [col for col in spec['performance'] if col not in df.columns]
        if missing_perf:
            print(f"  âŒ ç¼ºå¤±æ€§èƒ½æŒ‡æ ‡: {missing_perf}")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå¤±æ€§èƒ½æŒ‡æ ‡ {missing_perf}")
            all_passed = False
        else:
            print(f"  âœ… æ€§èƒ½æŒ‡æ ‡å®Œæ•´ ({len(spec['performance'])}åˆ—)")

    report.add_check('å­—æ®µæ­£ç¡®æ€§', all_passed)


def check_model_encoding(group_data, report):
    """æ£€æŸ¥5: æ¨¡å‹å˜é‡ç¼–ç """
    print("\n" + "="*60)
    print("æ£€æŸ¥5: æ¨¡å‹å˜é‡ç¼–ç  (One-hot n-1)")
    print("="*60)

    all_passed = True

    for group_id, df in group_data.items():
        if group_id not in GROUP_SPECS:
            continue

        spec = GROUP_SPECS[group_id]
        expected_vars = spec['model_vars_count']

        print(f"\n{group_id}:")
        print(f"  æ¨¡å‹æ•°: {len(spec['models'])}")
        print(f"  é¢„æœŸæ¨¡å‹å˜é‡æ•°: {expected_vars}")

        # æŸ¥æ‰¾æ¨¡å‹å˜é‡åˆ—
        model_var_cols = [col for col in df.columns if col.startswith('model_')]
        actual_vars = len(model_var_cols)

        print(f"  å®é™…æ¨¡å‹å˜é‡æ•°: {actual_vars}")

        if actual_vars == expected_vars:
            print(f"  âœ… æ¨¡å‹å˜é‡ç¼–ç æ­£ç¡®")
            if model_var_cols:
                print(f"     å˜é‡: {model_var_cols}")
                # æ£€æŸ¥ç¼–ç å€¼ï¼ˆåº”è¯¥æ˜¯0æˆ–1ï¼‰
                for col in model_var_cols:
                    unique_vals = sorted(df[col].unique())
                    if set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                        print(f"     âœ… {col}: {unique_vals}")
                    else:
                        print(f"     âŒ {col}: éäºŒå€¼ç¼–ç  {unique_vals}")
                        report.add_issue('ERROR', f"{group_id}: {col}éäºŒå€¼ç¼–ç ")
                        all_passed = False
        else:
            print(f"  âŒ æ¨¡å‹å˜é‡æ•°ä¸åŒ¹é…")
            report.add_issue('ERROR', f"{group_id}: æ¨¡å‹å˜é‡æ•°ä¸åŒ¹é… ({actual_vars} vs {expected_vars})")
            all_passed = False

    report.add_check('æ¨¡å‹å˜é‡ç¼–ç ', all_passed)


def check_data_quality(group_data, report):
    """æ£€æŸ¥6: æ•°æ®è´¨é‡ï¼ˆç¼ºå¤±ç‡ã€å¼‚å¸¸å€¼ï¼‰"""
    print("\n" + "="*60)
    print("æ£€æŸ¥6: æ•°æ®è´¨é‡")
    print("="*60)

    quality_stats = {}

    for group_id, df in group_data.items():
        if group_id not in GROUP_SPECS:
            continue

        spec = GROUP_SPECS[group_id]
        print(f"\n{group_id}:")

        stats = {
            'total_rows': len(df),
            'missing_rates': {},
            'complete_rows': 0
        }

        # è®¡ç®—æ¯åˆ—ç¼ºå¤±ç‡
        print("  ç¼ºå¤±ç‡åˆ†æ:")

        # èƒ½è€—åˆ—
        energy_cols_present = [col for col in ENERGY_COLS if col in df.columns]
        if energy_cols_present:
            energy_missing = df[energy_cols_present].isnull().sum()
            for col in energy_cols_present:
                rate = energy_missing[col] / len(df) * 100
                stats['missing_rates'][col] = rate
                if rate > 0:
                    print(f"    {col}: {rate:.1f}%")

        # è¶…å‚æ•°
        hyper_cols_present = [col for col in spec['hyperparams'] if col in df.columns]
        if hyper_cols_present:
            hyper_missing = df[hyper_cols_present].isnull().sum()
            for col in hyper_cols_present:
                rate = hyper_missing[col] / len(df) * 100
                stats['missing_rates'][col] = rate
                if rate > 0:
                    print(f"    {col}: {rate:.1f}%")

        # æ€§èƒ½æŒ‡æ ‡
        perf_cols_present = [col for col in spec['performance'] if col in df.columns]
        if perf_cols_present:
            perf_missing = df[perf_cols_present].isnull().sum()
            for col in perf_cols_present:
                rate = perf_missing[col] / len(df) * 100
                stats['missing_rates'][col] = rate
                if rate > 0:
                    print(f"    {col}: {rate:.1f}%")

        # è®¡ç®—å®Œæ•´è®°å½•æ•°ï¼ˆæ‰€æœ‰å…³é”®åˆ—éƒ½éç©ºï¼‰
        key_cols = energy_cols_present + hyper_cols_present + perf_cols_present
        complete_mask = ~df[key_cols].isnull().any(axis=1)
        complete_rows = complete_mask.sum()
        complete_rate = complete_rows / len(df) * 100

        stats['complete_rows'] = complete_rows
        stats['complete_rate'] = complete_rate

        print(f"\n  âœ… å®Œæ•´è®°å½•: {complete_rows}/{len(df)} ({complete_rate:.1f}%)")

        quality_stats[group_id] = stats

    # æ€»ä½“è´¨é‡è¯„ä¼°
    total_complete = sum(s['complete_rows'] for s in quality_stats.values())
    total_rows = sum(s['total_rows'] for s in quality_stats.values())
    overall_rate = total_complete / total_rows * 100 if total_rows > 0 else 0

    print(f"\næ€»ä½“å®Œæ•´ç‡: {total_complete}/{total_rows} ({overall_rate:.1f}%)")

    report.add_stat('complete_rows', total_complete)
    report.add_stat('complete_rate', overall_rate)
    report.add_stat('quality_by_group', quality_stats)

    # è¯„åˆ†æ ‡å‡†: >95% ä¼˜ç§€, >90% è‰¯å¥½, >80% å¯æ¥å—
    if overall_rate >= 95:
        print("âœ… æ•°æ®è´¨é‡: ä¼˜ç§€")
        report.add_check('æ•°æ®è´¨é‡', True, f"å®Œæ•´ç‡: {overall_rate:.1f}%")
    elif overall_rate >= 90:
        print("âš ï¸  æ•°æ®è´¨é‡: è‰¯å¥½")
        report.add_check('æ•°æ®è´¨é‡', True, f"å®Œæ•´ç‡: {overall_rate:.1f}%")
        report.add_issue('WARNING', f"æ•°æ®å®Œæ•´ç‡ {overall_rate:.1f}% (å¯æ¥å—ä½†ä¸ç†æƒ³)")
    elif overall_rate >= 80:
        print("âš ï¸  æ•°æ®è´¨é‡: å¯æ¥å—")
        report.add_check('æ•°æ®è´¨é‡', True, f"å®Œæ•´ç‡: {overall_rate:.1f}%")
        report.add_issue('WARNING', f"æ•°æ®å®Œæ•´ç‡ {overall_rate:.1f}% (å»ºè®®æ”¹è¿›)")
    else:
        print("âŒ æ•°æ®è´¨é‡: ä¸è¶³")
        report.add_check('æ•°æ®è´¨é‡', False, f"å®Œæ•´ç‡: {overall_rate:.1f}%")
        report.add_issue('ERROR', f"æ•°æ®å®Œæ•´ç‡ {overall_rate:.1f}% (ä¸å¯æ¥å—)")


def check_special_cases(group_data, report):
    """æ£€æŸ¥7: ç‰¹æ®Šæƒ…å†µï¼ˆL2æ­£åˆ™åŒ–ã€å¹¶è¡Œæ¨¡å¼ï¼‰"""
    print("\n" + "="*60)
    print("æ£€æŸ¥7: ç‰¹æ®Šæƒ…å†µ")
    print("="*60)

    # æ£€æŸ¥L2æ­£åˆ™åŒ–åˆå¹¶
    print("\n7.1 L2æ­£åˆ™åŒ–è¯­ä¹‰åˆå¹¶:")
    l2_groups = ['group2_vulberta', 'group5_mrt_oast', 'group6_resnet']

    for group_id in l2_groups:
        if group_id in group_data:
            df = group_data[group_id]
            if 'hyperparam_l2_regularization' in df.columns:
                non_null = df['hyperparam_l2_regularization'].notna().sum()
                print(f"  âœ… {group_id}: hyperparam_l2_regularizationå­˜åœ¨ ({non_null}/{len(df)}æ¡æœ‰å€¼)")
            else:
                print(f"  âŒ {group_id}: ç¼ºå°‘hyperparam_l2_regularization")
                report.add_issue('ERROR', f"{group_id}: ç¼ºå°‘L2æ­£åˆ™åŒ–åˆ—")

    # æ£€æŸ¥å¹¶è¡Œæ¨¡å¼å¤„ç†
    print("\n7.2 å¹¶è¡Œæ¨¡å¼æ ‡è¯†:")
    for group_id, df in group_data.items():
        if 'is_parallel' in df.columns:
            parallel_count = (df['is_parallel'] == True).sum()
            non_parallel_count = (df['is_parallel'] == False).sum()
            print(f"  {group_id}:")
            print(f"    å¹¶è¡Œæ¨¡å¼: {parallel_count}")
            print(f"    éå¹¶è¡Œæ¨¡å¼: {non_parallel_count}")
            if parallel_count + non_parallel_count == len(df):
                print(f"    âœ… is_parallelæ ‡è¯†å®Œæ•´")
            else:
                print(f"    âš ï¸  is_parallelæœ‰ç¼ºå¤±å€¼")
                report.add_issue('WARNING', f"{group_id}: is_parallelæœ‰ç¼ºå¤±å€¼")
        else:
            print(f"  âŒ {group_id}: ç¼ºå°‘is_parallelåˆ—")
            report.add_issue('ERROR', f"{group_id}: ç¼ºå°‘is_parallelåˆ—")


def generate_report(report, output_file):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    summary = report.get_summary()

    content = f"""# 6åˆ†ç»„æ•°æ®éªŒè¯æŠ¥å‘Š

**éªŒè¯æ—¥æœŸ**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®ä½ç½®**: {DATA_DIR}
**éªŒè¯è„šæœ¬**: validate_6groups_data.py

---

## ğŸ“Š éªŒè¯æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ€»æ£€æŸ¥é¡¹** | {summary['total_checks']} |
| **âœ… é€šè¿‡** | {summary['passed_checks']} |
| **âŒ å¤±è´¥** | {summary['failed_checks']} |
| **ğŸš¨ é”™è¯¯** | {summary['errors']} |
| **âš ï¸  è­¦å‘Š** | {summary['warnings']} |
| **ğŸ¯ æ€»ä½“è¯„åˆ†** | {summary['overall_score']:.1f}/100 |

"""

    # æ·»åŠ æ€»ä½“çŠ¶æ€
    if summary['errors'] == 0 and summary['warnings'] == 0:
        content += "**âœ… æ€»ä½“çŠ¶æ€**: æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæ•°æ®è´¨é‡ä¼˜ç§€ï¼\n\n"
    elif summary['errors'] == 0:
        content += f"**âš ï¸  æ€»ä½“çŠ¶æ€**: é€šè¿‡ä½†æœ‰ {summary['warnings']} ä¸ªè­¦å‘Šï¼Œå»ºè®®æ”¹è¿›\n\n"
    else:
        content += f"**âŒ æ€»ä½“çŠ¶æ€**: å‘ç° {summary['errors']} ä¸ªé”™è¯¯ï¼Œéœ€è¦ä¿®å¤ï¼\n\n"

    content += "---\n\n"

    # æ£€æŸ¥é¡¹è¯¦æƒ…
    content += "## ğŸ“‹ æ£€æŸ¥é¡¹è¯¦æƒ…\n\n"
    for i, check in enumerate(report.checks, 1):
        status = "âœ…" if check['passed'] else "âŒ"
        content += f"### {i}. {status} {check['name']}\n\n"
        if check['details']:
            content += f"**è¯¦æƒ…**: {check['details']}\n\n"

    content += "---\n\n"

    # é—®é¢˜æ¸…å•
    if report.issues:
        content += "## ğŸš¨ é”™è¯¯æ¸…å•\n\n"
        for i, issue in enumerate(report.issues, 1):
            content += f"{i}. **{issue['severity']}**: {issue['description']}\n"
        content += "\n---\n\n"

    if report.warnings:
        content += "## âš ï¸  è­¦å‘Šæ¸…å•\n\n"
        for i, warning in enumerate(report.warnings, 1):
            content += f"{i}. {warning['description']}\n"
        content += "\n---\n\n"

    # ç»Ÿè®¡ä¿¡æ¯
    if report.stats:
        content += "## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡\n\n"

        # æ•°æ®é‡ç»Ÿè®¡
        if 'total_rows' in report.stats:
            content += "### æ•°æ®é‡ç»Ÿè®¡\n\n"
            content += f"- æ€»è¡Œæ•°: {report.stats['total_rows']}\n"
            content += f"- é¢„æœŸè¡Œæ•°: {report.stats.get('expected_rows', 'N/A')}\n"
            content += f"- åˆ©ç”¨ç‡: {report.stats['total_rows']/report.stats.get('expected_rows', 1)*100:.1f}%\n\n"

        # æ•°æ®è´¨é‡ç»Ÿè®¡
        if 'complete_rows' in report.stats:
            content += "### æ•°æ®è´¨é‡ç»Ÿè®¡\n\n"
            content += f"- å®Œæ•´è®°å½•æ•°: {report.stats['complete_rows']}\n"
            content += f"- å®Œæ•´ç‡: {report.stats['complete_rate']:.1f}%\n\n"

        # åˆ†ç»„è´¨é‡
        if 'quality_by_group' in report.stats:
            content += "### åˆ†ç»„æ•°æ®è´¨é‡\n\n"
            content += "| ç»„åˆ« | æ€»è¡Œæ•° | å®Œæ•´è¡Œæ•° | å®Œæ•´ç‡ |\n"
            content += "|------|--------|---------|--------|\n"
            for group_id, stats in report.stats['quality_by_group'].items():
                content += f"| {group_id} | {stats['total_rows']} | {stats['complete_rows']} | {stats['complete_rate']:.1f}% |\n"
            content += "\n"

    content += "---\n\n"

    # å»ºè®®
    content += "## ğŸ’¡ å»ºè®®\n\n"
    if summary['errors'] == 0 and summary['warnings'] == 0:
        content += "æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥å¼€å§‹åˆ†æå·¥ä½œã€‚\n\n"
    elif summary['errors'] == 0:
        content += "æ•°æ®åŸºæœ¬å¯ç”¨ï¼Œä½†å»ºè®®å¤„ç†ä»¥ä¸‹è­¦å‘Šä»¥æé«˜æ•°æ®è´¨é‡ï¼š\n\n"
        for warning in report.warnings:
            content += f"- {warning['description']}\n"
        content += "\n"
    else:
        content += "**å¿…é¡»ä¿®å¤ä»¥ä¸‹é”™è¯¯æ‰èƒ½ç»§ç»­**ï¼š\n\n"
        for issue in report.issues:
            content += f"- {issue['description']}\n"
        content += "\n"

    content += "---\n\n"
    content += "**ç”Ÿæˆå·¥å…·**: validate_6groups_data.py  \n"
    content += "**å‚è€ƒæ–‡æ¡£**: [6GROUPS_DATA_DESIGN_CORRECT_20260115.md](6GROUPS_DATA_DESIGN_CORRECT_20260115.md)\n"

    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("6åˆ†ç»„æ•°æ®ç‹¬ç«‹éªŒè¯")
    print("="*60)
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")

    report = ValidationReport()

    try:
        # æ£€æŸ¥1: æ•°æ®å®Œæ•´æ€§
        group_data = check_data_completeness(report)

        if not group_data:
            print("\nâŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œç»ˆæ­¢éªŒè¯")
            return

        # æ£€æŸ¥2: Timestampå”¯ä¸€æ€§
        check_timestamp_uniqueness(group_data, report)

        # æ£€æŸ¥3: åˆ†ç»„æ­£ç¡®æ€§
        check_grouping_correctness(group_data, report)

        # æ£€æŸ¥4: å­—æ®µæ­£ç¡®æ€§
        check_field_correctness(group_data, report)

        # æ£€æŸ¥5: æ¨¡å‹å˜é‡ç¼–ç 
        check_model_encoding(group_data, report)

        # æ£€æŸ¥6: æ•°æ®è´¨é‡
        check_data_quality(group_data, report)

        # æ£€æŸ¥7: ç‰¹æ®Šæƒ…å†µ
        check_special_cases(group_data, report)

        # ç”ŸæˆæŠ¥å‘Š
        output_file = '/home/green/energy_dl/nightly/analysis/docs/reports/6GROUPS_DATA_VALIDATION_REPORT_20260115.md'
        generate_report(report, output_file)

        # æ‰“å°æ‘˜è¦
        summary = report.get_summary()
        print("\n" + "="*60)
        print("éªŒè¯å®Œæˆ")
        print("="*60)
        print(f"æ€»æ£€æŸ¥é¡¹: {summary['total_checks']}")
        print(f"é€šè¿‡: {summary['passed_checks']}")
        print(f"å¤±è´¥: {summary['failed_checks']}")
        print(f"é”™è¯¯: {summary['errors']}")
        print(f"è­¦å‘Š: {summary['warnings']}")
        print(f"æ€»ä½“è¯„åˆ†: {summary['overall_score']:.1f}/100")

        if summary['errors'] == 0:
            print("\nâœ… éªŒè¯é€šè¿‡ï¼")
            return 0
        else:
            print("\nâŒ éªŒè¯å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šäº†è§£è¯¦æƒ…")
            return 1

    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
