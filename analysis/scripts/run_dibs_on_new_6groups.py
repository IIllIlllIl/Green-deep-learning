#!/usr/bin/env python3
"""
æ–°6åˆ†ç»„æ•°æ®çš„DiBSå› æœåˆ†æè„šæœ¬

ç›®çš„: åœ¨æ–°ç”Ÿæˆçš„6ç»„æ•°æ®ï¼ˆåŸºäºdata.csvï¼‰ä¸Šæ‰§è¡ŒDiBSå› æœå‘ç°
æ•°æ®æº: analysis/data/energy_research/dibs_training/ (2026-01-15ç”Ÿæˆ)
ç ”ç©¶é—®é¢˜:
  - é—®é¢˜1: è¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“
  - é—®é¢˜2: èƒ½è€—å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡å…³ç³»
  - é—®é¢˜3: ä¸­é—´å˜é‡çš„ä¸­ä»‹æ•ˆåº”

åˆ›å»ºæ—¥æœŸ: 2026-01-15
åŸºäº: run_dibs_for_questions_2_3.py (æˆåŠŸé…ç½®: alpha=0.05, beta=0.1, particles=20)
"""

import numpy as np
import pandas as pd
import sys
import os
import time
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.causal_discovery import CausalGraphLearner

# ========== æœ€ä¼˜é…ç½®ï¼ˆåŸºäº2026-01-05å‚æ•°è°ƒä¼˜ç»“æœï¼‰ ==========
OPTIMAL_CONFIG = {
    "alpha_linear": 0.05,        # DiBSé»˜è®¤å€¼ï¼Œæ•ˆæœè‰¯å¥½
    "beta_linear": 0.1,          # ä½æ— ç¯çº¦æŸï¼Œå…è®¸æ›´å¤šè¾¹æ¢ç´¢ â­
    "n_particles": 20,           # æœ€ä½³æ€§ä»·æ¯”
    "tau": 1.0,                  # Gumbel-softmaxæ¸©åº¦
    "n_steps": 5000,             # è¶³å¤Ÿæ”¶æ•›
    "n_grad_mc_samples": 128,    # MCæ¢¯åº¦æ ·æœ¬æ•°
    "n_acyclicity_mc_samples": 32  # æ— ç¯æ€§MCæ ·æœ¬æ•°
}

# ========== æ–°6ä¸ªä»»åŠ¡ç»„é…ç½®ï¼ˆ2026-01-15æ›´æ–°ï¼‰ ==========
TASK_GROUPS = [
    {
        "id": "group1_examples",
        "name": "examplesï¼ˆå›¾åƒåˆ†ç±»-å°å‹ï¼‰",
        "csv_file": "group1_examples.csv",
        "expected_samples": 126,
        "expected_features": 18
    },
    {
        "id": "group2_vulberta",
        "name": "VulBERTaï¼ˆä»£ç æ¼æ´æ£€æµ‹ï¼‰",
        "csv_file": "group2_vulberta.csv",
        "expected_samples": 52,
        "expected_features": 16
    },
    {
        "id": "group3_person_reid",
        "name": "Person_reIDï¼ˆè¡Œäººé‡è¯†åˆ«ï¼‰",
        "csv_file": "group3_person_reid.csv",
        "expected_samples": 118,
        "expected_features": 19
    },
    {
        "id": "group4_bug_localization",
        "name": "bug-localizationï¼ˆç¼ºé™·å®šä½ï¼‰",
        "csv_file": "group4_bug_localization.csv",
        "expected_samples": 40,
        "expected_features": 17
    },
    {
        "id": "group5_mrt_oast",
        "name": "MRT-OASTï¼ˆç¼ºé™·å®šä½ï¼‰",
        "csv_file": "group5_mrt_oast.csv",
        "expected_samples": 46,
        "expected_features": 16
    },
    {
        "id": "group6_resnet",
        "name": "pytorch_resnetï¼ˆå›¾åƒåˆ†ç±»-ResNetï¼‰",
        "csv_file": "group6_resnet.csv",
        "expected_samples": 41,
        "expected_features": 18
    }
]


def load_task_group_data(task_config):
    """
    åŠ è½½å•ä¸ªä»»åŠ¡ç»„çš„æ•°æ®

    å‚æ•°:
        task_config: ä»»åŠ¡ç»„é…ç½®å­—å…¸

    è¿”å›:
        data: æ ‡å‡†åŒ–åçš„DataFrame
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    """
    data_dir = Path(__file__).parent.parent / "data" / "energy_research" / "dibs_training"
    data_file = data_dir / task_config["csv_file"]

    if not data_file.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_file)

    print(f"  æ•°æ®è§„æ¨¡: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
    print(f"  é¢„æœŸè§„æ¨¡: {task_config['expected_samples']}è¡Œ Ã— {task_config['expected_features']}åˆ—")

    if len(df) != task_config["expected_samples"]:
        print(f"  âš ï¸ è­¦å‘Š: æ ·æœ¬æ•°ä¸ç¬¦åˆé¢„æœŸï¼ˆé¢„æœŸ{task_config['expected_samples']}ï¼Œå®é™…{len(df)}ï¼‰")

    if len(df.columns) != task_config["expected_features"]:
        print(f"  âš ï¸ è­¦å‘Š: ç‰¹å¾æ•°ä¸ç¬¦åˆé¢„æœŸï¼ˆé¢„æœŸ{task_config['expected_features']}ï¼Œå®é™…{len(df.columns)}ï¼‰")

    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"  âš ï¸ è­¦å‘Š: å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼ï¼DiBSè¦æ±‚é›¶ç¼ºå¤±å€¼ã€‚")
        raise ValueError(f"æ•°æ®åŒ…å«ç¼ºå¤±å€¼ï¼ŒDiBSæ— æ³•è¿è¡Œ")

    # æ£€æŸ¥å¸¸é‡ç‰¹å¾ï¼ˆDiBSä¼šå´©æºƒï¼‰
    const_features = []
    for col in df.columns:
        if df[col].nunique() == 1:
            const_features.append(col)

    if const_features:
        print(f"  âš ï¸ è­¦å‘Š: å‘ç° {len(const_features)} ä¸ªå¸¸é‡ç‰¹å¾ï¼ˆå°†è¢«ç§»é™¤ï¼‰:")
        for col in const_features:
            print(f"    - {col} = {df[col].iloc[0]}")

        # ç§»é™¤å¸¸é‡ç‰¹å¾
        df = df.drop(columns=const_features)
        print(f"  âœ… ç§»é™¤å: {len(df.columns)}åˆ—")

    # ä¿å­˜ç‰¹å¾åç§°
    feature_names = df.columns.tolist()

    # è¿”å›DataFrameï¼ˆCausalGraphLearneræœŸæœ›DataFrameè¾“å…¥ï¼‰
    return df, feature_names


def classify_variables(feature_names):
    """
    å°†å˜é‡åˆ†ç±»ä¸ºï¼šè¶…å‚æ•°ã€æ€§èƒ½ã€èƒ½è€—ã€ä¸­ä»‹å˜é‡ã€å…¶ä»–

    å‚æ•°:
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨

    è¿”å›:
        classification: å­—å…¸ï¼ŒåŒ…å«å„ç±»å˜é‡çš„ç´¢å¼•
    """
    classification = {
        "hyperparams": [],      # è¶…å‚æ•°ï¼ˆXï¼‰
        "performance": [],      # æ€§èƒ½æŒ‡æ ‡ï¼ˆY_perfï¼‰
        "energy": [],           # èƒ½è€—æŒ‡æ ‡ï¼ˆY_energyï¼‰
        "mediators": [],        # ä¸­ä»‹å˜é‡ï¼ˆMï¼‰
        "others": []            # å…¶ä»–æ§åˆ¶å˜é‡
    }

    for idx, name in enumerate(feature_names):
        if name.startswith("hyperparam_"):
            classification["hyperparams"].append(idx)
        elif name.startswith("perf_"):
            classification["performance"].append(idx)
        elif name.startswith("energy_gpu") or name.startswith("energy_cpu"):
            # ç‰¹æ®Šå¤„ç†ï¼šGPUåˆ©ç”¨ç‡ã€æ¸©åº¦ã€åŠŸç‡æ˜¯ä¸­ä»‹å˜é‡
            if "util" in name or "temp" in name or "watts" in name:
                classification["mediators"].append(idx)
            else:
                classification["energy"].append(idx)
        elif name in ["duration_seconds", "retries", "num_mutated_params"]:
            classification["others"].append(idx)
        else:
            classification["others"].append(idx)

    return classification


def extract_research_question_1_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜1çš„è¯æ®ï¼šè¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“

    å‚æ•°:
        causal_graph: å› æœå›¾çŸ©é˜µ (n_vars Ã— n_vars)
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        var_classification: å˜é‡åˆ†ç±»å­—å…¸
        threshold: è¾¹å¼ºåº¦é˜ˆå€¼

    è¿”å›:
        evidence: åŒ…å«è¶…å‚æ•°æ•ˆåº”çš„å­—å…¸
    """
    evidence = {
        "direct_hyperparam_to_energy": [],
        "mediated_hyperparam_to_energy": []
    }

    hyperparam_vars = var_classification["hyperparams"]
    energy_vars = var_classification["energy"]
    mediator_vars = var_classification["mediators"]

    # 1. æ£€æµ‹è¶…å‚æ•°â†’èƒ½è€—çš„ç›´æ¥å› æœè¾¹
    for hp_idx in hyperparam_vars:
        for e_idx in energy_vars:
            strength = causal_graph[hp_idx, e_idx]
            if strength > threshold:
                evidence["direct_hyperparam_to_energy"].append({
                    "hyperparam": feature_names[hp_idx],
                    "energy_var": feature_names[e_idx],
                    "strength": float(strength)
                })

    # 2. æ£€æµ‹è¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—çš„é—´æ¥è·¯å¾„
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_hm > threshold and strength_me > threshold:
                    evidence["mediated_hyperparam_to_energy"].append({
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "energy_var": feature_names[e_idx],
                        "strength_step1": float(strength_hm),
                        "strength_step2": float(strength_me),
                        "indirect_strength": float(strength_hm * strength_me)
                    })

    return evidence


def extract_research_question_2_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜2çš„è¯æ®ï¼šèƒ½è€—-æ€§èƒ½æƒè¡¡å…³ç³»

    å‚æ•°:
        causal_graph: å› æœå›¾çŸ©é˜µ (n_vars Ã— n_vars)
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        var_classification: å˜é‡åˆ†ç±»å­—å…¸
        threshold: è¾¹å¼ºåº¦é˜ˆå€¼

    è¿”å›:
        evidence: åŒ…å«æƒè¡¡å…³ç³»è¯æ®çš„å­—å…¸
    """
    evidence = {
        "direct_edges_perf_to_energy": [],
        "direct_edges_energy_to_perf": [],
        "common_hyperparams": [],
        "mediated_tradeoffs": []
    }

    perf_vars = var_classification["performance"]
    energy_vars = var_classification["energy"]
    hyperparam_vars = var_classification["hyperparams"]
    mediator_vars = var_classification["mediators"]

    # 1. æ£€æµ‹æ€§èƒ½â†’èƒ½è€—çš„ç›´æ¥å› æœè¾¹
    for i in perf_vars:
        for j in energy_vars:
            strength = causal_graph[i, j]
            if strength > threshold:
                evidence["direct_edges_perf_to_energy"].append({
                    "from": feature_names[i],
                    "to": feature_names[j],
                    "strength": float(strength)
                })

    # 2. æ£€æµ‹èƒ½è€—â†’æ€§èƒ½çš„ç›´æ¥å› æœè¾¹
    for i in energy_vars:
        for j in perf_vars:
            strength = causal_graph[i, j]
            if strength > threshold:
                evidence["direct_edges_energy_to_perf"].append({
                    "from": feature_names[i],
                    "to": feature_names[j],
                    "strength": float(strength)
                })

    # 3. æ£€æµ‹åŒæ—¶å½±å“æ€§èƒ½å’Œèƒ½è€—çš„è¶…å‚æ•°ï¼ˆæƒè¡¡å€™é€‰ï¼‰
    for hp_idx in hyperparam_vars:
        hp_name = feature_names[hp_idx]

        # æ‰¾åˆ°è¯¥è¶…å‚æ•°å½±å“çš„æ€§èƒ½æŒ‡æ ‡
        perf_targets = []
        for p_idx in perf_vars:
            if causal_graph[hp_idx, p_idx] > threshold:
                perf_targets.append({
                    "var": feature_names[p_idx],
                    "strength": float(causal_graph[hp_idx, p_idx])
                })

        # æ‰¾åˆ°è¯¥è¶…å‚æ•°å½±å“çš„èƒ½è€—æŒ‡æ ‡
        energy_targets = []
        for e_idx in energy_vars:
            if causal_graph[hp_idx, e_idx] > threshold:
                energy_targets.append({
                    "var": feature_names[e_idx],
                    "strength": float(causal_graph[hp_idx, e_idx])
                })

        # å¦‚æœåŒæ—¶å½±å“æ€§èƒ½å’Œèƒ½è€—ï¼Œè®°å½•ä¸ºå…±åŒè¶…å‚æ•°
        if perf_targets and energy_targets:
            evidence["common_hyperparams"].append({
                "hyperparam": hp_name,
                "affects_performance": perf_targets,
                "affects_energy": energy_targets
            })

    # 4. æ£€æµ‹é€šè¿‡ä¸­ä»‹å˜é‡çš„é—´æ¥æƒè¡¡å…³ç³»
    for p_idx in perf_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_pm = causal_graph[p_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_pm > threshold and strength_me > threshold:
                    evidence["mediated_tradeoffs"].append({
                        "path": f"{feature_names[p_idx]} â†’ {feature_names[m_idx]} â†’ {feature_names[e_idx]}",
                        "strength_step1": float(strength_pm),
                        "strength_step2": float(strength_me),
                        "path_strength": float(strength_pm * strength_me)
                    })

    return evidence


def extract_research_question_3_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜3çš„è¯æ®ï¼šä¸­ä»‹æ•ˆåº”è·¯å¾„

    å‚æ•°:
        causal_graph: å› æœå›¾çŸ©é˜µ
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        var_classification: å˜é‡åˆ†ç±»å­—å…¸
        threshold: è¾¹å¼ºåº¦é˜ˆå€¼

    è¿”å›:
        evidence: åŒ…å«ä¸­ä»‹è·¯å¾„çš„å­—å…¸
    """
    evidence = {
        "mediation_paths_to_energy": [],
        "mediation_paths_to_performance": [],
        "multi_step_paths": []
    }

    hyperparam_vars = var_classification["hyperparams"]
    perf_vars = var_classification["performance"]
    energy_vars = var_classification["energy"]
    mediator_vars = var_classification["mediators"]

    # 1. ä¸‰èŠ‚ç‚¹è·¯å¾„ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹å˜é‡ â†’ èƒ½è€—
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_hm > threshold and strength_me > threshold:
                    direct_strength = causal_graph[hp_idx, e_idx]

                    evidence["mediation_paths_to_energy"].append({
                        "path_id": f"HP{hp_idx}_M{m_idx}_E{e_idx}",
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "outcome": feature_names[e_idx],
                        "strength_X_to_M": float(strength_hm),
                        "strength_M_to_Y": float(strength_me),
                        "indirect_strength": float(strength_hm * strength_me),
                        "direct_strength": float(direct_strength),
                        "mediation_type": "partial" if direct_strength > 0.01 else "full"
                    })

    # 2. ä¸‰èŠ‚ç‚¹è·¯å¾„ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹å˜é‡ â†’ æ€§èƒ½
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for p_idx in perf_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_mp = causal_graph[m_idx, p_idx]

                if strength_hm > threshold and strength_mp > threshold:
                    direct_strength = causal_graph[hp_idx, p_idx]

                    evidence["mediation_paths_to_performance"].append({
                        "path_id": f"HP{hp_idx}_M{m_idx}_P{p_idx}",
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "outcome": feature_names[p_idx],
                        "strength_X_to_M": float(strength_hm),
                        "strength_M_to_Y": float(strength_mp),
                        "indirect_strength": float(strength_hm * strength_mp),
                        "direct_strength": float(direct_strength),
                        "mediation_type": "partial" if direct_strength > 0.01 else "full"
                    })

    # 3. å››èŠ‚ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹1 â†’ ä¸­ä»‹2 â†’ èƒ½è€—/æ€§èƒ½
    for hp_idx in hyperparam_vars:
        for m1_idx in mediator_vars:
            for m2_idx in mediator_vars:
                if m1_idx == m2_idx:
                    continue

                strength_hm1 = causal_graph[hp_idx, m1_idx]
                strength_m1m2 = causal_graph[m1_idx, m2_idx]

                if strength_hm1 > threshold and strength_m1m2 > threshold:
                    for e_idx in energy_vars + perf_vars:
                        strength_m2y = causal_graph[m2_idx, e_idx]

                        if strength_m2y > threshold:
                            evidence["multi_step_paths"].append({
                                "path": f"{feature_names[hp_idx]} â†’ {feature_names[m1_idx]} â†’ {feature_names[m2_idx]} â†’ {feature_names[e_idx]}",
                                "strength_step1": float(strength_hm1),
                                "strength_step2": float(strength_m1m2),
                                "strength_step3": float(strength_m2y),
                                "path_strength": float(strength_hm1 * strength_m1m2 * strength_m2y)
                            })

    return evidence


def run_dibs_analysis(task_config, data, feature_names, config, output_dir):
    """
    è¿è¡Œå•ä¸ªä»»åŠ¡ç»„çš„DiBSåˆ†æ

    å‚æ•°:
        task_config: ä»»åŠ¡ç»„é…ç½®
        data: æ•°æ®DataFrame
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        config: DiBSé…ç½®
        output_dir: è¾“å‡ºç›®å½•

    è¿”å›:
        result: åˆ†æç»“æœå­—å…¸
    """
    task_id = task_config["id"]
    task_name = task_config["name"]

    print("\n" + "="*80)
    print(f"ä»»åŠ¡ç»„: {task_id} - {task_name}")
    print("="*80)

    # åˆ†ç±»å˜é‡
    var_classification = classify_variables(feature_names)

    print(f"\nå˜é‡åˆ†ç±»:")
    print(f"  è¶…å‚æ•°: {len(var_classification['hyperparams'])}ä¸ª")
    for idx in var_classification['hyperparams']:
        print(f"    - {feature_names[idx]}")
    print(f"  æ€§èƒ½æŒ‡æ ‡: {len(var_classification['performance'])}ä¸ª")
    for idx in var_classification['performance']:
        print(f"    - {feature_names[idx]}")
    print(f"  èƒ½è€—æŒ‡æ ‡: {len(var_classification['energy'])}ä¸ª")
    for idx in var_classification['energy']:
        print(f"    - {feature_names[idx]}")
    print(f"  ä¸­ä»‹å˜é‡: {len(var_classification['mediators'])}ä¸ª")
    for idx in var_classification['mediators']:
        print(f"    - {feature_names[idx]}")
    print(f"  å…¶ä»–å˜é‡: {len(var_classification['others'])}ä¸ª")

    # åˆ›å»ºDiBS learner
    learner = CausalGraphLearner(
        n_vars=len(feature_names),
        alpha=config["alpha_linear"],
        n_particles=config["n_particles"],
        beta=config["beta_linear"],
        tau=config["tau"],
        n_steps=config["n_steps"],
        n_grad_mc_samples=config["n_grad_mc_samples"],
        n_acyclicity_mc_samples=config["n_acyclicity_mc_samples"],
        random_seed=42
    )

    # æ‰§è¡ŒDiBS
    print(f"\næ‰§è¡ŒDiBSå› æœå‘ç°...")
    print(f"  alpha_linear: {config['alpha_linear']}")
    print(f"  beta_linear: {config['beta_linear']}")
    print(f"  n_particles: {config['n_particles']}")
    print(f"  n_steps: {config['n_steps']}")

    start_time = time.time()

    try:
        causal_graph = learner.fit(data, verbose=True)
        elapsed_time = time.time() - start_time
        success = True
        error_msg = None

        print(f"\nâœ… DiBSæ‰§è¡ŒæˆåŠŸï¼è€—æ—¶: {elapsed_time/60:.2f}åˆ†é’Ÿ")

    except Exception as e:
        elapsed_time = time.time() - start_time
        causal_graph = None
        success = False
        error_msg = str(e)
        print(f"\nâŒ DiBSæ‰§è¡Œå¤±è´¥: {error_msg}")

        # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ªä»¥è°ƒè¯•
        import traceback
        print("\nå®Œæ•´é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()

        return {
            "task_id": task_id,
            "task_name": task_name,
            "success": False,
            "elapsed_time_minutes": elapsed_time / 60,
            "error_message": error_msg
        }

    # åˆ†æå› æœå›¾
    graph_min = float(causal_graph.min())
    graph_max = float(causal_graph.max())
    graph_mean = float(causal_graph.mean())
    graph_std = float(causal_graph.std())

    # ä¸åŒé˜ˆå€¼ä¸‹çš„è¾¹æ•°
    edges_001 = int(np.sum(causal_graph > 0.01))
    edges_01 = int(np.sum(causal_graph > 0.1))
    edges_03 = int(np.sum(causal_graph > 0.3))
    edges_05 = int(np.sum(causal_graph > 0.5))

    print(f"\nå› æœå›¾ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {graph_min:.6f}")
    print(f"  æœ€å¤§å€¼: {graph_max:.6f}")
    print(f"  å¹³å‡å€¼: {graph_mean:.6f}")
    print(f"  æ ‡å‡†å·®: {graph_std:.6f}")
    print(f"\nè¾¹æ•°ç»Ÿè®¡:")
    print(f"  >0.01: {edges_001}æ¡")
    print(f"  >0.1:  {edges_01}æ¡")
    print(f"  >0.3:  {edges_03}æ¡ â­ å¼ºè¾¹")
    print(f"  >0.5:  {edges_05}æ¡")

    # æå–ç ”ç©¶é—®é¢˜è¯æ®
    print(f"\næå–ç ”ç©¶é—®é¢˜1è¯æ®ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼‰...")
    q1_evidence = extract_research_question_1_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  ç›´æ¥è¾¹ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼‰: {len(q1_evidence['direct_hyperparam_to_energy'])}æ¡")
    print(f"  é—´æ¥è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—ï¼‰: {len(q1_evidence['mediated_hyperparam_to_energy'])}æ¡")

    print(f"\næå–ç ”ç©¶é—®é¢˜2è¯æ®ï¼ˆèƒ½è€—-æ€§èƒ½æƒè¡¡ï¼‰...")
    q2_evidence = extract_research_question_2_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  ç›´æ¥è¾¹ï¼ˆæ€§èƒ½â†’èƒ½è€—ï¼‰: {len(q2_evidence['direct_edges_perf_to_energy'])}æ¡")
    print(f"  ç›´æ¥è¾¹ï¼ˆèƒ½è€—â†’æ€§èƒ½ï¼‰: {len(q2_evidence['direct_edges_energy_to_perf'])}æ¡")
    print(f"  å…±åŒè¶…å‚æ•°: {len(q2_evidence['common_hyperparams'])}ä¸ª")
    print(f"  ä¸­ä»‹æƒè¡¡è·¯å¾„: {len(q2_evidence['mediated_tradeoffs'])}æ¡")

    print(f"\næå–ç ”ç©¶é—®é¢˜3è¯æ®ï¼ˆä¸­ä»‹æ•ˆåº”ï¼‰...")
    q3_evidence = extract_research_question_3_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  è¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—: {len(q3_evidence['mediation_paths_to_energy'])}æ¡")
    print(f"  è¶…å‚æ•°â†’ä¸­ä»‹â†’æ€§èƒ½: {len(q3_evidence['mediation_paths_to_performance'])}æ¡")
    print(f"  å¤šæ­¥è·¯å¾„: {len(q3_evidence['multi_step_paths'])}æ¡")

    # ä¿å­˜å› æœå›¾çŸ©é˜µ
    graph_file = output_dir / f"{task_id}_causal_graph.npy"
    np.save(graph_file, causal_graph)
    print(f"\nâœ… å› æœå›¾çŸ©é˜µå·²ä¿å­˜: {graph_file}")

    # ä¿å­˜ç‰¹å¾åç§°
    names_file = output_dir / f"{task_id}_feature_names.json"
    with open(names_file, 'w') as f:
        json.dump(feature_names, f, indent=2, ensure_ascii=False)

    # æ„å»ºç»“æœå­—å…¸
    result = {
        "task_id": task_id,
        "task_name": task_name,
        "success": True,
        "elapsed_time_minutes": elapsed_time / 60,
        "n_samples": len(data),
        "n_features": len(feature_names),
        "variable_classification": {
            "n_hyperparams": len(var_classification["hyperparams"]),
            "n_performance": len(var_classification["performance"]),
            "n_energy": len(var_classification["energy"]),
            "n_mediators": len(var_classification["mediators"]),
            "hyperparam_names": [feature_names[i] for i in var_classification["hyperparams"]],
            "performance_names": [feature_names[i] for i in var_classification["performance"]],
            "energy_names": [feature_names[i] for i in var_classification["energy"]],
            "mediator_names": [feature_names[i] for i in var_classification["mediators"]]
        },
        "graph_stats": {
            "min": graph_min,
            "max": graph_max,
            "mean": graph_mean,
            "std": graph_std
        },
        "edges": {
            "threshold_0.01": edges_001,
            "threshold_0.1": edges_01,
            "threshold_0.3": edges_03,
            "threshold_0.5": edges_05
        },
        "question1_evidence": q1_evidence,
        "question2_evidence": q2_evidence,
        "question3_evidence": q3_evidence,
        "config": config,
        "feature_names": feature_names
    }

    # ä¿å­˜å•ä¸ªä»»åŠ¡ç»“æœ
    result_file = output_dir / f"{task_id}_result.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_file}")

    return result


def generate_summary_report(all_results, output_dir):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""

    report_file = output_dir / "NEW_6GROUPS_DIBS_ANALYSIS_REPORT.md"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# æ–°6åˆ†ç»„æ•°æ®DiBSå› æœåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**åˆ†ææ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æº**: data.csv (970è¡Œ) â†’ 6ç»„æ¸…æ´—æ•°æ® (423æ ·æœ¬)\n")
        f.write(f"**ä»»åŠ¡ç»„æ•°**: {len(all_results)}ä¸ª\n")
        f.write(f"**DiBSé…ç½®**: alpha=0.05, beta=0.1, particles=20, steps=5000\n\n")

        f.write("---\n\n")

        # æ€»ä½“ç»Ÿè®¡
        f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")

        successful = sum(1 for r in all_results if r['success'])
        total_time = sum(r['elapsed_time_minutes'] for r in all_results)

        f.write(f"- **æˆåŠŸä»»åŠ¡ç»„**: {successful}/{len(all_results)}\n")
        f.write(f"- **æ€»è€—æ—¶**: {total_time:.1f}åˆ†é’Ÿ ({total_time/60:.2f}å°æ—¶)\n")
        f.write(f"- **å¹³å‡è€—æ—¶**: {total_time/len(all_results):.1f}åˆ†é’Ÿ/ç»„\n\n")

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        f.write("## ğŸ“‹ ä»»åŠ¡ç»„è¯¦ç»†ç»“æœ\n\n")
        f.write("| ä»»åŠ¡ç»„ | çŠ¶æ€ | è€—æ—¶(åˆ†) | æ ·æœ¬æ•° | ç‰¹å¾æ•° | è¶…å‚æ•° | æ€§èƒ½ | èƒ½è€— | ä¸­ä»‹ | å¼ºè¾¹(>0.3) | æ€»è¾¹(>0.01) |\n")
        f.write("|--------|------|---------|-------|-------|--------|------|------|------|-----------|------------|\n")

        for r in all_results:
            status = "âœ… æˆåŠŸ" if r['success'] else "âŒ å¤±è´¥"

            if r['success']:
                var_class = r['variable_classification']
                f.write(f"| {r['task_name'][:25]} | {status} | "
                       f"{r['elapsed_time_minutes']:.1f} | {r['n_samples']} | "
                       f"{r['n_features']} | {var_class['n_hyperparams']} | "
                       f"{var_class['n_performance']} | {var_class['n_energy']} | "
                       f"{var_class['n_mediators']} | {r['edges']['threshold_0.3']} | "
                       f"{r['edges']['threshold_0.01']} |\n")
            else:
                f.write(f"| {r['task_name'][:25]} | {status} | "
                       f"{r['elapsed_time_minutes']:.1f} | - | - | - | - | - | - | - | - |\n")

        f.write("\n")

        successful_results = [r for r in all_results if r['success']]

        # é—®é¢˜1è¯æ®æ±‡æ€»
        f.write("## ğŸ¯ ç ”ç©¶é—®é¢˜1ï¼šè¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“\n\n")

        if not successful_results:
            f.write("âŒ æ‰€æœ‰ä»»åŠ¡ç»„DiBSåˆ†æå¤±è´¥ï¼Œæ— æ³•æå–è¯æ®ã€‚\n\n")
        else:
            total_direct_hp_to_energy = sum(len(r['question1_evidence']['direct_hyperparam_to_energy']) for r in successful_results)
            total_mediated_hp_to_energy = sum(len(r['question1_evidence']['mediated_hyperparam_to_energy']) for r in successful_results)

            f.write(f"### æ€»ä½“å‘ç°\n\n")
            f.write(f"- **ç›´æ¥å› æœè¾¹ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼‰**: {total_direct_hp_to_energy}æ¡\n")
            f.write(f"- **é—´æ¥è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—ï¼‰**: {total_mediated_hp_to_energy}æ¡\n")
            f.write(f"- **æ€»å› æœè·¯å¾„**: {total_direct_hp_to_energy + total_mediated_hp_to_energy}æ¡\n\n")

            # è¯¦ç»†åˆ—å‡ºç›´æ¥æ•ˆåº”
            if total_direct_hp_to_energy > 0:
                f.write(f"### è¶…å‚æ•°â†’èƒ½è€—ç›´æ¥æ•ˆåº” (Top 10)\n\n")
                f.write("| ä»»åŠ¡ç»„ | è¶…å‚æ•° | èƒ½è€—æŒ‡æ ‡ | å¼ºåº¦ |\n")
                f.write("|--------|--------|----------|------|\n")

                all_direct = []
                for r in successful_results:
                    for edge in r['question1_evidence']['direct_hyperparam_to_energy']:
                        all_direct.append({
                            "task": r['task_name'][:20],
                            "hp": edge['hyperparam'],
                            "energy": edge['energy_var'],
                            "strength": edge['strength']
                        })

                all_direct.sort(key=lambda x: x['strength'], reverse=True)

                for edge in all_direct[:10]:
                    f.write(f"| {edge['task']} | {edge['hp']} | {edge['energy']} | {edge['strength']:.4f} |\n")

                f.write("\n")

        # é—®é¢˜2è¯æ®æ±‡æ€»
        f.write("## ğŸ”„ ç ”ç©¶é—®é¢˜2ï¼šèƒ½è€—-æ€§èƒ½æƒè¡¡å…³ç³»\n\n")

        if not successful_results:
            f.write("âŒ æ‰€æœ‰ä»»åŠ¡ç»„DiBSåˆ†æå¤±è´¥ï¼Œæ— æ³•æå–è¯æ®ã€‚\n\n")
        else:
            total_direct_perf_to_energy = sum(len(r['question2_evidence']['direct_edges_perf_to_energy']) for r in successful_results)
            total_direct_energy_to_perf = sum(len(r['question2_evidence']['direct_edges_energy_to_perf']) for r in successful_results)
            total_common_hyperparams = sum(len(r['question2_evidence']['common_hyperparams']) for r in successful_results)
            total_mediated_tradeoffs = sum(len(r['question2_evidence']['mediated_tradeoffs']) for r in successful_results)

            f.write(f"### æ€»ä½“å‘ç°\n\n")
            f.write(f"- **ç›´æ¥å› æœè¾¹ï¼ˆæ€§èƒ½â†’èƒ½è€—ï¼‰**: {total_direct_perf_to_energy}æ¡\n")
            f.write(f"- **ç›´æ¥å› æœè¾¹ï¼ˆèƒ½è€—â†’æ€§èƒ½ï¼‰**: {total_direct_energy_to_perf}æ¡\n")
            f.write(f"- **å…±åŒè¶…å‚æ•°**: {total_common_hyperparams}ä¸ªï¼ˆåŒæ—¶å½±å“èƒ½è€—å’Œæ€§èƒ½ï¼‰\n")
            f.write(f"- **ä¸­ä»‹æƒè¡¡è·¯å¾„**: {total_mediated_tradeoffs}æ¡\n\n")

        # é—®é¢˜3è¯æ®æ±‡æ€»
        f.write("## ğŸ”¬ ç ”ç©¶é—®é¢˜3ï¼šä¸­ä»‹æ•ˆåº”è·¯å¾„\n\n")

        if not successful_results:
            f.write("âŒ æ‰€æœ‰ä»»åŠ¡ç»„DiBSåˆ†æå¤±è´¥ï¼Œæ— æ³•æå–è¯æ®ã€‚\n\n")
        else:
            total_mediation_to_energy = sum(len(r['question3_evidence']['mediation_paths_to_energy']) for r in successful_results)
            total_mediation_to_perf = sum(len(r['question3_evidence']['mediation_paths_to_performance']) for r in successful_results)
            total_multi_step = sum(len(r['question3_evidence']['multi_step_paths']) for r in successful_results)

            f.write(f"### æ€»ä½“å‘ç°\n\n")
            f.write(f"- **ä¸­ä»‹è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—ï¼‰**: {total_mediation_to_energy}æ¡\n")
            f.write(f"- **ä¸­ä»‹è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’æ€§èƒ½ï¼‰**: {total_mediation_to_perf}æ¡\n")
            f.write(f"- **å¤šæ­¥è·¯å¾„ï¼ˆâ‰¥4èŠ‚ç‚¹ï¼‰**: {total_multi_step}æ¡\n")
            f.write(f"- **æ€»ä¸­ä»‹è·¯å¾„**: {total_mediation_to_energy + total_mediation_to_perf + total_multi_step}æ¡\n\n")

        # ç»“è®º
        f.write("## ğŸ’¡ ç»“è®ºä¸ä¸‹ä¸€æ­¥\n\n")

        if successful:
            f.write(f"âœ… DiBSæˆåŠŸåœ¨{successful}/{len(all_results)}ä¸ªä»»åŠ¡ç»„ä¸Šå®Œæˆå› æœå‘ç°ã€‚\n\n")
            f.write("### ä¸‹ä¸€æ­¥å»ºè®®\n\n")
            f.write("1. ä½¿ç”¨å›å½’åˆ†æé‡åŒ–DiBSå‘ç°çš„å› æœè¾¹å¼ºåº¦\n")
            f.write("2. å¯¹ä¸­ä»‹è·¯å¾„è¿›è¡ŒSobelæ£€éªŒéªŒè¯\n")
            f.write("3. ç”Ÿæˆå› æœå›¾å¯è§†åŒ–\n")
            f.write("4. æ’°å†™ç ”ç©¶å‘ç°æŠ¥å‘Š\n")
        else:
            f.write("âŒ æ‰€æœ‰ä»»åŠ¡ç»„DiBSåˆ†æå¤±è´¥ã€‚\n\n")

        f.write("\n---\n\n")
        f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    print(f"\nâœ… æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return report_file


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("æ–°6åˆ†ç»„æ•°æ®DiBSå› æœåˆ†æ")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ•°æ®æº: data.csv (2026-01-15ç”Ÿæˆ)")
    print(f"ä»»åŠ¡ç»„æ•°: {len(TASK_GROUPS)}")
    print(f"DiBSé…ç½®: alpha={OPTIMAL_CONFIG['alpha_linear']}, beta={OPTIMAL_CONFIG['beta_linear']}, particles={OPTIMAL_CONFIG['n_particles']}")
    print("="*80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent.parent / "results" / "energy_research" / "new_6groups_dibs" / datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")

    # è¿è¡Œæ‰€æœ‰ä»»åŠ¡ç»„
    all_results = []

    for i, task_config in enumerate(TASK_GROUPS, 1):
        print(f"\n{'='*80}")
        print(f"è¿›åº¦: {i}/{len(TASK_GROUPS)}")
        print(f"{'='*80}")

        try:
            # åŠ è½½æ•°æ®
            print(f"\nåŠ è½½æ•°æ®: {task_config['name']}")
            data, feature_names = load_task_group_data(task_config)

            # è¿è¡ŒDiBSåˆ†æ
            result = run_dibs_analysis(
                task_config,
                data,
                feature_names,
                OPTIMAL_CONFIG,
                output_dir
            )

            all_results.append(result)

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­åˆ†æ")
            break

        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ç»„æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

            all_results.append({
                "task_id": task_config["id"],
                "task_name": task_config["name"],
                "success": False,
                "elapsed_time_minutes": 0,
                "error_message": str(e)
            })

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*80)
    print("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
    print("="*80)

    if all_results:
        report_file = generate_summary_report(all_results, output_dir)

        print(f"\n{'='*80}")
        print("âœ… DiBSåˆ†æå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"  æˆåŠŸä»»åŠ¡ç»„: {sum(1 for r in all_results if r['success'])}/{len(all_results)}")
        print(f"  ç»“æœç›®å½•: {output_dir}")
        print(f"  æ€»ç»“æŠ¥å‘Š: {report_file}")
        print(f"{'='*80}\n")
    else:
        print("\nâŒ æ²¡æœ‰å®Œæˆä»»ä½•ä»»åŠ¡ç»„")

    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
