#!/usr/bin/env python3
"""
6åˆ†ç»„äº¤äº’é¡¹æ•°æ®DiBSå› æœåˆ†æè„šæœ¬

ç›®çš„: åœ¨äº¤äº’é¡¹æ•°æ®ä¸Šæ‰§è¡ŒDiBSå› æœå‘ç°ï¼Œæ¢æµ‹è°ƒèŠ‚æ•ˆåº”
æ•°æ®æº: analysis/data/energy_research/6groups_interaction/
ç ”ç©¶é—®é¢˜:
  - é—®é¢˜1: è¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“ï¼ˆåŒ…æ‹¬è°ƒèŠ‚æ•ˆåº”ï¼‰
  - é—®é¢˜2: èƒ½è€—å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡å…³ç³»
  - é—®é¢˜3: ä¸­é—´å˜é‡çš„ä¸­ä»‹æ•ˆåº”

åˆ›å»ºæ—¥æœŸ: 2026-01-16
åŸºäº: run_dibs_6groups_final.py
æ–°å¢: äº¤äº’é¡¹æ”¯æŒ (è¶…å‚æ•° Ã— is_parallel)
"""

import numpy as np
import pandas as pd
import sys
import os
import time
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.causal_discovery import CausalGraphLearner

# ========== æœ€ä¼˜é…ç½®ï¼ˆåŸºäº2026-01-05å‚æ•°è°ƒä¼˜ç»“æœï¼‰ ==========
OPTIMAL_CONFIG = {
    "alpha_linear": 0.05,        # DiBSé»˜è®¤å€¼ï¼Œæ•ˆæœè‰¯å¥½
    "beta_linear": 0.1,          # ä½æ— ç¯çº¦æŸï¼Œå…è®¸æ›´å¤šè¾¹æ¢ç´¢ â­å…³é”®å‚æ•°
    "n_particles": 20,           # æœ€ä½³æ€§ä»·æ¯”
    "tau": 1.0,                  # Gumbel-softmaxæ¸©åº¦
    "n_steps": 5000,             # è¶³å¤Ÿæ”¶æ•›
    "n_grad_mc_samples": 128,    # MCæ¢¯åº¦æ ·æœ¬æ•°
    "n_acyclicity_mc_samples": 32  # æ— ç¯æ€§MCæ ·æœ¬æ•°
}

# ========== 6ä¸ªä»»åŠ¡ç»„é…ç½®ï¼ˆäº¤äº’é¡¹ç‰ˆæœ¬ï¼Œ2026-01-16ï¼‰==========
TASK_GROUPS = [
    {
        "id": "group1_examples",
        "name": "examplesï¼ˆå›¾åƒåˆ†ç±»-å°å‹ï¼‰",
        "csv_file": "group1_examples_interaction.csv",
        "expected_samples": 304,
        "expected_features": 24,  # 21 + 3äº¤äº’é¡¹
        "n_interaction_terms": 3
    },
    {
        "id": "group2_vulberta",
        "name": "VulBERTaï¼ˆä»£ç æ¼æ´æ£€æµ‹ï¼‰",
        "csv_file": "group2_vulberta_interaction.csv",
        "expected_samples": 72,
        "expected_features": 23,  # 20 + 3äº¤äº’é¡¹
        "n_interaction_terms": 3
    },
    {
        "id": "group3_person_reid",
        "name": "Person_reIDï¼ˆè¡Œäººé‡è¯†åˆ«ï¼‰",
        "csv_file": "group3_person_reid_interaction.csv",
        "expected_samples": 206,
        "expected_features": 25,  # 22 + 3äº¤äº’é¡¹
        "n_interaction_terms": 3
    },
    {
        "id": "group4_bug_localization",
        "name": "bug-localizationï¼ˆç¼ºé™·å®šä½ï¼‰",
        "csv_file": "group4_bug_localization_interaction.csv",
        "expected_samples": 90,
        "expected_features": 24,  # 21 + 3äº¤äº’é¡¹
        "n_interaction_terms": 3
    },
    {
        "id": "group5_mrt_oast",
        "name": "MRT-OASTï¼ˆç¼ºé™·å®šä½ï¼‰",
        "csv_file": "group5_mrt_oast_interaction.csv",
        "expected_samples": 72,
        "expected_features": 25,  # 21 + 4äº¤äº’é¡¹
        "n_interaction_terms": 4
    },
    {
        "id": "group6_resnet",
        "name": "pytorch_resnetï¼ˆå›¾åƒåˆ†ç±»-ResNetï¼‰",
        "csv_file": "group6_resnet_interaction.csv",
        "expected_samples": 74,
        "expected_features": 22,  # 19 + 3äº¤äº’é¡¹
        "n_interaction_terms": 3
    }
]


def load_task_group_data(task_config):
    """
    åŠ è½½å•ä¸ªä»»åŠ¡ç»„çš„äº¤äº’é¡¹æ•°æ®

    å‚æ•°:
        task_config: ä»»åŠ¡ç»„é…ç½®å­—å…¸

    è¿”å›:
        data: å¤„ç†åçš„DataFrame
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    """
    # ä½¿ç”¨äº¤äº’é¡¹æ•°æ®è·¯å¾„
    data_dir = Path(__file__).parent.parent / "data" / "energy_research" / "6groups_interaction"
    data_file = data_dir / task_config["csv_file"]

    if not data_file.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_file)

    print(f"  æ•°æ®è§„æ¨¡: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
    print(f"  é¢„æœŸè§„æ¨¡: {task_config['expected_samples']}è¡Œ Ã— {task_config['expected_features']}åˆ—")
    print(f"  äº¤äº’é¡¹æ•°: {task_config['n_interaction_terms']}ä¸ª")

    if len(df) != task_config["expected_samples"]:
        print(f"  âš ï¸ è­¦å‘Š: æ ·æœ¬æ•°ä¸ç¬¦åˆé¢„æœŸï¼ˆé¢„æœŸ{task_config['expected_samples']}ï¼Œå®é™…{len(df)}ï¼‰")

    if len(df.columns) != task_config["expected_features"]:
        print(f"  âš ï¸ è­¦å‘Š: ç‰¹å¾æ•°ä¸ç¬¦åˆé¢„æœŸï¼ˆé¢„æœŸ{task_config['expected_features']}ï¼Œå®é™…{len(df.columns)}ï¼‰")

    # æ£€æŸ¥ç¼ºå¤±å€¼ - DiBSè¦æ±‚é›¶ç¼ºå¤±å€¼
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"  âš ï¸ è­¦å‘Š: å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼ï¼")
        print(f"  å¤„ç†æ–¹å¼: ä½¿ç”¨åˆ—å‡å€¼å¡«å……...")

        # å¯¹æ¯ä¸€åˆ—ä½¿ç”¨å‡å€¼å¡«å……
        for col in df.columns:
            if df[col].isnull().any():
                missing_before = df[col].isnull().sum()
                df[col] = df[col].fillna(df[col].mean())
                print(f"    - {col}: {missing_before}ä¸ªç¼ºå¤±å€¼å·²å¡«å……")

        # å†æ¬¡æ£€æŸ¥
        remaining_missing = df.isnull().sum().sum()
        if remaining_missing > 0:
            raise ValueError(f"å¡«å……åä»æœ‰ {remaining_missing} ä¸ªç¼ºå¤±å€¼ï¼")
        else:
            print(f"  âœ… æ‰€æœ‰ç¼ºå¤±å€¼å·²å¡«å……")

    # ç§»é™¤timestampåˆ—ï¼ˆDiBSæ— æ³•å¤„ç†å­—ç¬¦ä¸²ï¼‰
    if 'timestamp' in df.columns:
        print(f"  âš ï¸ ç§»é™¤timestampåˆ—ï¼ˆDiBSæ— æ³•å¤„ç†å­—ç¬¦ä¸²ï¼‰")
        df = df.drop(columns=['timestamp'])

    # æ£€æŸ¥å¸¸é‡ç‰¹å¾ï¼ˆDiBSä¼šå´©æºƒï¼‰
    const_features = []
    for col in df.columns:
        if df[col].nunique() == 1:
            const_features.append(col)

    if const_features:
        print(f"  âš ï¸ è­¦å‘Š: å‘ç° {len(const_features)} ä¸ªå¸¸é‡ç‰¹å¾ï¼ˆå°†è¢«ç§»é™¤ï¼‰:")
        for col in const_features:
            print(f"    - {col} = {df[col].iloc[0]}")

        # ç§»é™¤å¸¸é‡ç‰¹å¾
        df = df.drop(columns=const_features)
        print(f"  âœ… ç§»é™¤å: {len(df.columns)}åˆ—")

    # ä¿å­˜ç‰¹å¾åç§°
    feature_names = df.columns.tolist()

    # éªŒè¯äº¤äº’é¡¹å­˜åœ¨
    interaction_terms = [col for col in feature_names if col.endswith('_x_is_parallel')]
    print(f"  âœ… æ£€æµ‹åˆ° {len(interaction_terms)} ä¸ªäº¤äº’é¡¹:")
    for term in interaction_terms:
        print(f"    - {term}")

    return df, feature_names


def classify_variables(feature_names):
    """
    å°†å˜é‡åˆ†ç±»ä¸ºï¼šè¶…å‚æ•°ã€æ€§èƒ½ã€èƒ½è€—ã€ä¸­ä»‹å˜é‡ã€äº¤äº’é¡¹ã€å…¶ä»–

    å‚æ•°:
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨

    è¿”å›:
        classification: å­—å…¸ï¼ŒåŒ…å«å„ç±»å˜é‡çš„ç´¢å¼•
    """
    classification = {
        "hyperparams": [],      # è¶…å‚æ•°ï¼ˆXï¼‰- ä¸åŒ…æ‹¬äº¤äº’é¡¹
        "performance": [],      # æ€§èƒ½æŒ‡æ ‡ï¼ˆY_perfï¼‰
        "energy": [],           # èƒ½è€—æŒ‡æ ‡ï¼ˆY_energyï¼‰
        "mediators": [],        # ä¸­ä»‹å˜é‡ï¼ˆMï¼‰
        "interactions": [],     # äº¤äº’é¡¹ï¼ˆX Ã— is_parallelï¼‰â­ æ–°å¢
        "others": []            # å…¶ä»–æ§åˆ¶å˜é‡
    }

    for idx, name in enumerate(feature_names):
        # â­ ä¼˜å…ˆè¯†åˆ«äº¤äº’é¡¹ï¼ˆé¿å…è¢«è¯¯åˆ†ç±»ä¸ºè¶…å‚æ•°ï¼‰
        if name.endswith("_x_is_parallel"):
            classification["interactions"].append(idx)
        elif name.startswith("hyperparam_") and not name.endswith("_seed"):
            classification["hyperparams"].append(idx)
        elif name.startswith("perf_"):
            classification["performance"].append(idx)
        elif name.startswith("energy_"):
            # åŒºåˆ†èƒ½è€—ç»“æœå˜é‡å’Œä¸­ä»‹å˜é‡
            if "util" in name or "temp" in name or "watts" in name or "avg" in name or "max" in name or "min" in name:
                classification["mediators"].append(idx)
            else:
                # ç„¦è€³ï¼ˆjoulesï¼‰æ˜¯èƒ½è€—ç»“æœå˜é‡
                classification["energy"].append(idx)
        elif name.startswith("model_"):
            # æ¨¡å‹å˜é‡è§†ä¸ºæ§åˆ¶å˜é‡
            classification["others"].append(idx)
        elif name in ["is_parallel", "timestamp"]:
            classification["others"].append(idx)
        else:
            classification["others"].append(idx)

    return classification


def extract_research_question_1_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜1çš„è¯æ®ï¼šè¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“ï¼ˆåŒ…æ‹¬è°ƒèŠ‚æ•ˆåº”ï¼‰

    å‚æ•°:
        causal_graph: å› æœå›¾çŸ©é˜µ (n_vars Ã— n_vars)
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        var_classification: å˜é‡åˆ†ç±»å­—å…¸
        threshold: è¾¹å¼ºåº¦é˜ˆå€¼

    è¿”å›:
        evidence: åŒ…å«è¶…å‚æ•°æ•ˆåº”å’Œè°ƒèŠ‚æ•ˆåº”çš„å­—å…¸
    """
    evidence = {
        "direct_hyperparam_to_energy": [],
        "mediated_hyperparam_to_energy": [],
        "moderation_effects": []  # â­ æ–°å¢ï¼šè°ƒèŠ‚æ•ˆåº”ï¼ˆäº¤äº’é¡¹â†’èƒ½è€—ï¼‰
    }

    hyperparam_vars = var_classification["hyperparams"]
    energy_vars = var_classification["energy"]
    mediator_vars = var_classification["mediators"]
    interaction_vars = var_classification["interactions"]  # â­ æ–°å¢

    # 1. æ£€æµ‹è¶…å‚æ•°â†’èƒ½è€—çš„ç›´æ¥å› æœè¾¹ï¼ˆä¸»æ•ˆåº”ï¼‰
    for hp_idx in hyperparam_vars:
        for e_idx in energy_vars:
            strength = causal_graph[hp_idx, e_idx]
            if strength > threshold:
                evidence["direct_hyperparam_to_energy"].append({
                    "hyperparam": feature_names[hp_idx],
                    "energy_var": feature_names[e_idx],
                    "strength": float(strength),
                    "effect_type": "main_effect"
                })

    # 2. æ£€æµ‹è¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—çš„é—´æ¥è·¯å¾„
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_hm > threshold and strength_me > threshold:
                    evidence["mediated_hyperparam_to_energy"].append({
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "energy_var": feature_names[e_idx],
                        "strength_step1": float(strength_hm),
                        "strength_step2": float(strength_me),
                        "indirect_strength": float(strength_hm * strength_me)
                    })

    # â­ 3. æ£€æµ‹è°ƒèŠ‚æ•ˆåº”ï¼šäº¤äº’é¡¹â†’èƒ½è€—
    for int_idx in interaction_vars:
        interaction_name = feature_names[int_idx]
        base_hyperparam = interaction_name.replace("_x_is_parallel", "")

        for e_idx in energy_vars:
            strength = causal_graph[int_idx, e_idx]
            if strength > threshold:
                # æ£€æŸ¥ä¸»æ•ˆåº”æ˜¯å¦å­˜åœ¨
                base_hp_idx = None
                for hp_idx in hyperparam_vars:
                    if feature_names[hp_idx] == base_hyperparam:
                        base_hp_idx = hp_idx
                        break

                main_effect_strength = causal_graph[base_hp_idx, e_idx] if base_hp_idx is not None else 0.0

                evidence["moderation_effects"].append({
                    "interaction_term": interaction_name,
                    "base_hyperparam": base_hyperparam,
                    "energy_var": feature_names[e_idx],
                    "moderation_strength": float(strength),
                    "main_effect_strength": float(main_effect_strength),
                    "interpretation": "is_parallelè°ƒèŠ‚äº†è¯¥è¶…å‚æ•°å¯¹èƒ½è€—çš„æ•ˆåº”" if strength > 0.3 else "å¼±è°ƒèŠ‚æ•ˆåº”"
                })

    return evidence


def extract_research_question_2_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜2çš„è¯æ®ï¼šèƒ½è€—-æ€§èƒ½æƒè¡¡å…³ç³»

    ï¼ˆä¸åŸç‰ˆç›¸åŒï¼Œæ— éœ€ä¿®æ”¹ï¼‰
    """
    evidence = {
        "direct_edges_perf_to_energy": [],
        "direct_edges_energy_to_perf": [],
        "common_hyperparams": [],
        "mediated_tradeoffs": []
    }

    perf_vars = var_classification["performance"]
    energy_vars = var_classification["energy"]
    hyperparam_vars = var_classification["hyperparams"]
    mediator_vars = var_classification["mediators"]

    # 1. æ£€æµ‹æ€§èƒ½â†’èƒ½è€—çš„ç›´æ¥å› æœè¾¹
    for i in perf_vars:
        for j in energy_vars:
            strength = causal_graph[i, j]
            if strength > threshold:
                evidence["direct_edges_perf_to_energy"].append({
                    "from": feature_names[i],
                    "to": feature_names[j],
                    "strength": float(strength)
                })

    # 2. æ£€æµ‹èƒ½è€—â†’æ€§èƒ½çš„ç›´æ¥å› æœè¾¹
    for i in energy_vars:
        for j in perf_vars:
            strength = causal_graph[i, j]
            if strength > threshold:
                evidence["direct_edges_energy_to_perf"].append({
                    "from": feature_names[i],
                    "to": feature_names[j],
                    "strength": float(strength)
                })

    # 3. æ£€æµ‹åŒæ—¶å½±å“æ€§èƒ½å’Œèƒ½è€—çš„è¶…å‚æ•°ï¼ˆæƒè¡¡å€™é€‰ï¼‰
    for hp_idx in hyperparam_vars:
        hp_name = feature_names[hp_idx]

        # æ‰¾åˆ°è¯¥è¶…å‚æ•°å½±å“çš„æ€§èƒ½æŒ‡æ ‡
        perf_targets = []
        for p_idx in perf_vars:
            if causal_graph[hp_idx, p_idx] > threshold:
                perf_targets.append({
                    "var": feature_names[p_idx],
                    "strength": float(causal_graph[hp_idx, p_idx])
                })

        # æ‰¾åˆ°è¯¥è¶…å‚æ•°å½±å“çš„èƒ½è€—æŒ‡æ ‡
        energy_targets = []
        for e_idx in energy_vars:
            if causal_graph[hp_idx, e_idx] > threshold:
                energy_targets.append({
                    "var": feature_names[e_idx],
                    "strength": float(causal_graph[hp_idx, e_idx])
                })

        # å¦‚æœåŒæ—¶å½±å“æ€§èƒ½å’Œèƒ½è€—ï¼Œè®°å½•ä¸ºå…±åŒè¶…å‚æ•°
        if perf_targets and energy_targets:
            evidence["common_hyperparams"].append({
                "hyperparam": hp_name,
                "affects_performance": perf_targets,
                "affects_energy": energy_targets
            })

    # 4. æ£€æµ‹é€šè¿‡ä¸­ä»‹å˜é‡çš„é—´æ¥æƒè¡¡å…³ç³»
    for p_idx in perf_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_pm = causal_graph[p_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_pm > threshold and strength_me > threshold:
                    evidence["mediated_tradeoffs"].append({
                        "path": f"{feature_names[p_idx]} â†’ {feature_names[m_idx]} â†’ {feature_names[e_idx]}",
                        "strength_step1": float(strength_pm),
                        "strength_step2": float(strength_me),
                        "path_strength": float(strength_pm * strength_me)
                    })

    return evidence


def extract_research_question_3_evidence(causal_graph, feature_names, var_classification, threshold=0.3):
    """
    æå–ç ”ç©¶é—®é¢˜3çš„è¯æ®ï¼šä¸­ä»‹æ•ˆåº”è·¯å¾„

    ï¼ˆä¸åŸç‰ˆç›¸åŒï¼Œæ— éœ€ä¿®æ”¹ï¼‰
    """
    evidence = {
        "mediation_paths_to_energy": [],
        "mediation_paths_to_performance": [],
        "multi_step_paths": []
    }

    hyperparam_vars = var_classification["hyperparams"]
    perf_vars = var_classification["performance"]
    energy_vars = var_classification["energy"]
    mediator_vars = var_classification["mediators"]

    # 1. ä¸‰èŠ‚ç‚¹è·¯å¾„ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹å˜é‡ â†’ èƒ½è€—
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for e_idx in energy_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_me = causal_graph[m_idx, e_idx]

                if strength_hm > threshold and strength_me > threshold:
                    direct_strength = causal_graph[hp_idx, e_idx]

                    evidence["mediation_paths_to_energy"].append({
                        "path_id": f"HP{hp_idx}_M{m_idx}_E{e_idx}",
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "outcome": feature_names[e_idx],
                        "strength_X_to_M": float(strength_hm),
                        "strength_M_to_Y": float(strength_me),
                        "indirect_strength": float(strength_hm * strength_me),
                        "direct_strength": float(direct_strength),
                        "mediation_type": "partial" if direct_strength > 0.01 else "full"
                    })

    # 2. ä¸‰èŠ‚ç‚¹è·¯å¾„ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹å˜é‡ â†’ æ€§èƒ½
    for hp_idx in hyperparam_vars:
        for m_idx in mediator_vars:
            for p_idx in perf_vars:
                strength_hm = causal_graph[hp_idx, m_idx]
                strength_mp = causal_graph[m_idx, p_idx]

                if strength_hm > threshold and strength_mp > threshold:
                    direct_strength = causal_graph[hp_idx, p_idx]

                    evidence["mediation_paths_to_performance"].append({
                        "path_id": f"HP{hp_idx}_M{m_idx}_P{p_idx}",
                        "hyperparam": feature_names[hp_idx],
                        "mediator": feature_names[m_idx],
                        "outcome": feature_names[p_idx],
                        "strength_X_to_M": float(strength_hm),
                        "strength_M_to_Y": float(strength_mp),
                        "indirect_strength": float(strength_hm * strength_mp),
                        "direct_strength": float(direct_strength),
                        "mediation_type": "partial" if direct_strength > 0.01 else "full"
                    })

    # 3. å››èŠ‚ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼šè¶…å‚æ•° â†’ ä¸­ä»‹1 â†’ ä¸­ä»‹2 â†’ èƒ½è€—/æ€§èƒ½
    for hp_idx in hyperparam_vars:
        for m1_idx in mediator_vars:
            for m2_idx in mediator_vars:
                if m1_idx == m2_idx:
                    continue

                strength_hm1 = causal_graph[hp_idx, m1_idx]
                strength_m1m2 = causal_graph[m1_idx, m2_idx]

                if strength_hm1 > threshold and strength_m1m2 > threshold:
                    for e_idx in energy_vars + perf_vars:
                        strength_m2y = causal_graph[m2_idx, e_idx]

                        if strength_m2y > threshold:
                            evidence["multi_step_paths"].append({
                                "path": f"{feature_names[hp_idx]} â†’ {feature_names[m1_idx]} â†’ {feature_names[m2_idx]} â†’ {feature_names[e_idx]}",
                                "strength_step1": float(strength_hm1),
                                "strength_step2": float(strength_m1m2),
                                "strength_step3": float(strength_m2y),
                                "path_strength": float(strength_hm1 * strength_m1m2 * strength_m2y)
                            })

    return evidence


def run_dibs_analysis(task_config, data, feature_names, config, output_dir):
    """
    è¿è¡Œå•ä¸ªä»»åŠ¡ç»„çš„DiBSåˆ†æï¼ˆäº¤äº’é¡¹ç‰ˆæœ¬ï¼‰
    """
    task_id = task_config["id"]
    task_name = task_config["name"]

    print("\n" + "="*80)
    print(f"ä»»åŠ¡ç»„: {task_id} - {task_name}")
    print("="*80)

    # åˆ†ç±»å˜é‡ï¼ˆåŒ…æ‹¬äº¤äº’é¡¹ï¼‰
    var_classification = classify_variables(feature_names)

    print(f"\nå˜é‡åˆ†ç±»:")
    print(f"  è¶…å‚æ•°ï¼ˆä¸»æ•ˆåº”ï¼‰: {len(var_classification['hyperparams'])}ä¸ª")
    for idx in var_classification['hyperparams']:
        print(f"    - {feature_names[idx]}")
    print(f"  â­ äº¤äº’é¡¹ï¼ˆè°ƒèŠ‚æ•ˆåº”ï¼‰: {len(var_classification['interactions'])}ä¸ª")
    for idx in var_classification['interactions']:
        print(f"    - {feature_names[idx]}")
    print(f"  æ€§èƒ½æŒ‡æ ‡: {len(var_classification['performance'])}ä¸ª")
    for idx in var_classification['performance']:
        print(f"    - {feature_names[idx]}")
    print(f"  èƒ½è€—æŒ‡æ ‡: {len(var_classification['energy'])}ä¸ª")
    for idx in var_classification['energy']:
        print(f"    - {feature_names[idx]}")
    print(f"  ä¸­ä»‹å˜é‡: {len(var_classification['mediators'])}ä¸ª")
    for idx in var_classification['mediators']:
        print(f"    - {feature_names[idx]}")
    print(f"  å…¶ä»–å˜é‡: {len(var_classification['others'])}ä¸ª")

    # åˆ›å»ºDiBS learner
    learner = CausalGraphLearner(
        n_vars=len(feature_names),
        alpha=config["alpha_linear"],
        n_particles=config["n_particles"],
        beta=config["beta_linear"],
        tau=config["tau"],
        n_steps=config["n_steps"],
        n_grad_mc_samples=config["n_grad_mc_samples"],
        n_acyclicity_mc_samples=config["n_acyclicity_mc_samples"],
        random_seed=42
    )

    # æ‰§è¡ŒDiBS
    print(f"\næ‰§è¡ŒDiBSå› æœå‘ç°...")
    print(f"  alpha_linear: {config['alpha_linear']}")
    print(f"  beta_linear: {config['beta_linear']} â­ å…³é”®å‚æ•°")
    print(f"  n_particles: {config['n_particles']}")
    print(f"  n_steps: {config['n_steps']}")

    start_time = time.time()

    try:
        causal_graph = learner.fit(data, verbose=True)
        elapsed_time = time.time() - start_time
        success = True
        error_msg = None

        print(f"\nâœ… DiBSæ‰§è¡ŒæˆåŠŸï¼è€—æ—¶: {elapsed_time/60:.2f}åˆ†é’Ÿ")

    except Exception as e:
        elapsed_time = time.time() - start_time
        causal_graph = None
        success = False
        error_msg = str(e)
        print(f"\nâŒ DiBSæ‰§è¡Œå¤±è´¥: {error_msg}")

        # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ªä»¥è°ƒè¯•
        import traceback
        print("\nå®Œæ•´é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()

        return {
            "task_id": task_id,
            "task_name": task_name,
            "success": False,
            "elapsed_time_minutes": elapsed_time / 60,
            "error_message": error_msg
        }

    # åˆ†æå› æœå›¾
    graph_min = float(causal_graph.min())
    graph_max = float(causal_graph.max())
    graph_mean = float(causal_graph.mean())
    graph_std = float(causal_graph.std())

    # ä¸åŒé˜ˆå€¼ä¸‹çš„è¾¹æ•°
    edges_001 = int(np.sum(causal_graph > 0.01))
    edges_01 = int(np.sum(causal_graph > 0.1))
    edges_03 = int(np.sum(causal_graph > 0.3))
    edges_05 = int(np.sum(causal_graph > 0.5))

    print(f"\nå› æœå›¾ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {graph_min:.6f}")
    print(f"  æœ€å¤§å€¼: {graph_max:.6f}")
    print(f"  å¹³å‡å€¼: {graph_mean:.6f}")
    print(f"  æ ‡å‡†å·®: {graph_std:.6f}")
    print(f"\nè¾¹æ•°ç»Ÿè®¡:")
    print(f"  >0.01: {edges_001}æ¡")
    print(f"  >0.1:  {edges_01}æ¡")
    print(f"  >0.3:  {edges_03}æ¡ â­ å¼ºè¾¹")
    print(f"  >0.5:  {edges_05}æ¡")

    # æå–ç ”ç©¶é—®é¢˜è¯æ®
    print(f"\næå–ç ”ç©¶é—®é¢˜1è¯æ®ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼ŒåŒ…æ‹¬è°ƒèŠ‚æ•ˆåº”ï¼‰...")
    q1_evidence = extract_research_question_1_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  ä¸»æ•ˆåº”ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼‰: {len(q1_evidence['direct_hyperparam_to_energy'])}æ¡")
    print(f"  â­ è°ƒèŠ‚æ•ˆåº”ï¼ˆäº¤äº’é¡¹â†’èƒ½è€—ï¼‰: {len(q1_evidence['moderation_effects'])}æ¡")
    print(f"  é—´æ¥è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—ï¼‰: {len(q1_evidence['mediated_hyperparam_to_energy'])}æ¡")

    print(f"\næå–ç ”ç©¶é—®é¢˜2è¯æ®ï¼ˆèƒ½è€—-æ€§èƒ½æƒè¡¡ï¼‰...")
    q2_evidence = extract_research_question_2_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  ç›´æ¥è¾¹ï¼ˆæ€§èƒ½â†’èƒ½è€—ï¼‰: {len(q2_evidence['direct_edges_perf_to_energy'])}æ¡")
    print(f"  ç›´æ¥è¾¹ï¼ˆèƒ½è€—â†’æ€§èƒ½ï¼‰: {len(q2_evidence['direct_edges_energy_to_perf'])}æ¡")
    print(f"  å…±åŒè¶…å‚æ•°: {len(q2_evidence['common_hyperparams'])}ä¸ª")
    print(f"  ä¸­ä»‹æƒè¡¡è·¯å¾„: {len(q2_evidence['mediated_tradeoffs'])}æ¡")

    print(f"\næå–ç ”ç©¶é—®é¢˜3è¯æ®ï¼ˆä¸­ä»‹æ•ˆåº”ï¼‰...")
    q3_evidence = extract_research_question_3_evidence(
        causal_graph, feature_names, var_classification, threshold=0.3
    )
    print(f"  è¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—: {len(q3_evidence['mediation_paths_to_energy'])}æ¡")
    print(f"  è¶…å‚æ•°â†’ä¸­ä»‹â†’æ€§èƒ½: {len(q3_evidence['mediation_paths_to_performance'])}æ¡")
    print(f"  å¤šæ­¥è·¯å¾„: {len(q3_evidence['multi_step_paths'])}æ¡")

    # ä¿å­˜å› æœå›¾çŸ©é˜µ
    graph_file = output_dir / f"{task_id}_causal_graph.npy"
    np.save(graph_file, causal_graph)
    print(f"\nâœ… å› æœå›¾çŸ©é˜µå·²ä¿å­˜: {graph_file}")

    # ä¿å­˜ç‰¹å¾åç§°
    names_file = output_dir / f"{task_id}_feature_names.json"
    with open(names_file, 'w') as f:
        json.dump(feature_names, f, indent=2, ensure_ascii=False)

    # æ„å»ºç»“æœå­—å…¸
    result = {
        "task_id": task_id,
        "task_name": task_name,
        "success": True,
        "elapsed_time_minutes": elapsed_time / 60,
        "n_samples": len(data),
        "n_features": len(feature_names),
        "variable_classification": {
            "n_hyperparams": len(var_classification["hyperparams"]),
            "n_interactions": len(var_classification["interactions"]),  # â­ æ–°å¢
            "n_performance": len(var_classification["performance"]),
            "n_energy": len(var_classification["energy"]),
            "n_mediators": len(var_classification["mediators"]),
            "hyperparam_names": [feature_names[i] for i in var_classification["hyperparams"]],
            "interaction_names": [feature_names[i] for i in var_classification["interactions"]],  # â­ æ–°å¢
            "performance_names": [feature_names[i] for i in var_classification["performance"]],
            "energy_names": [feature_names[i] for i in var_classification["energy"]],
            "mediator_names": [feature_names[i] for i in var_classification["mediators"]]
        },
        "graph_stats": {
            "min": graph_min,
            "max": graph_max,
            "mean": graph_mean,
            "std": graph_std
        },
        "edges": {
            "threshold_0.01": edges_001,
            "threshold_0.1": edges_01,
            "threshold_0.3": edges_03,
            "threshold_0.5": edges_05
        },
        "question1_evidence": q1_evidence,
        "question2_evidence": q2_evidence,
        "question3_evidence": q3_evidence,
        "config": config,
        "feature_names": feature_names
    }

    # ä¿å­˜å•ä¸ªä»»åŠ¡ç»“æœ
    result_file = output_dir / f"{task_id}_result.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_file}")

    return result


def generate_summary_report(all_results, output_dir):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼ˆäº¤äº’é¡¹ç‰ˆæœ¬ï¼‰"""

    report_file = output_dir / "DIBS_INTERACTION_ANALYSIS_REPORT.md"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# 6åˆ†ç»„äº¤äº’é¡¹æ•°æ®DiBSå› æœåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**åˆ†ææ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ•°æ®æº**: analysis/data/energy_research/6groups_interaction/\n")
        f.write(f"**æ•°æ®ç±»å‹**: æ ‡å‡†åŒ– + äº¤äº’é¡¹ (è¶…å‚æ•° Ã— is_parallel)\n")
        f.write(f"**ä»»åŠ¡ç»„æ•°**: {len(all_results)}ä¸ª\n")
        f.write(f"**DiBSé…ç½®**: alpha=0.05, beta=0.1 â­, particles=20, steps=5000\n\n")

        f.write("---\n\n")

        # æ€»ä½“ç»Ÿè®¡
        f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")

        successful = sum(1 for r in all_results if r['success'])
        total_time = sum(r['elapsed_time_minutes'] for r in all_results)

        f.write(f"- **æˆåŠŸä»»åŠ¡ç»„**: {successful}/{len(all_results)}\n")
        f.write(f"- **æ€»è€—æ—¶**: {total_time:.1f}åˆ†é’Ÿ ({total_time/60:.2f}å°æ—¶)\n")
        f.write(f"- **å¹³å‡è€—æ—¶**: {total_time/len(all_results):.1f}åˆ†é’Ÿ/ç»„\n\n")

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        f.write("## ğŸ“‹ ä»»åŠ¡ç»„è¯¦ç»†ç»“æœ\n\n")
        f.write("| ä»»åŠ¡ç»„ | çŠ¶æ€ | è€—æ—¶(åˆ†) | æ ·æœ¬æ•° | ç‰¹å¾æ•° | è¶…å‚æ•° | äº¤äº’é¡¹â­ | æ€§èƒ½ | èƒ½è€— | å¼ºè¾¹(>0.3) |\n")
        f.write("|--------|------|---------|-------|-------|--------|----------|------|------|----------|\n")

        for r in all_results:
            status = "âœ… æˆåŠŸ" if r['success'] else "âŒ å¤±è´¥"

            if r['success']:
                var_class = r['variable_classification']
                f.write(f"| {r['task_name'][:25]} | {status} | "
                       f"{r['elapsed_time_minutes']:.1f} | {r['n_samples']} | "
                       f"{r['n_features']} | {var_class['n_hyperparams']} | "
                       f"{var_class['n_interactions']} | "
                       f"{var_class['n_performance']} | {var_class['n_energy']} | "
                       f"{r['edges']['threshold_0.3']} |\n")
            else:
                f.write(f"| {r['task_name'][:25]} | {status} | "
                       f"{r['elapsed_time_minutes']:.1f} | - | - | - | - | - | - | - |\n")

        f.write("\n")

        successful_results = [r for r in all_results if r['success']]

        # é—®é¢˜1è¯æ®æ±‡æ€»ï¼ˆåŒ…æ‹¬è°ƒèŠ‚æ•ˆåº”ï¼‰
        f.write("## ğŸ¯ ç ”ç©¶é—®é¢˜1ï¼šè¶…å‚æ•°å¯¹èƒ½è€—çš„å½±å“ï¼ˆåŒ…æ‹¬è°ƒèŠ‚æ•ˆåº”ï¼‰\n\n")

        if not successful_results:
            f.write("âŒ æ‰€æœ‰ä»»åŠ¡ç»„DiBSåˆ†æå¤±è´¥ï¼Œæ— æ³•æå–è¯æ®ã€‚\n\n")
        else:
            total_main_effects = sum(len(r['question1_evidence']['direct_hyperparam_to_energy']) for r in successful_results)
            total_moderation_effects = sum(len(r['question1_evidence']['moderation_effects']) for r in successful_results)
            total_mediated = sum(len(r['question1_evidence']['mediated_hyperparam_to_energy']) for r in successful_results)

            f.write(f"### æ€»ä½“å‘ç°\n\n")
            f.write(f"- **ä¸»æ•ˆåº”ï¼ˆè¶…å‚æ•°â†’èƒ½è€—ï¼‰**: {total_main_effects}æ¡\n")
            f.write(f"- **â­ è°ƒèŠ‚æ•ˆåº”ï¼ˆäº¤äº’é¡¹â†’èƒ½è€—ï¼‰**: {total_moderation_effects}æ¡\n")
            f.write(f"- **é—´æ¥è·¯å¾„ï¼ˆè¶…å‚æ•°â†’ä¸­ä»‹â†’èƒ½è€—ï¼‰**: {total_mediated}æ¡\n")
            f.write(f"- **æ€»å› æœè·¯å¾„**: {total_main_effects + total_moderation_effects + total_mediated}æ¡\n\n")

            # è°ƒèŠ‚æ•ˆåº”è¯¦æƒ…
            if total_moderation_effects > 0:
                f.write(f"### â­ è°ƒèŠ‚æ•ˆåº”åˆ†æ (Top 10)\n\n")
                f.write("| ä»»åŠ¡ç»„ | äº¤äº’é¡¹ | èƒ½è€—æŒ‡æ ‡ | è°ƒèŠ‚å¼ºåº¦ | ä¸»æ•ˆåº”å¼ºåº¦ |\n")
                f.write("|--------|--------|----------|---------|----------|\n")

                all_moderations = []
                for r in successful_results:
                    for mod in r['question1_evidence']['moderation_effects']:
                        all_moderations.append({
                            "task": r['task_name'][:20],
                            "interaction": mod['interaction_term'],
                            "energy": mod['energy_var'],
                            "mod_strength": mod['moderation_strength'],
                            "main_strength": mod['main_effect_strength']
                        })

                all_moderations.sort(key=lambda x: x['mod_strength'], reverse=True)

                for mod in all_moderations[:10]:
                    f.write(f"| {mod['task']} | {mod['interaction']} | {mod['energy']} | "
                           f"{mod['mod_strength']:.4f} | {mod['main_strength']:.4f} |\n")

                f.write("\n")

        # é—®é¢˜2å’Œé—®é¢˜3ä¿æŒä¸å˜...
        f.write("## ğŸ”„ ç ”ç©¶é—®é¢˜2ï¼šèƒ½è€—-æ€§èƒ½æƒè¡¡å…³ç³»\n\n")
        f.write("ï¼ˆä¸åŸå§‹åˆ†æç›¸åŒï¼Œæœªå—äº¤äº’é¡¹å½±å“ï¼‰\n\n")

        f.write("## ğŸ”¬ ç ”ç©¶é—®é¢˜3ï¼šä¸­ä»‹æ•ˆåº”è·¯å¾„\n\n")
        f.write("ï¼ˆä¸åŸå§‹åˆ†æç›¸åŒï¼Œæœªå—äº¤äº’é¡¹å½±å“ï¼‰\n\n")

        # ç»“è®º
        f.write("## ğŸ’¡ ç»“è®ºä¸ä¸‹ä¸€æ­¥\n\n")

        if successful:
            f.write(f"âœ… DiBSæˆåŠŸåœ¨{successful}/{len(all_results)}ä¸ªä»»åŠ¡ç»„ä¸Šå®Œæˆå› æœå‘ç°ã€‚\n\n")
            f.write("### äº¤äº’é¡¹æ–¹æ¡ˆå…³é”®å‘ç°\n\n")
            f.write("1. DiBSèƒ½å¤Ÿè¯†åˆ«äº¤äº’é¡¹ï¼ˆè¶…å‚æ•° Ã— is_parallelï¼‰å¯¹èƒ½è€—çš„å½±å“\n")
            f.write("2. è°ƒèŠ‚æ•ˆåº”æ­ç¤ºäº†å¹¶è¡Œæ¨¡å¼å¦‚ä½•æ”¹å˜è¶…å‚æ•°çš„å› æœä½œç”¨\n")
            f.write("3. ä¸»æ•ˆåº”å’Œè°ƒèŠ‚æ•ˆåº”å¯ä»¥åŒæ—¶å­˜åœ¨ï¼Œæä¾›å®Œæ•´çš„å› æœå›¾æ™¯\n\n")

        f.write("\n---\n\n")
        f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ä½¿ç”¨è„šæœ¬**: run_dibs_6groups_interaction.py\n")
        f.write(f"**æ–¹æ³•å­¦å‚è€ƒ**: docs/INTERACTION_TERMS_TRANSFORMATION_PLAN.md\n")

    print(f"\nâœ… æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return report_file


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("6åˆ†ç»„äº¤äº’é¡¹æ•°æ®DiBSå› æœåˆ†æ")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ•°æ®æº: analysis/data/energy_research/6groups_interaction/")
    print(f"ä»»åŠ¡ç»„æ•°: {len(TASK_GROUPS)}")
    print(f"DiBSé…ç½®: alpha={OPTIMAL_CONFIG['alpha_linear']}, beta={OPTIMAL_CONFIG['beta_linear']} â­")
    print(f"â­ æ–°ç‰¹æ€§: æ”¯æŒäº¤äº’é¡¹ï¼ˆè¶…å‚æ•° Ã— is_parallelï¼‰åˆ†æè°ƒèŠ‚æ•ˆåº”")
    print("="*80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent.parent / "results" / "energy_research" / "dibs_interaction" / datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")

    # è¿è¡Œæ‰€æœ‰ä»»åŠ¡ç»„
    all_results = []

    for i, task_config in enumerate(TASK_GROUPS, 1):
        print(f"\n{'='*80}")
        print(f"è¿›åº¦: {i}/{len(TASK_GROUPS)}")
        print(f"{'='*80}")

        try:
            # åŠ è½½æ•°æ®
            print(f"\nåŠ è½½æ•°æ®: {task_config['name']}")
            data, feature_names = load_task_group_data(task_config)

            # è¿è¡ŒDiBSåˆ†æ
            result = run_dibs_analysis(
                task_config,
                data,
                feature_names,
                OPTIMAL_CONFIG,
                output_dir
            )

            all_results.append(result)

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­åˆ†æ")
            break

        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ç»„æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

            all_results.append({
                "task_id": task_config["id"],
                "task_name": task_config["name"],
                "success": False,
                "elapsed_time_minutes": 0,
                "error_message": str(e)
            })

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*80)
    print("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
    print("="*80)

    if all_results:
        report_file = generate_summary_report(all_results, output_dir)

        print(f"\n{'='*80}")
        print("âœ… DiBSåˆ†æå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"  æˆåŠŸä»»åŠ¡ç»„: {sum(1 for r in all_results if r['success'])}/{len(all_results)}")
        print(f"  ç»“æœç›®å½•: {output_dir}")
        print(f"  æ€»ç»“æŠ¥å‘Š: {report_file}")
        print(f"{'='*80}\n")
    else:
        print("\nâŒ æ²¡æœ‰å®Œæˆä»»ä½•ä»»åŠ¡ç»„")

    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
