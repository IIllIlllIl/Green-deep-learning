"""
DML (Double Machine Learning) 因果推断模块
实现论文中的因果效应估计方法

基于EconML库实现
论文参考: Chernozhukov et al. (2018) - Double/Debiased Machine Learning
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import warnings


class CausalInferenceEngine:
    """
    使用DML进行因果推断

    DML通过两阶段估计消除混淆偏差，提供无偏的平均处理效应(ATE)估计。

    参数:
        verbose: 是否输出详细信息

    示例:
        >>> engine = CausalInferenceEngine()
        >>> ate, ci = engine.estimate_ate(data, 'alpha', 'Te_Acc', confounders)
        >>> print(f"ATE: {ate:.3f}, 95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]")
    """

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.dml_models = {}  # 存储已拟合的模型
        self.ate_results = {}  # 存储ATE结果

    def estimate_ate(self,
                     data: pd.DataFrame,
                     treatment: str,
                     outcome: str,
                     confounders: List[str],
                     controls: Optional[List[str]] = None,
                     ref_df: Optional[pd.DataFrame] = None,
                     T0: Optional[float] = None,
                     T1: Optional[float] = None,
                     t_strategy: Optional[str] = None) -> Tuple[float, Tuple[float, float]]:
        """
        估计平均处理效应(ATE)

        使用DML方法估计干预(treatment)对结果(outcome)的因果效应。

        参数:
            data: 数据框
            treatment: 干预变量列名（如'alpha'）
            outcome: 结果变量列名（如'Te_Acc'）
            confounders: 混淆因素列名列表
            controls: 控制变量列名列表（可选）

        返回:
            (ate, (ci_lower, ci_upper))
            ate: 平均处理效应
            ci: 95%置信区间

        Raises:
            ValueError: 如果输入数据无效
            ImportError: 如果EconML未安装
        """
        # 输入验证
        if data is None or len(data) == 0:
            raise ValueError("data不能为空")
        if treatment not in data.columns:
            raise ValueError(f"treatment '{treatment}' 不在数据中")
        if outcome not in data.columns:
            raise ValueError(f"outcome '{outcome}' 不在数据中")
        for conf in confounders:
            if conf not in data.columns:
                raise ValueError(f"confounder '{conf}' 不在数据中")

        if self.verbose:
            print(f"\n估计因果效应: {treatment} → {outcome}")
            print(f"  混淆因素: {len(confounders)}个")
            print(f"  样本数: {len(data)}")

        try:
            from econml.dml import LinearDML

            # 准备数据
            T = data[treatment].values  # 干预变量
            Y = data[outcome].values    # 结果变量
            X = data[confounders].values if confounders else None  # 混淆因素
            W = data[controls].values if controls else None  # 控制变量

            # 检查数据变异性
            if len(np.unique(T)) < 2:
                warnings.warn(f"干预变量{treatment}缺乏变异性")
                return 0.0, (0.0, 0.0)

            # 创建和拟合DML模型
            dml = LinearDML(
                model_y='auto',  # 自动选择Y模型
                model_t='auto',  # 自动选择T模型
                random_state=42
            )

            dml.fit(Y, T, X=X, W=W)

            # 准备ATE计算参数（支持ref_df和T0/T1）
            # 确定使用哪个数据集计算ATE
            if ref_df is not None:
                # 使用ref_df（CTF风格）
                X_for_ate = ref_df[confounders].values if confounders else None
                if self.verbose:
                    print(f"  使用ref_df计算ATE (n={len(ref_df)})")
            else:
                # 使用原始数据
                X_for_ate = X
                if self.verbose:
                    print(f"  使用原始数据计算ATE (n={len(data)})")

            # 计算T0和T1（如果未提供）
            if T0 is None or T1 is None:
                if t_strategy == 'quantile':
                    # 使用25/75分位数（推荐）
                    T0_calc = data[treatment].quantile(0.25)
                    T1_calc = data[treatment].quantile(0.75)
                elif t_strategy == 'min_max':
                    # 使用最小值/最大值
                    T0_calc = data[treatment].min()
                    T1_calc = data[treatment].max()
                elif t_strategy == 'mean_std':
                    # 使用均值±标准差
                    mean_val = data[treatment].mean()
                    std_val = data[treatment].std()
                    T0_calc = mean_val - std_val
                    T1_calc = mean_val + std_val
                else:
                    # 默认：不使用T0/T1
                    T0_calc, T1_calc = None, None

                if self.verbose and (T0 is None or T1 is None):
                    if T0_calc is not None:
                        print(f"  计算T0/T1: {T0_calc:.4f} / {T1_calc:.4f} (策略={t_strategy or 'default'})")
                        # 验证T1 > T0
                        if T1_calc <= T0_calc:
                            warnings.warn(f"T1 ({T1_calc}) <= T0 ({T0_calc})，结果可能无效")
            else:
                T0_calc, T1_calc = T0, T1
                if self.verbose:
                    print(f"  使用提供的T0/T1: {T0_calc:.4f} / {T1_calc:.4f}")

            # 估计ATE
            if T0_calc is not None and T1_calc is not None:
                # 使用显式T0/T1计算ATE（CTF风格）
                ate = dml.ate(X=X_for_ate, T0=T0_calc, T1=T1_calc)
            else:
                # 使用默认ATE计算
                ate = dml.ate(X=X_for_ate)

            # 计算置信区间
            try:
                if T0_calc is not None and T1_calc is not None:
                    # 使用T0/T1计算效应
                    effects = dml.effect(X=X_for_ate, T0=T0_calc, T1=T1_calc)
                    ci_lower = np.percentile(effects, 2.5)
                    ci_upper = np.percentile(effects, 97.5)
                else:
                    ate_inference = dml.ate_inference(X=X_for_ate)
                    ci = ate_inference.conf_int()[0]  # 95%置信区间
                    ci_lower, ci_upper = float(ci[0]), float(ci[1])
            except Exception:
                # 如果置信区间计算失败，使用标准误估计
                stderr = dml.ate_stderr(X=X_for_ate) if hasattr(dml, 'ate_stderr') else 0.1 * abs(ate)
                ci_lower = ate - 1.96 * stderr
                ci_upper = ate + 1.96 * stderr

            # 存储模型和结果
            edge_key = f"{treatment}->{outcome}"
            self.dml_models[edge_key] = dml
            self.ate_results[edge_key] = {
                'ate': float(ate),
                'ci_lower': ci_lower,
                'ci_upper': ci_upper,
                'confounders': confounders
            }

            if self.verbose:
                print(f"  ATE: {ate:.4f}")
                print(f"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
                is_significant = not (ci_lower <= 0 <= ci_upper)
                print(f"  统计显著: {'是' if is_significant else '否'}")

            return float(ate), (ci_lower, ci_upper)

        except ImportError:
            # EconML未安装，使用简化方法
            warnings.warn("EconML未安装，使用简化的差值方法")
            return self._simple_ate_estimate(data, treatment, outcome)

        except Exception as e:
            warnings.warn(f"DML估计失败: {e}，使用简化方法")
            return self._simple_ate_estimate(data, treatment, outcome)

    def _simple_ate_estimate(self,
                            data: pd.DataFrame,
                            treatment: str,
                            outcome: str) -> Tuple[float, Tuple[float, float]]:
        """
        简化的ATE估计（后备方案）

        使用简单的分组均值差异作为ATE估计。
        注意：这个方法不控制混淆因素，结果可能有偏。
        """
        # 按treatment分组
        if data[treatment].dtype in [np.float64, np.float32]:
            # 连续变量：使用中位数分组
            median = data[treatment].median()
            low_group = data[data[treatment] <= median]
            high_group = data[data[treatment] > median]
        else:
            # 离散变量：使用唯一值
            unique_vals = sorted(data[treatment].unique())
            if len(unique_vals) < 2:
                return 0.0, (0.0, 0.0)
            low_group = data[data[treatment] == unique_vals[0]]
            high_group = data[data[treatment] == unique_vals[-1]]

        # 计算均值差异
        mean_low = low_group[outcome].mean()
        mean_high = high_group[outcome].mean()
        ate = mean_high - mean_low

        # 简单的标准误估计
        se_low = low_group[outcome].sem()
        se_high = high_group[outcome].sem()
        se = np.sqrt(se_low**2 + se_high**2)

        ci_lower = ate - 1.96 * se
        ci_upper = ate + 1.96 * se

        return float(ate), (float(ci_lower), float(ci_upper))

    def analyze_all_edges(self,
                         data: pd.DataFrame,
                         causal_graph: np.ndarray,
                         var_names: List[str],
                         threshold: float = 0.3) -> Dict[str, Dict]:
        """
        对因果图中的所有边进行因果推断

        参数:
            data: 数据框
            causal_graph: 邻接矩阵 (n_vars, n_vars)
            var_names: 变量名列表
            threshold: 边权重阈值

        返回:
            results: {
                'edge_name': {
                    'ate': float,
                    'ci_lower': float,
                    'ci_upper': float,
                    'is_significant': bool
                }
            }
        """
        if causal_graph is None:
            raise ValueError("causal_graph不能为None")
        if len(var_names) != causal_graph.shape[0]:
            raise ValueError(f"变量名数量({len(var_names)})与图大小({causal_graph.shape[0]})不匹配")

        results = {}
        n_vars = len(var_names)

        # 提取所有边
        edges = []
        for i in range(n_vars):
            for j in range(n_vars):
                if causal_graph[i, j] > threshold:
                    edges.append((i, j, causal_graph[i, j]))

        if self.verbose:
            print(f"\n开始分析 {len(edges)} 条因果边...")

        # 对每条边进行因果推断
        for idx, (source_idx, target_idx, weight) in enumerate(edges, 1):
            source = var_names[source_idx]
            target = var_names[target_idx]

            if self.verbose:
                print(f"\n[{idx}/{len(edges)}] 分析边: {source} → {target} (权重={weight:.3f})")

            # 识别混淆因素
            confounders = self._identify_confounders(
                causal_graph, source_idx, target_idx, var_names, threshold
            )

            # 估计ATE
            try:
                ate, ci = self.estimate_ate(data, source, target, confounders)

                is_significant = not (ci[0] <= 0 <= ci[1])

                results[f"{source}->{target}"] = {
                    'ate': ate,
                    'ci_lower': ci[0],
                    'ci_upper': ci[1],
                    'is_significant': is_significant,
                    'weight': weight,
                    'confounders': confounders
                }

            except Exception as e:
                warnings.warn(f"分析边 {source}->{target} 失败: {e}")
                continue

        if self.verbose:
            n_significant = sum(1 for r in results.values() if r['is_significant'])
            print(f"\n✓ 完成分析")
            print(f"  总边数: {len(edges)}")
            print(f"  成功分析: {len(results)}")
            print(f"  统计显著: {n_significant}")

        return results

    def _identify_confounders(self,
                            causal_graph: np.ndarray,
                            source_idx: int,
                            target_idx: int,
                            var_names: List[str],
                            threshold: float = 0.3) -> List[str]:
        """
        根据因果图识别混淆因素

        混淆因素定义：同时指向source和target的变量

        参数:
            causal_graph: 邻接矩阵
            source_idx: 源节点索引
            target_idx: 目标节点索引
            var_names: 变量名列表
            threshold: 边权重阈值

        返回:
            confounders: 混淆因素列表
        """
        confounders = []
        n_vars = len(var_names)

        for k in range(n_vars):
            if k == source_idx or k == target_idx:
                continue

            # 检查是否同时指向source和target
            points_to_source = causal_graph[k, source_idx] > threshold
            points_to_target = causal_graph[k, target_idx] > threshold

            if points_to_source or points_to_target:
                confounders.append(var_names[k])

        # 如果没有找到混淆因素，使用所有其他变量
        if len(confounders) == 0:
            confounders = [var_names[i] for i in range(n_vars)
                          if i != source_idx and i != target_idx]

        return confounders

    def _get_confounders_from_graph(self,
                                    parent: str,
                                    child: str,
                                    causal_graph: np.ndarray,
                                    var_names: List[str],
                                    threshold: float = 0.3) -> List[str]:
        """
        使用CTF风格的混淆因素识别（基于predecessors逻辑）

        与_identify_confounders的区别：
        - CTF风格：合并parent和child的所有父节点，然后移除parent本身
        - 原方法：查找所有指向parent或child的节点

        参数:
            parent: 处理变量名（如'learning_rate'）
            child: 结果变量名（如'energy_gpu_min_watts'）
            causal_graph: 邻接矩阵 (n_vars, n_vars)
            var_names: 变量名列表
            threshold: 边权重阈值

        返回:
            confounders: 混淆因素列表

        Raises:
            ValueError: 如果child在confounders中（逻辑错误）
        """
        # 获取索引
        if parent not in var_names:
            raise ValueError(f"Parent '{parent}' not in var_names")
        if child not in var_names:
            raise ValueError(f"Child '{child}' not in var_names")

        parent_idx = var_names.index(parent)
        child_idx = var_names.index(child)
        n_vars = len(var_names)

        # 步骤1: 获取parent的所有父节点（直接指向parent的节点）
        parent_parents = []
        for i in range(n_vars):
            if i != parent_idx and causal_graph[i, parent_idx] > threshold:
                parent_parents.append(var_names[i])

        # 步骤2: 获取child的所有父节点（直接指向child的节点）
        child_parents = []
        for i in range(n_vars):
            if i != child_idx and causal_graph[i, child_idx] > threshold:
                child_parents.append(var_names[i])

        # 步骤3: 合并并去重（使用set）
        X_cols = list(set(parent_parents + child_parents))

        # 步骤4: 关键 - 移除parent本身（CTF核心逻辑）
        # 这是防御性编程，防止图结构错误
        if parent in X_cols:
            X_cols.remove(parent)

        # 步骤5: 验证child不在confounders中
        if child in X_cols:
            raise ValueError(
                f"Child '{child}' should not be in confounders. "
                f"This indicates a causal graph error (circular dependency)."
            )

        return X_cols

    def analyze_all_edges_ctf_style(self,
                                    data: pd.DataFrame,
                                    causal_graph: np.ndarray,
                                    var_names: List[str],
                                    threshold: float = 0.3,
                                    ref_df: pd.DataFrame = None,
                                    t_strategy: str = None) -> Dict[str, Dict]:
        """
        对因果图中的所有边进行CTF风格的因果推断

        参数:
            data: 数据框
            causal_graph: 邻接矩阵 (n_vars, n_vars)
            var_names: 变量名列表
            threshold: 边权重阈值
            ref_df: 参考数据集（可选，迭代2功能）
            t_strategy: T0/T1计算策略（可选，迭代2功能）

        返回:
            results: {
                'edge_name': {
                    'ate': float,
                    'ci_lower': float,
                    'ci_upper': float,
                    'is_significant': bool,
                    'confounders': List[str]
                }
            }
        """
        if causal_graph is None:
            raise ValueError("causal_graph不能为None")
        if len(var_names) != causal_graph.shape[0]:
            raise ValueError(f"变量名数量({len(var_names)})与图大小({causal_graph.shape[0]})不匹配")

        results = {}
        n_vars = len(var_names)

        # 提取所有边
        edges = []
        for i in range(n_vars):
            for j in range(n_vars):
                if causal_graph[i, j] > threshold:
                    edges.append((i, j, causal_graph[i, j]))

        if self.verbose:
            print(f"\n开始CTF风格分析 {len(edges)} 条因果边...")

        # 对每条边进行因果推断
        for idx, (source_idx, target_idx, weight) in enumerate(edges, 1):
            source = var_names[source_idx]
            target = var_names[target_idx]

            if self.verbose:
                print(f"\n[{idx}/{len(edges)}] 分析边: {source} → {target} (权重={weight:.3f})")

            # 使用CTF风格识别混淆因素
            try:
                confounders = self._get_confounders_from_graph(
                    source, target, causal_graph, var_names, threshold
                )

                # 估计ATE（支持ref_df和T0/T1）
                ate, ci = self.estimate_ate(
                    data, source, target, confounders,
                    ref_df=ref_df, t_strategy=t_strategy
                )

                is_significant = not (ci[0] <= 0 <= ci[1])

                results[f"{source}->{target}"] = {
                    'ate': ate,
                    'ci_lower': ci[0],
                    'ci_upper': ci[1],
                    'is_significant': is_significant,
                    'weight': weight,
                    'confounders': confounders
                }

            except ValueError as e:
                # 跳过逻辑错误（如child在confounders中）
                if self.verbose:
                    print(f"  ⚠ 跳过边 {source}->{target}: {e}")
                continue
            except Exception as e:
                warnings.warn(f"分析边 {source}->{target} 失败: {e}")
                continue

        if self.verbose:
            n_significant = sum(1 for r in results.values() if r['is_significant'])
            print(f"\n✓ CTF风格分析完成")
            print(f"  总边数: {len(edges)}")
            print(f"  成功分析: {len(results)}")
            print(f"  统计显著: {n_significant}")
            if len(results) > 0:
                significance_rate = n_significant / len(results) * 100
                print(f"  显著率: {significance_rate:.1f}%")

        return results

    def get_significant_effects(self, alpha: float = 0.05) -> Dict[str, Dict]:
        """
        获取所有统计显著的因果效应

        参数:
            alpha: 显著性水平（默认0.05）

        返回:
            significant_effects: 统计显著的效应字典
        """
        significant = {}

        for edge, result in self.ate_results.items():
            if result['is_significant']:
                significant[edge] = result

        return significant

    def save_results(self, filepath: str):
        """
        保存因果推断结果到文件

        参数:
            filepath: 保存路径（.csv格式）
        """
        if not self.ate_results:
            warnings.warn("没有结果可保存")
            return

        # 转换为DataFrame
        records = []
        for edge, result in self.ate_results.items():
            source, target = edge.split('->')
            records.append({
                'source': source,
                'target': target,
                'ate': result['ate'],
                'ci_lower': result['ci_lower'],
                'ci_upper': result['ci_upper'],
                'is_significant': result['is_significant']
            })

        df = pd.DataFrame(records)
        df.to_csv(filepath, index=False)

        print(f"✓ 因果推断结果已保存到: {filepath}")
        print(f"  总边数: {len(records)}")
        print(f"  统计显著: {df['is_significant'].sum()}")

    def load_results(self, filepath: str):
        """
        从文件加载因果推断结果

        参数:
            filepath: 结果文件路径（.csv格式）
        """
        df = pd.read_csv(filepath)

        self.ate_results = {}
        for _, row in df.iterrows():
            edge = f"{row['source']}->{row['target']}"
            self.ate_results[edge] = {
                'ate': row['ate'],
                'ci_lower': row['ci_lower'],
                'ci_upper': row['ci_upper'],
                'is_significant': row['is_significant']
            }

        print(f"✓ 因果推断结果已从 {filepath} 加载")
        print(f"  加载边数: {len(self.ate_results)}")


# 辅助函数
def is_effect_significant(ci_lower: float, ci_upper: float) -> bool:
    """
    判断因果效应是否统计显著

    参数:
        ci_lower: 置信区间下界
        ci_upper: 置信区间上界

    返回:
        is_significant: True如果置信区间不包含0
    """
    return not (ci_lower <= 0 <= ci_upper)


def format_ate_result(ate: float, ci: Tuple[float, float]) -> str:
    """
    格式化ATE结果为可读字符串

    参数:
        ate: 平均处理效应
        ci: 置信区间 (lower, upper)

    返回:
        formatted_string: 格式化的字符串
    """
    is_sig = is_effect_significant(ci[0], ci[1])
    sig_marker = "***" if is_sig else ""

    return f"ATE={ate:.4f} {sig_marker}, 95% CI=[{ci[0]:.4f}, {ci[1]:.4f}]"

    def build_reference_df(self,
                          data: pd.DataFrame,
                          strategy: str = "non_parallel",
                          groupby_cols: Optional[List[str]] = None) -> pd.DataFrame:
        """
        构建参考数据集（ref_df）用于CTF风格的ATE计算

        参数:
            data: 原始数据集
            strategy: 构建策略
                - "non_parallel": 使用非并行模式作为baseline（推荐）
                - "mean": 使用全局均值
                - "group_mean": 按组计算均值
            groupby_cols: 分组列（仅当strategy="group_mean"时使用）

        返回:
            ref_df: 参考数据集

        示例:
            >>> # 使用非并行模式作为baseline
            >>> ref_df = ci.build_reference_df(data, strategy="non_parallel")
            >>>
            >>> # 使用全局均值
            >>> ref_df = ci.build_reference_df(data, strategy="mean")
        """
        if strategy == "non_parallel":
            # 使用非并行模式作为baseline（推荐用于能耗分析）
            if 'is_parallel' in data.columns:
                ref_df = data[data['is_parallel'] == 0].copy()
                if self.verbose:
                    print(f"  构建ref_df: 非并行模式 (n={len(ref_df)})")
            else:
                warnings.warn("数据中无is_parallel列，使用全部数据")
                ref_df = data.copy()
                if self.verbose:
                    print(f"  构建ref_df: 全部数据 (n={len(ref_df)})")

        elif strategy == "mean":
            # 使用全局均值
            ref_df = data.mean().to_frame().T
            if self.verbose:
                print(f"  构建ref_df: 全局均值 (1行)")

        elif strategy == "group_mean":
            # 按组计算均值
            if groupby_cols is None:
                raise ValueError("group_mean策略需要指定groupby_cols参数")

            # 检查列是否存在
            missing_cols = [col for col in groupby_cols if col not in data.columns]
            if missing_cols:
                raise ValueError(f"groupby_cols中的列不存在: {missing_cols}")

            ref_df = data.groupby(groupby_cols).mean().reset_index()
            if self.verbose:
                print(f"  构建ref_df: 按{groupby_cols}分组 (n={len(ref_df)})")

        else:
            raise ValueError(f"未知的strategy: {strategy}")

        # 验证ref_df不为空
        if len(ref_df) == 0:
            raise ValueError("构建的ref_df为空，请检查数据和策略")

        return ref_df

    def compute_T0_T1(self,
                     data: pd.DataFrame,
                     treatment: str,
                     strategy: str = "quantile") -> Tuple[float, float]:
        """
        计算T0（控制组值）和T1（处理组值）

        参数:
            data: 数据集
            treatment: 处理变量名
            strategy: 计算策略
                - "quantile": 25/75分位数（推荐，鲁棒性强）
                - "min_max": 最小值/最大值
                - "mean_std": 均值±标准差

        返回:
            (T0, T1): 控制组值和处理组值

        Raises:
            ValueError: 如果T1 <= T0（数据变异性不足）

        示例:
            >>> # 使用分位数策略（推荐）
            >>> T0, T1 = ci.compute_T0_T1(data, 'learning_rate', strategy='quantile')
            >>>
            >>> # 使用最小最大值
            >>> T0, T1 = ci.compute_T0_T1(data, 'learning_rate', strategy='min_max')
        """
        if treatment not in data.columns:
            raise ValueError(f"treatment '{treatment}' 不在数据中")

        if strategy == "quantile":
            # 使用25/75分位数（推荐，更鲁棒）
            T0 = data[treatment].quantile(0.25)
            T1 = data[treatment].quantile(0.75)

        elif strategy == "min_max":
            # 使用最小值/最大值
            T0 = data[treatment].min()
            T1 = data[treatment].max()

        elif strategy == "mean_std":
            # 使用均值±标准差
            mean_val = data[treatment].mean()
            std_val = data[treatment].std()
            T0 = mean_val - std_val
            T1 = mean_val + std_val

        else:
            raise ValueError(f"未知的strategy: {strategy}")

        # 验证T1 > T0
        if T1 <= T0:
            raise ValueError(
                f"T1 ({T1:.4f}) <= T0 ({T0:.4f})。"
                f"数据变异性不足，请检查treatment变量'{treatment}'"
            )

        # 警告：如果差异太小
        diff = T1 - T0
        threshold = 0.1 * data[treatment].std()
        if diff < threshold:
            warnings.warn(
                f"T1-T0差异较小: {diff:.4f} (阈值={threshold:.4f})。"
                f"ATE估计可能不稳定。"
            )

        if self.verbose:
            print(f"  计算T0/T1: {T0:.4f} / {T1:.4f} (策略={strategy}, 差异={diff:.4f})")

        return T0, T1
