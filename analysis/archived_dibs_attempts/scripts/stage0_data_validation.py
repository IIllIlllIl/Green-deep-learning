#!/usr/bin/env python3
"""
é˜¶æ®µ0: æ•°æ®éªŒè¯ (Data Validation)

åŠŸèƒ½:
1. åŠ è½½åŸå§‹æ•°æ® (data.csv)
2. éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡
3. ç”ŸæˆéªŒè¯æŠ¥å‘Š
4. è¾“å‡º: stage0_validated.csv (å¦‚æœéªŒè¯é€šè¿‡)

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
INPUT_FILE = RAW_DIR / "energy_data_original.csv"
OUTPUT_FILE = PROCESSED_DIR / "stage0_validated.csv"

# éªŒè¯æŠ¥å‘Šè·¯å¾„
REPORT_FILE = PROCESSED_DIR / "stage0_validation_report.txt"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)


def validate_file_exists(filepath):
    """éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if not filepath.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    print(f"âœ… æ–‡ä»¶å­˜åœ¨: {filepath}")
    return True


def load_data(filepath):
    """åŠ è½½CSVæ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {filepath}")
    df = pd.read_csv(filepath)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    return df


def validate_structure(df):
    """éªŒè¯æ•°æ®ç»“æ„"""
    print("\nğŸ” éªŒè¯æ•°æ®ç»“æ„...")

    issues = []

    # é¢„æœŸåˆ—æ•° (data.csvåº”è¯¥æ˜¯56åˆ—)
    expected_columns = 56
    actual_columns = len(df.columns)

    if actual_columns != expected_columns:
        issues.append(f"âš ï¸  åˆ—æ•°ä¸åŒ¹é…: é¢„æœŸ{expected_columns}åˆ—, å®é™…{actual_columns}åˆ—")
    else:
        print(f"âœ… åˆ—æ•°æ­£ç¡®: {actual_columns}åˆ—")

    # é¢„æœŸè¡Œæ•° (è‡³å°‘åº”è¯¥æœ‰æ•°æ®)
    if len(df) < 100:
        issues.append(f"âš ï¸  æ•°æ®é‡è¿‡å°‘: åªæœ‰{len(df)}è¡Œ")
    else:
        print(f"âœ… æ•°æ®é‡å……è¶³: {len(df)}è¡Œ")

    return issues


def validate_required_columns(df):
    """éªŒè¯å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ” éªŒè¯å¿…éœ€åˆ—...")

    required_columns = {
        'å…ƒä¿¡æ¯': ['experiment_id', 'timestamp', 'repository', 'model', 'mode', 'is_parallel'],
        'è¶…å‚æ•°': ['hyperparam_learning_rate', 'hyperparam_batch_size', 'hyperparam_epochs'],
        'èƒ½è€—': ['energy_cpu_total_joules', 'energy_gpu_total_joules'],
        'æ€§èƒ½': ['perf_test_accuracy', 'perf_map', 'perf_eval_loss', 'perf_top1_accuracy']
    }

    issues = []

    for category, columns in required_columns.items():
        print(f"\n  æ£€æŸ¥ {category} åˆ—:")
        for col in columns:
            if col not in df.columns:
                issues.append(f"âŒ ç¼ºå¤±å¿…éœ€åˆ—: {col} ({category})")
                print(f"    âŒ {col}")
            else:
                print(f"    âœ… {col}")

    if not issues:
        print(f"\nâœ… æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")

    return issues


def validate_data_types(df):
    """éªŒè¯æ•°æ®ç±»å‹"""
    print("\nğŸ” éªŒè¯æ•°æ®ç±»å‹...")

    issues = []

    # æ£€æŸ¥is_parallelåˆ—åº”è¯¥æ˜¯å¸ƒå°”å‹æˆ–0/1
    if 'is_parallel' in df.columns:
        unique_values = df['is_parallel'].dropna().unique()
        if not all(v in [True, False, 0, 1, 'True', 'False'] for v in unique_values):
            issues.append(f"âš ï¸  is_parallelåˆ—åŒ…å«éå¸ƒå°”å€¼: {unique_values}")
        else:
            print(f"âœ… is_parallelåˆ—ç±»å‹æ­£ç¡®")

    # æ£€æŸ¥è¶…å‚æ•°åˆ—åº”è¯¥æ˜¯æ•°å€¼å‹
    hyperparam_cols = [c for c in df.columns if 'hyperparam_' in c]
    for col in hyperparam_cols:
        non_numeric = df[col].dropna().apply(lambda x: not isinstance(x, (int, float, np.int64, np.float64)))
        if non_numeric.any():
            count = non_numeric.sum()
            issues.append(f"âš ï¸  {col} åŒ…å« {count} ä¸ªéæ•°å€¼é¡¹")

    if not issues:
        print(f"âœ… æ•°æ®ç±»å‹éªŒè¯é€šè¿‡")

    return issues


def check_missing_values(df):
    """æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ"""
    print("\nğŸ” æ£€æŸ¥ç¼ºå¤±å€¼...")

    total_cells = df.shape[0] * df.shape[1]
    missing_cells = df.isna().sum().sum()
    missing_rate = (missing_cells / total_cells) * 100

    print(f"  æ€»å•å…ƒæ ¼æ•°: {total_cells:,}")
    print(f"  ç¼ºå¤±å•å…ƒæ ¼æ•°: {missing_cells:,}")
    print(f"  æ€»ä½“ç¼ºå¤±ç‡: {missing_rate:.2f}%")

    # æ£€æŸ¥å…³é”®åˆ—çš„ç¼ºå¤±ç‡
    critical_columns = ['experiment_id', 'timestamp', 'repository', 'model', 'mode']

    print(f"\n  å…³é”®åˆ—ç¼ºå¤±ç‡:")
    issues = []

    for col in critical_columns:
        if col in df.columns:
            col_missing = df[col].isna().sum()
            col_missing_rate = (col_missing / len(df)) * 100

            if col_missing_rate > 0:
                issues.append(f"âŒ {col}: ç¼ºå¤± {col_missing} è¡Œ ({col_missing_rate:.2f}%)")
                print(f"    âŒ {col}: {col_missing_rate:.2f}%")
            else:
                print(f"    âœ… {col}: 0%")

    # æ£€æŸ¥èƒ½è€—å’Œæ€§èƒ½åˆ—çš„ç¼ºå¤±ç‡
    energy_cols = [c for c in df.columns if 'energy_' in c]
    perf_cols = [c for c in df.columns if 'perf_' in c]

    energy_missing = df[energy_cols].isna().all(axis=1).sum()
    perf_missing = df[perf_cols].isna().all(axis=1).sum()

    print(f"\n  æ•°æ®å®Œæ•´æ€§:")
    print(f"    èƒ½è€—æ•°æ®å…¨ç¼ºå¤±: {energy_missing} è¡Œ ({energy_missing/len(df)*100:.2f}%)")
    print(f"    æ€§èƒ½æ•°æ®å…¨ç¼ºå¤±: {perf_missing} è¡Œ ({perf_missing/len(df)*100:.2f}%)")

    if energy_missing > len(df) * 0.2:  # è¶…è¿‡20%
        issues.append(f"âš ï¸  èƒ½è€—æ•°æ®ç¼ºå¤±ä¸¥é‡: {energy_missing} è¡Œ")
    if perf_missing > len(df) * 0.2:  # è¶…è¿‡20%
        issues.append(f"âš ï¸  æ€§èƒ½æ•°æ®ç¼ºå¤±ä¸¥é‡: {perf_missing} è¡Œ")

    return issues, {
        'total_missing_rate': missing_rate,
        'energy_missing_rows': energy_missing,
        'perf_missing_rows': perf_missing
    }


def check_data_ranges(df):
    """æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§"""
    print("\nğŸ” æ£€æŸ¥æ•°æ®èŒƒå›´...")

    issues = []

    # æ£€æŸ¥èƒ½è€—æ•°æ®ä¸åº”ä¸ºè´Ÿæ•°
    energy_cols = [c for c in df.columns if 'energy_' in c and 'joules' in c]
    for col in energy_cols:
        if col in df.columns:
            negative_count = (df[col] < 0).sum()
            if negative_count > 0:
                issues.append(f"âŒ {col} åŒ…å« {negative_count} ä¸ªè´Ÿå€¼")
                print(f"  âŒ {col}: {negative_count} ä¸ªè´Ÿå€¼")

    # æ£€æŸ¥å‡†ç¡®ç‡åº”è¯¥åœ¨0-1ä¹‹é—´
    accuracy_cols = [c for c in df.columns if 'accuracy' in c.lower()]
    for col in accuracy_cols:
        if col in df.columns:
            out_of_range = ((df[col] < 0) | (df[col] > 1)).sum()
            if out_of_range > 0:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åˆ†æ¯”å½¢å¼ (0-100)
                if df[col].max() > 1:
                    print(f"  â„¹ï¸  {col}: å¯èƒ½æ˜¯ç™¾åˆ†æ¯”å½¢å¼ (èŒƒå›´: {df[col].min():.2f}-{df[col].max():.2f})")
                else:
                    issues.append(f"âŒ {col} åŒ…å« {out_of_range} ä¸ªè¶…å‡ºèŒƒå›´[0,1]çš„å€¼")
                    print(f"  âŒ {col}: {out_of_range} ä¸ªè¶…å‡ºèŒƒå›´çš„å€¼")

    if not issues:
        print(f"âœ… æ•°æ®èŒƒå›´éªŒè¯é€šè¿‡")

    return issues


def check_duplicates(df):
    """æ£€æŸ¥é‡å¤è®°å½•"""
    print("\nğŸ” æ£€æŸ¥é‡å¤è®°å½•...")

    issues = []

    # æ£€æŸ¥experiment_id + timestampçš„å”¯ä¸€æ€§
    if 'experiment_id' in df.columns and 'timestamp' in df.columns:
        df['_composite_key'] = df['experiment_id'].astype(str) + '|' + df['timestamp'].astype(str)
        duplicates = df['_composite_key'].duplicated().sum()

        if duplicates > 0:
            issues.append(f"âš ï¸  å‘ç° {duplicates} ä¸ªé‡å¤è®°å½• (experiment_id + timestamp)")
            print(f"  âš ï¸  é‡å¤è®°å½•: {duplicates} ä¸ª")

            # æ˜¾ç¤ºé‡å¤çš„è®°å½•
            dup_keys = df[df['_composite_key'].duplicated(keep=False)]['_composite_key'].unique()
            print(f"  é‡å¤çš„é”® (å‰5ä¸ª): {list(dup_keys[:5])}")
        else:
            print(f"âœ… æ— é‡å¤è®°å½•")

        df.drop('_composite_key', axis=1, inplace=True)

    return issues


def generate_validation_report(df, all_issues, stats):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”ŸæˆéªŒè¯æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ0: æ•°æ®éªŒè¯æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
    report_lines.append("")

    # æ•°æ®æ¦‚è§ˆ
    report_lines.append("=" * 80)
    report_lines.append("1. æ•°æ®æ¦‚è§ˆ")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»è¡Œæ•°: {len(df):,}")
    report_lines.append(f"æ€»åˆ—æ•°: {len(df.columns)}")
    report_lines.append(f"æ€»ä½“ç¼ºå¤±ç‡: {stats['total_missing_rate']:.2f}%")
    report_lines.append(f"èƒ½è€—æ•°æ®å…¨ç¼ºå¤±: {stats['energy_missing_rows']} è¡Œ")
    report_lines.append(f"æ€§èƒ½æ•°æ®å…¨ç¼ºå¤±: {stats['perf_missing_rows']} è¡Œ")
    report_lines.append("")

    # Repositoryåˆ†å¸ƒ
    if 'repository' in df.columns:
        report_lines.append("Repositoryåˆ†å¸ƒ:")
        repo_dist = df['repository'].value_counts()
        for repo, count in repo_dist.items():
            report_lines.append(f"  {repo}: {count} ({count/len(df)*100:.1f}%)")
        report_lines.append("")

    # Modeåˆ†å¸ƒ
    if 'mode' in df.columns:
        report_lines.append("Modeåˆ†å¸ƒ:")
        mode_dist = df['mode'].value_counts()
        for mode, count in mode_dist.items():
            report_lines.append(f"  {mode}: {count} ({count/len(df)*100:.1f}%)")
        report_lines.append("")

    # é—®é¢˜æ±‡æ€»
    report_lines.append("=" * 80)
    report_lines.append("2. éªŒè¯é—®é¢˜æ±‡æ€»")
    report_lines.append("=" * 80)

    if all_issues:
        report_lines.append(f"å‘ç° {len(all_issues)} ä¸ªé—®é¢˜:")
        report_lines.append("")
        for i, issue in enumerate(all_issues, 1):
            report_lines.append(f"{i}. {issue}")
    else:
        report_lines.append("âœ… æœªå‘ç°é—®é¢˜ï¼Œæ•°æ®éªŒè¯é€šè¿‡ï¼")

    report_lines.append("")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)

    return len(all_issues) == 0


def save_validated_data(df):
    """ä¿å­˜éªŒè¯é€šè¿‡çš„æ•°æ®"""
    print(f"\nğŸ’¾ ä¿å­˜éªŒè¯æ•°æ®...")

    df.to_csv(OUTPUT_FILE, index=False)

    print(f"âœ… éªŒè¯æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")
    print(f"  è¡Œæ•°: {len(df):,}")
    print(f"  åˆ—æ•°: {len(df.columns)}")
    print(f"  æ–‡ä»¶å¤§å°: {OUTPUT_FILE.stat().st_size / 1024:.1f} KB")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ0: æ•°æ®éªŒè¯ (Data Validation)")
    print("=" * 80)

    try:
        # 1. éªŒè¯æ–‡ä»¶å­˜åœ¨
        validate_file_exists(INPUT_FILE)

        # 2. åŠ è½½æ•°æ®
        df = load_data(INPUT_FILE)

        # 3. æ‰§è¡Œå„é¡¹éªŒè¯
        all_issues = []

        # éªŒè¯æ•°æ®ç»“æ„
        all_issues.extend(validate_structure(df))

        # éªŒè¯å¿…éœ€åˆ—
        all_issues.extend(validate_required_columns(df))

        # éªŒè¯æ•°æ®ç±»å‹
        all_issues.extend(validate_data_types(df))

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_issues, stats = check_missing_values(df)
        all_issues.extend(missing_issues)

        # æ£€æŸ¥æ•°æ®èŒƒå›´
        all_issues.extend(check_data_ranges(df))

        # æ£€æŸ¥é‡å¤è®°å½•
        all_issues.extend(check_duplicates(df))

        # 4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        validation_passed = generate_validation_report(df, all_issues, stats)

        # 5. ä¿å­˜éªŒè¯æ•°æ®
        if validation_passed:
            save_validated_data(df)
            print("\n" + "=" * 80)
            print("âœ… é˜¶æ®µ0å®Œæˆ: æ•°æ®éªŒè¯é€šè¿‡")
            print("=" * 80)
            return 0
        else:
            print("\n" + "=" * 80)
            print(f"âš ï¸  é˜¶æ®µ0å®Œæˆ: å‘ç° {len(all_issues)} ä¸ªé—®é¢˜")
            print("=" * 80)
            print("\nå»ºè®®:")
            print("1. æŸ¥çœ‹éªŒè¯æŠ¥å‘Šäº†è§£è¯¦æƒ…")
            print("2. æ ¹æ®é—®é¢˜ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­å¤„ç†")
            print("3. å¦‚æœæ˜¯è­¦å‘Šçº§åˆ«é—®é¢˜ï¼Œå¯ä»¥ç»§ç»­ï¼›å¦‚æœæ˜¯é”™è¯¯çº§åˆ«é—®é¢˜ï¼Œéœ€è¦ä¿®å¤æ•°æ®æº")

            # å³ä½¿æœ‰è­¦å‘Šä¹Ÿä¿å­˜æ•°æ®ï¼ˆä¾›æ£€æŸ¥ï¼‰
            save_validated_data(df)
            return 1

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
