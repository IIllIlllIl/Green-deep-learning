#!/usr/bin/env python3
"""
å•ä»»åŠ¡DiBSå› æœåˆ†æå¿«é€Ÿæµ‹è¯•è„šæœ¬ (v3 - æœ€ç»ˆä¿®å¤ç‰ˆæœ¬)

ä¸»è¦æ”¹è¿›:
1. æ•°æ®æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰
2. Alphaå‚æ•°: 0.1 â†’ 0.9 (è®ºæ–‡å»ºè®®å€¼)
3. è¿­ä»£æ­¥æ•°: 3000 â†’ 10000
4. ç²’å­æ•°: 10 â†’ 20
5. â­â­â­ æ–°å¢ï¼šç§»é™¤ç¼ºå¤±ç‡>30%çš„åˆ—

æ—¥æœŸ: 2025-12-26
"""
import numpy as np
import pandas as pd
import sys
import os
import time
import pickle
import argparse
from datetime import datetime
from pathlib import Path
from sklearn.preprocessing import StandardScaler

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from utils.causal_discovery import CausalGraphLearner

def main():
    parser = argparse.ArgumentParser(description='å•ä»»åŠ¡DiBSå› æœåˆ†æ (v3æœ€ç»ˆä¿®å¤ç‰ˆæœ¬)')
    parser.add_argument('--task', type=str, required=True, help='ä»»åŠ¡åç§°ï¼ˆå¦‚mrt_oastï¼‰')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print(f"DiBSå› æœåˆ†æ (v3æœ€ç»ˆä¿®å¤ç‰ˆæœ¬): {args.task}")
    print("=" * 80)
    print(f"è¾“å…¥: {args.input}")
    print(f"è¾“å‡º: {args.output}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    start_time = time.time()

    # 1. åŠ è½½æ•°æ®
    print("\n[æ­¥éª¤1] åŠ è½½æ•°æ®...")
    df = pd.read_csv(args.input)
    print(f"  âœ… æ•°æ®åŠ è½½: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

    # å‡†å¤‡æ•°å€¼å‹æ•°æ®
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    causal_data = df[numeric_cols].copy()
    causal_data = causal_data.dropna(axis=1, how='all')

    print(f"  åŸå§‹æ•°å€¼å‹å˜é‡: {len(causal_data.columns)}ä¸ª")
    print(f"  æœ‰æ•ˆæ ·æœ¬: {len(causal_data)}è¡Œ")

    missing_rate_before = causal_data.isna().sum().sum() / (len(causal_data) * len(causal_data.columns))
    print(f"  åŸå§‹ç¼ºå¤±ç‡: {missing_rate_before*100:.2f}%")

    # 2. ç§»é™¤é«˜ç¼ºå¤±åˆ— â­â­â­ æ–°å¢
    print("\n[æ­¥éª¤2] ç§»é™¤é«˜ç¼ºå¤±åˆ—...")

    # è®¡ç®—æ¯åˆ—çš„ç¼ºå¤±ç‡
    missing_per_col = causal_data.isna().sum() / len(causal_data)

    # æ˜¾ç¤ºç¼ºå¤±ç‡ç»Ÿè®¡
    print(f"  åˆ—ç¼ºå¤±ç‡åˆ†å¸ƒ:")
    for col, miss_rate in missing_per_col.items():
        if miss_rate > 0:
            print(f"    {col}: {miss_rate*100:.1f}%")

    # ä¿ç•™ç¼ºå¤±ç‡<=30%çš„åˆ—
    MISSING_THRESHOLD = 0.30
    cols_to_keep = missing_per_col[missing_per_col <= MISSING_THRESHOLD].index.tolist()
    cols_removed = [col for col in causal_data.columns if col not in cols_to_keep]

    print(f"\n  é˜ˆå€¼: ç¼ºå¤±ç‡<={MISSING_THRESHOLD*100:.0f}%")
    print(f"  ä¿ç•™: {len(cols_to_keep)}åˆ—")
    print(f"  ç§»é™¤: {len(cols_removed)}åˆ—")
    if cols_removed:
        print(f"  ç§»é™¤çš„åˆ—: {cols_removed}")

    causal_data = causal_data[cols_to_keep]
    numeric_cols = cols_to_keep

    missing_rate_after = causal_data.isna().sum().sum() / (len(causal_data) * len(numeric_cols))
    print(f"  è¿‡æ»¤åç¼ºå¤±ç‡: {missing_rate_after*100:.2f}%")

    # æ˜¾ç¤ºæ•°æ®èŒƒå›´ï¼ˆæ ‡å‡†åŒ–å‰ï¼‰
    print(f"\n  æ•°æ®èŒƒå›´ï¼ˆæ ‡å‡†åŒ–å‰ï¼‰:")
    for col in numeric_cols[:5]:  # æ˜¾ç¤ºå‰5åˆ—
        col_min = causal_data[col].min()
        col_max = causal_data[col].max()
        col_mean = causal_data[col].mean()
        col_std = causal_data[col].std()
        print(f"    {col}: [{col_min:.2f}, {col_max:.2f}], mean={col_mean:.2f}, std={col_std:.2f}")
    if len(numeric_cols) > 5:
        print(f"    ... (å…±{len(numeric_cols)}åˆ—)")

    # 3. æ•°æ®æ ‡å‡†åŒ–
    print("\n[æ­¥éª¤3] æ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    causal_data_scaled = pd.DataFrame(
        scaler.fit_transform(causal_data),
        columns=causal_data.columns,
        index=causal_data.index
    )
    print(f"  âœ… æ ‡å‡†åŒ–å®Œæˆ (mean=0, std=1)")

    # éªŒè¯æ ‡å‡†åŒ–
    print(f"  éªŒè¯: mean={causal_data_scaled.mean().mean():.6f}, std={causal_data_scaled.std().mean():.6f}")

    # 4. DiBSå› æœå›¾å­¦ä¹ 
    print("\n[æ­¥éª¤4] DiBSå› æœå›¾å­¦ä¹ ...")

    graph_file = output_dir / 'causal_graph.npy'
    edges_file = output_dir / 'causal_edges.pkl'

    # DiBSå‚æ•° â­â­â­ v3ä¿®æ”¹
    N_STEPS = 10000    # ä»3000å¢åŠ åˆ°10000
    ALPHA = 0.9        # ä»0.5å¢åŠ åˆ°0.9 (è®ºæ–‡å»ºè®®å€¼)
    THRESHOLD = 0.3    # ä¿æŒä¸å˜

    print(f"  é…ç½® (v3æœ€ç»ˆä¿®å¤ç‰ˆæœ¬):")
    print(f"    n_steps: {N_STEPS} (ä»3000å¢åŠ )")
    print(f"    alpha: {ALPHA} â­â­â­ (ä»0.5å¢åŠ åˆ°0.9ï¼Œè®ºæ–‡å»ºè®®å€¼)")
    print(f"    threshold: {THRESHOLD}")
    print(f"    n_particles: 20 (ä»10å¢åŠ )")
    print(f"  å˜é‡æ•°: {len(numeric_cols)}")
    print(f"  æ ·æœ¬æ•°: {len(causal_data_scaled)}")
    print(f"  é¢„è®¡æ—¶é—´: 15-30åˆ†é’Ÿ")

    learner = CausalGraphLearner(
        n_vars=len(numeric_cols),
        n_steps=N_STEPS,
        alpha=ALPHA,
        random_seed=42
    )

    print(f"\n  å¼€å§‹DiBSå­¦ä¹ ...")
    print(f"  æ³¨æ„: alpha=0.9å¯èƒ½äº§ç”Ÿæ›´å¤šè¾¹ï¼ˆæ›´ç¨ å¯†çš„å›¾ï¼‰")
    dibs_start = time.time()

    # ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®
    causal_graph = learner.fit(causal_data_scaled, verbose=args.verbose)

    dibs_time = time.time() - dibs_start
    print(f"\n  âœ… DiBSå®Œæˆï¼Œè€—æ—¶: {dibs_time/60:.1f}åˆ†é’Ÿ")

    # åˆ†æè¾¹
    edges = learner.get_edges(threshold=THRESHOLD)
    print(f"  æ£€æµ‹åˆ° {len(edges)} æ¡å› æœè¾¹ (threshold={THRESHOLD})")

    # æ˜¾ç¤ºå›¾çŸ©é˜µç»Ÿè®¡
    print(f"\n  å›¾çŸ©é˜µç»Ÿè®¡:")
    print(f"    æœ€å¤§æƒé‡: {causal_graph.max():.6f}")
    print(f"    æœ€å°æƒé‡: {causal_graph.min():.6f}")
    print(f"    å¹³å‡æƒé‡: {causal_graph.mean():.6f}")
    print(f"    éé›¶å…ƒç´ æ•°: {np.count_nonzero(causal_graph)}")

    # æ˜¾ç¤ºä¸åŒé˜ˆå€¼ä¸‹çš„è¾¹æ•°
    edges_01 = np.sum(causal_graph > 0.1)
    edges_02 = np.sum(causal_graph > 0.2)
    edges_03 = np.sum(causal_graph > 0.3)
    edges_04 = np.sum(causal_graph > 0.4)
    edges_05 = np.sum(causal_graph > 0.5)
    print(f"    æƒé‡>0.1çš„è¾¹æ•°: {edges_01}")
    print(f"    æƒé‡>0.2çš„è¾¹æ•°: {edges_02}")
    print(f"    æƒé‡>0.3çš„è¾¹æ•°: {edges_03}")
    print(f"    æƒé‡>0.4çš„è¾¹æ•°: {edges_04}")
    print(f"    æƒé‡>0.5çš„è¾¹æ•°: {edges_05}")

    # ä¿å­˜ç»“æœ
    learner.save_graph(str(graph_file))
    with open(edges_file, 'wb') as f:
        pickle.dump({
            'edges': edges,
            'numeric_cols': numeric_cols,
            'task_name': args.task,
            'dibs_params': {
                'n_steps': N_STEPS,
                'alpha': ALPHA,
                'threshold': THRESHOLD,
                'version': 'v3',
                'standardized': True,
                'missing_filter': MISSING_THRESHOLD
            },
            'data_stats': {
                'n_samples': len(causal_data_scaled),
                'n_vars': len(numeric_cols),
                'missing_rate': missing_rate_after,
                'cols_removed': cols_removed
            },
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }, f)

    print(f"  âœ… ç»“æœå·²ä¿å­˜")

    # æ˜¾ç¤ºå…³é”®è¾¹
    if len(edges) > 0:
        print(f"\n  ğŸ‰ æˆåŠŸæ£€æµ‹åˆ°å› æœè¾¹ï¼")
        print(f"  å‰10æ¡æœ€å¼ºå› æœè¾¹:")
        for i, (source, target, weight) in enumerate(edges[:10], 1):
            print(f"    {i}. {numeric_cols[source]} â†’ {numeric_cols[target]}: {weight:.3f}")
    else:
        print(f"\n  âš ï¸  æœªæ£€æµ‹åˆ°ç½®ä¿¡åº¦>{THRESHOLD}çš„å› æœè¾¹")
        if edges_01 > 0:
            print(f"  æç¤º: æœ‰{edges_01}æ¡è¾¹æƒé‡>0.1ï¼Œå¯ä»¥é™ä½thresholdæŸ¥çœ‹")
        else:
            print(f"  ä¸¥é‡è­¦å‘Š: å›¾çŸ©é˜µä»ç„¶å…¨ä¸º0ï¼")
            print(f"  å¯èƒ½åŸå› : æ•°æ®ä¸­ç¡®å®æ²¡æœ‰æ˜æ˜¾çš„å› æœå…³ç³»")

    # 5. ç”ŸæˆæŠ¥å‘Š
    report_file = output_dir / 'analysis_report.md'
    with open(report_file, 'w') as f:
        f.write(f"# {args.task} å› æœåˆ†ææŠ¥å‘Š (v3æœ€ç»ˆä¿®å¤ç‰ˆæœ¬)\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"## æ•°æ®æ¦‚å†µ\n\n")
        f.write(f"- æ ·æœ¬æ•°: {len(causal_data)}\n")
        f.write(f"- åŸå§‹å˜é‡æ•°: {len(df.select_dtypes(include=[np.number]).columns)}\n")
        f.write(f"- è¿‡æ»¤åå˜é‡æ•°: {len(numeric_cols)}\n")
        f.write(f"- ç§»é™¤åˆ—æ•°: {len(cols_removed)}\n")
        if cols_removed:
            f.write(f"- ç§»é™¤çš„åˆ—: {', '.join(cols_removed)}\n")
        f.write(f"- åŸå§‹ç¼ºå¤±ç‡: {missing_rate_before*100:.2f}%\n")
        f.write(f"- è¿‡æ»¤åç¼ºå¤±ç‡: {missing_rate_after*100:.2f}%\n")
        f.write(f"- æ•°æ®é¢„å¤„ç†: æ ‡å‡†åŒ– (StandardScaler)\n\n")
        f.write(f"## DiBSå‚æ•° (v3æœ€ç»ˆä¿®å¤ç‰ˆæœ¬)\n\n")
        f.write(f"- n_steps: {N_STEPS}\n")
        f.write(f"- alpha: {ALPHA} (è®ºæ–‡å»ºè®®å€¼)\n")
        f.write(f"- threshold: {THRESHOLD}\n")
        f.write(f"- n_particles: 20\n")
        f.write(f"- ç¼ºå¤±å€¼è¿‡æ»¤é˜ˆå€¼: {MISSING_THRESHOLD*100:.0f}%\n\n")
        f.write(f"## DiBSç»“æœ\n\n")
        f.write(f"- å› æœè¾¹æ•°: {len(edges)}\n")
        f.write(f"- è¿è¡Œæ—¶é—´: {dibs_time/60:.1f}åˆ†é’Ÿ\n")
        f.write(f"- å›¾çŸ©é˜µæœ€å¤§æƒé‡: {causal_graph.max():.6f}\n")
        f.write(f"- å›¾çŸ©é˜µå¹³å‡æƒé‡: {causal_graph.mean():.6f}\n")
        f.write(f"- éé›¶å…ƒç´ æ•°: {np.count_nonzero(causal_graph)}\n\n")

        if edges:
            f.write(f"### æ£€æµ‹åˆ°çš„å› æœè¾¹\n\n")
            for i, (source, target, weight) in enumerate(edges, 1):
                f.write(f"{i}. {numeric_cols[source]} â†’ {numeric_cols[target]}: {weight:.3f}\n")
        else:
            f.write(f"æœªæ£€æµ‹åˆ°ç½®ä¿¡åº¦>{THRESHOLD}çš„å› æœè¾¹ã€‚\n\n")
            f.write(f"### æƒé‡åˆ†å¸ƒ\n\n")
            f.write(f"- æƒé‡>0.1: {edges_01}æ¡\n")
            f.write(f"- æƒé‡>0.2: {edges_02}æ¡\n")
            f.write(f"- æƒé‡>0.3: {edges_03}æ¡\n")
            f.write(f"- æƒé‡>0.4: {edges_04}æ¡\n")
            f.write(f"- æƒé‡>0.5: {edges_05}æ¡\n")

    print(f"\n  âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # æ€»ç»“
    total_time = time.time() - start_time
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"å› æœè¾¹æ•°: {len(edges)}")
    print(f"å›¾çŸ©é˜µæœ€å¤§æƒé‡: {causal_graph.max():.6f}")

    if len(edges) > 0:
        print(f"âœ…âœ…âœ… v3ä¿®å¤æˆåŠŸï¼æ£€æµ‹åˆ°{len(edges)}æ¡å› æœè¾¹")
    elif edges_01 > 0:
        print(f"âš ï¸  é˜ˆå€¼0.3æœªæ£€æµ‹åˆ°è¾¹ï¼Œä½†æœ‰{edges_01}æ¡è¾¹æƒé‡>0.1")
        print(f"å»ºè®®: é™ä½thresholdé‡æ–°åˆ†æ")
    else:
        print(f"âŒâŒâŒ v3ä¿®å¤å¤±è´¥ï¼å›¾çŸ©é˜µä»ç„¶å…¨ä¸º0")
        print(f"ç»“è®º: DiBSå¯èƒ½ä¸é€‚ç”¨äºæ­¤æ•°æ®é›†")
    print("=" * 80)

if __name__ == '__main__':
    main()
