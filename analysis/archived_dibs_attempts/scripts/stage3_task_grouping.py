#!/usr/bin/env python3
"""
é˜¶æ®µ3: ä»»åŠ¡åˆ†ç»„ (Task Grouping)

åŠŸèƒ½:
1. åŠ è½½stage2_mediators.csv
2. æŒ‰4ä¸ªä»»åŠ¡ç»„åˆ†å‰²æ•°æ®:
   - å›¾åƒåˆ†ç±» (examples + pytorch_resnet_cifar10)
   - Person_reID (Person_reID_baseline_pytorch)
   - VulBERTa (VulBERTa)
   - Bugå®šä½ (bug-localization-by-dnn-and-rvsm)
3. ä¸ºæ¯ä¸ªä»»åŠ¡ç»„ç”Ÿæˆç‹¬ç«‹CSVæ–‡ä»¶
4. è¾“å‡º: 4ä¸ªä»»åŠ¡ç»„CSVæ–‡ä»¶

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
PROCESSED_DIR = DATA_DIR / "processed"
INPUT_FILE = PROCESSED_DIR / "stage2_mediators.csv"
REPORT_FILE = PROCESSED_DIR / "stage3_task_grouping_report.txt"

# ä»»åŠ¡ç»„å®šä¹‰
TASK_GROUPS = {
    'image_classification': {
        'name': 'å›¾åƒåˆ†ç±»',
        'repositories': ['examples', 'pytorch_resnet_cifar10'],
        'output_file': PROCESSED_DIR / "stage3_image_classification.csv"
    },
    'person_reid': {
        'name': 'Person_reIDæ£€ç´¢',
        'repositories': ['Person_reID_baseline_pytorch'],
        'output_file': PROCESSED_DIR / "stage3_person_reid.csv"
    },
    'vulberta': {
        'name': 'VulBERTaæ¼æ´æ£€æµ‹',
        'repositories': ['VulBERTa'],
        'output_file': PROCESSED_DIR / "stage3_vulberta.csv"
    },
    'bug_localization': {
        'name': 'Bugå®šä½',
        'repositories': ['bug-localization-by-dnn-and-rvsm'],
        'output_file': PROCESSED_DIR / "stage3_bug_localization.csv"
    }
}


def load_data(filepath):
    """åŠ è½½CSVæ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {filepath}")
    df = pd.read_csv(filepath)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    return df


def verify_repository_column(df):
    """éªŒè¯repositoryåˆ—"""
    print("\nğŸ” éªŒè¯repositoryåˆ—...")

    if 'repository' not in df.columns:
        raise ValueError("âŒ ç¼ºå°‘repositoryåˆ—")

    repos = df['repository'].unique()
    print(f"âœ… å‘ç° {len(repos)} ä¸ªrepository:")
    for repo in sorted(repos):
        count = (df['repository'] == repo).sum()
        print(f"  - {repo}: {count} è¡Œ")

    return repos


def split_into_task_groups(df):
    """åˆ†å‰²æ•°æ®åˆ°ä»»åŠ¡ç»„"""
    print("\nğŸ”§ åˆ†å‰²æ•°æ®åˆ°ä»»åŠ¡ç»„...")

    task_group_data = {}

    for group_id, group_info in TASK_GROUPS.items():
        print(f"\n  å¤„ç†: {group_info['name']}...")

        # ç­›é€‰å±äºæ­¤ä»»åŠ¡ç»„çš„æ•°æ®
        mask = df['repository'].isin(group_info['repositories'])
        group_df = df[mask].copy()

        task_group_data[group_id] = {
            'data': group_df,
            'info': group_info,
            'sample_count': len(group_df)
        }

        print(f"    æ ·æœ¬æ•°: {len(group_df)}")
        print(f"    Repository: {', '.join(group_info['repositories'])}")

        # æ˜¾ç¤ºrepositoryåˆ†å¸ƒï¼ˆå¦‚æœå¤šä¸ªï¼‰
        if len(group_info['repositories']) > 1:
            for repo in group_info['repositories']:
                repo_count = (group_df['repository'] == repo).sum()
                print(f"      - {repo}: {repo_count} è¡Œ")

    return task_group_data


def verify_split_completeness(df, task_group_data):
    """éªŒè¯åˆ†å‰²å®Œæ•´æ€§"""
    print("\nğŸ” éªŒè¯åˆ†å‰²å®Œæ•´æ€§...")

    total_samples = sum(group['sample_count'] for group in task_group_data.values())
    original_samples = len(df)

    print(f"  åŸå§‹æ ·æœ¬æ•°: {original_samples}")
    print(f"  åˆ†ç»„åæ€»æ•°: {total_samples}")

    if total_samples == original_samples:
        print(f"  âœ… æ ·æœ¬æ•°ä¸€è‡´")
    else:
        diff = original_samples - total_samples
        print(f"  âš ï¸  å·®å¼‚: {diff} è¡Œ")

        # æ£€æŸ¥æœªåˆ†é…çš„è¡Œ
        all_repos = []
        for group_info in TASK_GROUPS.values():
            all_repos.extend(group_info['repositories'])

        unassigned = df[~df['repository'].isin(all_repos)]
        if len(unassigned) > 0:
            print(f"  âš ï¸  æœªåˆ†é…çš„repository:")
            for repo in unassigned['repository'].unique():
                count = (unassigned['repository'] == repo).sum()
                print(f"    - {repo}: {count} è¡Œ")

    # æ£€æŸ¥é‡å 
    print("\n  æ£€æŸ¥ä»»åŠ¡ç»„é—´é‡å :")
    has_overlap = False

    group_ids = list(task_group_data.keys())
    for i in range(len(group_ids)):
        for j in range(i+1, len(group_ids)):
            group_i = task_group_data[group_ids[i]]
            group_j = task_group_data[group_ids[j]]

            # æ£€æŸ¥experiment_idæ˜¯å¦æœ‰é‡å 
            ids_i = set(group_i['data']['experiment_id'])
            ids_j = set(group_j['data']['experiment_id'])
            overlap = ids_i & ids_j

            if overlap:
                has_overlap = True
                print(f"  âš ï¸  {group_i['info']['name']} å’Œ {group_j['info']['name']} é‡å : {len(overlap)} ä¸ªå®éªŒ")

    if not has_overlap:
        print(f"  âœ… æ— é‡å ")

    return total_samples == original_samples and not has_overlap


def analyze_task_group_quality(task_group_data):
    """åˆ†æå„ä»»åŠ¡ç»„æ•°æ®è´¨é‡"""
    print("\nğŸ“Š ä»»åŠ¡ç»„æ•°æ®è´¨é‡åˆ†æ...")

    key_vars = ['energy_cpu_total_joules', 'energy_gpu_total_joules',
                'training_duration', 'gpu_util_avg']

    results = {}

    for group_id, group in task_group_data.items():
        group_df = group['data']
        group_name = group['info']['name']

        print(f"\n  {group_name}:")
        print(f"    æ ·æœ¬æ•°: {len(group_df)}")

        # è®¡ç®—å…³é”®å˜é‡å¡«å……ç‡
        group_results = {}
        for var in key_vars:
            if var in group_df.columns:
                filled = group_df[var].notna().sum()
                fill_rate = (filled / len(group_df)) * 100
                group_results[var] = fill_rate

                if fill_rate < 70:
                    status = "âš ï¸ "
                else:
                    status = "âœ…"

                print(f"    {status} {var:30s}: {fill_rate:5.1f}%")

        results[group_id] = group_results

    return results


def save_task_groups(task_group_data):
    """ä¿å­˜ä»»åŠ¡ç»„æ•°æ®"""
    print("\nğŸ’¾ ä¿å­˜ä»»åŠ¡ç»„æ•°æ®...")

    saved_files = []

    for group_id, group in task_group_data.items():
        group_df = group['data']
        output_file = group['info']['output_file']
        group_name = group['info']['name']

        group_df.to_csv(output_file, index=False)

        file_size = output_file.stat().st_size / 1024

        print(f"\n  âœ… {group_name}:")
        print(f"     æ–‡ä»¶: {output_file.name}")
        print(f"     è¡Œæ•°: {len(group_df)}")
        print(f"     åˆ—æ•°: {len(group_df.columns)}")
        print(f"     å¤§å°: {file_size:.1f} KB")

        saved_files.append({
            'group_id': group_id,
            'group_name': group_name,
            'file_path': output_file,
            'sample_count': len(group_df),
            'column_count': len(group_df.columns),
            'file_size_kb': file_size
        })

    return saved_files


def generate_grouping_report(df, task_group_data, quality_results, saved_files):
    """ç”Ÿæˆä»»åŠ¡åˆ†ç»„æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆä»»åŠ¡åˆ†ç»„æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ3: ä»»åŠ¡åˆ†ç»„æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
    report_lines.append("")

    # æ•°æ®æ¦‚è§ˆ
    report_lines.append("=" * 80)
    report_lines.append("1. æ•°æ®æ¦‚è§ˆ")
    report_lines.append("=" * 80)
    report_lines.append(f"åŸå§‹æ€»è¡Œæ•°: {len(df):,}")
    report_lines.append(f"ä»»åŠ¡ç»„æ•°é‡: {len(task_group_data)}")
    report_lines.append("")

    # ä»»åŠ¡ç»„ç»Ÿè®¡
    report_lines.append("=" * 80)
    report_lines.append("2. ä»»åŠ¡ç»„ç»Ÿè®¡")
    report_lines.append("=" * 80)

    for file_info in saved_files:
        report_lines.append(f"\n{file_info['group_name']}:")
        report_lines.append(f"  æ–‡ä»¶å: {file_info['file_path'].name}")
        report_lines.append(f"  æ ·æœ¬æ•°: {file_info['sample_count']}")
        report_lines.append(f"  åˆ—æ•°: {file_info['column_count']}")
        report_lines.append(f"  æ–‡ä»¶å¤§å°: {file_info['file_size_kb']:.1f} KB")

    report_lines.append("")

    # æ•°æ®è´¨é‡æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("3. æ•°æ®è´¨é‡æ‘˜è¦")
    report_lines.append("=" * 80)

    for group_id, quality in quality_results.items():
        group_name = task_group_data[group_id]['info']['name']
        report_lines.append(f"\n{group_name}:")

        for var, fill_rate in quality.items():
            status = "âœ…" if fill_rate >= 70 else "âš ï¸ "
            report_lines.append(f"  {status} {var}: {fill_rate:.1f}%")

    report_lines.append("")

    # æ ·æœ¬é‡å¯¹æ¯”
    report_lines.append("=" * 80)
    report_lines.append("4. æ ·æœ¬é‡å¯¹æ¯”")
    report_lines.append("=" * 80)

    total_grouped = sum(f['sample_count'] for f in saved_files)
    report_lines.append(f"åŸå§‹æ ·æœ¬: {len(df)}")
    report_lines.append(f"åˆ†ç»„åæ€»æ•°: {total_grouped}")
    report_lines.append(f"æ ·æœ¬å®Œæ•´æ€§: {'âœ… ä¸€è‡´' if total_grouped == len(df) else 'âš ï¸  ä¸ä¸€è‡´'}")

    report_lines.append("")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… ä»»åŠ¡åˆ†ç»„æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ3: ä»»åŠ¡åˆ†ç»„ (Task Grouping)")
    print("=" * 80)

    try:
        # 1. åŠ è½½æ•°æ®
        df = load_data(INPUT_FILE)

        # 2. éªŒè¯repositoryåˆ—
        repos = verify_repository_column(df)

        # 3. åˆ†å‰²åˆ°ä»»åŠ¡ç»„
        task_group_data = split_into_task_groups(df)

        # 4. éªŒè¯åˆ†å‰²å®Œæ•´æ€§
        is_complete = verify_split_completeness(df, task_group_data)

        # 5. åˆ†æä»»åŠ¡ç»„è´¨é‡
        quality_results = analyze_task_group_quality(task_group_data)

        # 6. ä¿å­˜ä»»åŠ¡ç»„æ•°æ®
        saved_files = save_task_groups(task_group_data)

        # 7. ç”ŸæˆæŠ¥å‘Š
        generate_grouping_report(df, task_group_data, quality_results, saved_files)

        print("\n" + "=" * 80)
        print("âœ… é˜¶æ®µ3å®Œæˆ: ä»»åŠ¡åˆ†ç»„æˆåŠŸ")
        print("=" * 80)
        print(f"\nç”Ÿæˆçš„ä»»åŠ¡ç»„æ–‡ä»¶:")
        for file_info in saved_files:
            print(f"  - {file_info['file_path'].name} ({file_info['sample_count']} æ ·æœ¬)")

        if is_complete:
            return 0
        else:
            print("\nâš ï¸  è­¦å‘Š: æ ·æœ¬å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")
            return 1

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
