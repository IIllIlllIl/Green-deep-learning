#!/usr/bin/env python3
"""
é˜¶æ®µ4: One-Hotç¼–ç  (One-Hot Encoding)

åŠŸèƒ½:
1. ä¸ºæ¯ä¸ªä»»åŠ¡ç»„æ·»åŠ One-Hotç¼–ç åˆ—
2. å›¾åƒåˆ†ç±»ç»„: is_mnist, is_cifar10
3. Person_reIDç»„: is_densenet121, is_hrnet18, is_pcb
4. VulBERTaå’ŒBugå®šä½: å•ä¸€repository/modelï¼Œæ— éœ€ç¼–ç 
5. è¾“å‡º: 4ä¸ªä»»åŠ¡ç»„çš„ç¼–ç åCSVæ–‡ä»¶

ä½œè€…: Analysis Module Team
æ—¥æœŸ: 2025-12-23
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ•°æ®è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "energy_research"
PROCESSED_DIR = DATA_DIR / "processed"
REPORT_FILE = PROCESSED_DIR / "stage4_onehot_report.txt"

# ä»»åŠ¡ç»„æ–‡ä»¶
TASK_FILES = {
    'image_classification': {
        'input': PROCESSED_DIR / 'stage3_image_classification.csv',
        'output': PROCESSED_DIR / 'stage4_image_classification.csv',
        'name': 'å›¾åƒåˆ†ç±»',
        'onehot_config': {
            'type': 'repository',
            'columns': {
                'is_mnist': 'examples',
                'is_cifar10': 'pytorch_resnet_cifar10'
            }
        }
    },
    'person_reid': {
        'input': PROCESSED_DIR / 'stage3_person_reid.csv',
        'output': PROCESSED_DIR / 'stage4_person_reid.csv',
        'name': 'Person_reIDæ£€ç´¢',
        'onehot_config': {
            'type': 'model',
            'columns': {
                'is_densenet121': 'densenet121',
                'is_hrnet18': 'hrnet18',
                'is_pcb': 'pcb'
            }
        }
    },
    'vulberta': {
        'input': PROCESSED_DIR / 'stage3_vulberta.csv',
        'output': PROCESSED_DIR / 'stage4_vulberta.csv',
        'name': 'VulBERTaæ¼æ´æ£€æµ‹',
        'onehot_config': None  # å•ä¸€repository/modelï¼Œæ— éœ€ç¼–ç 
    },
    'bug_localization': {
        'input': PROCESSED_DIR / 'stage3_bug_localization.csv',
        'output': PROCESSED_DIR / 'stage4_bug_localization.csv',
        'name': 'Bugå®šä½',
        'onehot_config': None  # å•ä¸€repository/modelï¼Œæ— éœ€ç¼–ç 
    }
}


def load_task_group(filepath, task_name):
    """åŠ è½½ä»»åŠ¡ç»„æ•°æ®"""
    print(f"\nğŸ“‚ åŠ è½½ {task_name}...")
    df = pd.read_csv(filepath)
    print(f"   è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
    return df


def add_onehot_encoding(df, onehot_config, task_name):
    """
    æ·»åŠ One-Hotç¼–ç åˆ—

    å‚æ•°:
        df: DataFrame
        onehot_config: One-Hoté…ç½® {'type': 'repository'/'model', 'columns': {...}}
        task_name: ä»»åŠ¡ç»„åç§°

    è¿”å›:
        ç¼–ç åçš„DataFrame
    """
    if onehot_config is None:
        print(f"   â„¹ï¸  {task_name}: å•ä¸€repository/modelï¼Œæ— éœ€One-Hotç¼–ç ")
        return df, 0

    print(f"\nğŸ”§ æ·»åŠ One-Hotç¼–ç  ({task_name})...")

    encoding_type = onehot_config['type']  # 'repository' or 'model'
    columns_map = onehot_config['columns']  # {new_col: value}

    # æ ¹æ®ç±»å‹é€‰æ‹©æºåˆ—
    source_column = encoding_type

    print(f"   ç¼–ç ç±»å‹: {encoding_type}")
    print(f"   æºåˆ—: {source_column}")
    print(f"   æ–°å¢åˆ—æ•°: {len(columns_map)}")

    # åˆ›å»ºOne-Hotåˆ—
    added_columns = []
    for new_col, target_value in columns_map.items():
        df[new_col] = (df[source_column] == target_value).astype(int)
        added_columns.append(new_col)

        # ç»Ÿè®¡
        count = df[new_col].sum()
        percentage = (count / len(df)) * 100
        print(f"   âœ… {new_col}: {count} è¡Œ ({percentage:.1f}%)")

    return df, len(added_columns)


def verify_onehot_encoding(df, onehot_config, task_name):
    """
    éªŒè¯One-Hotç¼–ç çš„æ­£ç¡®æ€§

    æ£€æŸ¥:
    1. åˆ—æ˜¯å¦äºŒå€¼åŒ– (0æˆ–1)
    2. æ¯è¡Œæ˜¯å¦æ°å¥½æœ‰ä¸€ä¸ª1 (äº’æ–¥æ€§)
    3. æ€»å’Œæ˜¯å¦ç­‰äºè¡Œæ•°
    """
    if onehot_config is None:
        return True

    print(f"\nğŸ” éªŒè¯One-Hotç¼–ç  ({task_name})...")

    columns_map = onehot_config['columns']
    onehot_cols = list(columns_map.keys())

    all_valid = True

    # 1. æ£€æŸ¥äºŒå€¼åŒ–
    for col in onehot_cols:
        unique_values = df[col].unique()
        if not set(unique_values).issubset({0, 1}):
            print(f"   âŒ {col}: åŒ…å«éäºŒå€¼æ•°æ® {unique_values}")
            all_valid = False
        else:
            print(f"   âœ… {col}: äºŒå€¼åŒ–æ­£ç¡® (0æˆ–1)")

    # 2. æ£€æŸ¥äº’æ–¥æ€§ (æ¯è¡Œæ°å¥½æœ‰ä¸€ä¸ª1)
    row_sums = df[onehot_cols].sum(axis=1)
    if (row_sums != 1).any():
        invalid_count = (row_sums != 1).sum()
        print(f"   âš ï¸  äº’æ–¥æ€§è¿è§„: {invalid_count} è¡Œä¸æ»¡è¶³æ°å¥½ä¸€ä¸ª1")
        all_valid = False
    else:
        print(f"   âœ… äº’æ–¥æ€§: æ‰€æœ‰è¡Œæ°å¥½æœ‰ä¸€ä¸ª1")

    # 3. æ£€æŸ¥æ€»å’Œ
    total_sum = df[onehot_cols].sum().sum()
    expected_sum = len(df)
    if total_sum == expected_sum:
        print(f"   âœ… æ€»å’ŒéªŒè¯: {total_sum} = {expected_sum}")
    else:
        print(f"   âŒ æ€»å’ŒéªŒè¯: {total_sum} â‰  {expected_sum}")
        all_valid = False

    return all_valid


def save_encoded_data(df, output_file, task_name):
    """ä¿å­˜ç¼–ç åçš„æ•°æ®"""
    df.to_csv(output_file, index=False)
    file_size = output_file.stat().st_size / 1024

    print(f"\nğŸ’¾ ä¿å­˜ {task_name}:")
    print(f"   æ–‡ä»¶: {output_file.name}")
    print(f"   è¡Œæ•°: {len(df)}")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   å¤§å°: {file_size:.1f} KB")

    return {
        'task_name': task_name,
        'file_path': output_file,
        'row_count': len(df),
        'column_count': len(df.columns),
        'file_size_kb': file_size
    }


def generate_onehot_report(results):
    """ç”ŸæˆOne-Hotç¼–ç æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”ŸæˆOne-Hotç¼–ç æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("é˜¶æ®µ4: One-Hotç¼–ç æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")

    # ä»»åŠ¡ç»„æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("1. ä»»åŠ¡ç»„ç¼–ç æ‘˜è¦")
    report_lines.append("=" * 80)

    for result in results:
        report_lines.append(f"\n{result['task_name']}:")
        report_lines.append(f"  è¾“å‡ºæ–‡ä»¶: {result['file_path'].name}")
        report_lines.append(f"  è¡Œæ•°: {result['row_count']}")
        report_lines.append(f"  åˆ—æ•°: {result['column_count']} (æ–°å¢: {result.get('added_columns', 0)}åˆ—)")
        report_lines.append(f"  æ–‡ä»¶å¤§å°: {result['file_size_kb']:.1f} KB")

        if result.get('added_columns', 0) > 0:
            report_lines.append(f"  éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if result.get('validation', False) else 'âŒ å¤±è´¥'}")

    report_lines.append("")

    # ç»Ÿè®¡æ‘˜è¦
    report_lines.append("=" * 80)
    report_lines.append("2. ç¼–ç ç»Ÿè®¡")
    report_lines.append("=" * 80)

    total_samples = sum(r['row_count'] for r in results)
    tasks_with_encoding = sum(1 for r in results if r.get('added_columns', 0) > 0)
    total_onehot_cols = sum(r.get('added_columns', 0) for r in results)

    report_lines.append(f"ä»»åŠ¡ç»„æ€»æ•°: {len(results)}")
    report_lines.append(f"éœ€è¦ç¼–ç çš„ä»»åŠ¡ç»„: {tasks_with_encoding}")
    report_lines.append(f"æ–°å¢One-Hotåˆ—æ€»æ•°: {total_onehot_cols}")
    report_lines.append(f"æ€»æ ·æœ¬æ•°: {total_samples}")

    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("âœ… é˜¶æ®µ4: One-Hotç¼–ç å®Œæˆ")
    report_lines.append("=" * 80)

    # å†™å…¥æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"âœ… One-Hotç¼–ç æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é˜¶æ®µ4: One-Hotç¼–ç  (One-Hot Encoding)")
    print("=" * 80)

    try:
        results = []

        for task_id, task_config in TASK_FILES.items():
            # 1. åŠ è½½æ•°æ®
            df = load_task_group(task_config['input'], task_config['name'])

            # 2. æ·»åŠ One-Hotç¼–ç 
            df_encoded, added_count = add_onehot_encoding(
                df,
                task_config['onehot_config'],
                task_config['name']
            )

            # 3. éªŒè¯ç¼–ç 
            validation_passed = verify_onehot_encoding(
                df_encoded,
                task_config['onehot_config'],
                task_config['name']
            )

            # 4. ä¿å­˜æ•°æ®
            result = save_encoded_data(
                df_encoded,
                task_config['output'],
                task_config['name']
            )

            result['added_columns'] = added_count
            result['validation'] = validation_passed
            results.append(result)

        # 5. ç”ŸæˆæŠ¥å‘Š
        generate_onehot_report(results)

        print("\n" + "=" * 80)
        print("âœ… é˜¶æ®µ4å®Œæˆ: One-Hotç¼–ç æˆåŠŸ")
        print("=" * 80)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰éªŒè¯é€šè¿‡
        all_validated = all(r.get('validation', True) for r in results)

        if all_validated:
            return 0
        else:
            print("\nâš ï¸  è­¦å‘Š: éƒ¨åˆ†ä»»åŠ¡ç»„éªŒè¯å¤±è´¥")
            return 1

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
