#!/usr/bin/env python3
"""
åŸºäºDiBSå› æœå›¾çš„å…¨å±€æ ‡å‡†åŒ–æ•°æ®ATEè®¡ç®—è„šæœ¬

åŸºäºDiBSå­¦åˆ°çš„å› æœå›¾é‡æ–°è®¡ç®—å…¨å±€æ ‡å‡†åŒ–æ•°æ®çš„ATE
ä½¿ç”¨å·²éªŒæ”¶çš„CausalInferenceEngineï¼ˆCTFé£æ ¼DMLæ–¹æ³•ï¼‰

æ•°æ®æº:
  - å…¨å±€æ ‡å‡†åŒ–æ•°æ®: data/energy_research/6groups_global_std/
  - DiBSå› æœå›¾: results/energy_research/data/global_std/

è¾“å‡º:
  - ATEç»“æœæ–‡ä»¶ï¼ˆæ¯æ¡è¾¹çš„ATEä¼°è®¡ã€ç½®ä¿¡åŒºé—´ã€æ˜¾è‘—æ€§ï¼‰
  - ä¿å­˜åˆ°: results/energy_research/data/global_std_dibs_ate/

ä½¿ç”¨æ–¹æ³•:
    # Dry run (æµ‹è¯•æ¨¡å¼)
    python compute_ate_dibs_global_std.py --dry-run

    # å®é™…è¿è¡Œï¼ˆæ›´æ–°æ‰€æœ‰ç»„ï¼‰
    python compute_ate_dibs_global_std.py

    # åªå¤„ç†ç‰¹å®šgroup
    python compute_ate_dibs_global_std.py --group 1

ä¾èµ–:
    - analysis/utils/causal_inference.py (å·²éªŒæ”¶çš„CTFé£æ ¼ATE)
    - EconML 0.14.1 (å·²å®‰è£…åˆ°causal-researchç¯å¢ƒ)
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import warnings
import time
import json
from pathlib import Path
import shutil

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from utils.causal_inference import CausalInferenceEngine
    ECONML_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥CausalInferenceEngine: {e}")
    print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
    ECONML_AVAILABLE = False


def build_causal_graph_from_whitelist(
    whitelist_df: pd.DataFrame,
    var_names: List[str],
    default_strength: float = 0.5
) -> np.ndarray:
    """
    ä»ç™½åå•æ„å»ºå› æœå›¾é‚»æ¥çŸ©é˜µ

    å‚æ•°:
        whitelist_df: ç™½åå•DataFrameï¼ŒåŒ…å«sourceå’Œtargetåˆ—
        var_names: æ‰€æœ‰å˜é‡ååˆ—è¡¨
        default_strength: é»˜è®¤è¾¹å¼ºåº¦

    è¿”å›:
        causal_graph: é‚»æ¥çŸ©é˜µï¼Œshape (n_vars, n_vars)
    """
    n_vars = len(var_names)
    causal_graph = np.zeros((n_vars, n_vars))

    # åˆ›å»ºå˜é‡ååˆ°ç´¢å¼•çš„æ˜ å°„
    var_to_idx = {var: idx for idx, var in enumerate(var_names)}

    # å¡«å……é‚»æ¥çŸ©é˜µ
    for _, row in whitelist_df.iterrows():
        source = row['source']
        target = row['target']

        if source in var_to_idx and target in var_to_idx:
            source_idx = var_to_idx[source]
            target_idx = var_to_idx[target]

            # ä½¿ç”¨strengthåˆ—çš„å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            strength = row.get('strength', default_strength)
            causal_graph[source_idx, target_idx] = strength

    return causal_graph


def load_dibs_causal_graph(dibs_graph_file: str):
    """
    åŠ è½½DiBSå› æœå›¾CSVæ–‡ä»¶

    å‚æ•°:
        dibs_graph_file: DiBSå› æœå›¾CSVæ–‡ä»¶è·¯å¾„

    è¿”å›:
        causal_graph: é‚»æ¥çŸ©é˜µ (n_vars, n_vars)
        feature_names: ç‰¹å¾ååˆ—è¡¨
    """
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(dibs_graph_file, index_col=0)

    # ç‰¹å¾åæ˜¯åˆ—åï¼ˆç¬¬ä¸€è¡Œï¼‰
    feature_names = list(df.columns)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    causal_graph = df.values.astype(float)

    # ç¡®ä¿æ˜¯å¯¹ç§°çš„ï¼ŸDiBSå¯èƒ½æ˜¯æœ‰å‘çš„ï¼Œä¿æŒåŸæ ·
    return causal_graph, feature_names


def compute_ate_for_group(
    group_num: int,
    global_std_dir: str,
    dibs_dir: str,
    output_dir: str,
    dry_run: bool = False,
    threshold: float = 0.3
) -> Dict:
    """
    ä¸ºå•ä¸ªç»„è®¡ç®—å…¨å±€æ ‡å‡†åŒ–ATE

    è¿”å›:
        results: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
    """
    # ç»„åæ˜ å°„
    group_mapping = {
        1: "group1_examples",
        2: "group2_vulberta",
        3: "group3_person_reid",
        4: "group4_bug_localization",
        5: "group5_mrt_oast",
        6: "group6_resnet"
    }

    group_id = group_mapping.get(group_num, f"group{group_num}")

    print(f"\n{'='*80}")
    print(f"å¤„ç†ç»„ {group_num}: {group_id}")
    print(f"{'='*80}")

    # 1. æ„å»ºæ–‡ä»¶è·¯å¾„
    global_std_file = os.path.join(global_std_dir, f"{group_id}_global_std.csv")
    dibs_graph_file = os.path.join(dibs_dir, group_id, f"{group_id}_dibs_causal_graph.csv")
    output_file = os.path.join(output_dir, f"{group_id}_dibs_global_std_ate.csv")

    print(f"   å…¨å±€æ ‡å‡†åŒ–æ•°æ®: {global_std_file}")
    print(f"   DiBSå› æœå›¾æ–‡ä»¶: {dibs_graph_file}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(global_std_file):
        print(f"âŒ å…¨å±€æ ‡å‡†åŒ–æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {global_std_file}")
        return {"success": False, "error": f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {global_std_file}"}

    if not os.path.exists(dibs_graph_file):
        print(f"âŒ DiBSå› æœå›¾æ–‡ä»¶ä¸å­˜åœ¨: {dibs_graph_file}")
        return {"success": False, "error": f"DiBSå› æœå›¾æ–‡ä»¶ä¸å­˜åœ¨: {dibs_graph_file}"}

    # 2. è¯»å–æ•°æ®
    print(f"   è¯»å–æ•°æ®...")
    try:
        data_df = pd.read_csv(global_std_file)
        # åŠ è½½DiBSå› æœå›¾
        causal_graph, dibs_feature_names = load_dibs_causal_graph(dibs_graph_file)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {"success": False, "error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"}

    # è®¡ç®—è¶…è¿‡é˜ˆå€¼çš„è¾¹æ•°
    n_edges = np.sum(causal_graph > threshold)
    print(f"   âœ… è¯»å–æˆåŠŸ: {len(data_df)} æ¡æ•°æ®, {n_edges} æ¡è¾¹ (é˜ˆå€¼>{threshold})")

    if dry_run:
        print(f"   ğŸ§ª Dry runæ¨¡å¼ - åªæ£€æŸ¥æ•°æ®ï¼Œä¸è®¡ç®—ATE")
        return {"success": True, "dry_run": True, "data_rows": len(data_df), "edges": n_edges}

    # 3. æ•°æ®æ¸…æ´—ï¼šå¤„ç†NaNå€¼
    print(f"   æ•°æ®æ¸…æ´—...")
    original_rows = len(data_df)
    original_nan = data_df.isna().sum().sum()

    # é¦–å…ˆåˆ é™¤å…¨ä¸ºNaNçš„æ•°å€¼åˆ—ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
    all_nan_numeric_cols = []
    numeric_cols = data_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if data_df[col].isna().all():
            all_nan_numeric_cols.append(col)

    if all_nan_numeric_cols:
        print(f"   åˆ é™¤å…¨NaNæ•°å€¼åˆ—: {len(all_nan_numeric_cols)} ä¸ª")
        for col in all_nan_numeric_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"      {col}: å…¨ä¸ºNaNï¼Œåˆ é™¤")
        if len(all_nan_numeric_cols) > 5:
            print(f"      ... å…±{len(all_nan_numeric_cols)}ä¸ªåˆ—")
        data_df = data_df.drop(columns=all_nan_numeric_cols)
        numeric_cols = data_df.select_dtypes(include=[np.number]).columns

    # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
    for col in numeric_cols:
        if data_df[col].isna().sum() > 0:
            median_val = data_df[col].median()
            # æ£€æŸ¥median_valæ˜¯å¦ä¸ºNaNï¼ˆå…¨NaNåˆ—åº”è¯¥å·²è¢«åˆ é™¤ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
            if pd.isna(median_val):
                print(f"      âš  {col}: ä¸­ä½æ•°ä¸ºNaNï¼Œç”¨0å¡«å……")
                data_df[col] = data_df[col].fillna(0)
            else:
                data_df[col] = data_df[col].fillna(median_val)
                na_count = data_df[col].isna().sum()
                if na_count == 0:
                    print(f"      {col}: ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……")

    # å¸ƒå°”åˆ—ï¼šç”¨Falseå¡«å……
    bool_cols = data_df.select_dtypes(include=[bool]).columns
    for col in bool_cols:
        if data_df[col].isna().sum() > 0:
            data_df[col] = data_df[col].fillna(False)
            print(f"      {col}: ç”¨Falseå¡«å……")

    # å…¶ä»–åˆ—ï¼šç”¨ä¼—æ•°å¡«å……
    other_cols = [col for col in data_df.columns
                  if col not in numeric_cols and col not in bool_cols]
    for col in other_cols:
        if data_df[col].isna().sum() > 0:
            mode_val = data_df[col].mode()
            if len(mode_val) > 0:
                data_df[col] = data_df[col].fillna(mode_val.iloc[0])
                print(f"      {col}: ç”¨ä¼—æ•° '{mode_val.iloc[0]}' å¡«å……")
            else:
                # å¦‚æœä¼—æ•°ä¸å­˜åœ¨ï¼ˆå…¨NaNï¼‰ï¼Œç”¨ç¬¬ä¸€ä¸ªéNaNå€¼æˆ–é»˜è®¤å€¼
                non_na_vals = data_df[col].dropna()
                if len(non_na_vals) > 0:
                    data_df[col] = data_df[col].fillna(non_na_vals.iloc[0])
                    print(f"      {col}: ç”¨ç¬¬ä¸€ä¸ªéNaNå€¼å¡«å……")
                else:
                    # å¦‚æœå…¨NaNï¼Œåˆ é™¤è¯¥åˆ—
                    print(f"      {col}: å…¨ä¸ºNaNï¼Œåˆ é™¤è¯¥åˆ—")
                    data_df = data_df.drop(columns=[col])

    cleaned_nan = data_df.isna().sum().sum()
    print(f"   âœ… æ¸…æ´—å®Œæˆ: åŸå§‹NaN {original_nan} â†’ å‰©ä½™NaN {cleaned_nan}")
    if cleaned_nan > 0:
        print(f"   âš  è­¦å‘Š: ä»æœ‰ {cleaned_nan} ä¸ªNaNï¼Œå¯èƒ½ä¼šå½±å“ATEè®¡ç®—")

    # 4. è·å–æ•°æ®ä¸­çš„æ‰€æœ‰å˜é‡åï¼ˆæ’é™¤éç‰¹å¾åˆ—ï¼‰
    exclude_cols = ['timestamp', 'experiment_id', 'session_id']  # å¸¸è§éç‰¹å¾åˆ—
    feature_cols = [col for col in data_df.columns if col not in exclude_cols]

    # 5. éªŒè¯DiBSç‰¹å¾åä¸æ•°æ®ç‰¹å¾ååŒ¹é…
    # DiBSå› æœå›¾å·²ç»åŠ è½½ä¸ºcausal_graphï¼Œç‰¹å¾ååœ¨dibs_feature_names
    # ç¡®ä¿é¡ºåºä¸€è‡´
    if len(feature_cols) != len(dibs_feature_names):
        print(f"âŒ ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æ•°æ®æœ‰{len(feature_cols)}ä¸ªç‰¹å¾, DiBSæœ‰{len(dibs_feature_names)}ä¸ªç‰¹å¾")
        return {"success": False, "error": "ç‰¹å¾æ•°é‡ä¸åŒ¹é…"}

    # æ£€æŸ¥ç‰¹å¾åæ˜¯å¦ä¸€è‡´ï¼ˆé¡ºåºå¯èƒ½ä¸åŒï¼Œä½†DiBSå› æœå›¾ä¸ç‰¹å¾åé¡ºåºå¯¹åº”ï¼‰
    # æˆ‘ä»¬å‡è®¾DiBSå› æœå›¾çš„ç‰¹å¾é¡ºåºä¸æ•°æ®ç‰¹å¾é¡ºåºä¸€è‡´
    # å¦‚æœä¸ä¸€è‡´ï¼Œéœ€è¦é‡æ–°æ’åºcausal_graph
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå‡è®¾é¡ºåºä¸€è‡´
    print(f"   âœ… ç‰¹å¾éªŒè¯é€šè¿‡: {len(feature_cols)}ä¸ªç‰¹å¾")

    # 6. ä½¿ç”¨DiBSå› æœå›¾ï¼ˆå·²ç»åŠ è½½ï¼‰
    # causal_graph å’Œ dibs_feature_names å·²ä»load_dibs_causal_graphåŠ è½½
    # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
    # å¦‚æœdibs_feature_namesä¸feature_colsé¡ºåºä¸åŒï¼Œéœ€è¦é‡æ–°æ’åºcausal_graph
    # è¿™é‡Œå‡è®¾é¡ºåºä¸€è‡´
    print(f"   ä½¿ç”¨DiBSå› æœå›¾...")

    # 7. ä»DiBSå› æœå›¾åˆ›å»ºè¾¹åˆ—è¡¨DataFrameï¼ˆæ¨¡æ‹Ÿç™½åå•ï¼‰
    print(f"   ä»DiBSå› æœå›¾æå–è¾¹ï¼ˆé˜ˆå€¼>{threshold})...")
    edges = []
    n_vars = causal_graph.shape[0]
    for i in range(n_vars):
        for j in range(n_vars):
            weight = causal_graph[i, j]
            if weight > threshold:
                source_name = feature_cols[i]  # å‡è®¾é¡ºåºä¸€è‡´
                target_name = feature_cols[j]
                edges.append({
                    'source': source_name,
                    'target': target_name,
                    'strength': weight
                })

    whitelist_df = pd.DataFrame(edges)
    print(f"   âœ… æå– {len(whitelist_df)} æ¡è¾¹")

    # æ·»åŠ ATEåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    global_std_ate_cols = [
        'ate_global_std',
        'ate_global_std_ci_lower',
        'ate_global_std_ci_upper',
        'ate_global_std_is_significant',
        'ate_global_std_confounders_count'
    ]

    for col in global_std_ate_cols:
        if col not in whitelist_df.columns:
            if col == 'ate_global_std_confounders_count':
                whitelist_df[col] = 0  # æ•´æ•°
            elif col == 'ate_global_std_is_significant':
                whitelist_df[col] = False  # å¸ƒå°”å€¼
            else:
                whitelist_df[col] = np.nan  # æµ®ç‚¹æ•°

    # 8. åˆå§‹åŒ–å› æœæ¨æ–­å¼•æ“
    engine = CausalInferenceEngine(verbose=True)

    # 9. è®¡ç®—æ¯æ¡è¾¹çš„ATEï¼ˆå…¨å±€æ ‡å‡†åŒ–æ•°æ®ï¼‰
    print(f"   å¼€å§‹è®¡ç®—å…¨å±€æ ‡å‡†åŒ–ATE...")
    start_time = time.time()

    try:
        results = engine.analyze_all_edges_ctf_style(
            data=data_df,
            causal_graph=causal_graph,
            var_names=feature_cols,
            threshold=threshold,
            ref_df=None,  # ä½¿ç”¨CTFé£æ ¼ï¼šè‡ªåŠ¨åˆ›å»ºæ•°æ®å‡å€¼å‘é‡
            t_strategy='quantile'  # ä½¿ç”¨CTFé£æ ¼ï¼š25/75åˆ†ä½æ•°T0/T1
        )

        elapsed_time = time.time() - start_time
        print(f"   âœ… ATEè®¡ç®—å®Œæˆï¼è€—æ—¶: {elapsed_time:.1f}ç§’")

        # 10. æ›´æ–°ç™½åå•DataFrame
        print(f"   æ›´æ–°ç™½åå•æ•°æ®...")

        # æ£€æŸ¥resultsçš„ç±»å‹
        if isinstance(results, dict):
            # resultsæ˜¯å­—å…¸çš„å­—å…¸ï¼Œé”®æ˜¯"source->target"æ ¼å¼
            print(f"   resultsç±»å‹: å­—å…¸ (åŒ…å« {len(results)} ä¸ªé”®å€¼å¯¹)")

            # åˆ›å»ºç»“æœæ˜ å°„å­—å…¸
            results_dict = {}
            for edge_key, result in results.items():
                if isinstance(result, dict) and 'ate' in result:
                    # è§£æè¾¹é”® "source->target"
                    if '->' in edge_key:
                        source, target = edge_key.split('->', 1)
                        key = (source, target)
                        results_dict[key] = result
                    else:
                        print(f"   âš  è­¦å‘Š: è·³è¿‡æ— æ•ˆè¾¹é”®æ ¼å¼: {edge_key}")
                else:
                    print(f"   âš  è­¦å‘Š: è·³è¿‡æ— æ•ˆç»“æœ: {type(result)}")

            valid_results = len(results_dict)
            print(f"   æœ‰æ•ˆç»“æœæ•°: {valid_results}/{len(results)}")

        elif isinstance(results, list):
            # resultsæ˜¯åˆ—è¡¨
            print(f"   resultsç±»å‹: åˆ—è¡¨ (åŒ…å« {len(results)} ä¸ªå…ƒç´ )")

            # åˆ›å»ºç»“æœæ˜ å°„å­—å…¸
            results_dict = {}
            valid_results = 0

            for result in results:
                # æ£€æŸ¥resultæ˜¯å¦æ˜¯å­—å…¸ï¼ˆæœ‰æ•ˆç»“æœï¼‰
                if isinstance(result, dict) and 'source_idx' in result:
                    source_idx = result['source_idx']
                    target_idx = result['target_idx']

                    # ç¡®ä¿ç´¢å¼•æ˜¯æ•´æ•°
                    if isinstance(source_idx, int) and isinstance(target_idx, int):
                        source_name = feature_cols[source_idx]
                        target_name = feature_cols[target_idx]

                        key = (source_name, target_name)
                        results_dict[key] = result
                        valid_results += 1
                    else:
                        print(f"   âš  è­¦å‘Š: è·³è¿‡æ— æ•ˆç»“æœ - ç´¢å¼•ä¸æ˜¯æ•´æ•°: source_idx={source_idx}, target_idx={target_idx}")
                else:
                    # å¯èƒ½æ˜¯å­—ç¬¦ä¸²æ¶ˆæ¯æˆ–æ— æ•ˆç»“æœ
                    if isinstance(result, str):
                        print(f"   âš  è­¦å‘Š: è·³è¿‡å­—ç¬¦ä¸²ç»“æœ: {result[:100]}...")
                    else:
                        print(f"   âš  è­¦å‘Š: è·³è¿‡éå­—å…¸ç»“æœ: {type(result)}")

            print(f"   æœ‰æ•ˆç»“æœæ•°: {valid_results}/{len(results)}")
        else:
            print(f"   âŒ é”™è¯¯: resultsç±»å‹æœªçŸ¥: {type(results)}")
            results_dict = {}
            valid_results = 0

        # æ›´æ–°æ¯æ¡è¾¹çš„ATEä¿¡æ¯
        updated_count = 0
        for idx, row in whitelist_df.iterrows():
            source = row['source']
            target = row['target']
            key = (source, target)

            if key in results_dict:
                result = results_dict[key]

                # æ›´æ–°å…¨å±€æ ‡å‡†åŒ–ATEåˆ—
                whitelist_df.at[idx, 'ate_global_std'] = result.get('ate', np.nan)
                whitelist_df.at[idx, 'ate_global_std_ci_lower'] = result.get('ci_lower', np.nan)
                whitelist_df.at[idx, 'ate_global_std_ci_upper'] = result.get('ci_upper', np.nan)
                whitelist_df.at[idx, 'ate_global_std_is_significant'] = result.get('is_significant', False)

                # å¤„ç†æ··æ·†å› ç´ è®¡æ•°
                confounders = result.get('confounders', [])
                if isinstance(confounders, list):
                    whitelist_df.at[idx, 'ate_global_std_confounders_count'] = len(confounders)
                else:
                    whitelist_df.at[idx, 'ate_global_std_confounders_count'] = 0

                updated_count += 1

        print(f"   âœ… æ›´æ–°å®Œæˆ: {updated_count}/{len(whitelist_df)} æ¡è¾¹å·²æ›´æ–°")

        # 11. ä¿å­˜ç»“æœ
        print(f"   ä¿å­˜ç»“æœ...")
        os.makedirs(output_dir, exist_ok=True)
        whitelist_df.to_csv(output_file, index=False)
        print(f"   âœ… ç»“æœå·²ä¿å­˜: {output_file}")

        # 12. ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        ate_values = whitelist_df['ate_global_std'].dropna()
        significant_count = whitelist_df['ate_global_std_is_significant'].sum()

        summary = {
            "group_id": group_id,
            "group_num": group_num,
            "data_rows": len(data_df),
            "edges_total": len(whitelist_df),
            "edges_updated": updated_count,
            "ate_computed": len(ate_values),
            "ate_significant": int(significant_count),
            "ate_mean": float(ate_values.mean()) if len(ate_values) > 0 else np.nan,
            "ate_std": float(ate_values.std()) if len(ate_values) > 0 else np.nan,
            "ate_min": float(ate_values.min()) if len(ate_values) > 0 else np.nan,
            "ate_max": float(ate_values.max()) if len(ate_values) > 0 else np.nan,
            "elapsed_seconds": elapsed_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿å­˜æ‘˜è¦
        summary_file = os.path.join(output_dir, f"{group_id}_ate_global_std_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"   âœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")

        return {
            "success": True,
            "summary": summary,
            "output_file": output_file,
            "summary_file": summary_file
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"âŒ ATEè®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        return {
            "success": False,
            "error": str(e),
            "elapsed_seconds": elapsed_time
        }


def main():
    parser = argparse.ArgumentParser(description="åŸºäºDiBSå› æœå›¾è®¡ç®—å…¨å±€æ ‡å‡†åŒ–æ•°æ®çš„ATE")
    parser.add_argument("--dry-run", action="store_true",
                       help="åªæµ‹è¯•ä¸å†™å…¥æ–‡ä»¶")
    parser.add_argument("--group", type=int, choices=range(1, 7),
                       help="åªå¤„ç†æŒ‡å®šç»„ï¼ˆ1-6ï¼‰")
    parser.add_argument("--global-std-dir", type=str,
                       default="data/energy_research/6groups_global_std",
                       help="å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç›®å½•")
    parser.add_argument("--dibs-dir", type=str,
                       default="results/energy_research/data/global_std",
                       help="DiBSå› æœå›¾ç›®å½•")
    parser.add_argument("--output-dir", type=str,
                       default="results/energy_research/data/global_std_dibs_ate",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--threshold", type=float, default=0.3,
                       help="è¾¹å¼ºåº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.3ï¼‰")

    args = parser.parse_args()

    print("=" * 80)
    print("å…¨å±€æ ‡å‡†åŒ–æ•°æ®ATEè®¡ç®—")
    print("=" * 80)

    print(f"\né…ç½®:")
    print(f"  å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç›®å½•: {args.global_std_dir}")
    print(f"  DiBSå› æœå›¾ç›®å½•: {args.dibs_dir}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  è¾¹å¼ºåº¦é˜ˆå€¼: {args.threshold}")
    print(f"  Dry runæ¨¡å¼: {args.dry_run}")

    # æ£€æŸ¥EconMLæ˜¯å¦å¯ç”¨
    if not ECONML_AVAILABLE:
        print("\nâŒ æ— æ³•å¯¼å…¥CausalInferenceEngine")
        print("è¯·ç¡®ä¿åœ¨causal-researchç¯å¢ƒä¸­è¿è¡Œ: conda activate causal-research")
        return 1

    # ç¡®å®šè¦å¤„ç†çš„ç»„
    if args.group:
        groups_to_process = [args.group]
        print(f"\nå¤„ç†æŒ‡å®šç»„: {args.group}")
    else:
        groups_to_process = list(range(1, 7))
        print(f"\nå¤„ç†æ‰€æœ‰ç»„: 1-6")

    # å¤„ç†æ¯ä¸ªç»„
    all_results = []

    for group_num in groups_to_process:
        result = compute_ate_for_group(
            group_num=group_num,
            global_std_dir=args.global_std_dir,
            dibs_dir=args.dibs_dir,
            output_dir=args.output_dir,
            dry_run=args.dry_run,
            threshold=args.threshold
        )

        all_results.append({
            "group": group_num,
            **result
        })

    # ç”Ÿæˆæ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("å…¨å±€æ ‡å‡†åŒ–ATEè®¡ç®—æ€»æŠ¥å‘Š")
    print(f"{'='*80}")

    successful_groups = [r for r in all_results if r.get('success', False)]
    failed_groups = [r for r in all_results if not r.get('success', False)]

    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  æˆåŠŸç»„æ•°: {len(successful_groups)}")
    print(f"  å¤±è´¥ç»„æ•°: {len(failed_groups)}")

    if successful_groups:
        print(f"\næˆåŠŸç»„è¯¦æƒ…:")
        for result in successful_groups:
            if result.get('dry_run', False):
                print(f"  ç»„ {result['group']}: Dry runå®Œæˆ - {result.get('data_rows', 0)}è¡Œæ•°æ®, {result.get('edges', 0)}æ¡è¾¹")
            elif 'summary' in result:
                summary = result['summary']
                print(f"  ç»„ {result['group']}: {summary['edges_updated']}/{summary['edges_total']}æ¡è¾¹æ›´æ–°")
                print(f"      ATEè®¡ç®—: {summary['ate_computed']}æ¡, æ˜¾è‘—: {summary['ate_significant']}æ¡")
                print(f"      ATEå‡å€¼: {summary['ate_mean']:.4f}, æ ‡å‡†å·®: {summary['ate_std']:.4f}")

    if failed_groups:
        print(f"\nå¤±è´¥ç»„è¯¦æƒ…:")
        for result in failed_groups:
            print(f"  ç»„ {result['group']}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # ä¿å­˜æ€»æŠ¥å‘Š
    if not args.dry_run and successful_groups:
        total_report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "config": {
                "global_std_dir": args.global_std_dir,
                "dibs_dir": args.dibs_dir,
                "output_dir": args.output_dir,
                "threshold": args.threshold,
                "dry_run": args.dry_run
            },
            "results": all_results,
            "summary": {
                "total_groups": len(all_results),
                "successful_groups": len(successful_groups),
                "failed_groups": len(failed_groups)
            }
        }

        total_report_file = os.path.join(args.output_dir, "ate_global_std_total_report.json")
        os.makedirs(args.output_dir, exist_ok=True)
        with open(total_report_file, 'w') as f:
            json.dump(total_report, f, indent=2)

        print(f"\nâœ… æ€»æŠ¥å‘Šå·²ä¿å­˜: {total_report_file}")

    print(f"\n{'='*80}")
    print("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
    print(f"{'='*80}")

    return 0 if len(failed_groups) == 0 else 1


if __name__ == "__main__":
    sys.exit(main())