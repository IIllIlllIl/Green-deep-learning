#!/usr/bin/env python3
"""
åˆ†æå„æ¨¡å‹çš„èµ„æºå ç”¨æƒ…å†µï¼Œè¯„ä¼°å¹¶å‘è®­ç»ƒçš„å¯è¡Œæ€§
Analyze resource usage of each model to evaluate concurrent training feasibility
"""
import json
from pathlib import Path
from collections import defaultdict
import statistics

# Hardware configuration
GPU_TOTAL_MEMORY_MB = 10240  # RTX 3080: 10GB
CPU_CORES = 20
RAM_TOTAL_GB = 30

# Load model config to get all models
with open('config/models_config.json') as f:
    config = json.load(f)

all_models = []
for repo_name, repo_info in config['models'].items():
    for model_name in repo_info['models']:
        all_models.append({
            'repo': repo_name,
            'model': model_name,
            'key': f"{repo_name}_{model_name}"
        })

print("="*100)
print("æ¨¡å‹å¹¶å‘è®­ç»ƒå¯è¡Œæ€§åˆ†æ")
print("Concurrent Training Feasibility Analysis")
print("="*100)
print(f"\nç¡¬ä»¶é…ç½®:")
print(f"  GPU: RTX 3080, {GPU_TOTAL_MEMORY_MB}MBæ˜¾å­˜")
print(f"  CPU: {CPU_CORES}æ ¸")
print(f"  RAM: {RAM_TOTAL_GB}GB\n")

# Analyze results
results_dir = Path('results')
model_stats = defaultdict(lambda: {
    'gpu_util': [], 'gpu_power': [], 'duration': [],
    'gpu_memory_est': [], 'success_count': 0, 'total_count': 0
})

print("æ­£åœ¨åˆ†æå†å²è®­ç»ƒæ•°æ®...\n")

for result_file in sorted(results_dir.glob('*.json')):
    try:
        with open(result_file) as f:
            data = json.load(f)

        repo = data.get('repository', '')
        model = data.get('model', '')
        model_key = f"{repo}_{model}"

        model_stats[model_key]['total_count'] += 1

        if data.get('training_success'):
            model_stats[model_key]['success_count'] += 1
            energy = data.get('energy_metrics', {})

            if energy:
                model_stats[model_key]['gpu_util'].append(energy.get('gpu_util_avg_percent', 0))
                model_stats[model_key]['gpu_power'].append(energy.get('gpu_power_avg_watts', 0))
                model_stats[model_key]['duration'].append(data.get('duration_seconds', 0))
    except:
        pass

# Estimate GPU memory usage based on model characteristics and GPU utilization
# Higher utilization usually correlates with higher memory usage
def estimate_gpu_memory(gpu_util_avg, model_key):
    """Estimate GPU memory based on model type and utilization"""
    # Base estimates from typical model sizes
    base_estimates = {
        'mnist': 500,      # Small CNN
        'resnet20': 800,   # Small ResNet
        'resnet32': 1000,
        'resnet44': 1200,
        'resnet56': 1500,
        'resnet110': 2500,
        'densenet121': 3000,  # Larger model
        'hrnet18': 2500,
        'pcb': 2000,
        'MRT-OAST': 3500,     # Object tracking, complex
        'VulBERTa': 4000,     # Transformer-based
        'bug-localization': 2000,
    }

    # Find base estimate
    base = 1500  # default
    for key, mem in base_estimates.items():
        if key in model_key.lower():
            base = mem
            break

    # Adjust based on utilization (high util often means high memory)
    if gpu_util_avg > 80:
        multiplier = 1.3
    elif gpu_util_avg > 50:
        multiplier = 1.1
    else:
        multiplier = 0.9

    return int(base * multiplier)

# Print detailed analysis
print("="*100)
print(f"{'æ¨¡å‹':<45} {'è®­ç»ƒæ¬¡æ•°':<10} {'æˆåŠŸç‡':<10} {'GPUåˆ©ç”¨ç‡%':<12} {'åŠŸç‡(W)':<12} {'æ—¶é•¿(s)':<12} {'æ˜¾å­˜ä¼°è®¡(MB)':<15}")
print("="*100)

model_analysis = {}
for model_info in all_models:
    model_key = model_info['key']
    stats = model_stats[model_key]

    if stats['success_count'] > 0:
        avg_util = statistics.mean(stats['gpu_util'])
        avg_power = statistics.mean(stats['gpu_power'])
        avg_duration = statistics.mean(stats['duration'])
        max_util = max(stats['gpu_util']) if stats['gpu_util'] else 0
        success_rate = stats['success_count'] / stats['total_count'] * 100

        # Estimate memory
        est_memory = estimate_gpu_memory(avg_util, model_key)

        model_analysis[model_key] = {
            'avg_gpu_util': avg_util,
            'max_gpu_util': max_util,
            'avg_power': avg_power,
            'avg_duration': avg_duration,
            'est_memory_mb': est_memory,
            'success_count': stats['success_count'],
            'success_rate': success_rate,
            'repo': model_info['repo'],
            'model': model_info['model']
        }

        print(f"{model_key:<45} {stats['success_count']:<10} {success_rate:<10.1f} "
              f"{avg_util:<12.1f} {avg_power:<12.1f} {avg_duration:<12.0f} {est_memory:<15}")
    else:
        # No data available
        # Use conservative estimates
        est_memory = estimate_gpu_memory(50, model_key)
        model_analysis[model_key] = {
            'avg_gpu_util': 50,  # assume moderate
            'max_gpu_util': 70,
            'avg_power': 150,
            'avg_duration': 600,
            'est_memory_mb': est_memory,
            'success_count': 0,
            'success_rate': 0,
            'repo': model_info['repo'],
            'model': model_info['model']
        }
        print(f"{model_key:<45} {'æ— æ•°æ®':<10} {'N/A':<10} "
              f"{'ä¼°è®¡:50':<12} {'ä¼°è®¡:150':<12} {'ä¼°è®¡:600':<12} {est_memory:<15} (ä¼°è®¡)")

# Analyze concurrent training feasibility
print("\n" + "="*100)
print("å¹¶å‘è®­ç»ƒå¯è¡Œæ€§åˆ†æ")
print("="*100)

# Sort models by memory usage
sorted_models = sorted(model_analysis.items(), key=lambda x: x[1]['est_memory_mb'], reverse=True)

print("\næŒ‰æ˜¾å­˜å ç”¨æ’åº:")
print(f"{'æ’å':<6} {'æ¨¡å‹':<45} {'ä¼°è®¡æ˜¾å­˜(MB)':<15} {'GPUåˆ©ç”¨ç‡%':<15}")
print("-"*90)
for i, (model_key, data) in enumerate(sorted_models, 1):
    print(f"{i:<6} {model_key:<45} {data['est_memory_mb']:<15} {data['avg_gpu_util']:<15.1f}")

# Identify incompatible pairs
print("\n" + "="*100)
print("å¹¶å‘å†²çªåˆ†æ - ä¸èƒ½åŒæ—¶è®­ç»ƒçš„æ¨¡å‹å¯¹")
print("="*100)

high_memory_models = []
medium_memory_models = []
low_memory_models = []

for model_key, data in model_analysis.items():
    mem = data['est_memory_mb']
    if mem > 2500:
        high_memory_models.append((model_key, mem))
    elif mem > 1500:
        medium_memory_models.append((model_key, mem))
    else:
        low_memory_models.append((model_key, mem))

print(f"\né«˜æ˜¾å­˜æ¨¡å‹ (>2500MB): {len(high_memory_models)}ä¸ª")
for model, mem in sorted(high_memory_models, key=lambda x: x[1], reverse=True):
    print(f"  - {model:<45} {mem:>6}MB")

print(f"\nä¸­æ˜¾å­˜æ¨¡å‹ (1500-2500MB): {len(medium_memory_models)}ä¸ª")
for model, mem in sorted(medium_memory_models, key=lambda x: x[1], reverse=True):
    print(f"  - {model:<45} {mem:>6}MB")

print(f"\nä½æ˜¾å­˜æ¨¡å‹ (<1500MB): {len(low_memory_models)}ä¸ª")
for model, mem in sorted(low_memory_models, key=lambda x: x[1], reverse=True):
    print(f"  - {model:<45} {mem:>6}MB")

# Check incompatible pairs
print("\n" + "="*100)
print("âŒ ä¸èƒ½åŒæ—¶è®­ç»ƒçš„æ¨¡å‹ç»„åˆ (æ€»æ˜¾å­˜ > 9GB, ç•™1GBç¼“å†²)")
print("="*100)

incompatible_pairs = []
for i, (model1, data1) in enumerate(sorted_models):
    for model2, data2 in sorted_models[i+1:]:
        total_memory = data1['est_memory_mb'] + data2['est_memory_mb']
        if total_memory > 9000:  # Leave 1GB buffer
            incompatible_pairs.append((model1, model2, total_memory))

if incompatible_pairs:
    print(f"\nå‘ç° {len(incompatible_pairs)} ä¸ªä¸å…¼å®¹çš„æ¨¡å‹å¯¹:\n")
    for model1, model2, total in sorted(incompatible_pairs, key=lambda x: x[2], reverse=True):
        print(f"  âŒ {model1:<40} + {model2:<40} = {total:>5}MB (è¶…å‡º)")
else:
    print("\nâœ… æœªå‘ç°ä¸å…¼å®¹çš„æ¨¡å‹å¯¹ï¼ˆåŸºäºä¼°è®¡ï¼‰")

# Check compatible pairs
print("\n" + "="*100)
print("âœ… å¯ä»¥åŒæ—¶è®­ç»ƒçš„æ¨¡å‹ç»„åˆ (æ€»æ˜¾å­˜ <= 9GB)")
print("="*100)

compatible_pairs = []
for i, (model1, data1) in enumerate(sorted_models):
    for model2, data2 in sorted_models[i+1:]:
        total_memory = data1['est_memory_mb'] + data2['est_memory_mb']
        if total_memory <= 9000:
            # Also check if combined GPU utilization is reasonable
            total_util = data1['avg_gpu_util'] + data2['avg_gpu_util']
            compatible_pairs.append((model1, model2, total_memory, total_util))

if compatible_pairs:
    # Sort by memory usage
    compatible_pairs.sort(key=lambda x: x[2], reverse=True)
    print(f"\nå‘ç° {len(compatible_pairs)} ä¸ªå…¼å®¹çš„æ¨¡å‹å¯¹:\n")
    print(f"{'æ¨¡å‹1':<40} {'æ¨¡å‹2':<40} {'æ€»æ˜¾å­˜(MB)':<12} {'æ€»GPUåˆ©ç”¨ç‡%':<15}")
    print("-"*110)
    for model1, model2, total_mem, total_util in compatible_pairs[:20]:  # Show top 20
        status = "âš ï¸ é«˜è´Ÿè½½" if total_util > 150 else "âœ… åˆç†"
        print(f"{model1:<40} {model2:<40} {total_mem:<12} {total_util:<15.1f} {status}")

    if len(compatible_pairs) > 20:
        print(f"\n... è¿˜æœ‰ {len(compatible_pairs) - 20} ä¸ªå…¼å®¹ç»„åˆæœªæ˜¾ç¤º")

# Best concurrent pairs
print("\n" + "="*100)
print("ğŸ’¡ æ¨èçš„å¹¶å‘è®­ç»ƒç»„åˆ (æ˜¾å­˜å®‰å…¨ + GPUåˆ©ç”¨ç‡äº’è¡¥)")
print("="*100)

# Find pairs where one is high-util and one is low-util (good complementary)
recommended_pairs = []
for model1, data1 in model_analysis.items():
    for model2, data2 in model_analysis.items():
        if model1 >= model2:  # Avoid duplicates
            continue

        total_memory = data1['est_memory_mb'] + data2['est_memory_mb']
        if total_memory > 9000:
            continue

        # Check if GPU utilization is complementary (one high, one low)
        util1 = data1['avg_gpu_util']
        util2 = data2['avg_gpu_util']

        is_complementary = (util1 > 70 and util2 < 30) or (util1 < 30 and util2 > 70)
        total_util = util1 + util2

        if is_complementary or total_util < 120:
            recommended_pairs.append({
                'model1': model1,
                'model2': model2,
                'total_memory': total_memory,
                'total_util': total_util,
                'util1': util1,
                'util2': util2,
                'complementary': is_complementary
            })

if recommended_pairs:
    recommended_pairs.sort(key=lambda x: (x['complementary'], -x['total_util']), reverse=True)
    print(f"\nå‘ç° {len(recommended_pairs)} ä¸ªæ¨èç»„åˆ:\n")
    print(f"{'æ¨¡å‹1':<40} {'GPU%1':<8} {'æ¨¡å‹2':<40} {'GPU%2':<8} {'æ€»æ˜¾å­˜MB':<12} {'æ€»GPU%':<10} {'äº’è¡¥':<8}")
    print("-"*125)
    for pair in recommended_pairs[:15]:
        comp_mark = "âœ…æ˜¯" if pair['complementary'] else "  -"
        print(f"{pair['model1']:<40} {pair['util1']:<8.1f} {pair['model2']:<40} "
              f"{pair['util2']:<8.1f} {pair['total_memory']:<12} {pair['total_util']:<10.1f} {comp_mark}")

# Summary and recommendations
print("\n" + "="*100)
print("ğŸ“Š æ€»ç»“ä¸å»ºè®®")
print("="*100)

high_mem_count = len([m for m in model_analysis.values() if m['est_memory_mb'] > 2500])
medium_mem_count = len([m for m in model_analysis.values() if 1500 < m['est_memory_mb'] <= 2500])
low_mem_count = len([m for m in model_analysis.values() if m['est_memory_mb'] <= 1500])

print(f"""
1. æ¨¡å‹åˆ†ç±»:
   - é«˜æ˜¾å­˜æ¨¡å‹ (>2.5GB): {high_mem_count}ä¸ª - è¿™äº›æ¨¡å‹ç›¸äº’ä¹‹é—´å¯èƒ½æ— æ³•å¹¶å‘
   - ä¸­æ˜¾å­˜æ¨¡å‹ (1.5-2.5GB): {medium_mem_count}ä¸ª - å¯ä»¥ä¸ä½æ˜¾å­˜æ¨¡å‹å¹¶å‘
   - ä½æ˜¾å­˜æ¨¡å‹ (<1.5GB): {low_mem_count}ä¸ª - å¯ä»¥ä¸å¤§å¤šæ•°æ¨¡å‹å¹¶å‘

2. å¹¶å‘ç­–ç•¥:
   âœ… æ¨è: 1ä¸ªé«˜æ˜¾å­˜æ¨¡å‹ + 1ä¸ªä½æ˜¾å­˜æ¨¡å‹
   âœ… æ¨è: 2ä¸ªä½æ˜¾å­˜æ¨¡å‹ + 1ä¸ªä¸­æ˜¾å­˜æ¨¡å‹
   âš ï¸  è°¨æ…: 2ä¸ªä¸­æ˜¾å­˜æ¨¡å‹
   âŒ é¿å…: 2ä¸ªé«˜æ˜¾å­˜æ¨¡å‹

3. GPUåˆ©ç”¨ç‡äº’è¡¥åŸåˆ™:
   - é«˜GPUåˆ©ç”¨ç‡æ¨¡å‹ (>70%): {len([m for m in model_analysis.values() if m['avg_gpu_util'] > 70])}ä¸ª
   - ä½GPUåˆ©ç”¨ç‡æ¨¡å‹ (<30%): {len([m for m in model_analysis.values() if m['avg_gpu_util'] < 30])}ä¸ª
   - é…å¯¹é«˜åˆ©ç”¨ç‡+ä½åˆ©ç”¨ç‡å¯ä»¥æé«˜GPUæ•´ä½“åˆ©ç”¨ç‡

4. æ½œåœ¨é—®é¢˜:
   - æ˜¾å­˜ä¸è¶³ (OOM): é«˜æ˜¾å­˜æ¨¡å‹ä¹‹é—´å¹¶å‘ä¼šå¯¼è‡´Out of Memory
   - GPUç®—åŠ›ç«äº‰: ä¸¤ä¸ªé«˜åˆ©ç”¨ç‡æ¨¡å‹å¹¶å‘ä¼šç›¸äº’æ‹–æ…¢
   - CPU/ç£ç›˜I/Oç“¶é¢ˆ: æ•°æ®åŠ è½½å¯†é›†çš„æ¨¡å‹å¯èƒ½ç«äº‰I/O

5. é¿å…æªæ–½:
   âœ… ä½¿ç”¨mutation.pyçš„max_parallelå‚æ•°é™åˆ¶å¹¶å‘æ•°
   âœ… ä¸ºé«˜æ˜¾å­˜æ¨¡å‹å•ç‹¬åˆ†ç»„ï¼Œé¿å…å¹¶å‘
   âœ… ç›‘æ§nvidia-smiï¼ŒåŠ¨æ€è°ƒæ•´å¹¶å‘ç­–ç•¥
   âœ… ä½¿ç”¨è¾ƒå°çš„batch_sizeå‡å°‘æ˜¾å­˜å ç”¨

6. æœ€ä½³å®è·µ:
   - ä½¿ç”¨--max-parallel 2 é™åˆ¶æœ€å¤š2ä¸ªæ¨¡å‹åŒæ—¶è®­ç»ƒ
   - ä¼˜å…ˆä¸²è¡Œè®­ç»ƒé«˜æ˜¾å­˜æ¨¡å‹
   - å¹¶å‘è®­ç»ƒäº’è¡¥çš„æ¨¡å‹ç»„åˆï¼ˆå¦‚MNIST + MRT-OASTï¼‰
""")

print("="*100)
print("åˆ†æå®Œæˆï¼")
print("="*100)
