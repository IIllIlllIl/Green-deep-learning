#!/usr/bin/env python3
"""
æ€§èƒ½åº¦é‡å¯è§†åŒ–è„šæœ¬

ç”Ÿæˆé¡¹ç›®ä¸­æ‰€æœ‰æ¨¡å‹æ€§èƒ½åº¦é‡çš„å¯è§†åŒ–å›¾è¡¨
"""

import json
from pathlib import Path
from collections import defaultdict

def analyze_metrics():
    """åˆ†æé…ç½®æ–‡ä»¶ä¸­çš„æ€§èƒ½åº¦é‡"""

    config_path = Path(__file__).parent.parent / "config" / "models_config.json"

    with open(config_path, 'r') as f:
        config = json.load(f)

    # ç»Ÿè®¡æ•°æ®
    metrics_count = defaultdict(int)
    task_types = {"classification": [], "retrieval": []}
    model_metrics = []

    for repo_name, repo_config in config["models"].items():
        models = repo_config.get("models", [])
        log_patterns = repo_config.get("performance_metrics", {}).get("log_patterns", {})

        for model_name in models:
            model_info = {
                "repo": repo_name,
                "model": model_name,
                "metrics": list(log_patterns.keys())
            }
            model_metrics.append(model_info)

            # ç»Ÿè®¡åº¦é‡å‡ºç°æ¬¡æ•°
            for metric in log_patterns.keys():
                metrics_count[metric] += 1

            # åˆ¤æ–­ä»»åŠ¡ç±»å‹
            if any(m in log_patterns for m in ["accuracy", "test_accuracy"]):
                task_types["classification"].append(f"{repo_name}/{model_name}")
            elif any(m in log_patterns for m in ["map", "rank1", "rank5"]):
                task_types["retrieval"].append(f"{repo_name}/{model_name}")

    return metrics_count, task_types, model_metrics

def print_metrics_report():
    """æ‰“å°æ€§èƒ½åº¦é‡åˆ†ææŠ¥å‘Š"""

    metrics_count, task_types, model_metrics = analyze_metrics()

    total_models = len(model_metrics)

    print("=" * 80)
    print("æ€§èƒ½åº¦é‡åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()

    # 1. æ€»ä½“ç»Ÿè®¡
    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print("-" * 80)
    print(f"æ€»æ¨¡å‹æ•°: {total_models}")
    print(f"åˆ†ç±»ä»»åŠ¡æ¨¡å‹: {len(task_types['classification'])} ({len(task_types['classification'])/total_models*100:.1f}%)")
    print(f"æ£€ç´¢ä»»åŠ¡æ¨¡å‹: {len(task_types['retrieval'])} ({len(task_types['retrieval'])/total_models*100:.1f}%)")
    print()

    # 2. åº¦é‡ç»Ÿè®¡
    print("ğŸ“ˆ æ€§èƒ½åº¦é‡ç»Ÿè®¡ (æŒ‰å‡ºç°æ¬¡æ•°æ’åº)")
    print("-" * 80)
    sorted_metrics = sorted(metrics_count.items(), key=lambda x: x[1], reverse=True)

    for metric, count in sorted_metrics:
        percentage = count / total_models * 100
        bar_length = int(percentage / 2)  # æ¯2%ä¸€ä¸ªå­—ç¬¦
        bar = "â–ˆ" * bar_length
        print(f"{metric:20s} {bar:50s} {count:2d} ä¸ªæ¨¡å‹ ({percentage:5.1f}%)")
    print()

    # 3. ä»»åŠ¡ç±»å‹è¯¦æƒ…
    print("ğŸ¯ åˆ†ç±»ä»»åŠ¡æ¨¡å‹ (11ä¸ª)")
    print("-" * 80)
    for i, model in enumerate(task_types['classification'], 1):
        print(f"{i:2d}. {model}")
    print()

    print("ğŸ” æ£€ç´¢ä»»åŠ¡æ¨¡å‹ (4ä¸ª)")
    print("-" * 80)
    for i, model in enumerate(task_types['retrieval'], 1):
        print(f"{i:2d}. {model}")
    print()

    # 4. å…¬å…±åº¦é‡åˆ†æ
    print("ğŸ’¡ å…¬å…±åº¦é‡åˆ†æ")
    print("-" * 80)

    # åˆ†ç±»ä»»åŠ¡çš„å…¬å…±åº¦é‡
    classification_metrics = defaultdict(int)
    for model_full in task_types['classification']:
        for model_info in model_metrics:
            if f"{model_info['repo']}/{model_info['model']}" == model_full:
                for metric in model_info['metrics']:
                    if 'accuracy' in metric.lower():
                        classification_metrics['accuracy'] += 1

    # æ£€ç´¢ä»»åŠ¡çš„å…¬å…±åº¦é‡
    retrieval_metrics = defaultdict(int)
    for model_full in task_types['retrieval']:
        for model_info in model_metrics:
            if f"{model_info['repo']}/{model_info['model']}" == model_full:
                for metric in model_info['metrics']:
                    if 'map' in metric.lower():
                        retrieval_metrics['map'] += 1

    print(f"âœ… åˆ†ç±»ä»»åŠ¡å…¬å…±åº¦é‡: Accuracy")
    print(f"   è¦†ç›–ç‡: {classification_metrics['accuracy']}/{len(task_types['classification'])} = {classification_metrics['accuracy']/len(task_types['classification'])*100:.1f}%")
    print()
    print(f"âœ… æ£€ç´¢ä»»åŠ¡å…¬å…±åº¦é‡: mAP")
    print(f"   è¦†ç›–ç‡: {retrieval_metrics['map']}/{len(task_types['retrieval'])} = {retrieval_metrics['map']/len(task_types['retrieval'])*100:.1f}%")
    print()

    # 5. è¯¦ç»†æ¨¡å‹åˆ—è¡¨
    print("ğŸ“‹ è¯¦ç»†æ¨¡å‹åº¦é‡åˆ—è¡¨")
    print("-" * 80)
    print(f"{'#':<3} {'ä»“åº“':<30} {'æ¨¡å‹':<20} {'åº¦é‡æŒ‡æ ‡':<30}")
    print("-" * 80)

    for i, model_info in enumerate(model_metrics, 1):
        metrics_str = ", ".join(model_info['metrics'])
        if len(metrics_str) > 28:
            metrics_str = metrics_str[:25] + "..."
        print(f"{i:<3} {model_info['repo']:<30} {model_info['model']:<20} {metrics_str:<30}")

    print()
    print("=" * 80)
    print("ç»“è®º: ä¸å­˜åœ¨å…¨å±€å…¬å…±åº¦é‡ï¼Œå»ºè®®é‡‡ç”¨åˆ†å±‚åº¦é‡ç­–ç•¥")
    print("  - åˆ†ç±»ä»»åŠ¡: ä½¿ç”¨ Accuracy")
    print("  - æ£€ç´¢ä»»åŠ¡: ä½¿ç”¨ mAP")
    print("=" * 80)

if __name__ == "__main__":
    print_metrics_report()
